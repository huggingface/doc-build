import{S as Qd,i as Kd,s as Xd,e as l,k as f,t as i,c as o,a as n,m as c,h as p,d as t,b as h,g as r,F as a,Q as Hl,q as b,l as kh,n as br,o as j,B as k,p as jr,w as E,y as q,j as Ch,G as Oh,a0 as Ah,x as P,a1 as Hh,T as Fh,Y as Sh,Z as Th,M as Lh,v as Rh}from"../chunks/vendor-8138ceec.js";import{T as ua}from"../chunks/Tip-12722dfc.js";import{I as C}from"../chunks/IconCopyLink-2dd3a6ac.js";import{a as Dh,C as D}from"../chunks/CodeBlock-fc89709f.js";import{b as Nh,I as Mh,a as Vh}from"../chunks/IconTensorflow-7f573d67.js";function Eh(x,d,g){const u=x.slice();return u[8]=d[g],u[10]=g,u}function qh(x){let d,g,u;var y=x[8].icon;function $(_){return{props:{classNames:"mr-1.5"}}}return y&&(d=new y($())),{c(){d&&E(d.$$.fragment),g=kh()},l(_){d&&P(d.$$.fragment,_),g=kh()},m(_,m){d&&q(d,_,m),r(_,g,m),u=!0},p(_,m){if(y!==(y=_[8].icon)){if(d){br();const w=d;j(w.$$.fragment,1,0,()=>{k(w,1)}),jr()}y?(d=new y($()),E(d.$$.fragment),b(d.$$.fragment,1),q(d,g.parentNode,g)):d=null}},i(_){u||(d&&b(d.$$.fragment,_),u=!0)},o(_){d&&j(d.$$.fragment,_),u=!1},d(_){_&&t(g),d&&k(d,_)}}}function Ph(x){let d,g,u,y=x[8].name+"",$,_,m,w,v,A,S,T=x[8].icon&&qh(x);function B(){return x[6](x[8])}return{c(){d=l("button"),T&&T.c(),g=f(),u=l("p"),$=i(y),m=f(),this.h()},l(I){d=o(I,"BUTTON",{class:!0});var N=n(d);T&&T.l(N),g=c(N),u=o(N,"P",{class:!0});var U=n(u);$=p(U,y),U.forEach(t),m=c(N),N.forEach(t),this.h()},h(){h(u,"class",_="!m-0 "+x[8].classNames),h(d,"class",w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(x[10]?"r":"l")+" "+(x[8].group!==x[1]&&"text-gray-500 filter grayscale"))},m(I,N){r(I,d,N),T&&T.m(d,null),a(d,g),a(d,u),a(u,$),a(d,m),v=!0,A||(S=Hl(d,"click",B),A=!0)},p(I,N){x=I,x[8].icon?T?(T.p(x,N),N&1&&b(T,1)):(T=qh(x),T.c(),b(T,1),T.m(d,g)):T&&(br(),j(T,1,1,()=>{T=null}),jr()),(!v||N&1)&&y!==(y=x[8].name+"")&&Ch($,y),(!v||N&1&&_!==(_="!m-0 "+x[8].classNames))&&h(u,"class",_),(!v||N&3&&w!==(w="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(x[10]?"r":"l")+" "+(x[8].group!==x[1]&&"text-gray-500 filter grayscale")))&&h(d,"class",w)},i(I){v||(b(T),v=!0)},o(I){j(T),v=!1},d(I){I&&t(d),T&&T.d(),A=!1,S()}}}function zh(x){let d,g,u,y=x[3].filter(x[5]),$=[];for(let m=0;m<y.length;m+=1)$[m]=Ph(Eh(x,y,m));const _=m=>j($[m],1,1,()=>{$[m]=null});return{c(){d=l("div"),g=l("div");for(let m=0;m<$.length;m+=1)$[m].c();this.h()},l(m){d=o(m,"DIV",{});var w=n(d);g=o(w,"DIV",{class:!0});var v=n(g);for(let A=0;A<$.length;A+=1)$[A].l(v);v.forEach(t),w.forEach(t),this.h()},h(){h(g,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m(m,w){r(m,d,w),a(d,g);for(let v=0;v<$.length;v+=1)$[v].m(g,null);u=!0},p(m,[w]){if(w&27){y=m[3].filter(m[5]);let v;for(v=0;v<y.length;v+=1){const A=Eh(m,y,v);$[v]?($[v].p(A,w),b($[v],1)):($[v]=Ph(A),$[v].c(),b($[v],1),$[v].m(g,null))}for(br(),v=y.length;v<$.length;v+=1)_(v);jr()}},i(m){if(!u){for(let w=0;w<y.length;w+=1)b($[w]);u=!0}},o(m){$=$.filter(Boolean);for(let w=0;w<$.length;w+=1)j($[w]);u=!1},d(m){m&&t(d),Oh($,m)}}}function Uh(x,d,g){let u,{ids:y}=d;const $=y.join("-"),_=Nh($);Ah(x,_,S=>g(1,u=S));const m=[{id:"pt",classNames:"",icon:Mh,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:Vh,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function w(S){Hh(_,u=S,u)}const v=S=>y.includes(S.id),A=S=>w(S.group);return x.$$set=S=>{"ids"in S&&g(0,y=S.ids)},[y,u,_,m,w,v,A]}class Ih extends Qd{constructor(d){super();Kd(this,d,Uh,zh,Xd,{ids:0})}}function Jh(x){let d,g,u,y,$,_,m=x[1].highlighted+"",w;return g=new Dh({props:{classNames:"transition duration-200 ease-in-out "+(x[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:x[1].code}}),$=new Ih({props:{ids:x[4]}}),{c(){d=l("div"),E(g.$$.fragment),u=f(),y=l("pre"),E($.$$.fragment),_=new Sh,this.h()},l(v){d=o(v,"DIV",{class:!0});var A=n(d);P(g.$$.fragment,A),A.forEach(t),u=c(v),y=o(v,"PRE",{});var S=n(y);P($.$$.fragment,S),_=Th(S),S.forEach(t),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),_.a=null},m(v,A){r(v,d,A),q(g,d,null),r(v,u,A),r(v,y,A),q($,y,null),_.m(m,y),w=!0},p(v,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(v[2]&&"opacity-0")),A&2&&(S.value=v[1].code),g.$set(S),(!w||A&2)&&m!==(m=v[1].highlighted+"")&&_.p(m)},i(v){w||(b(g.$$.fragment,v),b($.$$.fragment,v),w=!0)},o(v){j(g.$$.fragment,v),j($.$$.fragment,v),w=!1},d(v){v&&t(d),k(g),v&&t(u),v&&t(y),k($)}}}function Bh(x){let d,g,u,y,$,_,m=x[0].highlighted+"",w;return g=new Dh({props:{classNames:"transition duration-200 ease-in-out "+(x[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:x[0].code}}),$=new Ih({props:{ids:x[4]}}),{c(){d=l("div"),E(g.$$.fragment),u=f(),y=l("pre"),E($.$$.fragment),_=new Sh,this.h()},l(v){d=o(v,"DIV",{class:!0});var A=n(d);P(g.$$.fragment,A),A.forEach(t),u=c(v),y=o(v,"PRE",{});var S=n(y);P($.$$.fragment,S),_=Th(S),S.forEach(t),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),_.a=null},m(v,A){r(v,d,A),q(g,d,null),r(v,u,A),r(v,y,A),q($,y,null),_.m(m,y),w=!0},p(v,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(v[2]&&"opacity-0")),A&1&&(S.value=v[0].code),g.$set(S),(!w||A&1)&&m!==(m=v[0].highlighted+"")&&_.p(m)},i(v){w||(b(g.$$.fragment,v),b($.$$.fragment,v),w=!0)},o(v){j(g.$$.fragment,v),j($.$$.fragment,v),w=!1},d(v){v&&t(d),k(g),v&&t(u),v&&t(y),k($)}}}function Yh(x){let d,g,u,y,$,_;const m=[Bh,Jh],w=[];function v(A,S){return A[3]==="group1"?0:1}return g=v(x),u=w[g]=m[g](x),{c(){d=l("div"),u.c(),this.h()},l(A){d=o(A,"DIV",{class:!0});var S=n(d);u.l(S),S.forEach(t),this.h()},h(){h(d,"class","code-block relative")},m(A,S){r(A,d,S),w[g].m(d,null),y=!0,$||(_=[Hl(d,"mouseover",x[6]),Hl(d,"focus",x[6]),Hl(d,"mouseout",x[7]),Hl(d,"focus",x[7])],$=!0)},p(A,[S]){let T=g;g=v(A),g===T?w[g].p(A,S):(br(),j(w[T],1,1,()=>{w[T]=null}),jr(),u=w[g],u?u.p(A,S):(u=w[g]=m[g](A),u.c()),b(u,1),u.m(d,null))},i(A){y||(b(u),y=!0)},o(A){j(u),y=!1},d(A){A&&t(d),w[g].d(),$=!1,Fh(_)}}}function Wh(x,d,g){let u,{group1:y}=d,{group2:$}=d;const _=[y.id,$.id],m=_.join("-"),w=Nh(m);Ah(x,w,T=>g(3,u=T));let v=!0;function A(){g(2,v=!1)}function S(){g(2,v=!0)}return x.$$set=T=>{"group1"in T&&g(0,y=T.group1),"group2"in T&&g(1,$=T.group2)},[y,$,v,u,_,w,A,S]}class Ol extends Qd{constructor(d){super();Kd(this,d,Wh,Yh,Xd,{group1:0,group2:1})}}function Gh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Refer to the "),u=l("a"),y=i("Upload"),$=i(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Refer to the "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Upload"),w.forEach(t),$=p(m," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),m.forEach(t),this.h()},h(){h(u,"href","./upload_dataset")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Qh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("If you don\u2019t specify which data files to use, "),u=l("code"),y=i("load_dataset"),$=i(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"If you don\u2019t specify which data files to use, "),u=o(m,"CODE",{});var w=n(u);y=p(w,"load_dataset"),w.forEach(t),$=p(m," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),m.forEach(t)},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Kh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Curious about how to load datasets for vision? Check out the image loading guide "),u=l("a"),y=i("here"),$=i("!"),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Curious about how to load datasets for vision? Check out the image loading guide "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"here"),w.forEach(t),$=p(m,"!"),m.forEach(t),this.h()},h(){h(u,"href","./image_process")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function Xh(x){let d,g,u,y,$,_,m,w,v,A,S,T,B,I,N,U,O;return{c(){d=l("p"),g=i("An object data type in "),u=l("a"),y=i("pandas.Series"),$=i(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=l("a"),m=i("Features"),w=i(" using the "),v=l("code"),A=i("from_dict"),S=i(" or "),T=l("code"),B=i("from_pandas"),I=i(" methods. See the "),N=l("a"),U=i("troubleshoot"),O=i(" for more details on how to explicitly specify your own features."),this.h()},l(J){d=o(J,"P",{});var H=n(d);g=p(H,"An object data type in "),u=o(H,"A",{href:!0,rel:!0});var ma=n(u);y=p(ma,"pandas.Series"),ma.forEach(t),$=p(H," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),_=o(H,"A",{href:!0});var bs=n(_);m=p(bs,"Features"),bs.forEach(t),w=p(H," using the "),v=o(H,"CODE",{});var ga=n(v);A=p(ga,"from_dict"),ga.forEach(t),S=p(H," or "),T=o(H,"CODE",{});var _a=n(T);B=p(_a,"from_pandas"),_a.forEach(t),I=p(H," methods. See the "),N=o(H,"A",{href:!0});var js=n(N);U=p(js,"troubleshoot"),js.forEach(t),O=p(H," for more details on how to explicitly specify your own features."),H.forEach(t),this.h()},h(){h(u,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),h(u,"rel","nofollow"),h(_,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.Features"),h(N,"href","./loading#specify-features")},m(J,H){r(J,d,H),a(d,g),a(d,u),a(u,y),a(d,$),a(d,_),a(_,m),a(d,w),a(d,v),a(v,A),a(d,S),a(d,T),a(T,B),a(d,I),a(d,N),a(N,U),a(d,O)},d(J){J&&t(d)}}}function Zh(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Using "),u=l("code"),y=i("pct1_dropremainder"),$=i(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Using "),u=o(m,"CODE",{});var w=n(u);y=p(w,"pct1_dropremainder"),w.forEach(t),$=p(m," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),m.forEach(t)},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function su(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("See the "),u=l("a"),y=i("Metrics"),$=i(" guide for more details on how to write your own metric loading script."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"See the "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Metrics"),w.forEach(t),$=p(m," guide for more details on how to write your own metric loading script."),m.forEach(t),this.h()},h(){h(u,"href","./how_to_metrics#custom-metric-loading-script")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function tu(x){let d,g,u,y,$;return{c(){d=l("p"),g=i("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=l("a"),y=i("Metric.compute()"),$=i(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(_){d=o(_,"P",{});var m=n(d);g=p(m,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=o(m,"A",{href:!0});var w=n(u);y=p(w,"Metric.compute()"),w.forEach(t),$=p(m," gathers all the predictions and references from the nodes, and computes the final metric."),m.forEach(t),this.h()},h(){h(u,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.Metric.compute")},m(_,m){r(_,d,m),a(d,g),a(d,u),a(u,y),a(d,$)},d(_){_&&t(d)}}}function au(x){let d,g,u,y,$,_,m,w,v,A,S,T,B,I,N,U,O,J,H,ma,bs,ga,_a,js,xr,kr,ge,Er,qr,_e,Pr,Fl,va,Ar,Ll,$a,Rl,es,xs,ve,it,Sr,$e,Tr,Ml,ks,Dr,ye,Nr,Ir,Vl,Y,Cr,ya,Or,Hr,pt,Fr,Lr,zl,dt,Ul,wa,Rr,Jl,Es,Mr,we,Vr,zr,Bl,ft,Yl,qs,Wl,F,Ur,be,Jr,Br,je,Yr,Wr,xe,Gr,Qr,ke,Kr,Xr,Ee,Zr,si,Gl,ct,Ql,Ps,Kl,W,ti,qe,ai,ei,ht,li,oi,Xl,ut,Zl,As,ni,Pe,ri,ii,so,mt,to,ls,Ss,Ae,gt,pi,Se,di,ao,L,fi,Te,ci,hi,De,ui,mi,Ne,gi,_i,Ie,vi,$i,ba,yi,wi,eo,Ts,lo,os,Ds,Ce,_t,bi,Oe,ji,oo,ja,xi,no,vt,ro,xa,ki,io,$t,po,ka,Ei,fo,yt,co,Ea,qi,ho,wt,uo,qa,Pi,mo,bt,go,ns,Ns,He,jt,Ai,Fe,Si,_o,Is,Ti,Pa,Di,Ni,vo,xt,$o,Aa,Ii,yo,kt,wo,Cs,Ci,Le,Oi,Hi,bo,Et,jo,Sa,Fi,xo,qt,ko,Ta,Li,Eo,rs,Os,Re,Pt,Ri,Me,Mi,qo,Da,Vi,Po,At,Ao,Na,zi,So,St,To,is,Hs,Ve,Tt,Ui,ze,Ji,Do,Ia,Bi,No,Dt,Io,Ca,Yi,Co,Nt,Oo,ps,Fs,Ue,It,Wi,Je,Gi,Ho,Ls,Qi,Oa,Ki,Xi,Fo,ds,Rs,Be,Ct,Zi,Ye,sp,Lo,Ms,tp,Ha,ap,ep,Ro,Ot,Mo,fs,Vs,We,Ht,lp,Ge,op,Vo,zs,np,Fa,rp,ip,zo,Ft,Uo,Us,Jo,cs,Js,Qe,Lt,pp,Ke,dp,Bo,La,fp,Yo,G,cp,Xe,hp,up,Ze,mp,gp,Wo,hs,Bs,sl,Rt,_p,tl,vp,Go,Q,$p,Ra,yp,wp,Ma,bp,jp,Qo,K,xp,al,kp,Ep,el,qp,Pp,Ko,Mt,Xo,Ys,Ap,ll,Sp,Tp,Zo,Vt,sn,Va,Dp,tn,zt,an,za,Np,en,Ut,ln,Ua,Ip,on,Jt,nn,us,Ws,ol,Bt,Cp,nl,Op,rn,Ja,Hp,pn,Yt,dn,Gs,Fp,rl,Lp,Rp,fn,Wt,cn,Qs,hn,Ba,un,ms,Ks,il,Gt,Mp,pl,Vp,mn,Ya,zp,gn,gs,Xs,dl,Qt,Up,fl,Jp,_n,M,Bp,Wa,Yp,Wp,cl,Gp,Qp,hl,Kp,Xp,vn,Zs,Zp,Kt,sd,td,$n,Xt,yn,_s,st,ul,Zt,ad,ml,ed,wn,X,ld,Ga,od,nd,sa,rd,id,bn,Z,pd,Qa,dd,fd,Ka,cd,hd,jn,ta,xn,ss,ud,gl,md,gd,Xa,_d,vd,kn,aa,En,Za,$d,qn,ea,Pn,vs,tt,_l,la,yd,vl,wd,An,se,bd,Sn,oa,Tn,at,Dn,$s,et,$l,na,jd,yl,xd,Nn,ts,kd,wl,Ed,qd,te,Pd,Ad,In,ra,Cn,ys,lt,bl,ia,Sd,jl,Td,On,ae,Dd,Hn,ee,Nd,Fn,as,xl,pa,Id,kl,Cd,Od,Hd,El,ws,Fd,ql,Ld,Rd,Pl,Md,Vd,zd,Al,da,Ud,le,Jd,Bd,Ln,fa,Rn,ot,Mn,nt,Yd,Sl,Wd,Gd,Vn,ca,zn;return _=new C({}),it=new C({}),dt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),ft=new D({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),qs=new ua({props:{$$slots:{default:[Gh]},$$scope:{ctx:x}}}),ct=new D({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),Ps=new ua({props:{warning:!0,$$slots:{default:[Qh]},$$scope:{ctx:x}}}),ut=new D({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),mt=new D({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),gt=new C({}),Ts=new ua({props:{$$slots:{default:[Kh]},$$scope:{ctx:x}}}),_t=new C({}),vt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),$t=new D({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),yt=new D({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),wt=new D({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),bt=new D({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),jt=new C({}),xt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),kt=new D({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),Et=new D({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),qt=new D({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Pt=new C({}),At=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),St=new D({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),Tt=new C({}),Dt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Nt=new D({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),It=new C({}),Ct=new C({}),Ot=new D({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ht=new C({}),Ft=new D({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Us=new ua({props:{warning:!0,$$slots:{default:[Xh]},$$scope:{ctx:x}}}),Lt=new C({}),Rt=new C({}),Mt=new Ol({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Vt=new Ol({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),zt=new Ol({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),Ut=new Ol({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Jt=new Ol({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),Bt=new C({}),Yt=new D({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),Wt=new D({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Qs=new ua({props:{warning:!0,$$slots:{default:[Zh]},$$scope:{ctx:x}}}),Gt=new C({}),Qt=new C({}),Xt=new D({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),Zt=new C({}),ta=new D({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),aa=new D({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),ea=new D({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),la=new C({}),oa=new D({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),at=new ua({props:{$$slots:{default:[su]},$$scope:{ctx:x}}}),na=new C({}),ra=new D({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),ia=new C({}),fa=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),ot=new ua({props:{$$slots:{default:[tu]},$$scope:{ctx:x}}}),ca=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){d=l("meta"),g=f(),u=l("h1"),y=l("a"),$=l("span"),E(_.$$.fragment),m=f(),w=l("span"),v=i("Load"),A=f(),S=l("p"),T=i("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),B=f(),I=l("p"),N=i("This guide will show you how to load a dataset from:"),U=f(),O=l("ul"),J=l("li"),H=i("The Hub without a dataset loading script"),ma=f(),bs=l("li"),ga=i("Local files"),_a=f(),js=l("li"),xr=i("In-memory data"),kr=f(),ge=l("li"),Er=i("Offline"),qr=f(),_e=l("li"),Pr=i("A specific slice of a split"),Fl=f(),va=l("p"),Ar=i("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Ll=f(),$a=l("a"),Rl=f(),es=l("h2"),xs=l("a"),ve=l("span"),E(it.$$.fragment),Sr=f(),$e=l("span"),Tr=i("Hugging Face Hub"),Ml=f(),ks=l("p"),Dr=i("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),ye=l("strong"),Nr=i("without"),Ir=i(" a loading script!"),Vl=f(),Y=l("p"),Cr=i("First, create a dataset repository and upload your data files. Then you can use "),ya=l("a"),Or=i("load_dataset()"),Hr=i(" like you learned in the tutorial. For example, load the files from this "),pt=l("a"),Fr=i("demo repository"),Lr=i(" by providing the repository namespace and dataset name:"),zl=f(),E(dt.$$.fragment),Ul=f(),wa=l("p"),Rr=i("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Jl=f(),Es=l("p"),Mr=i("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),we=l("code"),Vr=i("revision"),zr=i(" flag to specify which dataset version you want to load:"),Bl=f(),E(ft.$$.fragment),Yl=f(),E(qs.$$.fragment),Wl=f(),F=l("p"),Ur=i("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),be=l("code"),Jr=i("train"),Br=i(" split. Use the "),je=l("code"),Yr=i("data_files"),Wr=i(" parameter to map data files to splits like "),xe=l("code"),Gr=i("train"),Qr=i(", "),ke=l("code"),Kr=i("validation"),Xr=i(" and "),Ee=l("code"),Zr=i("test"),si=i(":"),Gl=f(),E(ct.$$.fragment),Ql=f(),E(Ps.$$.fragment),Kl=f(),W=l("p"),ti=i("You can also load a specific subset of the files with the "),qe=l("code"),ai=i("data_files"),ei=i(" parameter. The example below loads files from the "),ht=l("a"),li=i("C4 dataset"),oi=i(":"),Xl=f(),E(ut.$$.fragment),Zl=f(),As=l("p"),ni=i("Specify a custom split with the "),Pe=l("code"),ri=i("split"),ii=i(" parameter:"),so=f(),E(mt.$$.fragment),to=f(),ls=l("h2"),Ss=l("a"),Ae=l("span"),E(gt.$$.fragment),pi=f(),Se=l("span"),di=i("Local and remote files"),ao=f(),L=l("p"),fi=i("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Te=l("code"),ci=i("csv"),hi=i(", "),De=l("code"),ui=i("json"),mi=i(", "),Ne=l("code"),gi=i("txt"),_i=i(" or "),Ie=l("code"),vi=i("parquet"),$i=i(" file. The "),ba=l("a"),yi=i("load_dataset()"),wi=i(" method is able to load each of these file types."),eo=f(),E(Ts.$$.fragment),lo=f(),os=l("h3"),Ds=l("a"),Ce=l("span"),E(_t.$$.fragment),bi=f(),Oe=l("span"),ji=i("CSV"),oo=f(),ja=l("p"),xi=i("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),no=f(),E(vt.$$.fragment),ro=f(),xa=l("p"),ki=i("If you have more than one CSV file:"),io=f(),E($t.$$.fragment),po=f(),ka=l("p"),Ei=i("You can also map the training and test splits to specific CSV files:"),fo=f(),E(yt.$$.fragment),co=f(),Ea=l("p"),qi=i("To load remote CSV files via HTTP, you can pass the URLs:"),ho=f(),E(wt.$$.fragment),uo=f(),qa=l("p"),Pi=i("To load zipped CSV files:"),mo=f(),E(bt.$$.fragment),go=f(),ns=l("h3"),Ns=l("a"),He=l("span"),E(jt.$$.fragment),Ai=f(),Fe=l("span"),Si=i("JSON"),_o=f(),Is=l("p"),Ti=i("JSON files are loaded directly with "),Pa=l("a"),Di=i("load_dataset()"),Ni=i(" as shown below:"),vo=f(),E(xt.$$.fragment),$o=f(),Aa=l("p"),Ii=i("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),yo=f(),E(kt.$$.fragment),wo=f(),Cs=l("p"),Ci=i("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Le=l("code"),Oi=i("field"),Hi=i(" argument as shown in the following:"),bo=f(),E(Et.$$.fragment),jo=f(),Sa=l("p"),Fi=i("To load remote JSON files via HTTP, you can pass the URLs:"),xo=f(),E(qt.$$.fragment),ko=f(),Ta=l("p"),Li=i("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Eo=f(),rs=l("h3"),Os=l("a"),Re=l("span"),E(Pt.$$.fragment),Ri=f(),Me=l("span"),Mi=i("Text files"),qo=f(),Da=l("p"),Vi=i("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Po=f(),E(At.$$.fragment),Ao=f(),Na=l("p"),zi=i("To load remote TXT files via HTTP, you can pass the URLs:"),So=f(),E(St.$$.fragment),To=f(),is=l("h3"),Hs=l("a"),Ve=l("span"),E(Tt.$$.fragment),Ui=f(),ze=l("span"),Ji=i("Parquet"),Do=f(),Ia=l("p"),Bi=i("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),No=f(),E(Dt.$$.fragment),Io=f(),Ca=l("p"),Yi=i("To load remote parquet files via HTTP, you can pass the URLs:"),Co=f(),E(Nt.$$.fragment),Oo=f(),ps=l("h2"),Fs=l("a"),Ue=l("span"),E(It.$$.fragment),Wi=f(),Je=l("span"),Gi=i("In-memory data"),Ho=f(),Ls=l("p"),Qi=i("\u{1F917} Datasets will also allow you to create a "),Oa=l("a"),Ki=i("Dataset"),Xi=i(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Fo=f(),ds=l("h3"),Rs=l("a"),Be=l("span"),E(Ct.$$.fragment),Zi=f(),Ye=l("span"),sp=i("Python dictionary"),Lo=f(),Ms=l("p"),tp=i("Load Python dictionaries with "),Ha=l("a"),ap=i("Dataset.from_dict()"),ep=i(":"),Ro=f(),E(Ot.$$.fragment),Mo=f(),fs=l("h3"),Vs=l("a"),We=l("span"),E(Ht.$$.fragment),lp=f(),Ge=l("span"),op=i("Pandas DataFrame"),Vo=f(),zs=l("p"),np=i("Load Pandas DataFrames with "),Fa=l("a"),rp=i("Dataset.from_pandas()"),ip=i(":"),zo=f(),E(Ft.$$.fragment),Uo=f(),E(Us.$$.fragment),Jo=f(),cs=l("h2"),Js=l("a"),Qe=l("span"),E(Lt.$$.fragment),pp=f(),Ke=l("span"),dp=i("Offline"),Bo=f(),La=l("p"),fp=i("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Yo=f(),G=l("p"),cp=i("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Xe=l("code"),hp=i("HF_DATASETS_OFFLINE"),up=i(" to "),Ze=l("code"),mp=i("1"),gp=i(" to enable full offline mode."),Wo=f(),hs=l("h2"),Bs=l("a"),sl=l("span"),E(Rt.$$.fragment),_p=f(),tl=l("span"),vp=i("Slice splits"),Go=f(),Q=l("p"),$p=i("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ra=l("a"),yp=i("ReadInstruction"),wp=i(". Strings are more compact and readable for simple cases, while "),Ma=l("a"),bp=i("ReadInstruction"),jp=i(" is easier to use with variable slicing parameters."),Qo=f(),K=l("p"),xp=i("Concatenate the "),al=l("code"),kp=i("train"),Ep=i(" and "),el=l("code"),qp=i("test"),Pp=i(" split by:"),Ko=f(),E(Mt.$$.fragment),Xo=f(),Ys=l("p"),Ap=i("Select specific rows of the "),ll=l("code"),Sp=i("train"),Tp=i(" split:"),Zo=f(),E(Vt.$$.fragment),sn=f(),Va=l("p"),Dp=i("Or select a percentage of the split with:"),tn=f(),E(zt.$$.fragment),an=f(),za=l("p"),Np=i("You can even select a combination of percentages from each split:"),en=f(),E(Ut.$$.fragment),ln=f(),Ua=l("p"),Ip=i("Finally, create cross-validated dataset splits by:"),on=f(),E(Jt.$$.fragment),nn=f(),us=l("h3"),Ws=l("a"),ol=l("span"),E(Bt.$$.fragment),Cp=f(),nl=l("span"),Op=i("Percent slicing and rounding"),rn=f(),Ja=l("p"),Hp=i("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),pn=f(),E(Yt.$$.fragment),dn=f(),Gs=l("p"),Fp=i("If you want equal sized splits, use "),rl=l("code"),Lp=i("pct1_dropremainder"),Rp=i(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),fn=f(),E(Wt.$$.fragment),cn=f(),E(Qs.$$.fragment),hn=f(),Ba=l("a"),un=f(),ms=l("h2"),Ks=l("a"),il=l("span"),E(Gt.$$.fragment),Mp=f(),pl=l("span"),Vp=i("Troubleshooting"),mn=f(),Ya=l("p"),zp=i("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),gn=f(),gs=l("h3"),Xs=l("a"),dl=l("span"),E(Qt.$$.fragment),Up=f(),fl=l("span"),Jp=i("Manual download"),_n=f(),M=l("p"),Bp=i("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Wa=l("a"),Yp=i("load_dataset()"),Wp=i(" to throw an "),cl=l("code"),Gp=i("AssertionError"),Qp=i(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),hl=l("code"),Kp=i("data_dir"),Xp=i(" argument to specify the path to the files you just downloaded."),vn=f(),Zs=l("p"),Zp=i("For example, if you try to download a configuration from the "),Kt=l("a"),sd=i("MATINF"),td=i(" dataset:"),$n=f(),E(Xt.$$.fragment),yn=f(),_s=l("h3"),st=l("a"),ul=l("span"),E(Zt.$$.fragment),ad=f(),ml=l("span"),ed=i("Specify features"),wn=f(),X=l("p"),ld=i("When you create a dataset from local files, the "),Ga=l("a"),od=i("Features"),nd=i(" are automatically inferred by "),sa=l("a"),rd=i("Apache Arrow"),id=i(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),bn=f(),Z=l("p"),pd=i("The following example shows how you can add custom labels with "),Qa=l("a"),dd=i("ClassLabel"),fd=i(". First, define your own labels using the "),Ka=l("a"),cd=i("Features"),hd=i(" class:"),jn=f(),E(ta.$$.fragment),xn=f(),ss=l("p"),ud=i("Next, specify the "),gl=l("code"),md=i("features"),gd=i(" argument in "),Xa=l("a"),_d=i("load_dataset()"),vd=i(" with the features you just created:"),kn=f(),E(aa.$$.fragment),En=f(),Za=l("p"),$d=i("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),qn=f(),E(ea.$$.fragment),Pn=f(),vs=l("h2"),tt=l("a"),_l=l("span"),E(la.$$.fragment),yd=f(),vl=l("span"),wd=i("Metrics"),An=f(),se=l("p"),bd=i("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Sn=f(),E(oa.$$.fragment),Tn=f(),E(at.$$.fragment),Dn=f(),$s=l("h3"),et=l("a"),$l=l("span"),E(na.$$.fragment),jd=f(),yl=l("span"),xd=i("Load configurations"),Nn=f(),ts=l("p"),kd=i("It is possible for a metric to have different configurations. The configurations are stored in the "),wl=l("code"),Ed=i("config_name"),qd=i(" parameter in "),te=l("a"),Pd=i("MetricInfo"),Ad=i(" attribute. When you load a metric, provide the configuration name as shown in the following:"),In=f(),E(ra.$$.fragment),Cn=f(),ys=l("h3"),lt=l("a"),bl=l("span"),E(ia.$$.fragment),Sd=f(),jl=l("span"),Td=i("Distributed setup"),On=f(),ae=l("p"),Dd=i("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Hn=f(),ee=l("p"),Nd=i("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Fn=f(),as=l("ol"),xl=l("li"),pa=l("p"),Id=i("Define the total number of processes with the "),kl=l("code"),Cd=i("num_process"),Od=i(" argument."),Hd=f(),El=l("li"),ws=l("p"),Fd=i("Set the process "),ql=l("code"),Ld=i("rank"),Rd=i(" as an integer between zero and "),Pl=l("code"),Md=i("num_process - 1"),Vd=i("."),zd=f(),Al=l("li"),da=l("p"),Ud=i("Load your metric with "),le=l("a"),Jd=i("load_metric()"),Bd=i(" with these arguments:"),Ln=f(),E(fa.$$.fragment),Rn=f(),E(ot.$$.fragment),Mn=f(),nt=l("p"),Yd=i("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Sl=l("code"),Wd=i("experiment_id"),Gd=i(" to distinguish the separate evaluations:"),Vn=f(),E(ca.$$.fragment),this.h()},l(s){const e=Lh('[data-svelte="svelte-1phssyn"]',document.head);d=o(e,"META",{name:!0,content:!0}),e.forEach(t),g=c(s),u=o(s,"H1",{class:!0});var ha=n(u);y=o(ha,"A",{id:!0,class:!0,href:!0});var Tl=n(y);$=o(Tl,"SPAN",{});var Dl=n($);P(_.$$.fragment,Dl),Dl.forEach(t),Tl.forEach(t),m=c(ha),w=o(ha,"SPAN",{});var Nl=n(w);v=p(Nl,"Load"),Nl.forEach(t),ha.forEach(t),A=c(s),S=o(s,"P",{});var Il=n(S);T=p(Il,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Il.forEach(t),B=c(s),I=o(s,"P",{});var Cl=n(I);N=p(Cl,"This guide will show you how to load a dataset from:"),Cl.forEach(t),U=c(s),O=o(s,"UL",{});var R=n(O);J=o(R,"LI",{});var Zd=n(J);H=p(Zd,"The Hub without a dataset loading script"),Zd.forEach(t),ma=c(R),bs=o(R,"LI",{});var sf=n(bs);ga=p(sf,"Local files"),sf.forEach(t),_a=c(R),js=o(R,"LI",{});var tf=n(js);xr=p(tf,"In-memory data"),tf.forEach(t),kr=c(R),ge=o(R,"LI",{});var af=n(ge);Er=p(af,"Offline"),af.forEach(t),qr=c(R),_e=o(R,"LI",{});var ef=n(_e);Pr=p(ef,"A specific slice of a split"),ef.forEach(t),R.forEach(t),Fl=c(s),va=o(s,"P",{});var lf=n(va);Ar=p(lf,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),lf.forEach(t),Ll=c(s),$a=o(s,"A",{id:!0}),n($a).forEach(t),Rl=c(s),es=o(s,"H2",{class:!0});var Un=n(es);xs=o(Un,"A",{id:!0,class:!0,href:!0});var of=n(xs);ve=o(of,"SPAN",{});var nf=n(ve);P(it.$$.fragment,nf),nf.forEach(t),of.forEach(t),Sr=c(Un),$e=o(Un,"SPAN",{});var rf=n($e);Tr=p(rf,"Hugging Face Hub"),rf.forEach(t),Un.forEach(t),Ml=c(s),ks=o(s,"P",{});var Jn=n(ks);Dr=p(Jn,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),ye=o(Jn,"STRONG",{});var pf=n(ye);Nr=p(pf,"without"),pf.forEach(t),Ir=p(Jn," a loading script!"),Jn.forEach(t),Vl=c(s),Y=o(s,"P",{});var oe=n(Y);Cr=p(oe,"First, create a dataset repository and upload your data files. Then you can use "),ya=o(oe,"A",{href:!0});var df=n(ya);Or=p(df,"load_dataset()"),df.forEach(t),Hr=p(oe," like you learned in the tutorial. For example, load the files from this "),pt=o(oe,"A",{href:!0,rel:!0});var ff=n(pt);Fr=p(ff,"demo repository"),ff.forEach(t),Lr=p(oe," by providing the repository namespace and dataset name:"),oe.forEach(t),zl=c(s),P(dt.$$.fragment,s),Ul=c(s),wa=o(s,"P",{});var cf=n(wa);Rr=p(cf,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),cf.forEach(t),Jl=c(s),Es=o(s,"P",{});var Bn=n(Es);Mr=p(Bn,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),we=o(Bn,"CODE",{});var hf=n(we);Vr=p(hf,"revision"),hf.forEach(t),zr=p(Bn," flag to specify which dataset version you want to load:"),Bn.forEach(t),Bl=c(s),P(ft.$$.fragment,s),Yl=c(s),P(qs.$$.fragment,s),Wl=c(s),F=o(s,"P",{});var V=n(F);Ur=p(V,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),be=o(V,"CODE",{});var uf=n(be);Jr=p(uf,"train"),uf.forEach(t),Br=p(V," split. Use the "),je=o(V,"CODE",{});var mf=n(je);Yr=p(mf,"data_files"),mf.forEach(t),Wr=p(V," parameter to map data files to splits like "),xe=o(V,"CODE",{});var gf=n(xe);Gr=p(gf,"train"),gf.forEach(t),Qr=p(V,", "),ke=o(V,"CODE",{});var _f=n(ke);Kr=p(_f,"validation"),_f.forEach(t),Xr=p(V," and "),Ee=o(V,"CODE",{});var vf=n(Ee);Zr=p(vf,"test"),vf.forEach(t),si=p(V,":"),V.forEach(t),Gl=c(s),P(ct.$$.fragment,s),Ql=c(s),P(Ps.$$.fragment,s),Kl=c(s),W=o(s,"P",{});var ne=n(W);ti=p(ne,"You can also load a specific subset of the files with the "),qe=o(ne,"CODE",{});var $f=n(qe);ai=p($f,"data_files"),$f.forEach(t),ei=p(ne," parameter. The example below loads files from the "),ht=o(ne,"A",{href:!0,rel:!0});var yf=n(ht);li=p(yf,"C4 dataset"),yf.forEach(t),oi=p(ne,":"),ne.forEach(t),Xl=c(s),P(ut.$$.fragment,s),Zl=c(s),As=o(s,"P",{});var Yn=n(As);ni=p(Yn,"Specify a custom split with the "),Pe=o(Yn,"CODE",{});var wf=n(Pe);ri=p(wf,"split"),wf.forEach(t),ii=p(Yn," parameter:"),Yn.forEach(t),so=c(s),P(mt.$$.fragment,s),to=c(s),ls=o(s,"H2",{class:!0});var Wn=n(ls);Ss=o(Wn,"A",{id:!0,class:!0,href:!0});var bf=n(Ss);Ae=o(bf,"SPAN",{});var jf=n(Ae);P(gt.$$.fragment,jf),jf.forEach(t),bf.forEach(t),pi=c(Wn),Se=o(Wn,"SPAN",{});var xf=n(Se);di=p(xf,"Local and remote files"),xf.forEach(t),Wn.forEach(t),ao=c(s),L=o(s,"P",{});var z=n(L);fi=p(z,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Te=o(z,"CODE",{});var kf=n(Te);ci=p(kf,"csv"),kf.forEach(t),hi=p(z,", "),De=o(z,"CODE",{});var Ef=n(De);ui=p(Ef,"json"),Ef.forEach(t),mi=p(z,", "),Ne=o(z,"CODE",{});var qf=n(Ne);gi=p(qf,"txt"),qf.forEach(t),_i=p(z," or "),Ie=o(z,"CODE",{});var Pf=n(Ie);vi=p(Pf,"parquet"),Pf.forEach(t),$i=p(z," file. The "),ba=o(z,"A",{href:!0});var Af=n(ba);yi=p(Af,"load_dataset()"),Af.forEach(t),wi=p(z," method is able to load each of these file types."),z.forEach(t),eo=c(s),P(Ts.$$.fragment,s),lo=c(s),os=o(s,"H3",{class:!0});var Gn=n(os);Ds=o(Gn,"A",{id:!0,class:!0,href:!0});var Sf=n(Ds);Ce=o(Sf,"SPAN",{});var Tf=n(Ce);P(_t.$$.fragment,Tf),Tf.forEach(t),Sf.forEach(t),bi=c(Gn),Oe=o(Gn,"SPAN",{});var Df=n(Oe);ji=p(Df,"CSV"),Df.forEach(t),Gn.forEach(t),oo=c(s),ja=o(s,"P",{});var Nf=n(ja);xi=p(Nf,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Nf.forEach(t),no=c(s),P(vt.$$.fragment,s),ro=c(s),xa=o(s,"P",{});var If=n(xa);ki=p(If,"If you have more than one CSV file:"),If.forEach(t),io=c(s),P($t.$$.fragment,s),po=c(s),ka=o(s,"P",{});var Cf=n(ka);Ei=p(Cf,"You can also map the training and test splits to specific CSV files:"),Cf.forEach(t),fo=c(s),P(yt.$$.fragment,s),co=c(s),Ea=o(s,"P",{});var Of=n(Ea);qi=p(Of,"To load remote CSV files via HTTP, you can pass the URLs:"),Of.forEach(t),ho=c(s),P(wt.$$.fragment,s),uo=c(s),qa=o(s,"P",{});var Hf=n(qa);Pi=p(Hf,"To load zipped CSV files:"),Hf.forEach(t),mo=c(s),P(bt.$$.fragment,s),go=c(s),ns=o(s,"H3",{class:!0});var Qn=n(ns);Ns=o(Qn,"A",{id:!0,class:!0,href:!0});var Ff=n(Ns);He=o(Ff,"SPAN",{});var Lf=n(He);P(jt.$$.fragment,Lf),Lf.forEach(t),Ff.forEach(t),Ai=c(Qn),Fe=o(Qn,"SPAN",{});var Rf=n(Fe);Si=p(Rf,"JSON"),Rf.forEach(t),Qn.forEach(t),_o=c(s),Is=o(s,"P",{});var Kn=n(Is);Ti=p(Kn,"JSON files are loaded directly with "),Pa=o(Kn,"A",{href:!0});var Mf=n(Pa);Di=p(Mf,"load_dataset()"),Mf.forEach(t),Ni=p(Kn," as shown below:"),Kn.forEach(t),vo=c(s),P(xt.$$.fragment,s),$o=c(s),Aa=o(s,"P",{});var Vf=n(Aa);Ii=p(Vf,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Vf.forEach(t),yo=c(s),P(kt.$$.fragment,s),wo=c(s),Cs=o(s,"P",{});var Xn=n(Cs);Ci=p(Xn,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Le=o(Xn,"CODE",{});var zf=n(Le);Oi=p(zf,"field"),zf.forEach(t),Hi=p(Xn," argument as shown in the following:"),Xn.forEach(t),bo=c(s),P(Et.$$.fragment,s),jo=c(s),Sa=o(s,"P",{});var Uf=n(Sa);Fi=p(Uf,"To load remote JSON files via HTTP, you can pass the URLs:"),Uf.forEach(t),xo=c(s),P(qt.$$.fragment,s),ko=c(s),Ta=o(s,"P",{});var Jf=n(Ta);Li=p(Jf,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Jf.forEach(t),Eo=c(s),rs=o(s,"H3",{class:!0});var Zn=n(rs);Os=o(Zn,"A",{id:!0,class:!0,href:!0});var Bf=n(Os);Re=o(Bf,"SPAN",{});var Yf=n(Re);P(Pt.$$.fragment,Yf),Yf.forEach(t),Bf.forEach(t),Ri=c(Zn),Me=o(Zn,"SPAN",{});var Wf=n(Me);Mi=p(Wf,"Text files"),Wf.forEach(t),Zn.forEach(t),qo=c(s),Da=o(s,"P",{});var Gf=n(Da);Vi=p(Gf,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Gf.forEach(t),Po=c(s),P(At.$$.fragment,s),Ao=c(s),Na=o(s,"P",{});var Qf=n(Na);zi=p(Qf,"To load remote TXT files via HTTP, you can pass the URLs:"),Qf.forEach(t),So=c(s),P(St.$$.fragment,s),To=c(s),is=o(s,"H3",{class:!0});var sr=n(is);Hs=o(sr,"A",{id:!0,class:!0,href:!0});var Kf=n(Hs);Ve=o(Kf,"SPAN",{});var Xf=n(Ve);P(Tt.$$.fragment,Xf),Xf.forEach(t),Kf.forEach(t),Ui=c(sr),ze=o(sr,"SPAN",{});var Zf=n(ze);Ji=p(Zf,"Parquet"),Zf.forEach(t),sr.forEach(t),Do=c(s),Ia=o(s,"P",{});var sc=n(Ia);Bi=p(sc,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),sc.forEach(t),No=c(s),P(Dt.$$.fragment,s),Io=c(s),Ca=o(s,"P",{});var tc=n(Ca);Yi=p(tc,"To load remote parquet files via HTTP, you can pass the URLs:"),tc.forEach(t),Co=c(s),P(Nt.$$.fragment,s),Oo=c(s),ps=o(s,"H2",{class:!0});var tr=n(ps);Fs=o(tr,"A",{id:!0,class:!0,href:!0});var ac=n(Fs);Ue=o(ac,"SPAN",{});var ec=n(Ue);P(It.$$.fragment,ec),ec.forEach(t),ac.forEach(t),Wi=c(tr),Je=o(tr,"SPAN",{});var lc=n(Je);Gi=p(lc,"In-memory data"),lc.forEach(t),tr.forEach(t),Ho=c(s),Ls=o(s,"P",{});var ar=n(Ls);Qi=p(ar,"\u{1F917} Datasets will also allow you to create a "),Oa=o(ar,"A",{href:!0});var oc=n(Oa);Ki=p(oc,"Dataset"),oc.forEach(t),Xi=p(ar," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),ar.forEach(t),Fo=c(s),ds=o(s,"H3",{class:!0});var er=n(ds);Rs=o(er,"A",{id:!0,class:!0,href:!0});var nc=n(Rs);Be=o(nc,"SPAN",{});var rc=n(Be);P(Ct.$$.fragment,rc),rc.forEach(t),nc.forEach(t),Zi=c(er),Ye=o(er,"SPAN",{});var ic=n(Ye);sp=p(ic,"Python dictionary"),ic.forEach(t),er.forEach(t),Lo=c(s),Ms=o(s,"P",{});var lr=n(Ms);tp=p(lr,"Load Python dictionaries with "),Ha=o(lr,"A",{href:!0});var pc=n(Ha);ap=p(pc,"Dataset.from_dict()"),pc.forEach(t),ep=p(lr,":"),lr.forEach(t),Ro=c(s),P(Ot.$$.fragment,s),Mo=c(s),fs=o(s,"H3",{class:!0});var or=n(fs);Vs=o(or,"A",{id:!0,class:!0,href:!0});var dc=n(Vs);We=o(dc,"SPAN",{});var fc=n(We);P(Ht.$$.fragment,fc),fc.forEach(t),dc.forEach(t),lp=c(or),Ge=o(or,"SPAN",{});var cc=n(Ge);op=p(cc,"Pandas DataFrame"),cc.forEach(t),or.forEach(t),Vo=c(s),zs=o(s,"P",{});var nr=n(zs);np=p(nr,"Load Pandas DataFrames with "),Fa=o(nr,"A",{href:!0});var hc=n(Fa);rp=p(hc,"Dataset.from_pandas()"),hc.forEach(t),ip=p(nr,":"),nr.forEach(t),zo=c(s),P(Ft.$$.fragment,s),Uo=c(s),P(Us.$$.fragment,s),Jo=c(s),cs=o(s,"H2",{class:!0});var rr=n(cs);Js=o(rr,"A",{id:!0,class:!0,href:!0});var uc=n(Js);Qe=o(uc,"SPAN",{});var mc=n(Qe);P(Lt.$$.fragment,mc),mc.forEach(t),uc.forEach(t),pp=c(rr),Ke=o(rr,"SPAN",{});var gc=n(Ke);dp=p(gc,"Offline"),gc.forEach(t),rr.forEach(t),Bo=c(s),La=o(s,"P",{});var _c=n(La);fp=p(_c,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),_c.forEach(t),Yo=c(s),G=o(s,"P",{});var re=n(G);cp=p(re,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Xe=o(re,"CODE",{});var vc=n(Xe);hp=p(vc,"HF_DATASETS_OFFLINE"),vc.forEach(t),up=p(re," to "),Ze=o(re,"CODE",{});var $c=n(Ze);mp=p($c,"1"),$c.forEach(t),gp=p(re," to enable full offline mode."),re.forEach(t),Wo=c(s),hs=o(s,"H2",{class:!0});var ir=n(hs);Bs=o(ir,"A",{id:!0,class:!0,href:!0});var yc=n(Bs);sl=o(yc,"SPAN",{});var wc=n(sl);P(Rt.$$.fragment,wc),wc.forEach(t),yc.forEach(t),_p=c(ir),tl=o(ir,"SPAN",{});var bc=n(tl);vp=p(bc,"Slice splits"),bc.forEach(t),ir.forEach(t),Go=c(s),Q=o(s,"P",{});var ie=n(Q);$p=p(ie,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ra=o(ie,"A",{href:!0});var jc=n(Ra);yp=p(jc,"ReadInstruction"),jc.forEach(t),wp=p(ie,". Strings are more compact and readable for simple cases, while "),Ma=o(ie,"A",{href:!0});var xc=n(Ma);bp=p(xc,"ReadInstruction"),xc.forEach(t),jp=p(ie," is easier to use with variable slicing parameters."),ie.forEach(t),Qo=c(s),K=o(s,"P",{});var pe=n(K);xp=p(pe,"Concatenate the "),al=o(pe,"CODE",{});var kc=n(al);kp=p(kc,"train"),kc.forEach(t),Ep=p(pe," and "),el=o(pe,"CODE",{});var Ec=n(el);qp=p(Ec,"test"),Ec.forEach(t),Pp=p(pe," split by:"),pe.forEach(t),Ko=c(s),P(Mt.$$.fragment,s),Xo=c(s),Ys=o(s,"P",{});var pr=n(Ys);Ap=p(pr,"Select specific rows of the "),ll=o(pr,"CODE",{});var qc=n(ll);Sp=p(qc,"train"),qc.forEach(t),Tp=p(pr," split:"),pr.forEach(t),Zo=c(s),P(Vt.$$.fragment,s),sn=c(s),Va=o(s,"P",{});var Pc=n(Va);Dp=p(Pc,"Or select a percentage of the split with:"),Pc.forEach(t),tn=c(s),P(zt.$$.fragment,s),an=c(s),za=o(s,"P",{});var Ac=n(za);Np=p(Ac,"You can even select a combination of percentages from each split:"),Ac.forEach(t),en=c(s),P(Ut.$$.fragment,s),ln=c(s),Ua=o(s,"P",{});var Sc=n(Ua);Ip=p(Sc,"Finally, create cross-validated dataset splits by:"),Sc.forEach(t),on=c(s),P(Jt.$$.fragment,s),nn=c(s),us=o(s,"H3",{class:!0});var dr=n(us);Ws=o(dr,"A",{id:!0,class:!0,href:!0});var Tc=n(Ws);ol=o(Tc,"SPAN",{});var Dc=n(ol);P(Bt.$$.fragment,Dc),Dc.forEach(t),Tc.forEach(t),Cp=c(dr),nl=o(dr,"SPAN",{});var Nc=n(nl);Op=p(Nc,"Percent slicing and rounding"),Nc.forEach(t),dr.forEach(t),rn=c(s),Ja=o(s,"P",{});var Ic=n(Ja);Hp=p(Ic,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),Ic.forEach(t),pn=c(s),P(Yt.$$.fragment,s),dn=c(s),Gs=o(s,"P",{});var fr=n(Gs);Fp=p(fr,"If you want equal sized splits, use "),rl=o(fr,"CODE",{});var Cc=n(rl);Lp=p(Cc,"pct1_dropremainder"),Cc.forEach(t),Rp=p(fr," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),fr.forEach(t),fn=c(s),P(Wt.$$.fragment,s),cn=c(s),P(Qs.$$.fragment,s),hn=c(s),Ba=o(s,"A",{id:!0}),n(Ba).forEach(t),un=c(s),ms=o(s,"H2",{class:!0});var cr=n(ms);Ks=o(cr,"A",{id:!0,class:!0,href:!0});var Oc=n(Ks);il=o(Oc,"SPAN",{});var Hc=n(il);P(Gt.$$.fragment,Hc),Hc.forEach(t),Oc.forEach(t),Mp=c(cr),pl=o(cr,"SPAN",{});var Fc=n(pl);Vp=p(Fc,"Troubleshooting"),Fc.forEach(t),cr.forEach(t),mn=c(s),Ya=o(s,"P",{});var Lc=n(Ya);zp=p(Lc,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Lc.forEach(t),gn=c(s),gs=o(s,"H3",{class:!0});var hr=n(gs);Xs=o(hr,"A",{id:!0,class:!0,href:!0});var Rc=n(Xs);dl=o(Rc,"SPAN",{});var Mc=n(dl);P(Qt.$$.fragment,Mc),Mc.forEach(t),Rc.forEach(t),Up=c(hr),fl=o(hr,"SPAN",{});var Vc=n(fl);Jp=p(Vc,"Manual download"),Vc.forEach(t),hr.forEach(t),_n=c(s),M=o(s,"P",{});var rt=n(M);Bp=p(rt,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Wa=o(rt,"A",{href:!0});var zc=n(Wa);Yp=p(zc,"load_dataset()"),zc.forEach(t),Wp=p(rt," to throw an "),cl=o(rt,"CODE",{});var Uc=n(cl);Gp=p(Uc,"AssertionError"),Uc.forEach(t),Qp=p(rt,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),hl=o(rt,"CODE",{});var Jc=n(hl);Kp=p(Jc,"data_dir"),Jc.forEach(t),Xp=p(rt," argument to specify the path to the files you just downloaded."),rt.forEach(t),vn=c(s),Zs=o(s,"P",{});var ur=n(Zs);Zp=p(ur,"For example, if you try to download a configuration from the "),Kt=o(ur,"A",{href:!0,rel:!0});var Bc=n(Kt);sd=p(Bc,"MATINF"),Bc.forEach(t),td=p(ur," dataset:"),ur.forEach(t),$n=c(s),P(Xt.$$.fragment,s),yn=c(s),_s=o(s,"H3",{class:!0});var mr=n(_s);st=o(mr,"A",{id:!0,class:!0,href:!0});var Yc=n(st);ul=o(Yc,"SPAN",{});var Wc=n(ul);P(Zt.$$.fragment,Wc),Wc.forEach(t),Yc.forEach(t),ad=c(mr),ml=o(mr,"SPAN",{});var Gc=n(ml);ed=p(Gc,"Specify features"),Gc.forEach(t),mr.forEach(t),wn=c(s),X=o(s,"P",{});var de=n(X);ld=p(de,"When you create a dataset from local files, the "),Ga=o(de,"A",{href:!0});var Qc=n(Ga);od=p(Qc,"Features"),Qc.forEach(t),nd=p(de," are automatically inferred by "),sa=o(de,"A",{href:!0,rel:!0});var Kc=n(sa);rd=p(Kc,"Apache Arrow"),Kc.forEach(t),id=p(de,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),de.forEach(t),bn=c(s),Z=o(s,"P",{});var fe=n(Z);pd=p(fe,"The following example shows how you can add custom labels with "),Qa=o(fe,"A",{href:!0});var Xc=n(Qa);dd=p(Xc,"ClassLabel"),Xc.forEach(t),fd=p(fe,". First, define your own labels using the "),Ka=o(fe,"A",{href:!0});var Zc=n(Ka);cd=p(Zc,"Features"),Zc.forEach(t),hd=p(fe," class:"),fe.forEach(t),jn=c(s),P(ta.$$.fragment,s),xn=c(s),ss=o(s,"P",{});var ce=n(ss);ud=p(ce,"Next, specify the "),gl=o(ce,"CODE",{});var sh=n(gl);md=p(sh,"features"),sh.forEach(t),gd=p(ce," argument in "),Xa=o(ce,"A",{href:!0});var th=n(Xa);_d=p(th,"load_dataset()"),th.forEach(t),vd=p(ce," with the features you just created:"),ce.forEach(t),kn=c(s),P(aa.$$.fragment,s),En=c(s),Za=o(s,"P",{});var ah=n(Za);$d=p(ah,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),ah.forEach(t),qn=c(s),P(ea.$$.fragment,s),Pn=c(s),vs=o(s,"H2",{class:!0});var gr=n(vs);tt=o(gr,"A",{id:!0,class:!0,href:!0});var eh=n(tt);_l=o(eh,"SPAN",{});var lh=n(_l);P(la.$$.fragment,lh),lh.forEach(t),eh.forEach(t),yd=c(gr),vl=o(gr,"SPAN",{});var oh=n(vl);wd=p(oh,"Metrics"),oh.forEach(t),gr.forEach(t),An=c(s),se=o(s,"P",{});var nh=n(se);bd=p(nh,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),nh.forEach(t),Sn=c(s),P(oa.$$.fragment,s),Tn=c(s),P(at.$$.fragment,s),Dn=c(s),$s=o(s,"H3",{class:!0});var _r=n($s);et=o(_r,"A",{id:!0,class:!0,href:!0});var rh=n(et);$l=o(rh,"SPAN",{});var ih=n($l);P(na.$$.fragment,ih),ih.forEach(t),rh.forEach(t),jd=c(_r),yl=o(_r,"SPAN",{});var ph=n(yl);xd=p(ph,"Load configurations"),ph.forEach(t),_r.forEach(t),Nn=c(s),ts=o(s,"P",{});var he=n(ts);kd=p(he,"It is possible for a metric to have different configurations. The configurations are stored in the "),wl=o(he,"CODE",{});var dh=n(wl);Ed=p(dh,"config_name"),dh.forEach(t),qd=p(he," parameter in "),te=o(he,"A",{href:!0});var fh=n(te);Pd=p(fh,"MetricInfo"),fh.forEach(t),Ad=p(he," attribute. When you load a metric, provide the configuration name as shown in the following:"),he.forEach(t),In=c(s),P(ra.$$.fragment,s),Cn=c(s),ys=o(s,"H3",{class:!0});var vr=n(ys);lt=o(vr,"A",{id:!0,class:!0,href:!0});var ch=n(lt);bl=o(ch,"SPAN",{});var hh=n(bl);P(ia.$$.fragment,hh),hh.forEach(t),ch.forEach(t),Sd=c(vr),jl=o(vr,"SPAN",{});var uh=n(jl);Td=p(uh,"Distributed setup"),uh.forEach(t),vr.forEach(t),On=c(s),ae=o(s,"P",{});var mh=n(ae);Dd=p(mh,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),mh.forEach(t),Hn=c(s),ee=o(s,"P",{});var gh=n(ee);Nd=p(gh,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),gh.forEach(t),Fn=c(s),as=o(s,"OL",{});var ue=n(as);xl=o(ue,"LI",{});var _h=n(xl);pa=o(_h,"P",{});var $r=n(pa);Id=p($r,"Define the total number of processes with the "),kl=o($r,"CODE",{});var vh=n(kl);Cd=p(vh,"num_process"),vh.forEach(t),Od=p($r," argument."),$r.forEach(t),_h.forEach(t),Hd=c(ue),El=o(ue,"LI",{});var $h=n(El);ws=o($h,"P",{});var me=n(ws);Fd=p(me,"Set the process "),ql=o(me,"CODE",{});var yh=n(ql);Ld=p(yh,"rank"),yh.forEach(t),Rd=p(me," as an integer between zero and "),Pl=o(me,"CODE",{});var wh=n(Pl);Md=p(wh,"num_process - 1"),wh.forEach(t),Vd=p(me,"."),me.forEach(t),$h.forEach(t),zd=c(ue),Al=o(ue,"LI",{});var bh=n(Al);da=o(bh,"P",{});var yr=n(da);Ud=p(yr,"Load your metric with "),le=o(yr,"A",{href:!0});var jh=n(le);Jd=p(jh,"load_metric()"),jh.forEach(t),Bd=p(yr," with these arguments:"),yr.forEach(t),bh.forEach(t),ue.forEach(t),Ln=c(s),P(fa.$$.fragment,s),Rn=c(s),P(ot.$$.fragment,s),Mn=c(s),nt=o(s,"P",{});var wr=n(nt);Yd=p(wr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Sl=o(wr,"CODE",{});var xh=n(Sl);Wd=p(xh,"experiment_id"),xh.forEach(t),Gd=p(wr," to distinguish the separate evaluations:"),wr.forEach(t),Vn=c(s),P(ca.$$.fragment,s),this.h()},h(){h(d,"name","hf:doc:metadata"),h(d,"content",JSON.stringify(eu)),h(y,"id","load"),h(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(y,"href","#load"),h(u,"class","relative group"),h($a,"id","load-from-the-hub"),h(xs,"id","hugging-face-hub"),h(xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xs,"href","#hugging-face-hub"),h(es,"class","relative group"),h(ya,"href","/docs/datasets/2.1.0/en/package_reference/loading_methods#datasets.load_dataset"),h(pt,"href","https://huggingface.co/datasets/lhoestq/demo1"),h(pt,"rel","nofollow"),h(ht,"href","https://huggingface.co/datasets/allenai/c4"),h(ht,"rel","nofollow"),h(Ss,"id","local-and-remote-files"),h(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ss,"href","#local-and-remote-files"),h(ls,"class","relative group"),h(ba,"href","/docs/datasets/2.1.0/en/package_reference/loading_methods#datasets.load_dataset"),h(Ds,"id","csv"),h(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ds,"href","#csv"),h(os,"class","relative group"),h(Ns,"id","json"),h(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ns,"href","#json"),h(ns,"class","relative group"),h(Pa,"href","/docs/datasets/2.1.0/en/package_reference/loading_methods#datasets.load_dataset"),h(Os,"id","text-files"),h(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Os,"href","#text-files"),h(rs,"class","relative group"),h(Hs,"id","parquet"),h(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Hs,"href","#parquet"),h(is,"class","relative group"),h(Fs,"id","inmemory-data"),h(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Fs,"href","#inmemory-data"),h(ps,"class","relative group"),h(Oa,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.Dataset"),h(Rs,"id","python-dictionary"),h(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Rs,"href","#python-dictionary"),h(ds,"class","relative group"),h(Ha,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.Dataset.from_dict"),h(Vs,"id","pandas-dataframe"),h(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Vs,"href","#pandas-dataframe"),h(fs,"class","relative group"),h(Fa,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.Dataset.from_pandas"),h(Js,"id","offline"),h(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Js,"href","#offline"),h(cs,"class","relative group"),h(Bs,"id","slice-splits"),h(Bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Bs,"href","#slice-splits"),h(hs,"class","relative group"),h(Ra,"href","/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Ma,"href","/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Ws,"id","percent-slicing-and-rounding"),h(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ws,"href","#percent-slicing-and-rounding"),h(us,"class","relative group"),h(Ba,"id","troubleshoot"),h(Ks,"id","troubleshooting"),h(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ks,"href","#troubleshooting"),h(ms,"class","relative group"),h(Xs,"id","manual-download"),h(Xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Xs,"href","#manual-download"),h(gs,"class","relative group"),h(Wa,"href","/docs/datasets/2.1.0/en/package_reference/loading_methods#datasets.load_dataset"),h(Kt,"href","https://huggingface.co/datasets/matinf"),h(Kt,"rel","nofollow"),h(st,"id","specify-features"),h(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(st,"href","#specify-features"),h(_s,"class","relative group"),h(Ga,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.Features"),h(sa,"href","https://arrow.apache.org/docs/"),h(sa,"rel","nofollow"),h(Qa,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.ClassLabel"),h(Ka,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.Features"),h(Xa,"href","/docs/datasets/2.1.0/en/package_reference/loading_methods#datasets.load_dataset"),h(tt,"id","metrics"),h(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(tt,"href","#metrics"),h(vs,"class","relative group"),h(et,"id","load-configurations"),h(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(et,"href","#load-configurations"),h($s,"class","relative group"),h(te,"href","/docs/datasets/2.1.0/en/package_reference/main_classes#datasets.MetricInfo"),h(lt,"id","distributed-setup"),h(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(lt,"href","#distributed-setup"),h(ys,"class","relative group"),h(le,"href","/docs/datasets/2.1.0/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){a(document.head,d),r(s,g,e),r(s,u,e),a(u,y),a(y,$),q(_,$,null),a(u,m),a(u,w),a(w,v),r(s,A,e),r(s,S,e),a(S,T),r(s,B,e),r(s,I,e),a(I,N),r(s,U,e),r(s,O,e),a(O,J),a(J,H),a(O,ma),a(O,bs),a(bs,ga),a(O,_a),a(O,js),a(js,xr),a(O,kr),a(O,ge),a(ge,Er),a(O,qr),a(O,_e),a(_e,Pr),r(s,Fl,e),r(s,va,e),a(va,Ar),r(s,Ll,e),r(s,$a,e),r(s,Rl,e),r(s,es,e),a(es,xs),a(xs,ve),q(it,ve,null),a(es,Sr),a(es,$e),a($e,Tr),r(s,Ml,e),r(s,ks,e),a(ks,Dr),a(ks,ye),a(ye,Nr),a(ks,Ir),r(s,Vl,e),r(s,Y,e),a(Y,Cr),a(Y,ya),a(ya,Or),a(Y,Hr),a(Y,pt),a(pt,Fr),a(Y,Lr),r(s,zl,e),q(dt,s,e),r(s,Ul,e),r(s,wa,e),a(wa,Rr),r(s,Jl,e),r(s,Es,e),a(Es,Mr),a(Es,we),a(we,Vr),a(Es,zr),r(s,Bl,e),q(ft,s,e),r(s,Yl,e),q(qs,s,e),r(s,Wl,e),r(s,F,e),a(F,Ur),a(F,be),a(be,Jr),a(F,Br),a(F,je),a(je,Yr),a(F,Wr),a(F,xe),a(xe,Gr),a(F,Qr),a(F,ke),a(ke,Kr),a(F,Xr),a(F,Ee),a(Ee,Zr),a(F,si),r(s,Gl,e),q(ct,s,e),r(s,Ql,e),q(Ps,s,e),r(s,Kl,e),r(s,W,e),a(W,ti),a(W,qe),a(qe,ai),a(W,ei),a(W,ht),a(ht,li),a(W,oi),r(s,Xl,e),q(ut,s,e),r(s,Zl,e),r(s,As,e),a(As,ni),a(As,Pe),a(Pe,ri),a(As,ii),r(s,so,e),q(mt,s,e),r(s,to,e),r(s,ls,e),a(ls,Ss),a(Ss,Ae),q(gt,Ae,null),a(ls,pi),a(ls,Se),a(Se,di),r(s,ao,e),r(s,L,e),a(L,fi),a(L,Te),a(Te,ci),a(L,hi),a(L,De),a(De,ui),a(L,mi),a(L,Ne),a(Ne,gi),a(L,_i),a(L,Ie),a(Ie,vi),a(L,$i),a(L,ba),a(ba,yi),a(L,wi),r(s,eo,e),q(Ts,s,e),r(s,lo,e),r(s,os,e),a(os,Ds),a(Ds,Ce),q(_t,Ce,null),a(os,bi),a(os,Oe),a(Oe,ji),r(s,oo,e),r(s,ja,e),a(ja,xi),r(s,no,e),q(vt,s,e),r(s,ro,e),r(s,xa,e),a(xa,ki),r(s,io,e),q($t,s,e),r(s,po,e),r(s,ka,e),a(ka,Ei),r(s,fo,e),q(yt,s,e),r(s,co,e),r(s,Ea,e),a(Ea,qi),r(s,ho,e),q(wt,s,e),r(s,uo,e),r(s,qa,e),a(qa,Pi),r(s,mo,e),q(bt,s,e),r(s,go,e),r(s,ns,e),a(ns,Ns),a(Ns,He),q(jt,He,null),a(ns,Ai),a(ns,Fe),a(Fe,Si),r(s,_o,e),r(s,Is,e),a(Is,Ti),a(Is,Pa),a(Pa,Di),a(Is,Ni),r(s,vo,e),q(xt,s,e),r(s,$o,e),r(s,Aa,e),a(Aa,Ii),r(s,yo,e),q(kt,s,e),r(s,wo,e),r(s,Cs,e),a(Cs,Ci),a(Cs,Le),a(Le,Oi),a(Cs,Hi),r(s,bo,e),q(Et,s,e),r(s,jo,e),r(s,Sa,e),a(Sa,Fi),r(s,xo,e),q(qt,s,e),r(s,ko,e),r(s,Ta,e),a(Ta,Li),r(s,Eo,e),r(s,rs,e),a(rs,Os),a(Os,Re),q(Pt,Re,null),a(rs,Ri),a(rs,Me),a(Me,Mi),r(s,qo,e),r(s,Da,e),a(Da,Vi),r(s,Po,e),q(At,s,e),r(s,Ao,e),r(s,Na,e),a(Na,zi),r(s,So,e),q(St,s,e),r(s,To,e),r(s,is,e),a(is,Hs),a(Hs,Ve),q(Tt,Ve,null),a(is,Ui),a(is,ze),a(ze,Ji),r(s,Do,e),r(s,Ia,e),a(Ia,Bi),r(s,No,e),q(Dt,s,e),r(s,Io,e),r(s,Ca,e),a(Ca,Yi),r(s,Co,e),q(Nt,s,e),r(s,Oo,e),r(s,ps,e),a(ps,Fs),a(Fs,Ue),q(It,Ue,null),a(ps,Wi),a(ps,Je),a(Je,Gi),r(s,Ho,e),r(s,Ls,e),a(Ls,Qi),a(Ls,Oa),a(Oa,Ki),a(Ls,Xi),r(s,Fo,e),r(s,ds,e),a(ds,Rs),a(Rs,Be),q(Ct,Be,null),a(ds,Zi),a(ds,Ye),a(Ye,sp),r(s,Lo,e),r(s,Ms,e),a(Ms,tp),a(Ms,Ha),a(Ha,ap),a(Ms,ep),r(s,Ro,e),q(Ot,s,e),r(s,Mo,e),r(s,fs,e),a(fs,Vs),a(Vs,We),q(Ht,We,null),a(fs,lp),a(fs,Ge),a(Ge,op),r(s,Vo,e),r(s,zs,e),a(zs,np),a(zs,Fa),a(Fa,rp),a(zs,ip),r(s,zo,e),q(Ft,s,e),r(s,Uo,e),q(Us,s,e),r(s,Jo,e),r(s,cs,e),a(cs,Js),a(Js,Qe),q(Lt,Qe,null),a(cs,pp),a(cs,Ke),a(Ke,dp),r(s,Bo,e),r(s,La,e),a(La,fp),r(s,Yo,e),r(s,G,e),a(G,cp),a(G,Xe),a(Xe,hp),a(G,up),a(G,Ze),a(Ze,mp),a(G,gp),r(s,Wo,e),r(s,hs,e),a(hs,Bs),a(Bs,sl),q(Rt,sl,null),a(hs,_p),a(hs,tl),a(tl,vp),r(s,Go,e),r(s,Q,e),a(Q,$p),a(Q,Ra),a(Ra,yp),a(Q,wp),a(Q,Ma),a(Ma,bp),a(Q,jp),r(s,Qo,e),r(s,K,e),a(K,xp),a(K,al),a(al,kp),a(K,Ep),a(K,el),a(el,qp),a(K,Pp),r(s,Ko,e),q(Mt,s,e),r(s,Xo,e),r(s,Ys,e),a(Ys,Ap),a(Ys,ll),a(ll,Sp),a(Ys,Tp),r(s,Zo,e),q(Vt,s,e),r(s,sn,e),r(s,Va,e),a(Va,Dp),r(s,tn,e),q(zt,s,e),r(s,an,e),r(s,za,e),a(za,Np),r(s,en,e),q(Ut,s,e),r(s,ln,e),r(s,Ua,e),a(Ua,Ip),r(s,on,e),q(Jt,s,e),r(s,nn,e),r(s,us,e),a(us,Ws),a(Ws,ol),q(Bt,ol,null),a(us,Cp),a(us,nl),a(nl,Op),r(s,rn,e),r(s,Ja,e),a(Ja,Hp),r(s,pn,e),q(Yt,s,e),r(s,dn,e),r(s,Gs,e),a(Gs,Fp),a(Gs,rl),a(rl,Lp),a(Gs,Rp),r(s,fn,e),q(Wt,s,e),r(s,cn,e),q(Qs,s,e),r(s,hn,e),r(s,Ba,e),r(s,un,e),r(s,ms,e),a(ms,Ks),a(Ks,il),q(Gt,il,null),a(ms,Mp),a(ms,pl),a(pl,Vp),r(s,mn,e),r(s,Ya,e),a(Ya,zp),r(s,gn,e),r(s,gs,e),a(gs,Xs),a(Xs,dl),q(Qt,dl,null),a(gs,Up),a(gs,fl),a(fl,Jp),r(s,_n,e),r(s,M,e),a(M,Bp),a(M,Wa),a(Wa,Yp),a(M,Wp),a(M,cl),a(cl,Gp),a(M,Qp),a(M,hl),a(hl,Kp),a(M,Xp),r(s,vn,e),r(s,Zs,e),a(Zs,Zp),a(Zs,Kt),a(Kt,sd),a(Zs,td),r(s,$n,e),q(Xt,s,e),r(s,yn,e),r(s,_s,e),a(_s,st),a(st,ul),q(Zt,ul,null),a(_s,ad),a(_s,ml),a(ml,ed),r(s,wn,e),r(s,X,e),a(X,ld),a(X,Ga),a(Ga,od),a(X,nd),a(X,sa),a(sa,rd),a(X,id),r(s,bn,e),r(s,Z,e),a(Z,pd),a(Z,Qa),a(Qa,dd),a(Z,fd),a(Z,Ka),a(Ka,cd),a(Z,hd),r(s,jn,e),q(ta,s,e),r(s,xn,e),r(s,ss,e),a(ss,ud),a(ss,gl),a(gl,md),a(ss,gd),a(ss,Xa),a(Xa,_d),a(ss,vd),r(s,kn,e),q(aa,s,e),r(s,En,e),r(s,Za,e),a(Za,$d),r(s,qn,e),q(ea,s,e),r(s,Pn,e),r(s,vs,e),a(vs,tt),a(tt,_l),q(la,_l,null),a(vs,yd),a(vs,vl),a(vl,wd),r(s,An,e),r(s,se,e),a(se,bd),r(s,Sn,e),q(oa,s,e),r(s,Tn,e),q(at,s,e),r(s,Dn,e),r(s,$s,e),a($s,et),a(et,$l),q(na,$l,null),a($s,jd),a($s,yl),a(yl,xd),r(s,Nn,e),r(s,ts,e),a(ts,kd),a(ts,wl),a(wl,Ed),a(ts,qd),a(ts,te),a(te,Pd),a(ts,Ad),r(s,In,e),q(ra,s,e),r(s,Cn,e),r(s,ys,e),a(ys,lt),a(lt,bl),q(ia,bl,null),a(ys,Sd),a(ys,jl),a(jl,Td),r(s,On,e),r(s,ae,e),a(ae,Dd),r(s,Hn,e),r(s,ee,e),a(ee,Nd),r(s,Fn,e),r(s,as,e),a(as,xl),a(xl,pa),a(pa,Id),a(pa,kl),a(kl,Cd),a(pa,Od),a(as,Hd),a(as,El),a(El,ws),a(ws,Fd),a(ws,ql),a(ql,Ld),a(ws,Rd),a(ws,Pl),a(Pl,Md),a(ws,Vd),a(as,zd),a(as,Al),a(Al,da),a(da,Ud),a(da,le),a(le,Jd),a(da,Bd),r(s,Ln,e),q(fa,s,e),r(s,Rn,e),q(ot,s,e),r(s,Mn,e),r(s,nt,e),a(nt,Yd),a(nt,Sl),a(Sl,Wd),a(nt,Gd),r(s,Vn,e),q(ca,s,e),zn=!0},p(s,[e]){const ha={};e&2&&(ha.$$scope={dirty:e,ctx:s}),qs.$set(ha);const Tl={};e&2&&(Tl.$$scope={dirty:e,ctx:s}),Ps.$set(Tl);const Dl={};e&2&&(Dl.$$scope={dirty:e,ctx:s}),Ts.$set(Dl);const Nl={};e&2&&(Nl.$$scope={dirty:e,ctx:s}),Us.$set(Nl);const Il={};e&2&&(Il.$$scope={dirty:e,ctx:s}),Qs.$set(Il);const Cl={};e&2&&(Cl.$$scope={dirty:e,ctx:s}),at.$set(Cl);const R={};e&2&&(R.$$scope={dirty:e,ctx:s}),ot.$set(R)},i(s){zn||(b(_.$$.fragment,s),b(it.$$.fragment,s),b(dt.$$.fragment,s),b(ft.$$.fragment,s),b(qs.$$.fragment,s),b(ct.$$.fragment,s),b(Ps.$$.fragment,s),b(ut.$$.fragment,s),b(mt.$$.fragment,s),b(gt.$$.fragment,s),b(Ts.$$.fragment,s),b(_t.$$.fragment,s),b(vt.$$.fragment,s),b($t.$$.fragment,s),b(yt.$$.fragment,s),b(wt.$$.fragment,s),b(bt.$$.fragment,s),b(jt.$$.fragment,s),b(xt.$$.fragment,s),b(kt.$$.fragment,s),b(Et.$$.fragment,s),b(qt.$$.fragment,s),b(Pt.$$.fragment,s),b(At.$$.fragment,s),b(St.$$.fragment,s),b(Tt.$$.fragment,s),b(Dt.$$.fragment,s),b(Nt.$$.fragment,s),b(It.$$.fragment,s),b(Ct.$$.fragment,s),b(Ot.$$.fragment,s),b(Ht.$$.fragment,s),b(Ft.$$.fragment,s),b(Us.$$.fragment,s),b(Lt.$$.fragment,s),b(Rt.$$.fragment,s),b(Mt.$$.fragment,s),b(Vt.$$.fragment,s),b(zt.$$.fragment,s),b(Ut.$$.fragment,s),b(Jt.$$.fragment,s),b(Bt.$$.fragment,s),b(Yt.$$.fragment,s),b(Wt.$$.fragment,s),b(Qs.$$.fragment,s),b(Gt.$$.fragment,s),b(Qt.$$.fragment,s),b(Xt.$$.fragment,s),b(Zt.$$.fragment,s),b(ta.$$.fragment,s),b(aa.$$.fragment,s),b(ea.$$.fragment,s),b(la.$$.fragment,s),b(oa.$$.fragment,s),b(at.$$.fragment,s),b(na.$$.fragment,s),b(ra.$$.fragment,s),b(ia.$$.fragment,s),b(fa.$$.fragment,s),b(ot.$$.fragment,s),b(ca.$$.fragment,s),zn=!0)},o(s){j(_.$$.fragment,s),j(it.$$.fragment,s),j(dt.$$.fragment,s),j(ft.$$.fragment,s),j(qs.$$.fragment,s),j(ct.$$.fragment,s),j(Ps.$$.fragment,s),j(ut.$$.fragment,s),j(mt.$$.fragment,s),j(gt.$$.fragment,s),j(Ts.$$.fragment,s),j(_t.$$.fragment,s),j(vt.$$.fragment,s),j($t.$$.fragment,s),j(yt.$$.fragment,s),j(wt.$$.fragment,s),j(bt.$$.fragment,s),j(jt.$$.fragment,s),j(xt.$$.fragment,s),j(kt.$$.fragment,s),j(Et.$$.fragment,s),j(qt.$$.fragment,s),j(Pt.$$.fragment,s),j(At.$$.fragment,s),j(St.$$.fragment,s),j(Tt.$$.fragment,s),j(Dt.$$.fragment,s),j(Nt.$$.fragment,s),j(It.$$.fragment,s),j(Ct.$$.fragment,s),j(Ot.$$.fragment,s),j(Ht.$$.fragment,s),j(Ft.$$.fragment,s),j(Us.$$.fragment,s),j(Lt.$$.fragment,s),j(Rt.$$.fragment,s),j(Mt.$$.fragment,s),j(Vt.$$.fragment,s),j(zt.$$.fragment,s),j(Ut.$$.fragment,s),j(Jt.$$.fragment,s),j(Bt.$$.fragment,s),j(Yt.$$.fragment,s),j(Wt.$$.fragment,s),j(Qs.$$.fragment,s),j(Gt.$$.fragment,s),j(Qt.$$.fragment,s),j(Xt.$$.fragment,s),j(Zt.$$.fragment,s),j(ta.$$.fragment,s),j(aa.$$.fragment,s),j(ea.$$.fragment,s),j(la.$$.fragment,s),j(oa.$$.fragment,s),j(at.$$.fragment,s),j(na.$$.fragment,s),j(ra.$$.fragment,s),j(ia.$$.fragment,s),j(fa.$$.fragment,s),j(ot.$$.fragment,s),j(ca.$$.fragment,s),zn=!1},d(s){t(d),s&&t(g),s&&t(u),k(_),s&&t(A),s&&t(S),s&&t(B),s&&t(I),s&&t(U),s&&t(O),s&&t(Fl),s&&t(va),s&&t(Ll),s&&t($a),s&&t(Rl),s&&t(es),k(it),s&&t(Ml),s&&t(ks),s&&t(Vl),s&&t(Y),s&&t(zl),k(dt,s),s&&t(Ul),s&&t(wa),s&&t(Jl),s&&t(Es),s&&t(Bl),k(ft,s),s&&t(Yl),k(qs,s),s&&t(Wl),s&&t(F),s&&t(Gl),k(ct,s),s&&t(Ql),k(Ps,s),s&&t(Kl),s&&t(W),s&&t(Xl),k(ut,s),s&&t(Zl),s&&t(As),s&&t(so),k(mt,s),s&&t(to),s&&t(ls),k(gt),s&&t(ao),s&&t(L),s&&t(eo),k(Ts,s),s&&t(lo),s&&t(os),k(_t),s&&t(oo),s&&t(ja),s&&t(no),k(vt,s),s&&t(ro),s&&t(xa),s&&t(io),k($t,s),s&&t(po),s&&t(ka),s&&t(fo),k(yt,s),s&&t(co),s&&t(Ea),s&&t(ho),k(wt,s),s&&t(uo),s&&t(qa),s&&t(mo),k(bt,s),s&&t(go),s&&t(ns),k(jt),s&&t(_o),s&&t(Is),s&&t(vo),k(xt,s),s&&t($o),s&&t(Aa),s&&t(yo),k(kt,s),s&&t(wo),s&&t(Cs),s&&t(bo),k(Et,s),s&&t(jo),s&&t(Sa),s&&t(xo),k(qt,s),s&&t(ko),s&&t(Ta),s&&t(Eo),s&&t(rs),k(Pt),s&&t(qo),s&&t(Da),s&&t(Po),k(At,s),s&&t(Ao),s&&t(Na),s&&t(So),k(St,s),s&&t(To),s&&t(is),k(Tt),s&&t(Do),s&&t(Ia),s&&t(No),k(Dt,s),s&&t(Io),s&&t(Ca),s&&t(Co),k(Nt,s),s&&t(Oo),s&&t(ps),k(It),s&&t(Ho),s&&t(Ls),s&&t(Fo),s&&t(ds),k(Ct),s&&t(Lo),s&&t(Ms),s&&t(Ro),k(Ot,s),s&&t(Mo),s&&t(fs),k(Ht),s&&t(Vo),s&&t(zs),s&&t(zo),k(Ft,s),s&&t(Uo),k(Us,s),s&&t(Jo),s&&t(cs),k(Lt),s&&t(Bo),s&&t(La),s&&t(Yo),s&&t(G),s&&t(Wo),s&&t(hs),k(Rt),s&&t(Go),s&&t(Q),s&&t(Qo),s&&t(K),s&&t(Ko),k(Mt,s),s&&t(Xo),s&&t(Ys),s&&t(Zo),k(Vt,s),s&&t(sn),s&&t(Va),s&&t(tn),k(zt,s),s&&t(an),s&&t(za),s&&t(en),k(Ut,s),s&&t(ln),s&&t(Ua),s&&t(on),k(Jt,s),s&&t(nn),s&&t(us),k(Bt),s&&t(rn),s&&t(Ja),s&&t(pn),k(Yt,s),s&&t(dn),s&&t(Gs),s&&t(fn),k(Wt,s),s&&t(cn),k(Qs,s),s&&t(hn),s&&t(Ba),s&&t(un),s&&t(ms),k(Gt),s&&t(mn),s&&t(Ya),s&&t(gn),s&&t(gs),k(Qt),s&&t(_n),s&&t(M),s&&t(vn),s&&t(Zs),s&&t($n),k(Xt,s),s&&t(yn),s&&t(_s),k(Zt),s&&t(wn),s&&t(X),s&&t(bn),s&&t(Z),s&&t(jn),k(ta,s),s&&t(xn),s&&t(ss),s&&t(kn),k(aa,s),s&&t(En),s&&t(Za),s&&t(qn),k(ea,s),s&&t(Pn),s&&t(vs),k(la),s&&t(An),s&&t(se),s&&t(Sn),k(oa,s),s&&t(Tn),k(at,s),s&&t(Dn),s&&t($s),k(na),s&&t(Nn),s&&t(ts),s&&t(In),k(ra,s),s&&t(Cn),s&&t(ys),k(ia),s&&t(On),s&&t(ae),s&&t(Hn),s&&t(ee),s&&t(Fn),s&&t(as),s&&t(Ln),k(fa,s),s&&t(Rn),k(ot,s),s&&t(Mn),s&&t(nt),s&&t(Vn),k(ca,s)}}}const eu={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function lu(x){return Rh(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class du extends Qd{constructor(d){super();Kd(this,d,lu,au,Xd,{})}}export{du as default,eu as metadata};
