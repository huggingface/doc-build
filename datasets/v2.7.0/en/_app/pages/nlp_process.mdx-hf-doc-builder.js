import{S as oe,i as he,s as me,e as p,k as c,w as v,t as l,M as ce,c as r,d as a,m as i,a as o,x as $,h as t,b as m,G as e,g as h,y as w,L as ie,q as k,o as y,B as x,v as ue}from"../chunks/vendor-hf-doc-builder.js";import{I as Ia}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as ts}from"../chunks/CodeBlock-hf-doc-builder.js";function be(Da){let j,js,d,E,ps,I,Js,rs,Rs,ds,K,Ks,gs,q,D,Qs,Q,Vs,Ws,Xs,os,Zs,_s,A,sa,M,aa,ea,vs,g,P,hs,C,na,ms,la,$s,T,ta,V,pa,ra,ws,z,oa,B,ha,ma,ks,F,ys,u,ca,cs,ia,ua,is,ba,fa,W,ja,da,xs,O,Es,b,ga,X,_a,va,us,$a,wa,qs,H,As,_,N,bs,U,ka,fs,ya,Ps,f,xa,Z,Ea,qa,Y,Aa,Pa,Ts,G,zs,ss,Ta,Ns,J,Ls,L,za,as,Na,La,Ss,R,Is,es,Sa,Ds;return I=new Ia({}),C=new Ia({}),F=new ts({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),O=new ts({props:{code:`dataset = dataset.map(lambda examples: tokenizer(examples["text"]), batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>]), batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;the rock is destined to be the 21st century\\&#x27;s new &quot; conan &quot; and that he\\&#x27;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#x27;</span>, 
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>, 
 <span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2600</span>, <span class="hljs-number">2003</span>, <span class="hljs-number">16036</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2022</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">7398</span>, <span class="hljs-number">2301</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">2047</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">16608</span>, <span class="hljs-number">1000</span>, <span class="hljs-number">1998</span>, <span class="hljs-number">2008</span>, <span class="hljs-number">2002</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1055</span>, <span class="hljs-number">2183</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2191</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">17624</span>, <span class="hljs-number">2130</span>, <span class="hljs-number">3618</span>, <span class="hljs-number">2084</span>, <span class="hljs-number">7779</span>, <span class="hljs-number">29058</span>, <span class="hljs-number">8625</span>, <span class="hljs-number">13327</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">3744</span>, <span class="hljs-number">1011</span>, <span class="hljs-number">18856</span>, <span class="hljs-number">19513</span>, <span class="hljs-number">3158</span>, <span class="hljs-number">5477</span>, <span class="hljs-number">4168</span>, <span class="hljs-number">2030</span>, <span class="hljs-number">7112</span>, <span class="hljs-number">16562</span>, <span class="hljs-number">2140</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),H=new ts({props:{code:'dataset = dataset.map(lambda examples: tokenizer(examples["text"], return_tensors="np"), batched=True)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;np&quot;</span>), batched=<span class="hljs-literal">True</span>)'}}),U=new Ia({}),G=new ts({props:{code:'label2id = {"entailment": 0, "neutral": 1, "contradiction": 2}',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {<span class="hljs-string">&quot;entailment&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;neutral&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;contradiction&quot;</span>: <span class="hljs-number">2</span>}'}}),J=new ts({props:{code:'label2id = {"contradiction": 0, "neutral": 1, "entailment": 2}',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {<span class="hljs-string">&quot;contradiction&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;neutral&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;entailment&quot;</span>: <span class="hljs-number">2</span>}'}}),R=new ts({props:{code:`from datasets import load_dataset

mnli = load_dataset("glue", "mnli", split="train")
mnli_aligned = mnli.align_labels_with_mapping(label2id, "label")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>mnli = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mnli&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>mnli_aligned = mnli.align_labels_with_mapping(label2id, <span class="hljs-string">&quot;label&quot;</span>)`}}),{c(){j=p("meta"),js=c(),d=p("h1"),E=p("a"),ps=p("span"),v(I.$$.fragment),Js=c(),rs=p("span"),Rs=l("Process text data"),ds=c(),K=p("p"),Ks=l("This guide shows specific methods for processing text datasets. Learn how to:"),gs=c(),q=p("ul"),D=p("li"),Qs=l("Tokenize a dataset with "),Q=p("a"),Vs=l("map()"),Ws=l("."),Xs=c(),os=p("li"),Zs=l("Align dataset labels with label ids for NLI datasets."),_s=c(),A=p("p"),sa=l("For a guide on how to process any type of dataset, take a look at the "),M=p("a"),aa=l("general process guide"),ea=l("."),vs=c(),g=p("h2"),P=p("a"),hs=p("span"),v(C.$$.fragment),na=c(),ms=p("span"),la=l("Map"),$s=c(),T=p("p"),ta=l("The "),V=p("a"),pa=l("map()"),ra=l(" function supports processing batches of examples at once which speeds up tokenization."),ws=c(),z=p("p"),oa=l("Load a tokenizer from \u{1F917} "),B=p("a"),ha=l("Transformers"),ma=l(":"),ks=c(),v(F.$$.fragment),ys=c(),u=p("p"),ca=l("Set the "),cs=p("code"),ia=l("batched"),ua=l(" parameter to "),is=p("code"),ba=l("True"),fa=l(" in the "),W=p("a"),ja=l("map()"),da=l(" function to apply the tokenizer to batches of examples:"),xs=c(),v(O.$$.fragment),Es=c(),b=p("p"),ga=l("The "),X=p("a"),_a=l("map()"),va=l(" function converts the returned values to a PyArrow-supported format. But explicitly returning the tensors as NumPy arrays is faster because it is a natively supported PyArrow format. Set "),us=p("code"),$a=l('return_tensors="np"'),wa=l(" when you tokenize your text:"),qs=c(),v(H.$$.fragment),As=c(),_=p("h2"),N=p("a"),bs=p("span"),v(U.$$.fragment),ka=c(),fs=p("span"),ya=l("Align"),Ps=c(),f=p("p"),xa=l("The "),Z=p("a"),Ea=l("align_labels_with_mapping()"),qa=l(" function aligns a dataset label id with the label name. Not all \u{1F917} Transformers models follow the prescribed label mapping of the original dataset, especially for NLI datasets. For example, the "),Y=p("a"),Aa=l("MNLI"),Pa=l(" dataset uses the following label mapping:"),Ts=c(),v(G.$$.fragment),zs=c(),ss=p("p"),Ta=l("To align the dataset label mapping with the mapping used by a model, create a dictionary of the label name and id to align on:"),Ns=c(),v(J.$$.fragment),Ls=c(),L=p("p"),za=l("Pass the dictionary of the label mappings to the "),as=p("a"),Na=l("align_labels_with_mapping()"),La=l(" function, and the column to align on:"),Ss=c(),v(R.$$.fragment),Is=c(),es=p("p"),Sa=l("You can also use this function to assign a custom mapping of labels to ids."),this.h()},l(s){const n=ce('[data-svelte="svelte-1phssyn"]',document.head);j=r(n,"META",{name:!0,content:!0}),n.forEach(a),js=i(s),d=r(s,"H1",{class:!0});var Ms=o(d);E=r(Ms,"A",{id:!0,class:!0,href:!0});var Ma=o(E);ps=r(Ma,"SPAN",{});var Ca=o(ps);$(I.$$.fragment,Ca),Ca.forEach(a),Ma.forEach(a),Js=i(Ms),rs=r(Ms,"SPAN",{});var Ba=o(rs);Rs=t(Ba,"Process text data"),Ba.forEach(a),Ms.forEach(a),ds=i(s),K=r(s,"P",{});var Fa=o(K);Ks=t(Fa,"This guide shows specific methods for processing text datasets. Learn how to:"),Fa.forEach(a),gs=i(s),q=r(s,"UL",{});var Cs=o(q);D=r(Cs,"LI",{});var Bs=o(D);Qs=t(Bs,"Tokenize a dataset with "),Q=r(Bs,"A",{href:!0});var Oa=o(Q);Vs=t(Oa,"map()"),Oa.forEach(a),Ws=t(Bs,"."),Bs.forEach(a),Xs=i(Cs),os=r(Cs,"LI",{});var Ha=o(os);Zs=t(Ha,"Align dataset labels with label ids for NLI datasets."),Ha.forEach(a),Cs.forEach(a),_s=i(s),A=r(s,"P",{});var Fs=o(A);sa=t(Fs,"For a guide on how to process any type of dataset, take a look at the "),M=r(Fs,"A",{class:!0,href:!0});var Ua=o(M);aa=t(Ua,"general process guide"),Ua.forEach(a),ea=t(Fs,"."),Fs.forEach(a),vs=i(s),g=r(s,"H2",{class:!0});var Os=o(g);P=r(Os,"A",{id:!0,class:!0,href:!0});var Ya=o(P);hs=r(Ya,"SPAN",{});var Ga=o(hs);$(C.$$.fragment,Ga),Ga.forEach(a),Ya.forEach(a),na=i(Os),ms=r(Os,"SPAN",{});var Ja=o(ms);la=t(Ja,"Map"),Ja.forEach(a),Os.forEach(a),$s=i(s),T=r(s,"P",{});var Hs=o(T);ta=t(Hs,"The "),V=r(Hs,"A",{href:!0});var Ra=o(V);pa=t(Ra,"map()"),Ra.forEach(a),ra=t(Hs," function supports processing batches of examples at once which speeds up tokenization."),Hs.forEach(a),ws=i(s),z=r(s,"P",{});var Us=o(z);oa=t(Us,"Load a tokenizer from \u{1F917} "),B=r(Us,"A",{href:!0,rel:!0});var Ka=o(B);ha=t(Ka,"Transformers"),Ka.forEach(a),ma=t(Us,":"),Us.forEach(a),ks=i(s),$(F.$$.fragment,s),ys=i(s),u=r(s,"P",{});var S=o(u);ca=t(S,"Set the "),cs=r(S,"CODE",{});var Qa=o(cs);ia=t(Qa,"batched"),Qa.forEach(a),ua=t(S," parameter to "),is=r(S,"CODE",{});var Va=o(is);ba=t(Va,"True"),Va.forEach(a),fa=t(S," in the "),W=r(S,"A",{href:!0});var Wa=o(W);ja=t(Wa,"map()"),Wa.forEach(a),da=t(S," function to apply the tokenizer to batches of examples:"),S.forEach(a),xs=i(s),$(O.$$.fragment,s),Es=i(s),b=r(s,"P",{});var ns=o(b);ga=t(ns,"The "),X=r(ns,"A",{href:!0});var Xa=o(X);_a=t(Xa,"map()"),Xa.forEach(a),va=t(ns," function converts the returned values to a PyArrow-supported format. But explicitly returning the tensors as NumPy arrays is faster because it is a natively supported PyArrow format. Set "),us=r(ns,"CODE",{});var Za=o(us);$a=t(Za,'return_tensors="np"'),Za.forEach(a),wa=t(ns," when you tokenize your text:"),ns.forEach(a),qs=i(s),$(H.$$.fragment,s),As=i(s),_=r(s,"H2",{class:!0});var Ys=o(_);N=r(Ys,"A",{id:!0,class:!0,href:!0});var se=o(N);bs=r(se,"SPAN",{});var ae=o(bs);$(U.$$.fragment,ae),ae.forEach(a),se.forEach(a),ka=i(Ys),fs=r(Ys,"SPAN",{});var ee=o(fs);ya=t(ee,"Align"),ee.forEach(a),Ys.forEach(a),Ps=i(s),f=r(s,"P",{});var ls=o(f);xa=t(ls,"The "),Z=r(ls,"A",{href:!0});var ne=o(Z);Ea=t(ne,"align_labels_with_mapping()"),ne.forEach(a),qa=t(ls," function aligns a dataset label id with the label name. Not all \u{1F917} Transformers models follow the prescribed label mapping of the original dataset, especially for NLI datasets. For example, the "),Y=r(ls,"A",{href:!0,rel:!0});var le=o(Y);Aa=t(le,"MNLI"),le.forEach(a),Pa=t(ls," dataset uses the following label mapping:"),ls.forEach(a),Ts=i(s),$(G.$$.fragment,s),zs=i(s),ss=r(s,"P",{});var te=o(ss);Ta=t(te,"To align the dataset label mapping with the mapping used by a model, create a dictionary of the label name and id to align on:"),te.forEach(a),Ns=i(s),$(J.$$.fragment,s),Ls=i(s),L=r(s,"P",{});var Gs=o(L);za=t(Gs,"Pass the dictionary of the label mappings to the "),as=r(Gs,"A",{href:!0});var pe=o(as);Na=t(pe,"align_labels_with_mapping()"),pe.forEach(a),La=t(Gs," function, and the column to align on:"),Gs.forEach(a),Ss=i(s),$(R.$$.fragment,s),Is=i(s),es=r(s,"P",{});var re=o(es);Sa=t(re,"You can also use this function to assign a custom mapping of labels to ids."),re.forEach(a),this.h()},h(){m(j,"name","hf:doc:metadata"),m(j,"content",JSON.stringify(fe)),m(E,"id","process-text-data"),m(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(E,"href","#process-text-data"),m(d,"class","relative group"),m(Q,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),m(M,"class","underline decoration-sky-400 decoration-2 font-semibold"),m(M,"href","./process"),m(P,"id","map"),m(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(P,"href","#map"),m(g,"class","relative group"),m(V,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),m(B,"href","https://huggingface.co/transformers/"),m(B,"rel","nofollow"),m(W,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),m(X,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),m(N,"id","align"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#align"),m(_,"class","relative group"),m(Z,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping"),m(Y,"href","https://huggingface.co/datasets/glue"),m(Y,"rel","nofollow"),m(as,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.align_labels_with_mapping")},m(s,n){e(document.head,j),h(s,js,n),h(s,d,n),e(d,E),e(E,ps),w(I,ps,null),e(d,Js),e(d,rs),e(rs,Rs),h(s,ds,n),h(s,K,n),e(K,Ks),h(s,gs,n),h(s,q,n),e(q,D),e(D,Qs),e(D,Q),e(Q,Vs),e(D,Ws),e(q,Xs),e(q,os),e(os,Zs),h(s,_s,n),h(s,A,n),e(A,sa),e(A,M),e(M,aa),e(A,ea),h(s,vs,n),h(s,g,n),e(g,P),e(P,hs),w(C,hs,null),e(g,na),e(g,ms),e(ms,la),h(s,$s,n),h(s,T,n),e(T,ta),e(T,V),e(V,pa),e(T,ra),h(s,ws,n),h(s,z,n),e(z,oa),e(z,B),e(B,ha),e(z,ma),h(s,ks,n),w(F,s,n),h(s,ys,n),h(s,u,n),e(u,ca),e(u,cs),e(cs,ia),e(u,ua),e(u,is),e(is,ba),e(u,fa),e(u,W),e(W,ja),e(u,da),h(s,xs,n),w(O,s,n),h(s,Es,n),h(s,b,n),e(b,ga),e(b,X),e(X,_a),e(b,va),e(b,us),e(us,$a),e(b,wa),h(s,qs,n),w(H,s,n),h(s,As,n),h(s,_,n),e(_,N),e(N,bs),w(U,bs,null),e(_,ka),e(_,fs),e(fs,ya),h(s,Ps,n),h(s,f,n),e(f,xa),e(f,Z),e(Z,Ea),e(f,qa),e(f,Y),e(Y,Aa),e(f,Pa),h(s,Ts,n),w(G,s,n),h(s,zs,n),h(s,ss,n),e(ss,Ta),h(s,Ns,n),w(J,s,n),h(s,Ls,n),h(s,L,n),e(L,za),e(L,as),e(as,Na),e(L,La),h(s,Ss,n),w(R,s,n),h(s,Is,n),h(s,es,n),e(es,Sa),Ds=!0},p:ie,i(s){Ds||(k(I.$$.fragment,s),k(C.$$.fragment,s),k(F.$$.fragment,s),k(O.$$.fragment,s),k(H.$$.fragment,s),k(U.$$.fragment,s),k(G.$$.fragment,s),k(J.$$.fragment,s),k(R.$$.fragment,s),Ds=!0)},o(s){y(I.$$.fragment,s),y(C.$$.fragment,s),y(F.$$.fragment,s),y(O.$$.fragment,s),y(H.$$.fragment,s),y(U.$$.fragment,s),y(G.$$.fragment,s),y(J.$$.fragment,s),y(R.$$.fragment,s),Ds=!1},d(s){a(j),s&&a(js),s&&a(d),x(I),s&&a(ds),s&&a(K),s&&a(gs),s&&a(q),s&&a(_s),s&&a(A),s&&a(vs),s&&a(g),x(C),s&&a($s),s&&a(T),s&&a(ws),s&&a(z),s&&a(ks),x(F,s),s&&a(ys),s&&a(u),s&&a(xs),x(O,s),s&&a(Es),s&&a(b),s&&a(qs),x(H,s),s&&a(As),s&&a(_),x(U),s&&a(Ps),s&&a(f),s&&a(Ts),x(G,s),s&&a(zs),s&&a(ss),s&&a(Ns),x(J,s),s&&a(Ls),s&&a(L),s&&a(Ss),x(R,s),s&&a(Is),s&&a(es)}}}const fe={local:"process-text-data",sections:[{local:"map",title:"Map"},{local:"align",title:"Align"}],title:"Process text data"};function je(Da){return ue(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ve extends oe{constructor(j){super();he(this,j,je,be,me,{})}}export{ve as default,fe as metadata};
