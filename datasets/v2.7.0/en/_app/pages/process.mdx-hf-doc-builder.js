import{S as Rv,i as Lv,s as Mv,e as l,k as c,w as f,t,M as Uv,c as r,d as a,m as h,a as o,x as d,h as n,b as u,G as e,g as i,y as m,q as g,o as _,B as j,v as Vv}from"../chunks/vendor-hf-doc-builder.js";import{T as so}from"../chunks/Tip-hf-doc-builder.js";import{I as E}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../chunks/CodeBlock-hf-doc-builder.js";function Bv(R){let v,q,b,y,k;return{c(){v=l("p"),q=t("All processing methods in this guide return a new "),b=l("a"),y=t("Dataset"),k=t(" object. Modification is not done in-place. Be careful about overriding your previous dataset!"),this.h()},l($){v=r($,"P",{});var x=o(v);q=n(x,"All processing methods in this guide return a new "),b=r(x,"A",{href:!0});var D=o(b);y=n(D,"Dataset"),D.forEach(a),k=n(x," object. Modification is not done in-place. Be careful about overriding your previous dataset!"),x.forEach(a),this.h()},h(){u(b,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset")},m($,x){i($,v,x),e(v,q),e(v,b),e(b,y),e(v,k)},d($){$&&a(v)}}}function Hv(R){let v,q,b,y,k,$,x,D;return{c(){v=l("p"),q=t("Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type "),b=l("code"),y=t('Value("int32")'),k=t(" to "),$=l("code"),x=t('Value("bool")'),D=t(" if the original column only contains ones and zeros.")},l(us){v=r(us,"P",{});var F=o(v);q=n(F,"Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type "),b=r(F,"CODE",{});var fs=o(b);y=n(fs,'Value("int32")'),fs.forEach(a),k=n(F," to "),$=r(F,"CODE",{});var ft=o($);x=n(ft,'Value("bool")'),ft.forEach(a),D=n(F," if the original column only contains ones and zeros."),F.forEach(a)},m(us,F){i(us,v,F),e(v,q),e(v,b),e(b,y),e(v,k),e(v,$),e($,x),e(v,D)},d(us){us&&a(v)}}}function Yv(R){let v,q,b,y,k;return{c(){v=l("p"),q=t("\u{1F917} Datasets also has a "),b=l("a"),y=t("remove_columns()"),k=t(" function which is faster because it doesn\u2019t copy the data of the remaining columns."),this.h()},l($){v=r($,"P",{});var x=o(v);q=n(x,"\u{1F917} Datasets also has a "),b=r(x,"A",{href:!0});var D=o(b);y=n(D,"remove_columns()"),D.forEach(a),k=n(x," function which is faster because it doesn\u2019t copy the data of the remaining columns."),x.forEach(a),this.h()},h(){u(b,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.remove_columns")},m($,x){i($,v,x),e(v,q),e(v,b),e(b,y),e(v,k)},d($){$&&a(v)}}}function Gv(R){let v,q,b,y,k;return{c(){v=l("p"),q=t("\u{1F917} Datasets also provides support for other common data formats such as NumPy, Pandas, and JAX. Check out the "),b=l("a"),y=t("Using Datasets with TensorFlow"),k=t(" guide for more details on how to efficiently create a TensorFlow dataset."),this.h()},l($){v=r($,"P",{});var x=o(v);q=n(x,"\u{1F917} Datasets also provides support for other common data formats such as NumPy, Pandas, and JAX. Check out the "),b=r(x,"A",{href:!0,rel:!0});var D=o(b);y=n(D,"Using Datasets with TensorFlow"),D.forEach(a),k=n(x," guide for more details on how to efficiently create a TensorFlow dataset."),x.forEach(a),this.h()},h(){u(b,"href","https://huggingface.co/docs/datasets/master/en/use_with_tensorflow#using-totfdataset"),u(b,"rel","nofollow")},m($,x){i($,v,x),e(v,q),e(v,b),e(b,y),e(v,k)},d($){$&&a(v)}}}function Wv(R){let v,q,b,y,k;return{c(){v=l("p"),q=t("Want to save your dataset to a cloud storage provider? Read our "),b=l("a"),y=t("Cloud Storage"),k=t(" guide to learn how to save your dataset to AWS or Google Cloud Storage."),this.h()},l($){v=r($,"P",{});var x=o(v);q=n(x,"Want to save your dataset to a cloud storage provider? Read our "),b=r(x,"A",{href:!0});var D=o(b);y=n(D,"Cloud Storage"),D.forEach(a),k=n(x," guide to learn how to save your dataset to AWS or Google Cloud Storage."),x.forEach(a),this.h()},h(){u(b,"href","./filesystems")},m($,x){i($,v,x),e(v,q),e(v,b),e(b,y),e(v,k)},d($){$&&a(v)}}}function Jv(R){let v,q,b,y,k,$,x,D,us,F,fs,ft,eo,dt,Dc,ao,P,Yn,Tc,Pc,Gn,Sc,Cc,Wn,Nc,Oc,Jn,zc,Fc,Kn,Ic,Rc,Qn,Lc,to,L,Mc,Le,Uc,Vc,Me,Bc,Hc,Ue,Yc,Gc,no,mt,Wc,lo,Ve,ro,Rs,oo,ds,Ls,Xn,Be,Jc,Zn,Kc,po,gt,Qc,io,ms,Ms,sl,He,Xc,el,Zc,co,Us,sh,_t,eh,ah,ho,Ye,uo,gs,Vs,al,Ge,th,tl,nh,fo,M,lh,jt,rh,oh,nl,ph,ih,ll,ch,hh,mo,We,go,_s,Bs,rl,Je,uh,ol,fh,_o,Y,dh,vt,mh,gh,bt,_h,jh,jo,wt,$t,xt,vh,bh,vo,Ke,bo,yt,kt,Et,wh,$h,wo,Qe,$o,js,qt,xh,yh,pl,kh,Eh,xo,Xe,yo,vs,Hs,il,Ze,qh,cl,Ah,ko,G,Dh,At,Th,Ph,hl,Sh,Ch,Eo,sa,qo,Ys,Nh,ul,Oh,zh,Ao,bs,Gs,fl,ea,Fh,dl,Ih,Do,U,Rh,ml,Lh,Mh,Dt,Uh,Vh,gl,Bh,Hh,To,Ws,Yh,aa,Gh,Wh,Po,ta,So,Tt,Jh,Co,na,No,ws,Js,_l,la,Kh,jl,Qh,Oo,Pt,Xh,zo,$s,Ks,vl,ra,Zh,bl,su,Fo,Qs,eu,St,au,tu,Io,Xs,nu,Ct,lu,ru,Ro,oa,Lo,xs,Zs,wl,pa,ou,$l,pu,Mo,se,iu,Nt,cu,hu,Uo,ia,Vo,ys,ee,xl,ca,uu,yl,fu,Bo,C,du,Ot,mu,gu,zt,_u,ju,Ft,vu,bu,It,wu,$u,Ho,ha,Yo,ae,Go,te,xu,Rt,yu,ku,Wo,ua,Jo,ks,ne,kl,fa,Eu,El,qu,Ko,Lt,Au,Qo,da,Xo,N,Du,ql,Tu,Pu,Al,Su,Cu,Dl,Nu,Ou,Mt,zu,Fu,Zo,ma,sp,W,Iu,Tl,Ru,Lu,Pl,Mu,Uu,ep,Es,le,Sl,ga,Vu,Cl,Bu,ap,J,Hu,Ut,Yu,Gu,Vt,Wu,Ju,tp,K,Ku,Nl,Qu,Xu,Ol,Zu,sf,np,Q,ef,zl,af,tf,Fl,nf,lf,lp,_a,rp,X,rf,Bt,of,pf,Il,cf,hf,op,ja,pp,re,uf,Ht,ff,df,ip,Z,mf,Rl,gf,_f,Yt,jf,vf,cp,va,hp,oe,up,ss,bf,Gt,wf,$f,Ll,xf,yf,fp,ba,dp,S,kf,Wt,Ef,qf,Ml,Af,Df,Ul,Tf,Pf,Vl,Sf,Cf,Bl,Nf,Of,mp,wa,gp,pe,zf,Hl,Ff,If,_p,$a,jp,qs,ie,Yl,xa,Rf,Gl,Lf,vp,es,Mf,Wl,Uf,Vf,Jt,Bf,Hf,bp,ya,wp,As,ce,Jl,ka,Yf,Kl,Gf,$p,V,Wf,Kt,Jf,Kf,Ql,Qf,Xf,Xl,Zf,sd,xp,Ds,he,Zl,Ea,ed,sr,ad,yp,Qt,td,kp,ue,er,qa,nd,ar,ld,rd,od,tr,nr,pd,Ep,Aa,qp,fe,id,Xt,cd,hd,Ap,Da,Dp,Zt,ud,Tp,Ta,Pp,Ts,de,lr,Pa,fd,rr,dd,Sp,me,md,sn,gd,_d,Cp,as,jd,Sa,vd,bd,Ca,wd,$d,Np,Na,Op,en,xd,zp,Oa,Fp,ge,yd,an,kd,Ed,Ip,za,Rp,O,qd,or,Ad,Dd,pr,Td,Pd,ir,Sd,Cd,cr,Nd,Od,Lp,Ps,_e,hr,Fa,zd,ur,Fd,Mp,ts,Id,tn,Rd,Ld,fr,Md,Ud,Up,Ia,Vp,Ss,je,dr,Ra,Vd,mr,Bd,Bp,ns,Hd,nn,Yd,Gd,La,Wd,Jd,Hp,ve,Kd,gr,Qd,Xd,Yp,Ma,Gp,Cs,be,_r,Ua,Zd,jr,sm,Wp,we,em,ln,am,tm,Jp,Va,Kp,$e,nm,vr,lm,rm,Qp,Ba,Xp,Ns,xe,br,Ha,om,wr,pm,Zp,A,im,$r,cm,hm,rn,um,fm,on,dm,mm,pn,gm,_m,cn,jm,vm,hn,bm,wm,un,$m,xm,fn,ym,km,si,dn,Em,ei,Ya,ai,z,qm,xr,Am,Dm,yr,Tm,Pm,kr,Sm,Cm,Er,Nm,Om,ti,Ga,ni,Os,ye,qr,Wa,zm,Ar,Fm,li,ls,Im,mn,Rm,Lm,Dr,Mm,Um,ri,ke,Vm,Tr,Bm,Hm,oi,Ja,pi,rs,Ym,gn,Gm,Wm,_n,Jm,Km,ii,Ka,ci,Ee,hi,qe,Qm,jn,Xm,Zm,ui,Qa,fi,zs,Ae,Pr,Xa,sg,Sr,eg,di,De,ag,vn,tg,ng,mi,Za,gi,Fs,Te,Cr,st,lg,Nr,rg,_i,Pe,og,bn,pg,ig,ji,wn,cg,vi,et,bi,Se,hg,$n,ug,fg,wi,at,$i,Ce,xi,Is,Ne,Or,tt,dg,zr,mg,yi,xn,gg,ki,Oe,Fr,nt,Ir,_g,jg,Rr,vg,bg,I,lt,Lr,wg,$g,Mr,yn,xg,yg,rt,Ur,kg,Eg,Vr,kn,qg,Ag,ot,Br,Dg,Tg,Hr,En,Pg,Sg,pt,Yr,Cg,Ng,Gr,qn,Og,zg,it,Wr,Fg,Ig,ct,An,Rg,Lg,Dn,Mg,Ei,Tn,Ug,qi,ht,Ai;return $=new E({}),Ve=new w({props:{code:`from datasets import load_dataset
dataset = load_dataset("glue", "mrpc", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;mrpc&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Rs=new so({props:{warning:!0,$$slots:{default:[Bv]},$$scope:{ctx:R}}}),Be=new E({}),He=new E({}),Ye=new w({props:{code:`dataset["label"][:10]
sorted_dataset = dataset.sort("label")
sorted_dataset["label"][:10]
sorted_dataset["label"][-10:]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&quot;label&quot;</span>][:<span class="hljs-number">10</span>]
[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>sorted_dataset = dataset.sort(<span class="hljs-string">&quot;label&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sorted_dataset[<span class="hljs-string">&quot;label&quot;</span>][:<span class="hljs-number">10</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>sorted_dataset[<span class="hljs-string">&quot;label&quot;</span>][-<span class="hljs-number">10</span>:]
[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),Ge=new E({}),We=new w({props:{code:`shuffled_dataset = sorted_dataset.shuffle(seed=42)
shuffled_dataset["label"][:10]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = sorted_dataset.shuffle(seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset[<span class="hljs-string">&quot;label&quot;</span>][:<span class="hljs-number">10</span>]
[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]`}}),Je=new E({}),Ke=new w({props:{code:`small_dataset = dataset.select([0, 10, 20, 30, 40, 50])
len(small_dataset)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>small_dataset = dataset.select([<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">30</span>, <span class="hljs-number">40</span>, <span class="hljs-number">50</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(small_dataset)
<span class="hljs-number">6</span>`}}),Qe=new w({props:{code:`start_with_ar = dataset.filter(lambda example: example["sentence1"].startswith("Ar"))
len(start_with_ar)
start_with_ar["sentence1"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">&quot;sentence1&quot;</span>].startswith(<span class="hljs-string">&quot;Ar&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(start_with_ar)
<span class="hljs-number">6</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar[<span class="hljs-string">&quot;sentence1&quot;</span>]
[<span class="hljs-string">&#x27;Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .&#x27;</span>,
<span class="hljs-string">&#x27;Arison said Mann may have been one of the pioneers of the world music movement and he had a deep love of Brazilian music .&#x27;</span>,
<span class="hljs-string">&#x27;Arts helped coach the youth on an eighth-grade football team at Lombardi Middle School in Green Bay .&#x27;</span>,
<span class="hljs-string">&#x27;Around 9 : 00 a.m. EDT ( 1300 GMT ) , the euro was at $ 1.1566 against the dollar , up 0.07 percent on the day .&#x27;</span>,
<span class="hljs-string">&quot;Arguing that the case was an isolated example , Canada has threatened a trade backlash if Tokyo &#x27;s ban is not justified on scientific grounds .&quot;</span>,
<span class="hljs-string">&#x27;Artists are worried the plan would harm those who need help most - performers who have a difficult time lining up shows .&#x27;</span>
]`}}),Xe=new w({props:{code:`even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
len(even_dataset)
len(dataset) / 2`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>even_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(even_dataset)
<span class="hljs-number">1834</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(dataset) / <span class="hljs-number">2</span>
<span class="hljs-number">1834.0</span>`}}),Ze=new E({}),sa=new w({props:{code:`dataset.train_test_split(test_size=0.1)
0.1 * len(dataset)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.train_test_split(test_size=<span class="hljs-number">0.1</span>)
{<span class="hljs-string">&#x27;train&#x27;</span>: Dataset(schema: {<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-string">&#x27;int32&#x27;</span>}, num_rows: <span class="hljs-number">3301</span>),
<span class="hljs-string">&#x27;test&#x27;</span>: Dataset(schema: {<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-string">&#x27;int32&#x27;</span>}, num_rows: <span class="hljs-number">367</span>)}
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-number">0.1</span> * <span class="hljs-built_in">len</span>(dataset)
<span class="hljs-number">366.8</span>`}}),ea=new E({}),ta=new w({props:{code:`from datasets import load_dataset
datasets = load_dataset("imdb", split="train")
print(dataset)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>datasets = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(dataset)
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">25000</span>
})`}}),na=new w({props:{code:`dataset.shard(num_shards=4, index=0)
print(25000/4)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.shard(num_shards=<span class="hljs-number">4</span>, index=<span class="hljs-number">0</span>)
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">6250</span>
})
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-number">25000</span>/<span class="hljs-number">4</span>)
<span class="hljs-number">6250.0</span>`}}),la=new E({}),ra=new E({}),oa=new w({props:{code:`dataset
dataset = dataset.rename_column("sentence1", "sentenceA")
dataset = dataset.rename_column("sentence2", "sentenceB")
dataset`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
    features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
    num_rows: <span class="hljs-number">3668</span>
})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentenceA&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;sentence2&quot;</span>, <span class="hljs-string">&quot;sentenceB&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
    features: [<span class="hljs-string">&#x27;sentenceA&#x27;</span>, <span class="hljs-string">&#x27;sentenceB&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
    num_rows: <span class="hljs-number">3668</span>
})`}}),pa=new E({}),ia=new w({props:{code:`dataset = dataset.remove_columns("label")
dataset
dataset = dataset.remove_columns(["sentence1", "sentence2"])
dataset`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&quot;label&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
    features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
    num_rows: <span class="hljs-number">3668</span>
})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns([<span class="hljs-string">&quot;sentence1&quot;</span>, <span class="hljs-string">&quot;sentence2&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
    features: [<span class="hljs-string">&#x27;idx&#x27;</span>],
    num_rows: <span class="hljs-number">3668</span>
})`}}),ca=new E({}),ha=new w({props:{code:`dataset.features

from datasets import ClassLabel, Value
new_features = dataset.features.copy()
new_features["label"] = ClassLabel(names=["negative", "positive"])
new_features["idx"] = Value("int64")
dataset = dataset.cast(new_features)
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, Value
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features = dataset.features.copy()
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;label&quot;</span>] = ClassLabel(names=[<span class="hljs-string">&quot;negative&quot;</span>, <span class="hljs-string">&quot;positive&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;idx&quot;</span>] = Value(<span class="hljs-string">&quot;int64&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast(new_features)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ae=new so({props:{$$slots:{default:[Hv]},$$scope:{ctx:R}}}),ua=new w({props:{code:`dataset.features

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">44100</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">16000</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),fa=new E({}),da=new w({props:{code:`from datasets import load_dataset
dataset = load_dataset("squad", split="train")
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;answers&#x27;</span>: <span class="hljs-type">Sequence</span>(feature={<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>), <span class="hljs-string">&#x27;answer_start&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}, length=-<span class="hljs-number">1</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;context&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;id&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;question&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;title&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ma=new w({props:{code:`flat_dataset = dataset.flatten()
flat_dataset`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>flat_dataset = dataset.flatten()
<span class="hljs-meta">&gt;&gt;&gt; </span>flat_dataset
Dataset({
    features: [<span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;context&#x27;</span>, <span class="hljs-string">&#x27;question&#x27;</span>, <span class="hljs-string">&#x27;answers.text&#x27;</span>, <span class="hljs-string">&#x27;answers.answer_start&#x27;</span>],
 num_rows: <span class="hljs-number">87599</span>
})`}}),ga=new E({}),_a=new w({props:{code:`def add_prefix(example):
    example["sentence1"] = 'My sentence: ' + example["sentence1"]
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&quot;sentence1&quot;</span>] = <span class="hljs-string">&#x27;My sentence: &#x27;</span> + example[<span class="hljs-string">&quot;sentence1&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),ja=new w({props:{code:`updated_dataset = small_dataset.map(add_prefix)
updated_dataset["sentence1"][:5]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = small_dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset[<span class="hljs-string">&quot;sentence1&quot;</span>][:<span class="hljs-number">5</span>]
[<span class="hljs-string">&#x27;My sentence: Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&quot;My sentence: Yucaipa owned Dominick &#x27;s before selling the chain to Safeway in 1998 for $ 2.5 billion .&quot;</span>,
<span class="hljs-string">&#x27;My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .&#x27;</span>,
<span class="hljs-string">&#x27;My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .&#x27;</span>,
]`}}),va=new w({props:{code:`updated_dataset = dataset.map(lambda example: {"new_sentence": example["sentence1"]}, remove_columns=["sentence1"])
updated_dataset.column_names`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> example: {<span class="hljs-string">&quot;new_sentence&quot;</span>: example[<span class="hljs-string">&quot;sentence1&quot;</span>]}, remove_columns=[<span class="hljs-string">&quot;sentence1&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset.column_names
[<span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;new_sentence&#x27;</span>]`}}),oe=new so({props:{$$slots:{default:[Yv]},$$scope:{ctx:R}}}),ba=new w({props:{code:`updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, with_indices=True)
updated_dataset["sentence2"][:5]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> example, idx: {<span class="hljs-string">&quot;sentence2&quot;</span>: <span class="hljs-string">f&quot;<span class="hljs-subst">{idx}</span>: &quot;</span> + example[<span class="hljs-string">&quot;sentence2&quot;</span>]}, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset[<span class="hljs-string">&quot;sentence2&quot;</span>][:<span class="hljs-number">5</span>]
[<span class="hljs-string">&#x27;0: Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
 <span class="hljs-string">&quot;1: Yucaipa bought Dominick &#x27;s in 1995 for $ 693 million and sold it to Safeway for $ 1.8 billion in 1998 .&quot;</span>,
 <span class="hljs-string">&quot;2: On June 10 , the ship &#x27;s owners had published an advertisement on the Internet , offering the explosives for sale .&quot;</span>,
 <span class="hljs-string">&#x27;3: Tab shares jumped 20 cents , or 4.6 % , to set a record closing high at A $ 4.57 .&#x27;</span>,
 <span class="hljs-string">&#x27;4: PG &amp; E Corp. shares jumped $ 1.63 or 8 percent to $ 21.03 on the New York Stock Exchange on Friday .&#x27;</span>
]`}}),wa=new w({props:{code:`from multiprocess import set_start_method
import torch
import os
set_start_method("spawn")
def gpu_computation(example, rank):
    os.environ["CUDA_VISIBLE_DEVICES"] = str(rank % torch.cuda.device_count())
    # Your big GPU call goes here
    return examples
updated_dataset = dataset.map(gpu_computation, with_rank=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> multiprocess <span class="hljs-keyword">import</span> set_start_method
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>set_start_method(<span class="hljs-string">&quot;spawn&quot;</span>)
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">gpu_computation</span>(<span class="hljs-params">example, rank</span>):
<span class="hljs-meta">&gt;&gt;&gt; </span>    os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-built_in">str</span>(rank % torch.cuda.device_count())
<span class="hljs-meta">&gt;&gt;&gt; </span>    <span class="hljs-comment"># Your big GPU call goes here</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>    <span class="hljs-keyword">return</span> examples
&gt;&gt;&gt;
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(gpu_computation, with_rank=<span class="hljs-literal">True</span>)`}}),$a=new w({props:{code:"RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.",highlighted:'RuntimeError: Cannot re-initialize CUDA <span class="hljs-keyword">in</span> forked subprocess. To use CUDA with multiprocessing, you must use the <span class="hljs-string">&#x27;spawn&#x27;</span> start method.'}}),xa=new E({}),ya=new w({props:{code:'updated_dataset = dataset.map(lambda example, idx: {"sentence2": f"{idx}: " + example["sentence2"]}, num_proc=4)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> example, idx: {<span class="hljs-string">&quot;sentence2&quot;</span>: <span class="hljs-string">f&quot;<span class="hljs-subst">{idx}</span>: &quot;</span> + example[<span class="hljs-string">&quot;sentence2&quot;</span>]}, num_proc=<span class="hljs-number">4</span>)'}}),ka=new E({}),Ea=new E({}),Aa=new w({props:{code:`def chunk_examples(examples):
    chunks = []
    for sentence in examples["sentence1"]:
        chunks += [sentence[i:i + 50] for i in range(0, len(sentence), 50)]
    return {"chunks": chunks}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">chunk_examples</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    chunks = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;sentence1&quot;</span>]:
<span class="hljs-meta">... </span>        chunks += [sentence[i:i + <span class="hljs-number">50</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(sentence), <span class="hljs-number">50</span>)]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;chunks&quot;</span>: chunks}`}}),Da=new w({props:{code:`chunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)
chunked_dataset[:10]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>chunked_dataset = dataset.<span class="hljs-built_in">map</span>(chunk_examples, batched=<span class="hljs-literal">True</span>, remove_columns=dataset.column_names)
<span class="hljs-meta">&gt;&gt;&gt; </span>chunked_dataset[:<span class="hljs-number">10</span>]
{<span class="hljs-string">&#x27;chunks&#x27;</span>: [<span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the &#x27;</span>,
            <span class="hljs-string">&#x27;witness &quot; , of deliberately distorting his evidenc&#x27;</span>,
            <span class="hljs-string">&#x27;e .&#x27;</span>,
            <span class="hljs-string">&quot;Yucaipa owned Dominick &#x27;s before selling the chain&quot;</span>,
            <span class="hljs-string">&#x27; to Safeway in 1998 for $ 2.5 billion .&#x27;</span>,
            <span class="hljs-string">&#x27;They had published an advertisement on the Interne&#x27;</span>,
            <span class="hljs-string">&#x27;t on June 10 , offering the cargo for sale , he ad&#x27;</span>,
            <span class="hljs-string">&#x27;ded .&#x27;</span>,
            <span class="hljs-string">&#x27;Around 0335 GMT , Tab shares were up 19 cents , or&#x27;</span>,
            <span class="hljs-string">&#x27; 4.4 % , at A $ 4.56 , having earlier set a record&#x27;</span>]}`}}),Ta=new w({props:{code:`dataset
chunked_dataset`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset
Dataset({
 features: [<span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;idx&#x27;</span>],
 num_rows: <span class="hljs-number">3668</span>
})
<span class="hljs-meta">&gt;&gt;&gt; </span>chunked_dataset
Dataset(schema: {<span class="hljs-string">&#x27;chunks&#x27;</span>: <span class="hljs-string">&#x27;string&#x27;</span>}, num_rows: <span class="hljs-number">10470</span>)`}}),Pa=new E({}),Na=new w({props:{code:`from random import randint
from transformers import pipeline

fillmask = pipeline("fill-mask", model="roberta-base")
mask_token = fillmask.tokenizer.mask_token
smaller_dataset = dataset.filter(lambda e, i: i<100, with_indices=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> random <span class="hljs-keyword">import</span> randint
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>fillmask = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, model=<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token = fillmask.tokenizer.mask_token
<span class="hljs-meta">&gt;&gt;&gt; </span>smaller_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> e, i: i&lt;<span class="hljs-number">100</span>, with_indices=<span class="hljs-literal">True</span>)`}}),Oa=new w({props:{code:`def augment_data(examples):
    outputs = []
    for sentence in examples["sentence1"]:
        words = sentence.split(' ')
        K = randint(1, len(words)-1)
        masked_sentence = " ".join(words[:K]  + [mask_token] + words[K+1:])
        predictions = fillmask(masked_sentence)
        augmented_sequences = [predictions[i]["sequence"] for i in range(3)]
        outputs += [sentence] + augmented_sequences
    return {"data": outputs}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">augment_data</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    outputs = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;sentence1&quot;</span>]:
<span class="hljs-meta">... </span>        words = sentence.split(<span class="hljs-string">&#x27; &#x27;</span>)
<span class="hljs-meta">... </span>        K = randint(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(words)-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>        masked_sentence = <span class="hljs-string">&quot; &quot;</span>.join(words[:K]  + [mask_token] + words[K+<span class="hljs-number">1</span>:])
<span class="hljs-meta">... </span>        predictions = fillmask(masked_sentence)
<span class="hljs-meta">... </span>        augmented_sequences = [predictions[i][<span class="hljs-string">&quot;sequence&quot;</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)]
<span class="hljs-meta">... </span>        outputs += [sentence] + augmented_sequences
...
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;data&quot;</span>: outputs}`}}),za=new w({props:{code:`augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)
augmented_dataset[:9]["data"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>augmented_dataset = smaller_dataset.<span class="hljs-built_in">map</span>(augment_data, batched=<span class="hljs-literal">True</span>, remove_columns=dataset.column_names, batch_size=<span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>augmented_dataset[:<span class="hljs-number">9</span>][<span class="hljs-string">&quot;data&quot;</span>]
[<span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
 <span class="hljs-string">&#x27;Amrozi accused his brother, whom he called &quot; the witness &quot;, of deliberately withholding his evidence.&#x27;</span>,
 <span class="hljs-string">&#x27;Amrozi accused his brother, whom he called &quot; the witness &quot;, of deliberately suppressing his evidence.&#x27;</span>,
 <span class="hljs-string">&#x27;Amrozi accused his brother, whom he called &quot; the witness &quot;, of deliberately destroying his evidence.&#x27;</span>,
 <span class="hljs-string">&quot;Yucaipa owned Dominick &#x27;s before selling the chain to Safeway in 1998 for $ 2.5 billion .&quot;</span>,
 <span class="hljs-string">&#x27;Yucaipa owned Dominick Stores before selling the chain to Safeway in 1998 for $ 2.5 billion.&#x27;</span>,
 <span class="hljs-string">&quot;Yucaipa owned Dominick&#x27;s before selling the chain to Safeway in 1998 for $ 2.5 billion.&quot;</span>,
 <span class="hljs-string">&#x27;Yucaipa owned Dominick Pizza before selling the chain to Safeway in 1998 for $ 2.5 billion.&#x27;</span>
]`}}),Fa=new E({}),Ia=new w({props:{code:`from datasets import load_dataset

dataset = load_dataset('glue', 'mrpc')
encoded_dataset = dataset.map(lambda examples: tokenizer(examples["sentence1"]), batched=True)
encoded_dataset["train"][0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># load all the splits</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: tokenizer(examples[<span class="hljs-string">&quot;sentence1&quot;</span>]), batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: [  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]
}`}}),Ra=new E({}),Ma=new w({props:{code:`from datasets import Dataset
import torch.distributed

dataset1 = Dataset.from_dict({"a": [0, 1, 2]})

if training_args.local_rank > 0:
    print("Waiting for main process to perform the mapping")
    torch.distributed.barrier()

dataset2 = dataset1.map(lambda x: {"a": x["a"] + 1})

if training_args.local_rank == 0:
    print("Loading results from main process")
    torch.distributed.barrier()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch.distributed

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset1 = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]})

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> training_args.local_rank &gt; <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Waiting for main process to perform the mapping&quot;</span>)
<span class="hljs-meta">... </span>    torch.distributed.barrier()

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset2 = dataset1.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: {<span class="hljs-string">&quot;a&quot;</span>: x[<span class="hljs-string">&quot;a&quot;</span>] + <span class="hljs-number">1</span>})

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> training_args.local_rank == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Loading results from main process&quot;</span>)
<span class="hljs-meta">... </span>    torch.distributed.barrier()`}}),Ua=new E({}),Va=new w({props:{code:`from datasets import concatenate_datasets, load_dataset

bookcorpus = load_dataset("bookcorpus", split="train")
wiki = load_dataset("wikipedia", "20220301.en", split="train")
wiki = wiki.remove_columns([col for col in wiki.column_names if col != "text"])  # only keep the 'text' column

assert bookcorpus.features.type == wiki.features.type
bert_dataset = concatenate_datasets([bookcorpus, wiki])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> concatenate_datasets, load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>bookcorpus = load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;wikipedia&quot;</span>, <span class="hljs-string">&quot;20220301.en&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = wiki.remove_columns([col <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> wiki.column_names <span class="hljs-keyword">if</span> col != <span class="hljs-string">&quot;text&quot;</span>])  <span class="hljs-comment"># only keep the &#x27;text&#x27; column</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> bookcorpus.features.<span class="hljs-built_in">type</span> == wiki.features.<span class="hljs-built_in">type</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bert_dataset = concatenate_datasets([bookcorpus, wiki])`}}),Ba=new w({props:{code:`from datasets import Dataset
bookcorpus_ids = Dataset.from_dict({"ids": list(range(len(bookcorpus)))})
bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>bookcorpus_ids = Dataset.from_dict({<span class="hljs-string">&quot;ids&quot;</span>: <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(bookcorpus)))})
<span class="hljs-meta">&gt;&gt;&gt; </span>bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=<span class="hljs-number">1</span>)`}}),Ha=new E({}),Ya=new w({props:{code:`seed = 42
probabilities = [0.3, 0.5, 0.2]
d1 = Dataset.from_dict({"a": [0, 1, 2]})
d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
d3 = Dataset.from_dict({"a": [20, 21, 22]})
dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
dataset["a"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>seed = <span class="hljs-number">42</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probabilities = [<span class="hljs-number">0.3</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.2</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>d1 = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>d2 = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>d3 = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&quot;a&quot;</span>]
[<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">20</span>, <span class="hljs-number">12</span>, <span class="hljs-number">0</span>, <span class="hljs-number">21</span>, <span class="hljs-number">13</span>]`}}),Ga=new w({props:{code:`d1 = Dataset.from_dict({"a": [0, 1, 2]})
d2 = Dataset.from_dict({"a": [10, 11, 12, 13]})
d3 = Dataset.from_dict({"a": [20, 21, 22]})
dataset = interleave_datasets([d1, d2, d3], stopping_strategy="all_exhausted")
dataset["a"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>d1 = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>d2 = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>d3 = Dataset.from_dict({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">20</span>, <span class="hljs-number">21</span>, <span class="hljs-number">22</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = interleave_datasets([d1, d2, d3], stopping_strategy=<span class="hljs-string">&quot;all_exhausted&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&quot;a&quot;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">1</span>, <span class="hljs-number">11</span>, <span class="hljs-number">21</span>, <span class="hljs-number">2</span>, <span class="hljs-number">12</span>, <span class="hljs-number">22</span>, <span class="hljs-number">0</span>, <span class="hljs-number">13</span>, <span class="hljs-number">20</span>]`}}),Wa=new E({}),Ja=new w({props:{code:`import torch
dataset.set_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;torch&quot;</span>, columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>])`}}),Ka=new w({props:{code:'dataset = dataset.with_format(type="torch", columns=["input_ids", "token_type_ids", "attention_mask", "label"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&quot;torch&quot;</span>, columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;token_type_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>])'}}),Ee=new so({props:{$$slots:{default:[Gv]},$$scope:{ctx:R}}}),Qa=new w({props:{code:`dataset.format
dataset.reset_format()
dataset.format`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.<span class="hljs-built_in">format</span>
{<span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;torch&#x27;</span>, <span class="hljs-string">&#x27;format_kwargs&#x27;</span>: {}, <span class="hljs-string">&#x27;columns&#x27;</span>: [<span class="hljs-string">&#x27;label&#x27;</span>], <span class="hljs-string">&#x27;output_all_columns&#x27;</span>: <span class="hljs-literal">False</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.reset_format()
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.<span class="hljs-built_in">format</span>
{<span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;python&#x27;</span>, <span class="hljs-string">&#x27;format_kwargs&#x27;</span>: {}, <span class="hljs-string">&#x27;columns&#x27;</span>: [<span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>], <span class="hljs-string">&#x27;output_all_columns&#x27;</span>: <span class="hljs-literal">False</span>}`}}),Xa=new E({}),Za=new w({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
def encode(batch):
    return tokenizer(batch["sentence1"], padding="longest", truncation=True, max_length=512, return_tensors="pt")
dataset.set_transform(encode)
dataset.format`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(batch[<span class="hljs-string">&quot;sentence1&quot;</span>], padding=<span class="hljs-string">&quot;longest&quot;</span>, truncation=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">512</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_transform(encode)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.<span class="hljs-built_in">format</span>
{<span class="hljs-string">&#x27;type&#x27;</span>: <span class="hljs-string">&#x27;custom&#x27;</span>, <span class="hljs-string">&#x27;format_kwargs&#x27;</span>: {<span class="hljs-string">&#x27;transform&#x27;</span>: &lt;function __main__.encode(batch)&gt;}, <span class="hljs-string">&#x27;columns&#x27;</span>: [<span class="hljs-string">&#x27;idx&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;sentence1&#x27;</span>, <span class="hljs-string">&#x27;sentence2&#x27;</span>], <span class="hljs-string">&#x27;output_all_columns&#x27;</span>: <span class="hljs-literal">False</span>}`}}),st=new E({}),et=new w({props:{code:'encoded_dataset.save_to_disk("path/of/my/dataset/directory")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;path/of/my/dataset/directory&quot;</span>)'}}),at=new w({props:{code:`from datasets import load_from_disk
reloaded_dataset = load_from_disk("path/of/my/dataset/directory")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-meta">&gt;&gt;&gt; </span>reloaded_dataset = load_from_disk(<span class="hljs-string">&quot;path/of/my/dataset/directory&quot;</span>)`}}),Ce=new so({props:{$$slots:{default:[Wv]},$$scope:{ctx:R}}}),tt=new E({}),ht=new w({props:{code:'encoded_dataset.to_csv("path/of/my/dataset.csv")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.to_csv(<span class="hljs-string">&quot;path/of/my/dataset.csv&quot;</span>)'}}),{c(){v=l("meta"),q=c(),b=l("h1"),y=l("a"),k=l("span"),f($.$$.fragment),x=c(),D=l("span"),us=t("Process"),F=c(),fs=l("p"),ft=t("\u{1F917} Datasets provides many tools for modifying the structure and content of a dataset. These tools are important for tidying up a dataset, creating additional columns, converting between features and formats, and much more."),eo=c(),dt=l("p"),Dc=t("This guide will show you how to:"),ao=c(),P=l("ul"),Yn=l("li"),Tc=t("Reorder rows and split the dataset."),Pc=c(),Gn=l("li"),Sc=t("Rename and remove columns, and other common column operations."),Cc=c(),Wn=l("li"),Nc=t("Apply processing functions to each example in a dataset."),Oc=c(),Jn=l("li"),zc=t("Concatenate datasets."),Fc=c(),Kn=l("li"),Ic=t("Apply a custom formatting transform."),Rc=c(),Qn=l("li"),Lc=t("Save and export processed datasets."),to=c(),L=l("p"),Mc=t("For more details specific to processing other dataset modalities, take a look at the "),Le=l("a"),Uc=t("process audio dataset guide"),Vc=t(", the "),Me=l("a"),Bc=t("process image dataset guide"),Hc=t(", or the "),Ue=l("a"),Yc=t("process text dataset guide"),Gc=t("."),no=c(),mt=l("p"),Wc=t("The examples in this guide use the MRPC dataset, but feel free to load any dataset of your choice and follow along!"),lo=c(),f(Ve.$$.fragment),ro=c(),f(Rs.$$.fragment),oo=c(),ds=l("h2"),Ls=l("a"),Xn=l("span"),f(Be.$$.fragment),Jc=c(),Zn=l("span"),Kc=t("Sort, shuffle, select, split, and shard"),po=c(),gt=l("p"),Qc=t("There are several functions for rearranging the structure of a dataset. These functions are useful for selecting only the rows you want, creating train and test splits, and sharding very large datasets into smaller chunks."),io=c(),ms=l("h3"),Ms=l("a"),sl=l("span"),f(He.$$.fragment),Xc=c(),el=l("span"),Zc=t("Sort"),co=c(),Us=l("p"),sh=t("Use "),_t=l("a"),eh=t("sort()"),ah=t(" to sort column values according to their numerical values. The provided column must be NumPy compatible."),ho=c(),f(Ye.$$.fragment),uo=c(),gs=l("h3"),Vs=l("a"),al=l("span"),f(Ge.$$.fragment),th=c(),tl=l("span"),nh=t("Shuffle"),fo=c(),M=l("p"),lh=t("The "),jt=l("a"),rh=t("shuffle()"),oh=t(" function randomly rearranges the column values. You can specify the "),nl=l("code"),ph=t("generator"),ih=t(" parameter in this function to use a different "),ll=l("code"),ch=t("numpy.random.Generator"),hh=t(" if you want more control over the algorithm used to shuffle the dataset."),mo=c(),f(We.$$.fragment),go=c(),_s=l("h3"),Bs=l("a"),rl=l("span"),f(Je.$$.fragment),uh=c(),ol=l("span"),fh=t("Select and Filter"),_o=c(),Y=l("p"),dh=t("There are two options for filtering rows in a dataset: "),vt=l("a"),mh=t("select()"),gh=t(" and "),bt=l("a"),_h=t("filter()"),jh=t("."),jo=c(),wt=l("ul"),$t=l("li"),xt=l("a"),vh=t("select()"),bh=t(" returns rows according to a list of indices:"),vo=c(),f(Ke.$$.fragment),bo=c(),yt=l("ul"),kt=l("li"),Et=l("a"),wh=t("filter()"),$h=t(" returns rows that match a specified condition:"),wo=c(),f(Qe.$$.fragment),$o=c(),js=l("p"),qt=l("a"),xh=t("filter()"),yh=t(" can also filter by indices if you set "),pl=l("code"),kh=t("with_indices=True"),Eh=t(":"),xo=c(),f(Xe.$$.fragment),yo=c(),vs=l("h3"),Hs=l("a"),il=l("span"),f(Ze.$$.fragment),qh=c(),cl=l("span"),Ah=t("Split"),ko=c(),G=l("p"),Dh=t("The "),At=l("a"),Th=t("train_test_split()"),Ph=t(" function creates train and test splits if your dataset doesn\u2019t already have them. This allows you to adjust the relative proportions or an absolute number of samples in each split. In the example below, use the "),hl=l("code"),Sh=t("test_size"),Ch=t(" parameter to create a test split that is 10% of the original dataset:"),Eo=c(),f(sa.$$.fragment),qo=c(),Ys=l("p"),Nh=t("The splits are shuffled by default, but you can set "),ul=l("code"),Oh=t("shuffle=False"),zh=t(" to prevent shuffling."),Ao=c(),bs=l("h3"),Gs=l("a"),fl=l("span"),f(ea.$$.fragment),Fh=c(),dl=l("span"),Ih=t("Shard"),Do=c(),U=l("p"),Rh=t("\u{1F917} Datasets supports sharding to divide a very large dataset into a predefined number of chunks. Specify the "),ml=l("code"),Lh=t("num_shards"),Mh=t(" parameter in "),Dt=l("a"),Uh=t("shard()"),Vh=t(" to determine the number of shards to split the dataset into. You\u2019ll also need to provide the shard you want to return with the "),gl=l("code"),Bh=t("index"),Hh=t(" parameter."),To=c(),Ws=l("p"),Yh=t("For example, the "),aa=l("a"),Gh=t("imdb"),Wh=t(" dataset has 25000 examples:"),Po=c(),f(ta.$$.fragment),So=c(),Tt=l("p"),Jh=t("After sharding the dataset into four chunks, the first shard will only have 6250 examples:"),Co=c(),f(na.$$.fragment),No=c(),ws=l("h2"),Js=l("a"),_l=l("span"),f(la.$$.fragment),Kh=c(),jl=l("span"),Qh=t("Rename, remove, cast, and flatten"),Oo=c(),Pt=l("p"),Xh=t("The following functions allow you to modify the columns of a dataset. These functions are useful for renaming or removing columns, changing columns to a new set of features, and flattening nested column structures."),zo=c(),$s=l("h3"),Ks=l("a"),vl=l("span"),f(ra.$$.fragment),Zh=c(),bl=l("span"),su=t("Rename"),Fo=c(),Qs=l("p"),eu=t("Use "),St=l("a"),au=t("rename_column()"),tu=t(" when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place."),Io=c(),Xs=l("p"),nu=t("Provide "),Ct=l("a"),lu=t("rename_column()"),ru=t(" with the name of the original column, and the new column name:"),Ro=c(),f(oa.$$.fragment),Lo=c(),xs=l("h3"),Zs=l("a"),wl=l("span"),f(pa.$$.fragment),ou=c(),$l=l("span"),pu=t("Remove"),Mo=c(),se=l("p"),iu=t("When you need to remove one or more columns, provide the column name to remove to the "),Nt=l("a"),cu=t("remove_columns()"),hu=t(" function. Remove more than one column by providing a list of column names:"),Uo=c(),f(ia.$$.fragment),Vo=c(),ys=l("h3"),ee=l("a"),xl=l("span"),f(ca.$$.fragment),uu=c(),yl=l("span"),fu=t("Cast"),Bo=c(),C=l("p"),du=t("The "),Ot=l("a"),mu=t("cast()"),gu=t(" function transforms the feature type of one or more columns. This function accepts your new "),zt=l("a"),_u=t("Features"),ju=t(" as its argument. The example below demonstrates how to change the "),Ft=l("a"),vu=t("ClassLabel"),bu=t(" and "),It=l("a"),wu=t("Value"),$u=t(" features:"),Ho=c(),f(ha.$$.fragment),Yo=c(),f(ae.$$.fragment),Go=c(),te=l("p"),xu=t("Use the "),Rt=l("a"),yu=t("cast_column()"),ku=t(" function to change the feature type of a single column. Pass the column name and its new feature type as arguments:"),Wo=c(),f(ua.$$.fragment),Jo=c(),ks=l("h3"),ne=l("a"),kl=l("span"),f(fa.$$.fragment),Eu=c(),El=l("span"),qu=t("Flatten"),Ko=c(),Lt=l("p"),Au=t("Sometimes a column can be a nested structure of several types. Take a look at the nested structure below from the SQuAD dataset:"),Qo=c(),f(da.$$.fragment),Xo=c(),N=l("p"),Du=t("The "),ql=l("code"),Tu=t("answers"),Pu=t(" field contains two subfields: "),Al=l("code"),Su=t("text"),Cu=t(" and "),Dl=l("code"),Nu=t("answer_start"),Ou=t(". Use the "),Mt=l("a"),zu=t("flatten()"),Fu=t(" function to extract the subfields into their own separate columns:"),Zo=c(),f(ma.$$.fragment),sp=c(),W=l("p"),Iu=t("Notice how the subfields are now their own independent columns: "),Tl=l("code"),Ru=t("answers.text"),Lu=t(" and "),Pl=l("code"),Mu=t("answers.answer_start"),Uu=t("."),ep=c(),Es=l("h2"),le=l("a"),Sl=l("span"),f(ga.$$.fragment),Vu=c(),Cl=l("span"),Bu=t("Map"),ap=c(),J=l("p"),Hu=t("Some of the more powerful applications of \u{1F917} Datasets come from using the "),Ut=l("a"),Yu=t("map()"),Gu=t(" function. The primary purpose of "),Vt=l("a"),Wu=t("map()"),Ju=t(" is to speed up processing functions. It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),tp=c(),K=l("p"),Ku=t("In the following example, prefix each "),Nl=l("code"),Qu=t("sentence1"),Xu=t(" value in the dataset with "),Ol=l("code"),Zu=t("'My sentence: '"),sf=t("."),np=c(),Q=l("p"),ef=t("Start by creating a function that adds "),zl=l("code"),af=t("'My sentence: '"),tf=t(" to the beginning of each sentence. The function needs to accept and output a "),Fl=l("code"),nf=t("dict"),lf=t(":"),lp=c(),f(_a.$$.fragment),rp=c(),X=l("p"),rf=t("Now use "),Bt=l("a"),of=t("map()"),pf=t(" to apply the "),Il=l("code"),cf=t("add_prefix"),hf=t(" function to the entire dataset:"),op=c(),f(ja.$$.fragment),pp=c(),re=l("p"),uf=t("Let\u2019s take a look at another example, except this time, you\u2019ll remove a column with "),Ht=l("a"),ff=t("map()"),df=t(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),ip=c(),Z=l("p"),mf=t("Specify the column to remove with the "),Rl=l("code"),gf=t("remove_columns"),_f=t(" parameter in "),Yt=l("a"),jf=t("map()"),vf=t(":"),cp=c(),f(va.$$.fragment),hp=c(),f(oe.$$.fragment),up=c(),ss=l("p"),bf=t("You can also use "),Gt=l("a"),wf=t("map()"),$f=t(" with indices if you set "),Ll=l("code"),xf=t("with_indices=True"),yf=t(". The example below adds the index to the beginning of each sentence:"),fp=c(),f(ba.$$.fragment),dp=c(),S=l("p"),kf=t("The "),Wt=l("a"),Ef=t("map()"),qf=t(" also works with the rank of the process if you set "),Ml=l("code"),Af=t("with_rank=True"),Df=t(". This is analogous to the "),Ul=l("code"),Tf=t("with_indices"),Pf=t(" parameter. The "),Vl=l("code"),Sf=t("with_rank"),Cf=t(" parameter in the mapped function goes after the "),Bl=l("code"),Nf=t("index"),Of=t(" one if it is already present."),mp=c(),f(wa.$$.fragment),gp=c(),pe=l("p"),zf=t("The main use-case for rank is to parallelize computation across several GPUs. This requires setting "),Hl=l("code"),Ff=t('multiprocess.set_start_method("spawn")'),If=t(". If you don\u2019t you\u2019ll receive the following CUDA error:"),_p=c(),f($a.$$.fragment),jp=c(),qs=l("h3"),ie=l("a"),Yl=l("span"),f(xa.$$.fragment),Rf=c(),Gl=l("span"),Lf=t("Multiprocessing"),vp=c(),es=l("p"),Mf=t("Multiprocessing significantly speeds up processing by parallelizing processes on the CPU. Set the "),Wl=l("code"),Uf=t("num_proc"),Vf=t(" parameter in "),Jt=l("a"),Bf=t("map()"),Hf=t(" to set the number of processes to use:"),bp=c(),f(ya.$$.fragment),wp=c(),As=l("h3"),ce=l("a"),Jl=l("span"),f(ka.$$.fragment),Yf=c(),Kl=l("span"),Gf=t("Batch processing"),$p=c(),V=l("p"),Wf=t("The "),Kt=l("a"),Jf=t("map()"),Kf=t(" function supports working with batches of examples. Operate on batches by setting "),Ql=l("code"),Qf=t("batched=True"),Xf=t(". The default batch size is 1000, but you can adjust it with the "),Xl=l("code"),Zf=t("batch_size"),sd=t(" parameter. Batch processing enables interesting applications such as splitting long sentences into shorter chunks and data augmentation."),xp=c(),Ds=l("h4"),he=l("a"),Zl=l("span"),f(Ea.$$.fragment),ed=c(),sr=l("span"),ad=t("Split long examples"),yp=c(),Qt=l("p"),td=t("When examples are too long, you may want to split them into several smaller chunks. Begin by creating a function that:"),kp=c(),ue=l("ol"),er=l("li"),qa=l("p"),nd=t("Splits the "),ar=l("code"),ld=t("sentence1"),rd=t(" field into chunks of 50 characters."),od=c(),tr=l("li"),nr=l("p"),pd=t("Stacks all the chunks together to create the new dataset."),Ep=c(),f(Aa.$$.fragment),qp=c(),fe=l("p"),id=t("Apply the function with "),Xt=l("a"),cd=t("map()"),hd=t(":"),Ap=c(),f(Da.$$.fragment),Dp=c(),Zt=l("p"),ud=t("Notice how the sentences are split into shorter chunks now, and there are more rows in the dataset."),Tp=c(),f(Ta.$$.fragment),Pp=c(),Ts=l("h4"),de=l("a"),lr=l("span"),f(Pa.$$.fragment),fd=c(),rr=l("span"),dd=t("Data augmentation"),Sp=c(),me=l("p"),md=t("The "),sn=l("a"),gd=t("map()"),_d=t(" function could also be used for data augmentation. The following example generates additional words for a masked token in a sentence."),Cp=c(),as=l("p"),jd=t("Load and use the "),Sa=l("a"),vd=t("RoBERTA"),bd=t(" model in \u{1F917} Transformers\u2019 "),Ca=l("a"),wd=t("FillMaskPipeline"),$d=t(":"),Np=c(),f(Na.$$.fragment),Op=c(),en=l("p"),xd=t("Create a function to randomly select a word to mask in the sentence. The function should also return the original sentence and the top two replacements generated by RoBERTA."),zp=c(),f(Oa.$$.fragment),Fp=c(),ge=l("p"),yd=t("Use "),an=l("a"),kd=t("map()"),Ed=t(" to apply the function over the whole dataset:"),Ip=c(),f(za.$$.fragment),Rp=c(),O=l("p"),qd=t("For each original sentence, RoBERTA augmented a random word with three alternatives. The original word "),or=l("code"),Ad=t("distorting"),Dd=t(" is supplemented by "),pr=l("code"),Td=t("withholding"),Pd=t(", "),ir=l("code"),Sd=t("suppressing"),Cd=t(", and "),cr=l("code"),Nd=t("destroying"),Od=t("."),Lp=c(),Ps=l("h3"),_e=l("a"),hr=l("span"),f(Fa.$$.fragment),zd=c(),ur=l("span"),Fd=t("Process multiple splits"),Mp=c(),ts=l("p"),Id=t("Many datasets have splits that can be processed simultaneously with "),tn=l("a"),Rd=t("DatasetDict.map()"),Ld=t(". For example, tokenize the "),fr=l("code"),Md=t("sentence1"),Ud=t(" field in the train and test split by:"),Up=c(),f(Ia.$$.fragment),Vp=c(),Ss=l("h3"),je=l("a"),dr=l("span"),f(Ra.$$.fragment),Vd=c(),mr=l("span"),Bd=t("Distributed usage"),Bp=c(),ns=l("p"),Hd=t("When you use "),nn=l("a"),Yd=t("map()"),Gd=t(" in a distributed setting, you should also use "),La=l("a"),Wd=t("torch.distributed.barrier"),Jd=t(". This ensures the main process performs the mapping, while the other processes load the results, thereby avoiding duplicate work."),Hp=c(),ve=l("p"),Kd=t("The following example shows how you can use "),gr=l("code"),Qd=t("torch.distributed.barrier"),Xd=t(" to synchronize the processes:"),Yp=c(),f(Ma.$$.fragment),Gp=c(),Cs=l("h2"),be=l("a"),_r=l("span"),f(Ua.$$.fragment),Zd=c(),jr=l("span"),sm=t("Concatenate"),Wp=c(),we=l("p"),em=t("Separate datasets can be concatenated if they share the same column types. Concatenate datasets with "),ln=l("a"),am=t("concatenate_datasets()"),tm=t(":"),Jp=c(),f(Va.$$.fragment),Kp=c(),$e=l("p"),nm=t("You can also concatenate two datasets horizontally by setting "),vr=l("code"),lm=t("axis=1"),rm=t(" as long as the datasets have the same number of rows:"),Qp=c(),f(Ba.$$.fragment),Xp=c(),Ns=l("h3"),xe=l("a"),br=l("span"),f(Ha.$$.fragment),om=c(),wr=l("span"),pm=t("Interleave"),Zp=c(),A=l("p"),im=t("You can also mix several datasets together by taking alternating examples from each one to create a new dataset. This is known as "),$r=l("em"),cm=t("interleaving"),hm=t(", which is enabled by the "),rn=l("a"),um=t("interleave_datasets()"),fm=t(" function. Both "),on=l("a"),dm=t("interleave_datasets()"),mm=t(" and "),pn=l("a"),gm=t("concatenate_datasets()"),_m=t(" work with regular "),cn=l("a"),jm=t("Dataset"),vm=t(" and "),hn=l("a"),bm=t("IterableDataset"),wm=t(` objects.
Refer to the `),un=l("a"),$m=t("Stream"),xm=t(" guide for an example of how to interleave "),fn=l("a"),ym=t("IterableDataset"),km=t(" objects."),si=c(),dn=l("p"),Em=t(`You can define sampling probabilities for each of the original datasets to specify how to interleave the datasets.
In this case, the new dataset is constructed by getting examples one by one from a random dataset until one of the datasets runs out of samples.`),ei=c(),f(Ya.$$.fragment),ai=c(),z=l("p"),qm=t("You can also specify the "),xr=l("code"),Am=t("stopping_strategy"),Dm=t(". The default strategy, "),yr=l("code"),Tm=t("first_exhausted"),Pm=t(`, is a subsampling strategy, i.e the dataset construction is stopped as soon one of the dataset runs out of samples.
You can specify `),kr=l("code"),Sm=t("stopping_strategy=all_exhausted"),Cm=t(` to execute an oversampling strategy. In this case, the dataset construction is stopped as soon as every samples in every dataset has been added at least once. In practice, it means that if a dataset is exhausted, it will return to the beginning of this dataset until the stop criterion has been reached.
Note that if no sampling probabilities are specified, the new dataset will have `),Er=l("code"),Nm=t("max_length_datasets*nb_dataset samples"),Om=t("."),ti=c(),f(Ga.$$.fragment),ni=c(),Os=l("h2"),ye=l("a"),qr=l("span"),f(Wa.$$.fragment),zm=c(),Ar=l("span"),Fm=t("Format"),li=c(),ls=l("p"),Im=t("The "),mn=l("a"),Rm=t("set_format()"),Lm=t(" function changes the format of a column to be compatible with some common data formats. Specify the output you\u2019d like in the "),Dr=l("code"),Mm=t("type"),Um=t(" parameter and the columns you want to format. Formatting is applied on-the-fly."),ri=c(),ke=l("p"),Vm=t("For example, create PyTorch tensors by setting "),Tr=l("code"),Bm=t('type="torch"'),Hm=t(":"),oi=c(),f(Ja.$$.fragment),pi=c(),rs=l("p"),Ym=t("The "),gn=l("a"),Gm=t("with_format()"),Wm=t(" function also changes the format of a column, except it returns a new "),_n=l("a"),Jm=t("Dataset"),Km=t(" object:"),ii=c(),f(Ka.$$.fragment),ci=c(),f(Ee.$$.fragment),hi=c(),qe=l("p"),Qm=t("If you need to reset the dataset to its original format, use the "),jn=l("a"),Xm=t("reset_format()"),Zm=t(" function:"),ui=c(),f(Qa.$$.fragment),fi=c(),zs=l("h3"),Ae=l("a"),Pr=l("span"),f(Xa.$$.fragment),sg=c(),Sr=l("span"),eg=t("Format transform"),di=c(),De=l("p"),ag=t("The "),vn=l("a"),tg=t("set_transform()"),ng=t(" function applies a custom formatting transform on-the-fly. This function replaces any previously specified format. For example, you can use this function to tokenize and pad tokens on-the-fly. Tokenization is only applied when examples are accessed:"),mi=c(),f(Za.$$.fragment),gi=c(),Fs=l("h2"),Te=l("a"),Cr=l("span"),f(st.$$.fragment),lg=c(),Nr=l("span"),rg=t("Save"),_i=c(),Pe=l("p"),og=t("Once you are done processing your dataset, you can save and reuse it later with "),bn=l("a"),pg=t("save_to_disk()"),ig=t("."),ji=c(),wn=l("p"),cg=t("Save your dataset by providing the path to the directory you wish to save it to:"),vi=c(),f(et.$$.fragment),bi=c(),Se=l("p"),hg=t("Use the "),$n=l("a"),ug=t("load_from_disk()"),fg=t(" function to reload the dataset:"),wi=c(),f(at.$$.fragment),$i=c(),f(Ce.$$.fragment),xi=c(),Is=l("h2"),Ne=l("a"),Or=l("span"),f(tt.$$.fragment),dg=c(),zr=l("span"),mg=t("Export"),yi=c(),xn=l("p"),gg=t("\u{1F917} Datasets supports exporting as well so you can work with your dataset in other applications. The following table shows currently supported file formats you can export to:"),ki=c(),Oe=l("table"),Fr=l("thead"),nt=l("tr"),Ir=l("th"),_g=t("File type"),jg=c(),Rr=l("th"),vg=t("Export method"),bg=c(),I=l("tbody"),lt=l("tr"),Lr=l("td"),wg=t("CSV"),$g=c(),Mr=l("td"),yn=l("a"),xg=t("Dataset.to_csv()"),yg=c(),rt=l("tr"),Ur=l("td"),kg=t("JSON"),Eg=c(),Vr=l("td"),kn=l("a"),qg=t("Dataset.to_json()"),Ag=c(),ot=l("tr"),Br=l("td"),Dg=t("Parquet"),Tg=c(),Hr=l("td"),En=l("a"),Pg=t("Dataset.to_parquet()"),Sg=c(),pt=l("tr"),Yr=l("td"),Cg=t("SQL"),Ng=c(),Gr=l("td"),qn=l("a"),Og=t("Dataset.to_sql()"),zg=c(),it=l("tr"),Wr=l("td"),Fg=t("In-memory Python object"),Ig=c(),ct=l("td"),An=l("a"),Rg=t("Dataset.to_pandas()"),Lg=t(" or "),Dn=l("a"),Mg=t("Dataset.to_dict()"),Ei=c(),Tn=l("p"),Ug=t("For example, export your dataset to a CSV file like this:"),qi=c(),f(ht.$$.fragment),this.h()},l(s){const p=Uv('[data-svelte="svelte-1phssyn"]',document.head);v=r(p,"META",{name:!0,content:!0}),p.forEach(a),q=h(s),b=r(s,"H1",{class:!0});var ut=o(b);y=r(ut,"A",{id:!0,class:!0,href:!0});var Jr=o(y);k=r(Jr,"SPAN",{});var Kr=o(k);d($.$$.fragment,Kr),Kr.forEach(a),Jr.forEach(a),x=h(ut),D=r(ut,"SPAN",{});var Qr=o(D);us=n(Qr,"Process"),Qr.forEach(a),ut.forEach(a),F=h(s),fs=r(s,"P",{});var Xr=o(fs);ft=n(Xr,"\u{1F917} Datasets provides many tools for modifying the structure and content of a dataset. These tools are important for tidying up a dataset, creating additional columns, converting between features and formats, and much more."),Xr.forEach(a),eo=h(s),dt=r(s,"P",{});var Hg=o(dt);Dc=n(Hg,"This guide will show you how to:"),Hg.forEach(a),ao=h(s),P=r(s,"UL",{});var B=o(P);Yn=r(B,"LI",{});var Yg=o(Yn);Tc=n(Yg,"Reorder rows and split the dataset."),Yg.forEach(a),Pc=h(B),Gn=r(B,"LI",{});var Gg=o(Gn);Sc=n(Gg,"Rename and remove columns, and other common column operations."),Gg.forEach(a),Cc=h(B),Wn=r(B,"LI",{});var Wg=o(Wn);Nc=n(Wg,"Apply processing functions to each example in a dataset."),Wg.forEach(a),Oc=h(B),Jn=r(B,"LI",{});var Jg=o(Jn);zc=n(Jg,"Concatenate datasets."),Jg.forEach(a),Fc=h(B),Kn=r(B,"LI",{});var Kg=o(Kn);Ic=n(Kg,"Apply a custom formatting transform."),Kg.forEach(a),Rc=h(B),Qn=r(B,"LI",{});var Qg=o(Qn);Lc=n(Qg,"Save and export processed datasets."),Qg.forEach(a),B.forEach(a),to=h(s),L=r(s,"P",{});var ze=o(L);Mc=n(ze,"For more details specific to processing other dataset modalities, take a look at the "),Le=r(ze,"A",{class:!0,href:!0});var Xg=o(Le);Uc=n(Xg,"process audio dataset guide"),Xg.forEach(a),Vc=n(ze,", the "),Me=r(ze,"A",{class:!0,href:!0});var Zg=o(Me);Bc=n(Zg,"process image dataset guide"),Zg.forEach(a),Hc=n(ze,", or the "),Ue=r(ze,"A",{class:!0,href:!0});var s_=o(Ue);Yc=n(s_,"process text dataset guide"),s_.forEach(a),Gc=n(ze,"."),ze.forEach(a),no=h(s),mt=r(s,"P",{});var e_=o(mt);Wc=n(e_,"The examples in this guide use the MRPC dataset, but feel free to load any dataset of your choice and follow along!"),e_.forEach(a),lo=h(s),d(Ve.$$.fragment,s),ro=h(s),d(Rs.$$.fragment,s),oo=h(s),ds=r(s,"H2",{class:!0});var Di=o(ds);Ls=r(Di,"A",{id:!0,class:!0,href:!0});var a_=o(Ls);Xn=r(a_,"SPAN",{});var t_=o(Xn);d(Be.$$.fragment,t_),t_.forEach(a),a_.forEach(a),Jc=h(Di),Zn=r(Di,"SPAN",{});var n_=o(Zn);Kc=n(n_,"Sort, shuffle, select, split, and shard"),n_.forEach(a),Di.forEach(a),po=h(s),gt=r(s,"P",{});var l_=o(gt);Qc=n(l_,"There are several functions for rearranging the structure of a dataset. These functions are useful for selecting only the rows you want, creating train and test splits, and sharding very large datasets into smaller chunks."),l_.forEach(a),io=h(s),ms=r(s,"H3",{class:!0});var Ti=o(ms);Ms=r(Ti,"A",{id:!0,class:!0,href:!0});var r_=o(Ms);sl=r(r_,"SPAN",{});var o_=o(sl);d(He.$$.fragment,o_),o_.forEach(a),r_.forEach(a),Xc=h(Ti),el=r(Ti,"SPAN",{});var p_=o(el);Zc=n(p_,"Sort"),p_.forEach(a),Ti.forEach(a),co=h(s),Us=r(s,"P",{});var Pi=o(Us);sh=n(Pi,"Use "),_t=r(Pi,"A",{href:!0});var i_=o(_t);eh=n(i_,"sort()"),i_.forEach(a),ah=n(Pi," to sort column values according to their numerical values. The provided column must be NumPy compatible."),Pi.forEach(a),ho=h(s),d(Ye.$$.fragment,s),uo=h(s),gs=r(s,"H3",{class:!0});var Si=o(gs);Vs=r(Si,"A",{id:!0,class:!0,href:!0});var c_=o(Vs);al=r(c_,"SPAN",{});var h_=o(al);d(Ge.$$.fragment,h_),h_.forEach(a),c_.forEach(a),th=h(Si),tl=r(Si,"SPAN",{});var u_=o(tl);nh=n(u_,"Shuffle"),u_.forEach(a),Si.forEach(a),fo=h(s),M=r(s,"P",{});var Fe=o(M);lh=n(Fe,"The "),jt=r(Fe,"A",{href:!0});var f_=o(jt);rh=n(f_,"shuffle()"),f_.forEach(a),oh=n(Fe," function randomly rearranges the column values. You can specify the "),nl=r(Fe,"CODE",{});var d_=o(nl);ph=n(d_,"generator"),d_.forEach(a),ih=n(Fe," parameter in this function to use a different "),ll=r(Fe,"CODE",{});var m_=o(ll);ch=n(m_,"numpy.random.Generator"),m_.forEach(a),hh=n(Fe," if you want more control over the algorithm used to shuffle the dataset."),Fe.forEach(a),mo=h(s),d(We.$$.fragment,s),go=h(s),_s=r(s,"H3",{class:!0});var Ci=o(_s);Bs=r(Ci,"A",{id:!0,class:!0,href:!0});var g_=o(Bs);rl=r(g_,"SPAN",{});var __=o(rl);d(Je.$$.fragment,__),__.forEach(a),g_.forEach(a),uh=h(Ci),ol=r(Ci,"SPAN",{});var j_=o(ol);fh=n(j_,"Select and Filter"),j_.forEach(a),Ci.forEach(a),_o=h(s),Y=r(s,"P",{});var Pn=o(Y);dh=n(Pn,"There are two options for filtering rows in a dataset: "),vt=r(Pn,"A",{href:!0});var v_=o(vt);mh=n(v_,"select()"),v_.forEach(a),gh=n(Pn," and "),bt=r(Pn,"A",{href:!0});var b_=o(bt);_h=n(b_,"filter()"),b_.forEach(a),jh=n(Pn,"."),Pn.forEach(a),jo=h(s),wt=r(s,"UL",{});var w_=o(wt);$t=r(w_,"LI",{});var Vg=o($t);xt=r(Vg,"A",{href:!0});var $_=o(xt);vh=n($_,"select()"),$_.forEach(a),bh=n(Vg," returns rows according to a list of indices:"),Vg.forEach(a),w_.forEach(a),vo=h(s),d(Ke.$$.fragment,s),bo=h(s),yt=r(s,"UL",{});var x_=o(yt);kt=r(x_,"LI",{});var Bg=o(kt);Et=r(Bg,"A",{href:!0});var y_=o(Et);wh=n(y_,"filter()"),y_.forEach(a),$h=n(Bg," returns rows that match a specified condition:"),Bg.forEach(a),x_.forEach(a),wo=h(s),d(Qe.$$.fragment,s),$o=h(s),js=r(s,"P",{});var Zr=o(js);qt=r(Zr,"A",{href:!0});var k_=o(qt);xh=n(k_,"filter()"),k_.forEach(a),yh=n(Zr," can also filter by indices if you set "),pl=r(Zr,"CODE",{});var E_=o(pl);kh=n(E_,"with_indices=True"),E_.forEach(a),Eh=n(Zr,":"),Zr.forEach(a),xo=h(s),d(Xe.$$.fragment,s),yo=h(s),vs=r(s,"H3",{class:!0});var Ni=o(vs);Hs=r(Ni,"A",{id:!0,class:!0,href:!0});var q_=o(Hs);il=r(q_,"SPAN",{});var A_=o(il);d(Ze.$$.fragment,A_),A_.forEach(a),q_.forEach(a),qh=h(Ni),cl=r(Ni,"SPAN",{});var D_=o(cl);Ah=n(D_,"Split"),D_.forEach(a),Ni.forEach(a),ko=h(s),G=r(s,"P",{});var Sn=o(G);Dh=n(Sn,"The "),At=r(Sn,"A",{href:!0});var T_=o(At);Th=n(T_,"train_test_split()"),T_.forEach(a),Ph=n(Sn," function creates train and test splits if your dataset doesn\u2019t already have them. This allows you to adjust the relative proportions or an absolute number of samples in each split. In the example below, use the "),hl=r(Sn,"CODE",{});var P_=o(hl);Sh=n(P_,"test_size"),P_.forEach(a),Ch=n(Sn," parameter to create a test split that is 10% of the original dataset:"),Sn.forEach(a),Eo=h(s),d(sa.$$.fragment,s),qo=h(s),Ys=r(s,"P",{});var Oi=o(Ys);Nh=n(Oi,"The splits are shuffled by default, but you can set "),ul=r(Oi,"CODE",{});var S_=o(ul);Oh=n(S_,"shuffle=False"),S_.forEach(a),zh=n(Oi," to prevent shuffling."),Oi.forEach(a),Ao=h(s),bs=r(s,"H3",{class:!0});var zi=o(bs);Gs=r(zi,"A",{id:!0,class:!0,href:!0});var C_=o(Gs);fl=r(C_,"SPAN",{});var N_=o(fl);d(ea.$$.fragment,N_),N_.forEach(a),C_.forEach(a),Fh=h(zi),dl=r(zi,"SPAN",{});var O_=o(dl);Ih=n(O_,"Shard"),O_.forEach(a),zi.forEach(a),Do=h(s),U=r(s,"P",{});var Ie=o(U);Rh=n(Ie,"\u{1F917} Datasets supports sharding to divide a very large dataset into a predefined number of chunks. Specify the "),ml=r(Ie,"CODE",{});var z_=o(ml);Lh=n(z_,"num_shards"),z_.forEach(a),Mh=n(Ie," parameter in "),Dt=r(Ie,"A",{href:!0});var F_=o(Dt);Uh=n(F_,"shard()"),F_.forEach(a),Vh=n(Ie," to determine the number of shards to split the dataset into. You\u2019ll also need to provide the shard you want to return with the "),gl=r(Ie,"CODE",{});var I_=o(gl);Bh=n(I_,"index"),I_.forEach(a),Hh=n(Ie," parameter."),Ie.forEach(a),To=h(s),Ws=r(s,"P",{});var Fi=o(Ws);Yh=n(Fi,"For example, the "),aa=r(Fi,"A",{href:!0,rel:!0});var R_=o(aa);Gh=n(R_,"imdb"),R_.forEach(a),Wh=n(Fi," dataset has 25000 examples:"),Fi.forEach(a),Po=h(s),d(ta.$$.fragment,s),So=h(s),Tt=r(s,"P",{});var L_=o(Tt);Jh=n(L_,"After sharding the dataset into four chunks, the first shard will only have 6250 examples:"),L_.forEach(a),Co=h(s),d(na.$$.fragment,s),No=h(s),ws=r(s,"H2",{class:!0});var Ii=o(ws);Js=r(Ii,"A",{id:!0,class:!0,href:!0});var M_=o(Js);_l=r(M_,"SPAN",{});var U_=o(_l);d(la.$$.fragment,U_),U_.forEach(a),M_.forEach(a),Kh=h(Ii),jl=r(Ii,"SPAN",{});var V_=o(jl);Qh=n(V_,"Rename, remove, cast, and flatten"),V_.forEach(a),Ii.forEach(a),Oo=h(s),Pt=r(s,"P",{});var B_=o(Pt);Xh=n(B_,"The following functions allow you to modify the columns of a dataset. These functions are useful for renaming or removing columns, changing columns to a new set of features, and flattening nested column structures."),B_.forEach(a),zo=h(s),$s=r(s,"H3",{class:!0});var Ri=o($s);Ks=r(Ri,"A",{id:!0,class:!0,href:!0});var H_=o(Ks);vl=r(H_,"SPAN",{});var Y_=o(vl);d(ra.$$.fragment,Y_),Y_.forEach(a),H_.forEach(a),Zh=h(Ri),bl=r(Ri,"SPAN",{});var G_=o(bl);su=n(G_,"Rename"),G_.forEach(a),Ri.forEach(a),Fo=h(s),Qs=r(s,"P",{});var Li=o(Qs);eu=n(Li,"Use "),St=r(Li,"A",{href:!0});var W_=o(St);au=n(W_,"rename_column()"),W_.forEach(a),tu=n(Li," when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place."),Li.forEach(a),Io=h(s),Xs=r(s,"P",{});var Mi=o(Xs);nu=n(Mi,"Provide "),Ct=r(Mi,"A",{href:!0});var J_=o(Ct);lu=n(J_,"rename_column()"),J_.forEach(a),ru=n(Mi," with the name of the original column, and the new column name:"),Mi.forEach(a),Ro=h(s),d(oa.$$.fragment,s),Lo=h(s),xs=r(s,"H3",{class:!0});var Ui=o(xs);Zs=r(Ui,"A",{id:!0,class:!0,href:!0});var K_=o(Zs);wl=r(K_,"SPAN",{});var Q_=o(wl);d(pa.$$.fragment,Q_),Q_.forEach(a),K_.forEach(a),ou=h(Ui),$l=r(Ui,"SPAN",{});var X_=o($l);pu=n(X_,"Remove"),X_.forEach(a),Ui.forEach(a),Mo=h(s),se=r(s,"P",{});var Vi=o(se);iu=n(Vi,"When you need to remove one or more columns, provide the column name to remove to the "),Nt=r(Vi,"A",{href:!0});var Z_=o(Nt);cu=n(Z_,"remove_columns()"),Z_.forEach(a),hu=n(Vi," function. Remove more than one column by providing a list of column names:"),Vi.forEach(a),Uo=h(s),d(ia.$$.fragment,s),Vo=h(s),ys=r(s,"H3",{class:!0});var Bi=o(ys);ee=r(Bi,"A",{id:!0,class:!0,href:!0});var s2=o(ee);xl=r(s2,"SPAN",{});var e2=o(xl);d(ca.$$.fragment,e2),e2.forEach(a),s2.forEach(a),uu=h(Bi),yl=r(Bi,"SPAN",{});var a2=o(yl);fu=n(a2,"Cast"),a2.forEach(a),Bi.forEach(a),Bo=h(s),C=r(s,"P",{});var os=o(C);du=n(os,"The "),Ot=r(os,"A",{href:!0});var t2=o(Ot);mu=n(t2,"cast()"),t2.forEach(a),gu=n(os," function transforms the feature type of one or more columns. This function accepts your new "),zt=r(os,"A",{href:!0});var n2=o(zt);_u=n(n2,"Features"),n2.forEach(a),ju=n(os," as its argument. The example below demonstrates how to change the "),Ft=r(os,"A",{href:!0});var l2=o(Ft);vu=n(l2,"ClassLabel"),l2.forEach(a),bu=n(os," and "),It=r(os,"A",{href:!0});var r2=o(It);wu=n(r2,"Value"),r2.forEach(a),$u=n(os," features:"),os.forEach(a),Ho=h(s),d(ha.$$.fragment,s),Yo=h(s),d(ae.$$.fragment,s),Go=h(s),te=r(s,"P",{});var Hi=o(te);xu=n(Hi,"Use the "),Rt=r(Hi,"A",{href:!0});var o2=o(Rt);yu=n(o2,"cast_column()"),o2.forEach(a),ku=n(Hi," function to change the feature type of a single column. Pass the column name and its new feature type as arguments:"),Hi.forEach(a),Wo=h(s),d(ua.$$.fragment,s),Jo=h(s),ks=r(s,"H3",{class:!0});var Yi=o(ks);ne=r(Yi,"A",{id:!0,class:!0,href:!0});var p2=o(ne);kl=r(p2,"SPAN",{});var i2=o(kl);d(fa.$$.fragment,i2),i2.forEach(a),p2.forEach(a),Eu=h(Yi),El=r(Yi,"SPAN",{});var c2=o(El);qu=n(c2,"Flatten"),c2.forEach(a),Yi.forEach(a),Ko=h(s),Lt=r(s,"P",{});var h2=o(Lt);Au=n(h2,"Sometimes a column can be a nested structure of several types. Take a look at the nested structure below from the SQuAD dataset:"),h2.forEach(a),Qo=h(s),d(da.$$.fragment,s),Xo=h(s),N=r(s,"P",{});var ps=o(N);Du=n(ps,"The "),ql=r(ps,"CODE",{});var u2=o(ql);Tu=n(u2,"answers"),u2.forEach(a),Pu=n(ps," field contains two subfields: "),Al=r(ps,"CODE",{});var f2=o(Al);Su=n(f2,"text"),f2.forEach(a),Cu=n(ps," and "),Dl=r(ps,"CODE",{});var d2=o(Dl);Nu=n(d2,"answer_start"),d2.forEach(a),Ou=n(ps,". Use the "),Mt=r(ps,"A",{href:!0});var m2=o(Mt);zu=n(m2,"flatten()"),m2.forEach(a),Fu=n(ps," function to extract the subfields into their own separate columns:"),ps.forEach(a),Zo=h(s),d(ma.$$.fragment,s),sp=h(s),W=r(s,"P",{});var Cn=o(W);Iu=n(Cn,"Notice how the subfields are now their own independent columns: "),Tl=r(Cn,"CODE",{});var g2=o(Tl);Ru=n(g2,"answers.text"),g2.forEach(a),Lu=n(Cn," and "),Pl=r(Cn,"CODE",{});var _2=o(Pl);Mu=n(_2,"answers.answer_start"),_2.forEach(a),Uu=n(Cn,"."),Cn.forEach(a),ep=h(s),Es=r(s,"H2",{class:!0});var Gi=o(Es);le=r(Gi,"A",{id:!0,class:!0,href:!0});var j2=o(le);Sl=r(j2,"SPAN",{});var v2=o(Sl);d(ga.$$.fragment,v2),v2.forEach(a),j2.forEach(a),Vu=h(Gi),Cl=r(Gi,"SPAN",{});var b2=o(Cl);Bu=n(b2,"Map"),b2.forEach(a),Gi.forEach(a),ap=h(s),J=r(s,"P",{});var Nn=o(J);Hu=n(Nn,"Some of the more powerful applications of \u{1F917} Datasets come from using the "),Ut=r(Nn,"A",{href:!0});var w2=o(Ut);Yu=n(w2,"map()"),w2.forEach(a),Gu=n(Nn," function. The primary purpose of "),Vt=r(Nn,"A",{href:!0});var $2=o(Vt);Wu=n($2,"map()"),$2.forEach(a),Ju=n(Nn," is to speed up processing functions. It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),Nn.forEach(a),tp=h(s),K=r(s,"P",{});var On=o(K);Ku=n(On,"In the following example, prefix each "),Nl=r(On,"CODE",{});var x2=o(Nl);Qu=n(x2,"sentence1"),x2.forEach(a),Xu=n(On," value in the dataset with "),Ol=r(On,"CODE",{});var y2=o(Ol);Zu=n(y2,"'My sentence: '"),y2.forEach(a),sf=n(On,"."),On.forEach(a),np=h(s),Q=r(s,"P",{});var zn=o(Q);ef=n(zn,"Start by creating a function that adds "),zl=r(zn,"CODE",{});var k2=o(zl);af=n(k2,"'My sentence: '"),k2.forEach(a),tf=n(zn," to the beginning of each sentence. The function needs to accept and output a "),Fl=r(zn,"CODE",{});var E2=o(Fl);nf=n(E2,"dict"),E2.forEach(a),lf=n(zn,":"),zn.forEach(a),lp=h(s),d(_a.$$.fragment,s),rp=h(s),X=r(s,"P",{});var Fn=o(X);rf=n(Fn,"Now use "),Bt=r(Fn,"A",{href:!0});var q2=o(Bt);of=n(q2,"map()"),q2.forEach(a),pf=n(Fn," to apply the "),Il=r(Fn,"CODE",{});var A2=o(Il);cf=n(A2,"add_prefix"),A2.forEach(a),hf=n(Fn," function to the entire dataset:"),Fn.forEach(a),op=h(s),d(ja.$$.fragment,s),pp=h(s),re=r(s,"P",{});var Wi=o(re);uf=n(Wi,"Let\u2019s take a look at another example, except this time, you\u2019ll remove a column with "),Ht=r(Wi,"A",{href:!0});var D2=o(Ht);ff=n(D2,"map()"),D2.forEach(a),df=n(Wi,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),Wi.forEach(a),ip=h(s),Z=r(s,"P",{});var In=o(Z);mf=n(In,"Specify the column to remove with the "),Rl=r(In,"CODE",{});var T2=o(Rl);gf=n(T2,"remove_columns"),T2.forEach(a),_f=n(In," parameter in "),Yt=r(In,"A",{href:!0});var P2=o(Yt);jf=n(P2,"map()"),P2.forEach(a),vf=n(In,":"),In.forEach(a),cp=h(s),d(va.$$.fragment,s),hp=h(s),d(oe.$$.fragment,s),up=h(s),ss=r(s,"P",{});var Rn=o(ss);bf=n(Rn,"You can also use "),Gt=r(Rn,"A",{href:!0});var S2=o(Gt);wf=n(S2,"map()"),S2.forEach(a),$f=n(Rn," with indices if you set "),Ll=r(Rn,"CODE",{});var C2=o(Ll);xf=n(C2,"with_indices=True"),C2.forEach(a),yf=n(Rn,". The example below adds the index to the beginning of each sentence:"),Rn.forEach(a),fp=h(s),d(ba.$$.fragment,s),dp=h(s),S=r(s,"P",{});var H=o(S);kf=n(H,"The "),Wt=r(H,"A",{href:!0});var N2=o(Wt);Ef=n(N2,"map()"),N2.forEach(a),qf=n(H," also works with the rank of the process if you set "),Ml=r(H,"CODE",{});var O2=o(Ml);Af=n(O2,"with_rank=True"),O2.forEach(a),Df=n(H,". This is analogous to the "),Ul=r(H,"CODE",{});var z2=o(Ul);Tf=n(z2,"with_indices"),z2.forEach(a),Pf=n(H," parameter. The "),Vl=r(H,"CODE",{});var F2=o(Vl);Sf=n(F2,"with_rank"),F2.forEach(a),Cf=n(H," parameter in the mapped function goes after the "),Bl=r(H,"CODE",{});var I2=o(Bl);Nf=n(I2,"index"),I2.forEach(a),Of=n(H," one if it is already present."),H.forEach(a),mp=h(s),d(wa.$$.fragment,s),gp=h(s),pe=r(s,"P",{});var Ji=o(pe);zf=n(Ji,"The main use-case for rank is to parallelize computation across several GPUs. This requires setting "),Hl=r(Ji,"CODE",{});var R2=o(Hl);Ff=n(R2,'multiprocess.set_start_method("spawn")'),R2.forEach(a),If=n(Ji,". If you don\u2019t you\u2019ll receive the following CUDA error:"),Ji.forEach(a),_p=h(s),d($a.$$.fragment,s),jp=h(s),qs=r(s,"H3",{class:!0});var Ki=o(qs);ie=r(Ki,"A",{id:!0,class:!0,href:!0});var L2=o(ie);Yl=r(L2,"SPAN",{});var M2=o(Yl);d(xa.$$.fragment,M2),M2.forEach(a),L2.forEach(a),Rf=h(Ki),Gl=r(Ki,"SPAN",{});var U2=o(Gl);Lf=n(U2,"Multiprocessing"),U2.forEach(a),Ki.forEach(a),vp=h(s),es=r(s,"P",{});var Ln=o(es);Mf=n(Ln,"Multiprocessing significantly speeds up processing by parallelizing processes on the CPU. Set the "),Wl=r(Ln,"CODE",{});var V2=o(Wl);Uf=n(V2,"num_proc"),V2.forEach(a),Vf=n(Ln," parameter in "),Jt=r(Ln,"A",{href:!0});var B2=o(Jt);Bf=n(B2,"map()"),B2.forEach(a),Hf=n(Ln," to set the number of processes to use:"),Ln.forEach(a),bp=h(s),d(ya.$$.fragment,s),wp=h(s),As=r(s,"H3",{class:!0});var Qi=o(As);ce=r(Qi,"A",{id:!0,class:!0,href:!0});var H2=o(ce);Jl=r(H2,"SPAN",{});var Y2=o(Jl);d(ka.$$.fragment,Y2),Y2.forEach(a),H2.forEach(a),Yf=h(Qi),Kl=r(Qi,"SPAN",{});var G2=o(Kl);Gf=n(G2,"Batch processing"),G2.forEach(a),Qi.forEach(a),$p=h(s),V=r(s,"P",{});var Re=o(V);Wf=n(Re,"The "),Kt=r(Re,"A",{href:!0});var W2=o(Kt);Jf=n(W2,"map()"),W2.forEach(a),Kf=n(Re," function supports working with batches of examples. Operate on batches by setting "),Ql=r(Re,"CODE",{});var J2=o(Ql);Qf=n(J2,"batched=True"),J2.forEach(a),Xf=n(Re,". The default batch size is 1000, but you can adjust it with the "),Xl=r(Re,"CODE",{});var K2=o(Xl);Zf=n(K2,"batch_size"),K2.forEach(a),sd=n(Re," parameter. Batch processing enables interesting applications such as splitting long sentences into shorter chunks and data augmentation."),Re.forEach(a),xp=h(s),Ds=r(s,"H4",{class:!0});var Xi=o(Ds);he=r(Xi,"A",{id:!0,class:!0,href:!0});var Q2=o(he);Zl=r(Q2,"SPAN",{});var X2=o(Zl);d(Ea.$$.fragment,X2),X2.forEach(a),Q2.forEach(a),ed=h(Xi),sr=r(Xi,"SPAN",{});var Z2=o(sr);ad=n(Z2,"Split long examples"),Z2.forEach(a),Xi.forEach(a),yp=h(s),Qt=r(s,"P",{});var sj=o(Qt);td=n(sj,"When examples are too long, you may want to split them into several smaller chunks. Begin by creating a function that:"),sj.forEach(a),kp=h(s),ue=r(s,"OL",{});var Zi=o(ue);er=r(Zi,"LI",{});var ej=o(er);qa=r(ej,"P",{});var sc=o(qa);nd=n(sc,"Splits the "),ar=r(sc,"CODE",{});var aj=o(ar);ld=n(aj,"sentence1"),aj.forEach(a),rd=n(sc," field into chunks of 50 characters."),sc.forEach(a),ej.forEach(a),od=h(Zi),tr=r(Zi,"LI",{});var tj=o(tr);nr=r(tj,"P",{});var nj=o(nr);pd=n(nj,"Stacks all the chunks together to create the new dataset."),nj.forEach(a),tj.forEach(a),Zi.forEach(a),Ep=h(s),d(Aa.$$.fragment,s),qp=h(s),fe=r(s,"P",{});var ec=o(fe);id=n(ec,"Apply the function with "),Xt=r(ec,"A",{href:!0});var lj=o(Xt);cd=n(lj,"map()"),lj.forEach(a),hd=n(ec,":"),ec.forEach(a),Ap=h(s),d(Da.$$.fragment,s),Dp=h(s),Zt=r(s,"P",{});var rj=o(Zt);ud=n(rj,"Notice how the sentences are split into shorter chunks now, and there are more rows in the dataset."),rj.forEach(a),Tp=h(s),d(Ta.$$.fragment,s),Pp=h(s),Ts=r(s,"H4",{class:!0});var ac=o(Ts);de=r(ac,"A",{id:!0,class:!0,href:!0});var oj=o(de);lr=r(oj,"SPAN",{});var pj=o(lr);d(Pa.$$.fragment,pj),pj.forEach(a),oj.forEach(a),fd=h(ac),rr=r(ac,"SPAN",{});var ij=o(rr);dd=n(ij,"Data augmentation"),ij.forEach(a),ac.forEach(a),Sp=h(s),me=r(s,"P",{});var tc=o(me);md=n(tc,"The "),sn=r(tc,"A",{href:!0});var cj=o(sn);gd=n(cj,"map()"),cj.forEach(a),_d=n(tc," function could also be used for data augmentation. The following example generates additional words for a masked token in a sentence."),tc.forEach(a),Cp=h(s),as=r(s,"P",{});var Mn=o(as);jd=n(Mn,"Load and use the "),Sa=r(Mn,"A",{href:!0,rel:!0});var hj=o(Sa);vd=n(hj,"RoBERTA"),hj.forEach(a),bd=n(Mn," model in \u{1F917} Transformers\u2019 "),Ca=r(Mn,"A",{href:!0,rel:!0});var uj=o(Ca);wd=n(uj,"FillMaskPipeline"),uj.forEach(a),$d=n(Mn,":"),Mn.forEach(a),Np=h(s),d(Na.$$.fragment,s),Op=h(s),en=r(s,"P",{});var fj=o(en);xd=n(fj,"Create a function to randomly select a word to mask in the sentence. The function should also return the original sentence and the top two replacements generated by RoBERTA."),fj.forEach(a),zp=h(s),d(Oa.$$.fragment,s),Fp=h(s),ge=r(s,"P",{});var nc=o(ge);yd=n(nc,"Use "),an=r(nc,"A",{href:!0});var dj=o(an);kd=n(dj,"map()"),dj.forEach(a),Ed=n(nc," to apply the function over the whole dataset:"),nc.forEach(a),Ip=h(s),d(za.$$.fragment,s),Rp=h(s),O=r(s,"P",{});var is=o(O);qd=n(is,"For each original sentence, RoBERTA augmented a random word with three alternatives. The original word "),or=r(is,"CODE",{});var mj=o(or);Ad=n(mj,"distorting"),mj.forEach(a),Dd=n(is," is supplemented by "),pr=r(is,"CODE",{});var gj=o(pr);Td=n(gj,"withholding"),gj.forEach(a),Pd=n(is,", "),ir=r(is,"CODE",{});var _j=o(ir);Sd=n(_j,"suppressing"),_j.forEach(a),Cd=n(is,", and "),cr=r(is,"CODE",{});var jj=o(cr);Nd=n(jj,"destroying"),jj.forEach(a),Od=n(is,"."),is.forEach(a),Lp=h(s),Ps=r(s,"H3",{class:!0});var lc=o(Ps);_e=r(lc,"A",{id:!0,class:!0,href:!0});var vj=o(_e);hr=r(vj,"SPAN",{});var bj=o(hr);d(Fa.$$.fragment,bj),bj.forEach(a),vj.forEach(a),zd=h(lc),ur=r(lc,"SPAN",{});var wj=o(ur);Fd=n(wj,"Process multiple splits"),wj.forEach(a),lc.forEach(a),Mp=h(s),ts=r(s,"P",{});var Un=o(ts);Id=n(Un,"Many datasets have splits that can be processed simultaneously with "),tn=r(Un,"A",{href:!0});var $j=o(tn);Rd=n($j,"DatasetDict.map()"),$j.forEach(a),Ld=n(Un,". For example, tokenize the "),fr=r(Un,"CODE",{});var xj=o(fr);Md=n(xj,"sentence1"),xj.forEach(a),Ud=n(Un," field in the train and test split by:"),Un.forEach(a),Up=h(s),d(Ia.$$.fragment,s),Vp=h(s),Ss=r(s,"H3",{class:!0});var rc=o(Ss);je=r(rc,"A",{id:!0,class:!0,href:!0});var yj=o(je);dr=r(yj,"SPAN",{});var kj=o(dr);d(Ra.$$.fragment,kj),kj.forEach(a),yj.forEach(a),Vd=h(rc),mr=r(rc,"SPAN",{});var Ej=o(mr);Bd=n(Ej,"Distributed usage"),Ej.forEach(a),rc.forEach(a),Bp=h(s),ns=r(s,"P",{});var Vn=o(ns);Hd=n(Vn,"When you use "),nn=r(Vn,"A",{href:!0});var qj=o(nn);Yd=n(qj,"map()"),qj.forEach(a),Gd=n(Vn," in a distributed setting, you should also use "),La=r(Vn,"A",{href:!0,rel:!0});var Aj=o(La);Wd=n(Aj,"torch.distributed.barrier"),Aj.forEach(a),Jd=n(Vn,". This ensures the main process performs the mapping, while the other processes load the results, thereby avoiding duplicate work."),Vn.forEach(a),Hp=h(s),ve=r(s,"P",{});var oc=o(ve);Kd=n(oc,"The following example shows how you can use "),gr=r(oc,"CODE",{});var Dj=o(gr);Qd=n(Dj,"torch.distributed.barrier"),Dj.forEach(a),Xd=n(oc," to synchronize the processes:"),oc.forEach(a),Yp=h(s),d(Ma.$$.fragment,s),Gp=h(s),Cs=r(s,"H2",{class:!0});var pc=o(Cs);be=r(pc,"A",{id:!0,class:!0,href:!0});var Tj=o(be);_r=r(Tj,"SPAN",{});var Pj=o(_r);d(Ua.$$.fragment,Pj),Pj.forEach(a),Tj.forEach(a),Zd=h(pc),jr=r(pc,"SPAN",{});var Sj=o(jr);sm=n(Sj,"Concatenate"),Sj.forEach(a),pc.forEach(a),Wp=h(s),we=r(s,"P",{});var ic=o(we);em=n(ic,"Separate datasets can be concatenated if they share the same column types. Concatenate datasets with "),ln=r(ic,"A",{href:!0});var Cj=o(ln);am=n(Cj,"concatenate_datasets()"),Cj.forEach(a),tm=n(ic,":"),ic.forEach(a),Jp=h(s),d(Va.$$.fragment,s),Kp=h(s),$e=r(s,"P",{});var cc=o($e);nm=n(cc,"You can also concatenate two datasets horizontally by setting "),vr=r(cc,"CODE",{});var Nj=o(vr);lm=n(Nj,"axis=1"),Nj.forEach(a),rm=n(cc," as long as the datasets have the same number of rows:"),cc.forEach(a),Qp=h(s),d(Ba.$$.fragment,s),Xp=h(s),Ns=r(s,"H3",{class:!0});var hc=o(Ns);xe=r(hc,"A",{id:!0,class:!0,href:!0});var Oj=o(xe);br=r(Oj,"SPAN",{});var zj=o(br);d(Ha.$$.fragment,zj),zj.forEach(a),Oj.forEach(a),om=h(hc),wr=r(hc,"SPAN",{});var Fj=o(wr);pm=n(Fj,"Interleave"),Fj.forEach(a),hc.forEach(a),Zp=h(s),A=r(s,"P",{});var T=o(A);im=n(T,"You can also mix several datasets together by taking alternating examples from each one to create a new dataset. This is known as "),$r=r(T,"EM",{});var Ij=o($r);cm=n(Ij,"interleaving"),Ij.forEach(a),hm=n(T,", which is enabled by the "),rn=r(T,"A",{href:!0});var Rj=o(rn);um=n(Rj,"interleave_datasets()"),Rj.forEach(a),fm=n(T," function. Both "),on=r(T,"A",{href:!0});var Lj=o(on);dm=n(Lj,"interleave_datasets()"),Lj.forEach(a),mm=n(T," and "),pn=r(T,"A",{href:!0});var Mj=o(pn);gm=n(Mj,"concatenate_datasets()"),Mj.forEach(a),_m=n(T," work with regular "),cn=r(T,"A",{href:!0});var Uj=o(cn);jm=n(Uj,"Dataset"),Uj.forEach(a),vm=n(T," and "),hn=r(T,"A",{href:!0});var Vj=o(hn);bm=n(Vj,"IterableDataset"),Vj.forEach(a),wm=n(T,` objects.
Refer to the `),un=r(T,"A",{href:!0});var Bj=o(un);$m=n(Bj,"Stream"),Bj.forEach(a),xm=n(T," guide for an example of how to interleave "),fn=r(T,"A",{href:!0});var Hj=o(fn);ym=n(Hj,"IterableDataset"),Hj.forEach(a),km=n(T," objects."),T.forEach(a),si=h(s),dn=r(s,"P",{});var Yj=o(dn);Em=n(Yj,`You can define sampling probabilities for each of the original datasets to specify how to interleave the datasets.
In this case, the new dataset is constructed by getting examples one by one from a random dataset until one of the datasets runs out of samples.`),Yj.forEach(a),ei=h(s),d(Ya.$$.fragment,s),ai=h(s),z=r(s,"P",{});var cs=o(z);qm=n(cs,"You can also specify the "),xr=r(cs,"CODE",{});var Gj=o(xr);Am=n(Gj,"stopping_strategy"),Gj.forEach(a),Dm=n(cs,". The default strategy, "),yr=r(cs,"CODE",{});var Wj=o(yr);Tm=n(Wj,"first_exhausted"),Wj.forEach(a),Pm=n(cs,`, is a subsampling strategy, i.e the dataset construction is stopped as soon one of the dataset runs out of samples.
You can specify `),kr=r(cs,"CODE",{});var Jj=o(kr);Sm=n(Jj,"stopping_strategy=all_exhausted"),Jj.forEach(a),Cm=n(cs,` to execute an oversampling strategy. In this case, the dataset construction is stopped as soon as every samples in every dataset has been added at least once. In practice, it means that if a dataset is exhausted, it will return to the beginning of this dataset until the stop criterion has been reached.
Note that if no sampling probabilities are specified, the new dataset will have `),Er=r(cs,"CODE",{});var Kj=o(Er);Nm=n(Kj,"max_length_datasets*nb_dataset samples"),Kj.forEach(a),Om=n(cs,"."),cs.forEach(a),ti=h(s),d(Ga.$$.fragment,s),ni=h(s),Os=r(s,"H2",{class:!0});var uc=o(Os);ye=r(uc,"A",{id:!0,class:!0,href:!0});var Qj=o(ye);qr=r(Qj,"SPAN",{});var Xj=o(qr);d(Wa.$$.fragment,Xj),Xj.forEach(a),Qj.forEach(a),zm=h(uc),Ar=r(uc,"SPAN",{});var Zj=o(Ar);Fm=n(Zj,"Format"),Zj.forEach(a),uc.forEach(a),li=h(s),ls=r(s,"P",{});var Bn=o(ls);Im=n(Bn,"The "),mn=r(Bn,"A",{href:!0});var sv=o(mn);Rm=n(sv,"set_format()"),sv.forEach(a),Lm=n(Bn," function changes the format of a column to be compatible with some common data formats. Specify the output you\u2019d like in the "),Dr=r(Bn,"CODE",{});var ev=o(Dr);Mm=n(ev,"type"),ev.forEach(a),Um=n(Bn," parameter and the columns you want to format. Formatting is applied on-the-fly."),Bn.forEach(a),ri=h(s),ke=r(s,"P",{});var fc=o(ke);Vm=n(fc,"For example, create PyTorch tensors by setting "),Tr=r(fc,"CODE",{});var av=o(Tr);Bm=n(av,'type="torch"'),av.forEach(a),Hm=n(fc,":"),fc.forEach(a),oi=h(s),d(Ja.$$.fragment,s),pi=h(s),rs=r(s,"P",{});var Hn=o(rs);Ym=n(Hn,"The "),gn=r(Hn,"A",{href:!0});var tv=o(gn);Gm=n(tv,"with_format()"),tv.forEach(a),Wm=n(Hn," function also changes the format of a column, except it returns a new "),_n=r(Hn,"A",{href:!0});var nv=o(_n);Jm=n(nv,"Dataset"),nv.forEach(a),Km=n(Hn," object:"),Hn.forEach(a),ii=h(s),d(Ka.$$.fragment,s),ci=h(s),d(Ee.$$.fragment,s),hi=h(s),qe=r(s,"P",{});var dc=o(qe);Qm=n(dc,"If you need to reset the dataset to its original format, use the "),jn=r(dc,"A",{href:!0});var lv=o(jn);Xm=n(lv,"reset_format()"),lv.forEach(a),Zm=n(dc," function:"),dc.forEach(a),ui=h(s),d(Qa.$$.fragment,s),fi=h(s),zs=r(s,"H3",{class:!0});var mc=o(zs);Ae=r(mc,"A",{id:!0,class:!0,href:!0});var rv=o(Ae);Pr=r(rv,"SPAN",{});var ov=o(Pr);d(Xa.$$.fragment,ov),ov.forEach(a),rv.forEach(a),sg=h(mc),Sr=r(mc,"SPAN",{});var pv=o(Sr);eg=n(pv,"Format transform"),pv.forEach(a),mc.forEach(a),di=h(s),De=r(s,"P",{});var gc=o(De);ag=n(gc,"The "),vn=r(gc,"A",{href:!0});var iv=o(vn);tg=n(iv,"set_transform()"),iv.forEach(a),ng=n(gc," function applies a custom formatting transform on-the-fly. This function replaces any previously specified format. For example, you can use this function to tokenize and pad tokens on-the-fly. Tokenization is only applied when examples are accessed:"),gc.forEach(a),mi=h(s),d(Za.$$.fragment,s),gi=h(s),Fs=r(s,"H2",{class:!0});var _c=o(Fs);Te=r(_c,"A",{id:!0,class:!0,href:!0});var cv=o(Te);Cr=r(cv,"SPAN",{});var hv=o(Cr);d(st.$$.fragment,hv),hv.forEach(a),cv.forEach(a),lg=h(_c),Nr=r(_c,"SPAN",{});var uv=o(Nr);rg=n(uv,"Save"),uv.forEach(a),_c.forEach(a),_i=h(s),Pe=r(s,"P",{});var jc=o(Pe);og=n(jc,"Once you are done processing your dataset, you can save and reuse it later with "),bn=r(jc,"A",{href:!0});var fv=o(bn);pg=n(fv,"save_to_disk()"),fv.forEach(a),ig=n(jc,"."),jc.forEach(a),ji=h(s),wn=r(s,"P",{});var dv=o(wn);cg=n(dv,"Save your dataset by providing the path to the directory you wish to save it to:"),dv.forEach(a),vi=h(s),d(et.$$.fragment,s),bi=h(s),Se=r(s,"P",{});var vc=o(Se);hg=n(vc,"Use the "),$n=r(vc,"A",{href:!0});var mv=o($n);ug=n(mv,"load_from_disk()"),mv.forEach(a),fg=n(vc," function to reload the dataset:"),vc.forEach(a),wi=h(s),d(at.$$.fragment,s),$i=h(s),d(Ce.$$.fragment,s),xi=h(s),Is=r(s,"H2",{class:!0});var bc=o(Is);Ne=r(bc,"A",{id:!0,class:!0,href:!0});var gv=o(Ne);Or=r(gv,"SPAN",{});var _v=o(Or);d(tt.$$.fragment,_v),_v.forEach(a),gv.forEach(a),dg=h(bc),zr=r(bc,"SPAN",{});var jv=o(zr);mg=n(jv,"Export"),jv.forEach(a),bc.forEach(a),yi=h(s),xn=r(s,"P",{});var vv=o(xn);gg=n(vv,"\u{1F917} Datasets supports exporting as well so you can work with your dataset in other applications. The following table shows currently supported file formats you can export to:"),vv.forEach(a),ki=h(s),Oe=r(s,"TABLE",{});var wc=o(Oe);Fr=r(wc,"THEAD",{});var bv=o(Fr);nt=r(bv,"TR",{});var $c=o(nt);Ir=r($c,"TH",{});var wv=o(Ir);_g=n(wv,"File type"),wv.forEach(a),jg=h($c),Rr=r($c,"TH",{});var $v=o(Rr);vg=n($v,"Export method"),$v.forEach(a),$c.forEach(a),bv.forEach(a),bg=h(wc),I=r(wc,"TBODY",{});var hs=o(I);lt=r(hs,"TR",{});var xc=o(lt);Lr=r(xc,"TD",{});var xv=o(Lr);wg=n(xv,"CSV"),xv.forEach(a),$g=h(xc),Mr=r(xc,"TD",{});var yv=o(Mr);yn=r(yv,"A",{href:!0});var kv=o(yn);xg=n(kv,"Dataset.to_csv()"),kv.forEach(a),yv.forEach(a),xc.forEach(a),yg=h(hs),rt=r(hs,"TR",{});var yc=o(rt);Ur=r(yc,"TD",{});var Ev=o(Ur);kg=n(Ev,"JSON"),Ev.forEach(a),Eg=h(yc),Vr=r(yc,"TD",{});var qv=o(Vr);kn=r(qv,"A",{href:!0});var Av=o(kn);qg=n(Av,"Dataset.to_json()"),Av.forEach(a),qv.forEach(a),yc.forEach(a),Ag=h(hs),ot=r(hs,"TR",{});var kc=o(ot);Br=r(kc,"TD",{});var Dv=o(Br);Dg=n(Dv,"Parquet"),Dv.forEach(a),Tg=h(kc),Hr=r(kc,"TD",{});var Tv=o(Hr);En=r(Tv,"A",{href:!0});var Pv=o(En);Pg=n(Pv,"Dataset.to_parquet()"),Pv.forEach(a),Tv.forEach(a),kc.forEach(a),Sg=h(hs),pt=r(hs,"TR",{});var Ec=o(pt);Yr=r(Ec,"TD",{});var Sv=o(Yr);Cg=n(Sv,"SQL"),Sv.forEach(a),Ng=h(Ec),Gr=r(Ec,"TD",{});var Cv=o(Gr);qn=r(Cv,"A",{href:!0});var Nv=o(qn);Og=n(Nv,"Dataset.to_sql()"),Nv.forEach(a),Cv.forEach(a),Ec.forEach(a),zg=h(hs),it=r(hs,"TR",{});var qc=o(it);Wr=r(qc,"TD",{});var Ov=o(Wr);Fg=n(Ov,"In-memory Python object"),Ov.forEach(a),Ig=h(qc),ct=r(qc,"TD",{});var Ac=o(ct);An=r(Ac,"A",{href:!0});var zv=o(An);Rg=n(zv,"Dataset.to_pandas()"),zv.forEach(a),Lg=n(Ac," or "),Dn=r(Ac,"A",{href:!0});var Fv=o(Dn);Mg=n(Fv,"Dataset.to_dict()"),Fv.forEach(a),Ac.forEach(a),qc.forEach(a),hs.forEach(a),wc.forEach(a),Ei=h(s),Tn=r(s,"P",{});var Iv=o(Tn);Ug=n(Iv,"For example, export your dataset to a CSV file like this:"),Iv.forEach(a),qi=h(s),d(ht.$$.fragment,s),this.h()},h(){u(v,"name","hf:doc:metadata"),u(v,"content",JSON.stringify(Kv)),u(y,"id","process"),u(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(y,"href","#process"),u(b,"class","relative group"),u(Le,"class","underline decoration-pink-400 decoration-2 font-semibold"),u(Le,"href","./audio_process"),u(Me,"class","underline decoration-yellow-400 decoration-2 font-semibold"),u(Me,"href","./image_process"),u(Ue,"class","underline decoration-green-400 decoration-2 font-semibold"),u(Ue,"href","./nlp_process"),u(Ls,"id","sort-shuffle-select-split-and-shard"),u(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ls,"href","#sort-shuffle-select-split-and-shard"),u(ds,"class","relative group"),u(Ms,"id","sort"),u(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ms,"href","#sort"),u(ms,"class","relative group"),u(_t,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.sort"),u(Vs,"id","shuffle"),u(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Vs,"href","#shuffle"),u(gs,"class","relative group"),u(jt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.shuffle"),u(Bs,"id","select-and-filter"),u(Bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Bs,"href","#select-and-filter"),u(_s,"class","relative group"),u(vt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.select"),u(bt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.filter"),u(xt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.select"),u(Et,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.filter"),u(qt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.filter"),u(Hs,"id","split"),u(Hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Hs,"href","#split"),u(vs,"class","relative group"),u(At,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.train_test_split"),u(Gs,"id","shard"),u(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Gs,"href","#shard"),u(bs,"class","relative group"),u(Dt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.shard"),u(aa,"href","https://huggingface.co/datasets/imdb"),u(aa,"rel","nofollow"),u(Js,"id","rename-remove-cast-and-flatten"),u(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Js,"href","#rename-remove-cast-and-flatten"),u(ws,"class","relative group"),u(Ks,"id","rename"),u(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ks,"href","#rename"),u($s,"class","relative group"),u(St,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.rename_column"),u(Ct,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.rename_column"),u(Zs,"id","remove"),u(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Zs,"href","#remove"),u(xs,"class","relative group"),u(Nt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.remove_columns"),u(ee,"id","cast"),u(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ee,"href","#cast"),u(ys,"class","relative group"),u(Ot,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.cast"),u(zt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Features"),u(Ft,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.ClassLabel"),u(It,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Value"),u(Rt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.cast_column"),u(ne,"id","flatten"),u(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ne,"href","#flatten"),u(ks,"class","relative group"),u(Mt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.flatten"),u(le,"id","map"),u(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(le,"href","#map"),u(Es,"class","relative group"),u(Ut,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(Vt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(Bt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(Ht,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(Yt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(Gt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(Wt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(ie,"id","multiprocessing"),u(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ie,"href","#multiprocessing"),u(qs,"class","relative group"),u(Jt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(ce,"id","batch-processing"),u(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ce,"href","#batch-processing"),u(As,"class","relative group"),u(Kt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(he,"id","split-long-examples"),u(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(he,"href","#split-long-examples"),u(Ds,"class","relative group"),u(Xt,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(de,"id","data-augmentation"),u(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(de,"href","#data-augmentation"),u(Ts,"class","relative group"),u(sn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(Sa,"href","https://huggingface.co/roberta-base"),u(Sa,"rel","nofollow"),u(Ca,"href","https://huggingface.co/transformers/main_classes/pipelines#transformers.FillMaskPipeline"),u(Ca,"rel","nofollow"),u(an,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(_e,"id","process-multiple-splits"),u(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(_e,"href","#process-multiple-splits"),u(Ps,"class","relative group"),u(tn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.DatasetDict.map"),u(je,"id","distributed-usage"),u(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(je,"href","#distributed-usage"),u(Ss,"class","relative group"),u(nn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.map"),u(La,"href","https://pytorch.org/docs/stable/distributed?highlight=barrier#torch.distributed.barrier"),u(La,"rel","nofollow"),u(be,"id","concatenate"),u(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(be,"href","#concatenate"),u(Cs,"class","relative group"),u(ln,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.concatenate_datasets"),u(xe,"id","interleave"),u(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(xe,"href","#interleave"),u(Ns,"class","relative group"),u(rn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.interleave_datasets"),u(on,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.interleave_datasets"),u(pn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.concatenate_datasets"),u(cn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset"),u(hn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.IterableDataset"),u(un,"href","./stream#interleave"),u(fn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.IterableDataset"),u(ye,"id","format"),u(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ye,"href","#format"),u(Os,"class","relative group"),u(mn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.set_format"),u(gn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.with_format"),u(_n,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset"),u(jn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.reset_format"),u(Ae,"id","format-transform"),u(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ae,"href","#format-transform"),u(zs,"class","relative group"),u(vn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.set_transform"),u(Te,"id","save"),u(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Te,"href","#save"),u(Fs,"class","relative group"),u(bn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),u($n,"href","/docs/datasets/v2.7.0/en/package_reference/loading_methods#datasets.load_from_disk"),u(Ne,"id","export"),u(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ne,"href","#export"),u(Is,"class","relative group"),u(yn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.to_csv"),u(kn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.to_json"),u(En,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.to_parquet"),u(qn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.to_sql"),u(An,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.to_pandas"),u(Dn,"href","/docs/datasets/v2.7.0/en/package_reference/main_classes#datasets.Dataset.to_dict")},m(s,p){e(document.head,v),i(s,q,p),i(s,b,p),e(b,y),e(y,k),m($,k,null),e(b,x),e(b,D),e(D,us),i(s,F,p),i(s,fs,p),e(fs,ft),i(s,eo,p),i(s,dt,p),e(dt,Dc),i(s,ao,p),i(s,P,p),e(P,Yn),e(Yn,Tc),e(P,Pc),e(P,Gn),e(Gn,Sc),e(P,Cc),e(P,Wn),e(Wn,Nc),e(P,Oc),e(P,Jn),e(Jn,zc),e(P,Fc),e(P,Kn),e(Kn,Ic),e(P,Rc),e(P,Qn),e(Qn,Lc),i(s,to,p),i(s,L,p),e(L,Mc),e(L,Le),e(Le,Uc),e(L,Vc),e(L,Me),e(Me,Bc),e(L,Hc),e(L,Ue),e(Ue,Yc),e(L,Gc),i(s,no,p),i(s,mt,p),e(mt,Wc),i(s,lo,p),m(Ve,s,p),i(s,ro,p),m(Rs,s,p),i(s,oo,p),i(s,ds,p),e(ds,Ls),e(Ls,Xn),m(Be,Xn,null),e(ds,Jc),e(ds,Zn),e(Zn,Kc),i(s,po,p),i(s,gt,p),e(gt,Qc),i(s,io,p),i(s,ms,p),e(ms,Ms),e(Ms,sl),m(He,sl,null),e(ms,Xc),e(ms,el),e(el,Zc),i(s,co,p),i(s,Us,p),e(Us,sh),e(Us,_t),e(_t,eh),e(Us,ah),i(s,ho,p),m(Ye,s,p),i(s,uo,p),i(s,gs,p),e(gs,Vs),e(Vs,al),m(Ge,al,null),e(gs,th),e(gs,tl),e(tl,nh),i(s,fo,p),i(s,M,p),e(M,lh),e(M,jt),e(jt,rh),e(M,oh),e(M,nl),e(nl,ph),e(M,ih),e(M,ll),e(ll,ch),e(M,hh),i(s,mo,p),m(We,s,p),i(s,go,p),i(s,_s,p),e(_s,Bs),e(Bs,rl),m(Je,rl,null),e(_s,uh),e(_s,ol),e(ol,fh),i(s,_o,p),i(s,Y,p),e(Y,dh),e(Y,vt),e(vt,mh),e(Y,gh),e(Y,bt),e(bt,_h),e(Y,jh),i(s,jo,p),i(s,wt,p),e(wt,$t),e($t,xt),e(xt,vh),e($t,bh),i(s,vo,p),m(Ke,s,p),i(s,bo,p),i(s,yt,p),e(yt,kt),e(kt,Et),e(Et,wh),e(kt,$h),i(s,wo,p),m(Qe,s,p),i(s,$o,p),i(s,js,p),e(js,qt),e(qt,xh),e(js,yh),e(js,pl),e(pl,kh),e(js,Eh),i(s,xo,p),m(Xe,s,p),i(s,yo,p),i(s,vs,p),e(vs,Hs),e(Hs,il),m(Ze,il,null),e(vs,qh),e(vs,cl),e(cl,Ah),i(s,ko,p),i(s,G,p),e(G,Dh),e(G,At),e(At,Th),e(G,Ph),e(G,hl),e(hl,Sh),e(G,Ch),i(s,Eo,p),m(sa,s,p),i(s,qo,p),i(s,Ys,p),e(Ys,Nh),e(Ys,ul),e(ul,Oh),e(Ys,zh),i(s,Ao,p),i(s,bs,p),e(bs,Gs),e(Gs,fl),m(ea,fl,null),e(bs,Fh),e(bs,dl),e(dl,Ih),i(s,Do,p),i(s,U,p),e(U,Rh),e(U,ml),e(ml,Lh),e(U,Mh),e(U,Dt),e(Dt,Uh),e(U,Vh),e(U,gl),e(gl,Bh),e(U,Hh),i(s,To,p),i(s,Ws,p),e(Ws,Yh),e(Ws,aa),e(aa,Gh),e(Ws,Wh),i(s,Po,p),m(ta,s,p),i(s,So,p),i(s,Tt,p),e(Tt,Jh),i(s,Co,p),m(na,s,p),i(s,No,p),i(s,ws,p),e(ws,Js),e(Js,_l),m(la,_l,null),e(ws,Kh),e(ws,jl),e(jl,Qh),i(s,Oo,p),i(s,Pt,p),e(Pt,Xh),i(s,zo,p),i(s,$s,p),e($s,Ks),e(Ks,vl),m(ra,vl,null),e($s,Zh),e($s,bl),e(bl,su),i(s,Fo,p),i(s,Qs,p),e(Qs,eu),e(Qs,St),e(St,au),e(Qs,tu),i(s,Io,p),i(s,Xs,p),e(Xs,nu),e(Xs,Ct),e(Ct,lu),e(Xs,ru),i(s,Ro,p),m(oa,s,p),i(s,Lo,p),i(s,xs,p),e(xs,Zs),e(Zs,wl),m(pa,wl,null),e(xs,ou),e(xs,$l),e($l,pu),i(s,Mo,p),i(s,se,p),e(se,iu),e(se,Nt),e(Nt,cu),e(se,hu),i(s,Uo,p),m(ia,s,p),i(s,Vo,p),i(s,ys,p),e(ys,ee),e(ee,xl),m(ca,xl,null),e(ys,uu),e(ys,yl),e(yl,fu),i(s,Bo,p),i(s,C,p),e(C,du),e(C,Ot),e(Ot,mu),e(C,gu),e(C,zt),e(zt,_u),e(C,ju),e(C,Ft),e(Ft,vu),e(C,bu),e(C,It),e(It,wu),e(C,$u),i(s,Ho,p),m(ha,s,p),i(s,Yo,p),m(ae,s,p),i(s,Go,p),i(s,te,p),e(te,xu),e(te,Rt),e(Rt,yu),e(te,ku),i(s,Wo,p),m(ua,s,p),i(s,Jo,p),i(s,ks,p),e(ks,ne),e(ne,kl),m(fa,kl,null),e(ks,Eu),e(ks,El),e(El,qu),i(s,Ko,p),i(s,Lt,p),e(Lt,Au),i(s,Qo,p),m(da,s,p),i(s,Xo,p),i(s,N,p),e(N,Du),e(N,ql),e(ql,Tu),e(N,Pu),e(N,Al),e(Al,Su),e(N,Cu),e(N,Dl),e(Dl,Nu),e(N,Ou),e(N,Mt),e(Mt,zu),e(N,Fu),i(s,Zo,p),m(ma,s,p),i(s,sp,p),i(s,W,p),e(W,Iu),e(W,Tl),e(Tl,Ru),e(W,Lu),e(W,Pl),e(Pl,Mu),e(W,Uu),i(s,ep,p),i(s,Es,p),e(Es,le),e(le,Sl),m(ga,Sl,null),e(Es,Vu),e(Es,Cl),e(Cl,Bu),i(s,ap,p),i(s,J,p),e(J,Hu),e(J,Ut),e(Ut,Yu),e(J,Gu),e(J,Vt),e(Vt,Wu),e(J,Ju),i(s,tp,p),i(s,K,p),e(K,Ku),e(K,Nl),e(Nl,Qu),e(K,Xu),e(K,Ol),e(Ol,Zu),e(K,sf),i(s,np,p),i(s,Q,p),e(Q,ef),e(Q,zl),e(zl,af),e(Q,tf),e(Q,Fl),e(Fl,nf),e(Q,lf),i(s,lp,p),m(_a,s,p),i(s,rp,p),i(s,X,p),e(X,rf),e(X,Bt),e(Bt,of),e(X,pf),e(X,Il),e(Il,cf),e(X,hf),i(s,op,p),m(ja,s,p),i(s,pp,p),i(s,re,p),e(re,uf),e(re,Ht),e(Ht,ff),e(re,df),i(s,ip,p),i(s,Z,p),e(Z,mf),e(Z,Rl),e(Rl,gf),e(Z,_f),e(Z,Yt),e(Yt,jf),e(Z,vf),i(s,cp,p),m(va,s,p),i(s,hp,p),m(oe,s,p),i(s,up,p),i(s,ss,p),e(ss,bf),e(ss,Gt),e(Gt,wf),e(ss,$f),e(ss,Ll),e(Ll,xf),e(ss,yf),i(s,fp,p),m(ba,s,p),i(s,dp,p),i(s,S,p),e(S,kf),e(S,Wt),e(Wt,Ef),e(S,qf),e(S,Ml),e(Ml,Af),e(S,Df),e(S,Ul),e(Ul,Tf),e(S,Pf),e(S,Vl),e(Vl,Sf),e(S,Cf),e(S,Bl),e(Bl,Nf),e(S,Of),i(s,mp,p),m(wa,s,p),i(s,gp,p),i(s,pe,p),e(pe,zf),e(pe,Hl),e(Hl,Ff),e(pe,If),i(s,_p,p),m($a,s,p),i(s,jp,p),i(s,qs,p),e(qs,ie),e(ie,Yl),m(xa,Yl,null),e(qs,Rf),e(qs,Gl),e(Gl,Lf),i(s,vp,p),i(s,es,p),e(es,Mf),e(es,Wl),e(Wl,Uf),e(es,Vf),e(es,Jt),e(Jt,Bf),e(es,Hf),i(s,bp,p),m(ya,s,p),i(s,wp,p),i(s,As,p),e(As,ce),e(ce,Jl),m(ka,Jl,null),e(As,Yf),e(As,Kl),e(Kl,Gf),i(s,$p,p),i(s,V,p),e(V,Wf),e(V,Kt),e(Kt,Jf),e(V,Kf),e(V,Ql),e(Ql,Qf),e(V,Xf),e(V,Xl),e(Xl,Zf),e(V,sd),i(s,xp,p),i(s,Ds,p),e(Ds,he),e(he,Zl),m(Ea,Zl,null),e(Ds,ed),e(Ds,sr),e(sr,ad),i(s,yp,p),i(s,Qt,p),e(Qt,td),i(s,kp,p),i(s,ue,p),e(ue,er),e(er,qa),e(qa,nd),e(qa,ar),e(ar,ld),e(qa,rd),e(ue,od),e(ue,tr),e(tr,nr),e(nr,pd),i(s,Ep,p),m(Aa,s,p),i(s,qp,p),i(s,fe,p),e(fe,id),e(fe,Xt),e(Xt,cd),e(fe,hd),i(s,Ap,p),m(Da,s,p),i(s,Dp,p),i(s,Zt,p),e(Zt,ud),i(s,Tp,p),m(Ta,s,p),i(s,Pp,p),i(s,Ts,p),e(Ts,de),e(de,lr),m(Pa,lr,null),e(Ts,fd),e(Ts,rr),e(rr,dd),i(s,Sp,p),i(s,me,p),e(me,md),e(me,sn),e(sn,gd),e(me,_d),i(s,Cp,p),i(s,as,p),e(as,jd),e(as,Sa),e(Sa,vd),e(as,bd),e(as,Ca),e(Ca,wd),e(as,$d),i(s,Np,p),m(Na,s,p),i(s,Op,p),i(s,en,p),e(en,xd),i(s,zp,p),m(Oa,s,p),i(s,Fp,p),i(s,ge,p),e(ge,yd),e(ge,an),e(an,kd),e(ge,Ed),i(s,Ip,p),m(za,s,p),i(s,Rp,p),i(s,O,p),e(O,qd),e(O,or),e(or,Ad),e(O,Dd),e(O,pr),e(pr,Td),e(O,Pd),e(O,ir),e(ir,Sd),e(O,Cd),e(O,cr),e(cr,Nd),e(O,Od),i(s,Lp,p),i(s,Ps,p),e(Ps,_e),e(_e,hr),m(Fa,hr,null),e(Ps,zd),e(Ps,ur),e(ur,Fd),i(s,Mp,p),i(s,ts,p),e(ts,Id),e(ts,tn),e(tn,Rd),e(ts,Ld),e(ts,fr),e(fr,Md),e(ts,Ud),i(s,Up,p),m(Ia,s,p),i(s,Vp,p),i(s,Ss,p),e(Ss,je),e(je,dr),m(Ra,dr,null),e(Ss,Vd),e(Ss,mr),e(mr,Bd),i(s,Bp,p),i(s,ns,p),e(ns,Hd),e(ns,nn),e(nn,Yd),e(ns,Gd),e(ns,La),e(La,Wd),e(ns,Jd),i(s,Hp,p),i(s,ve,p),e(ve,Kd),e(ve,gr),e(gr,Qd),e(ve,Xd),i(s,Yp,p),m(Ma,s,p),i(s,Gp,p),i(s,Cs,p),e(Cs,be),e(be,_r),m(Ua,_r,null),e(Cs,Zd),e(Cs,jr),e(jr,sm),i(s,Wp,p),i(s,we,p),e(we,em),e(we,ln),e(ln,am),e(we,tm),i(s,Jp,p),m(Va,s,p),i(s,Kp,p),i(s,$e,p),e($e,nm),e($e,vr),e(vr,lm),e($e,rm),i(s,Qp,p),m(Ba,s,p),i(s,Xp,p),i(s,Ns,p),e(Ns,xe),e(xe,br),m(Ha,br,null),e(Ns,om),e(Ns,wr),e(wr,pm),i(s,Zp,p),i(s,A,p),e(A,im),e(A,$r),e($r,cm),e(A,hm),e(A,rn),e(rn,um),e(A,fm),e(A,on),e(on,dm),e(A,mm),e(A,pn),e(pn,gm),e(A,_m),e(A,cn),e(cn,jm),e(A,vm),e(A,hn),e(hn,bm),e(A,wm),e(A,un),e(un,$m),e(A,xm),e(A,fn),e(fn,ym),e(A,km),i(s,si,p),i(s,dn,p),e(dn,Em),i(s,ei,p),m(Ya,s,p),i(s,ai,p),i(s,z,p),e(z,qm),e(z,xr),e(xr,Am),e(z,Dm),e(z,yr),e(yr,Tm),e(z,Pm),e(z,kr),e(kr,Sm),e(z,Cm),e(z,Er),e(Er,Nm),e(z,Om),i(s,ti,p),m(Ga,s,p),i(s,ni,p),i(s,Os,p),e(Os,ye),e(ye,qr),m(Wa,qr,null),e(Os,zm),e(Os,Ar),e(Ar,Fm),i(s,li,p),i(s,ls,p),e(ls,Im),e(ls,mn),e(mn,Rm),e(ls,Lm),e(ls,Dr),e(Dr,Mm),e(ls,Um),i(s,ri,p),i(s,ke,p),e(ke,Vm),e(ke,Tr),e(Tr,Bm),e(ke,Hm),i(s,oi,p),m(Ja,s,p),i(s,pi,p),i(s,rs,p),e(rs,Ym),e(rs,gn),e(gn,Gm),e(rs,Wm),e(rs,_n),e(_n,Jm),e(rs,Km),i(s,ii,p),m(Ka,s,p),i(s,ci,p),m(Ee,s,p),i(s,hi,p),i(s,qe,p),e(qe,Qm),e(qe,jn),e(jn,Xm),e(qe,Zm),i(s,ui,p),m(Qa,s,p),i(s,fi,p),i(s,zs,p),e(zs,Ae),e(Ae,Pr),m(Xa,Pr,null),e(zs,sg),e(zs,Sr),e(Sr,eg),i(s,di,p),i(s,De,p),e(De,ag),e(De,vn),e(vn,tg),e(De,ng),i(s,mi,p),m(Za,s,p),i(s,gi,p),i(s,Fs,p),e(Fs,Te),e(Te,Cr),m(st,Cr,null),e(Fs,lg),e(Fs,Nr),e(Nr,rg),i(s,_i,p),i(s,Pe,p),e(Pe,og),e(Pe,bn),e(bn,pg),e(Pe,ig),i(s,ji,p),i(s,wn,p),e(wn,cg),i(s,vi,p),m(et,s,p),i(s,bi,p),i(s,Se,p),e(Se,hg),e(Se,$n),e($n,ug),e(Se,fg),i(s,wi,p),m(at,s,p),i(s,$i,p),m(Ce,s,p),i(s,xi,p),i(s,Is,p),e(Is,Ne),e(Ne,Or),m(tt,Or,null),e(Is,dg),e(Is,zr),e(zr,mg),i(s,yi,p),i(s,xn,p),e(xn,gg),i(s,ki,p),i(s,Oe,p),e(Oe,Fr),e(Fr,nt),e(nt,Ir),e(Ir,_g),e(nt,jg),e(nt,Rr),e(Rr,vg),e(Oe,bg),e(Oe,I),e(I,lt),e(lt,Lr),e(Lr,wg),e(lt,$g),e(lt,Mr),e(Mr,yn),e(yn,xg),e(I,yg),e(I,rt),e(rt,Ur),e(Ur,kg),e(rt,Eg),e(rt,Vr),e(Vr,kn),e(kn,qg),e(I,Ag),e(I,ot),e(ot,Br),e(Br,Dg),e(ot,Tg),e(ot,Hr),e(Hr,En),e(En,Pg),e(I,Sg),e(I,pt),e(pt,Yr),e(Yr,Cg),e(pt,Ng),e(pt,Gr),e(Gr,qn),e(qn,Og),e(I,zg),e(I,it),e(it,Wr),e(Wr,Fg),e(it,Ig),e(it,ct),e(ct,An),e(An,Rg),e(ct,Lg),e(ct,Dn),e(Dn,Mg),i(s,Ei,p),i(s,Tn,p),e(Tn,Ug),i(s,qi,p),m(ht,s,p),Ai=!0},p(s,[p]){const ut={};p&2&&(ut.$$scope={dirty:p,ctx:s}),Rs.$set(ut);const Jr={};p&2&&(Jr.$$scope={dirty:p,ctx:s}),ae.$set(Jr);const Kr={};p&2&&(Kr.$$scope={dirty:p,ctx:s}),oe.$set(Kr);const Qr={};p&2&&(Qr.$$scope={dirty:p,ctx:s}),Ee.$set(Qr);const Xr={};p&2&&(Xr.$$scope={dirty:p,ctx:s}),Ce.$set(Xr)},i(s){Ai||(g($.$$.fragment,s),g(Ve.$$.fragment,s),g(Rs.$$.fragment,s),g(Be.$$.fragment,s),g(He.$$.fragment,s),g(Ye.$$.fragment,s),g(Ge.$$.fragment,s),g(We.$$.fragment,s),g(Je.$$.fragment,s),g(Ke.$$.fragment,s),g(Qe.$$.fragment,s),g(Xe.$$.fragment,s),g(Ze.$$.fragment,s),g(sa.$$.fragment,s),g(ea.$$.fragment,s),g(ta.$$.fragment,s),g(na.$$.fragment,s),g(la.$$.fragment,s),g(ra.$$.fragment,s),g(oa.$$.fragment,s),g(pa.$$.fragment,s),g(ia.$$.fragment,s),g(ca.$$.fragment,s),g(ha.$$.fragment,s),g(ae.$$.fragment,s),g(ua.$$.fragment,s),g(fa.$$.fragment,s),g(da.$$.fragment,s),g(ma.$$.fragment,s),g(ga.$$.fragment,s),g(_a.$$.fragment,s),g(ja.$$.fragment,s),g(va.$$.fragment,s),g(oe.$$.fragment,s),g(ba.$$.fragment,s),g(wa.$$.fragment,s),g($a.$$.fragment,s),g(xa.$$.fragment,s),g(ya.$$.fragment,s),g(ka.$$.fragment,s),g(Ea.$$.fragment,s),g(Aa.$$.fragment,s),g(Da.$$.fragment,s),g(Ta.$$.fragment,s),g(Pa.$$.fragment,s),g(Na.$$.fragment,s),g(Oa.$$.fragment,s),g(za.$$.fragment,s),g(Fa.$$.fragment,s),g(Ia.$$.fragment,s),g(Ra.$$.fragment,s),g(Ma.$$.fragment,s),g(Ua.$$.fragment,s),g(Va.$$.fragment,s),g(Ba.$$.fragment,s),g(Ha.$$.fragment,s),g(Ya.$$.fragment,s),g(Ga.$$.fragment,s),g(Wa.$$.fragment,s),g(Ja.$$.fragment,s),g(Ka.$$.fragment,s),g(Ee.$$.fragment,s),g(Qa.$$.fragment,s),g(Xa.$$.fragment,s),g(Za.$$.fragment,s),g(st.$$.fragment,s),g(et.$$.fragment,s),g(at.$$.fragment,s),g(Ce.$$.fragment,s),g(tt.$$.fragment,s),g(ht.$$.fragment,s),Ai=!0)},o(s){_($.$$.fragment,s),_(Ve.$$.fragment,s),_(Rs.$$.fragment,s),_(Be.$$.fragment,s),_(He.$$.fragment,s),_(Ye.$$.fragment,s),_(Ge.$$.fragment,s),_(We.$$.fragment,s),_(Je.$$.fragment,s),_(Ke.$$.fragment,s),_(Qe.$$.fragment,s),_(Xe.$$.fragment,s),_(Ze.$$.fragment,s),_(sa.$$.fragment,s),_(ea.$$.fragment,s),_(ta.$$.fragment,s),_(na.$$.fragment,s),_(la.$$.fragment,s),_(ra.$$.fragment,s),_(oa.$$.fragment,s),_(pa.$$.fragment,s),_(ia.$$.fragment,s),_(ca.$$.fragment,s),_(ha.$$.fragment,s),_(ae.$$.fragment,s),_(ua.$$.fragment,s),_(fa.$$.fragment,s),_(da.$$.fragment,s),_(ma.$$.fragment,s),_(ga.$$.fragment,s),_(_a.$$.fragment,s),_(ja.$$.fragment,s),_(va.$$.fragment,s),_(oe.$$.fragment,s),_(ba.$$.fragment,s),_(wa.$$.fragment,s),_($a.$$.fragment,s),_(xa.$$.fragment,s),_(ya.$$.fragment,s),_(ka.$$.fragment,s),_(Ea.$$.fragment,s),_(Aa.$$.fragment,s),_(Da.$$.fragment,s),_(Ta.$$.fragment,s),_(Pa.$$.fragment,s),_(Na.$$.fragment,s),_(Oa.$$.fragment,s),_(za.$$.fragment,s),_(Fa.$$.fragment,s),_(Ia.$$.fragment,s),_(Ra.$$.fragment,s),_(Ma.$$.fragment,s),_(Ua.$$.fragment,s),_(Va.$$.fragment,s),_(Ba.$$.fragment,s),_(Ha.$$.fragment,s),_(Ya.$$.fragment,s),_(Ga.$$.fragment,s),_(Wa.$$.fragment,s),_(Ja.$$.fragment,s),_(Ka.$$.fragment,s),_(Ee.$$.fragment,s),_(Qa.$$.fragment,s),_(Xa.$$.fragment,s),_(Za.$$.fragment,s),_(st.$$.fragment,s),_(et.$$.fragment,s),_(at.$$.fragment,s),_(Ce.$$.fragment,s),_(tt.$$.fragment,s),_(ht.$$.fragment,s),Ai=!1},d(s){a(v),s&&a(q),s&&a(b),j($),s&&a(F),s&&a(fs),s&&a(eo),s&&a(dt),s&&a(ao),s&&a(P),s&&a(to),s&&a(L),s&&a(no),s&&a(mt),s&&a(lo),j(Ve,s),s&&a(ro),j(Rs,s),s&&a(oo),s&&a(ds),j(Be),s&&a(po),s&&a(gt),s&&a(io),s&&a(ms),j(He),s&&a(co),s&&a(Us),s&&a(ho),j(Ye,s),s&&a(uo),s&&a(gs),j(Ge),s&&a(fo),s&&a(M),s&&a(mo),j(We,s),s&&a(go),s&&a(_s),j(Je),s&&a(_o),s&&a(Y),s&&a(jo),s&&a(wt),s&&a(vo),j(Ke,s),s&&a(bo),s&&a(yt),s&&a(wo),j(Qe,s),s&&a($o),s&&a(js),s&&a(xo),j(Xe,s),s&&a(yo),s&&a(vs),j(Ze),s&&a(ko),s&&a(G),s&&a(Eo),j(sa,s),s&&a(qo),s&&a(Ys),s&&a(Ao),s&&a(bs),j(ea),s&&a(Do),s&&a(U),s&&a(To),s&&a(Ws),s&&a(Po),j(ta,s),s&&a(So),s&&a(Tt),s&&a(Co),j(na,s),s&&a(No),s&&a(ws),j(la),s&&a(Oo),s&&a(Pt),s&&a(zo),s&&a($s),j(ra),s&&a(Fo),s&&a(Qs),s&&a(Io),s&&a(Xs),s&&a(Ro),j(oa,s),s&&a(Lo),s&&a(xs),j(pa),s&&a(Mo),s&&a(se),s&&a(Uo),j(ia,s),s&&a(Vo),s&&a(ys),j(ca),s&&a(Bo),s&&a(C),s&&a(Ho),j(ha,s),s&&a(Yo),j(ae,s),s&&a(Go),s&&a(te),s&&a(Wo),j(ua,s),s&&a(Jo),s&&a(ks),j(fa),s&&a(Ko),s&&a(Lt),s&&a(Qo),j(da,s),s&&a(Xo),s&&a(N),s&&a(Zo),j(ma,s),s&&a(sp),s&&a(W),s&&a(ep),s&&a(Es),j(ga),s&&a(ap),s&&a(J),s&&a(tp),s&&a(K),s&&a(np),s&&a(Q),s&&a(lp),j(_a,s),s&&a(rp),s&&a(X),s&&a(op),j(ja,s),s&&a(pp),s&&a(re),s&&a(ip),s&&a(Z),s&&a(cp),j(va,s),s&&a(hp),j(oe,s),s&&a(up),s&&a(ss),s&&a(fp),j(ba,s),s&&a(dp),s&&a(S),s&&a(mp),j(wa,s),s&&a(gp),s&&a(pe),s&&a(_p),j($a,s),s&&a(jp),s&&a(qs),j(xa),s&&a(vp),s&&a(es),s&&a(bp),j(ya,s),s&&a(wp),s&&a(As),j(ka),s&&a($p),s&&a(V),s&&a(xp),s&&a(Ds),j(Ea),s&&a(yp),s&&a(Qt),s&&a(kp),s&&a(ue),s&&a(Ep),j(Aa,s),s&&a(qp),s&&a(fe),s&&a(Ap),j(Da,s),s&&a(Dp),s&&a(Zt),s&&a(Tp),j(Ta,s),s&&a(Pp),s&&a(Ts),j(Pa),s&&a(Sp),s&&a(me),s&&a(Cp),s&&a(as),s&&a(Np),j(Na,s),s&&a(Op),s&&a(en),s&&a(zp),j(Oa,s),s&&a(Fp),s&&a(ge),s&&a(Ip),j(za,s),s&&a(Rp),s&&a(O),s&&a(Lp),s&&a(Ps),j(Fa),s&&a(Mp),s&&a(ts),s&&a(Up),j(Ia,s),s&&a(Vp),s&&a(Ss),j(Ra),s&&a(Bp),s&&a(ns),s&&a(Hp),s&&a(ve),s&&a(Yp),j(Ma,s),s&&a(Gp),s&&a(Cs),j(Ua),s&&a(Wp),s&&a(we),s&&a(Jp),j(Va,s),s&&a(Kp),s&&a($e),s&&a(Qp),j(Ba,s),s&&a(Xp),s&&a(Ns),j(Ha),s&&a(Zp),s&&a(A),s&&a(si),s&&a(dn),s&&a(ei),j(Ya,s),s&&a(ai),s&&a(z),s&&a(ti),j(Ga,s),s&&a(ni),s&&a(Os),j(Wa),s&&a(li),s&&a(ls),s&&a(ri),s&&a(ke),s&&a(oi),j(Ja,s),s&&a(pi),s&&a(rs),s&&a(ii),j(Ka,s),s&&a(ci),j(Ee,s),s&&a(hi),s&&a(qe),s&&a(ui),j(Qa,s),s&&a(fi),s&&a(zs),j(Xa),s&&a(di),s&&a(De),s&&a(mi),j(Za,s),s&&a(gi),s&&a(Fs),j(st),s&&a(_i),s&&a(Pe),s&&a(ji),s&&a(wn),s&&a(vi),j(et,s),s&&a(bi),s&&a(Se),s&&a(wi),j(at,s),s&&a($i),j(Ce,s),s&&a(xi),s&&a(Is),j(tt),s&&a(yi),s&&a(xn),s&&a(ki),s&&a(Oe),s&&a(Ei),s&&a(Tn),s&&a(qi),j(ht,s)}}}const Kv={local:"process",sections:[{local:"sort-shuffle-select-split-and-shard",sections:[{local:"sort",title:"Sort"},{local:"shuffle",title:"Shuffle"},{local:"select-and-filter",title:"Select and Filter"},{local:"split",title:"Split"},{local:"shard",title:"Shard"}],title:"Sort, shuffle, select, split, and shard"},{local:"rename-remove-cast-and-flatten",sections:[{local:"rename",title:"Rename"},{local:"remove",title:"Remove"},{local:"cast",title:"Cast"},{local:"flatten",title:"Flatten"}],title:"Rename, remove, cast, and flatten"},{local:"map",sections:[{local:"multiprocessing",title:"Multiprocessing"},{local:"batch-processing",sections:[{local:"split-long-examples",title:"Split long examples"},{local:"data-augmentation",title:"Data augmentation"}],title:"Batch processing"},{local:"process-multiple-splits",title:"Process multiple splits"},{local:"distributed-usage",title:"Distributed usage"}],title:"Map"},{local:"concatenate",sections:[{local:"interleave",title:"Interleave"}],title:"Concatenate"},{local:"format",sections:[{local:"format-transform",title:"Format transform"}],title:"Format"},{local:"save",title:"Save"},{local:"export",title:"Export"}],title:"Process"};function Qv(R){return Vv(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ab extends Rv{constructor(v){super();Lv(this,v,Qv,Jv,Mv,{})}}export{ab as default,Kv as metadata};
