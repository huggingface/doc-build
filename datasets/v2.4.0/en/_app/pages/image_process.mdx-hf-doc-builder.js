import{S as Xr,i as Zr,s as sp,e as n,k as c,w as u,t,M as ep,c as o,d as a,m,a as r,x as d,h as l,b as h,N as ja,G as e,g as i,y as g,q as b,o as x,B as j,v as ap}from"../chunks/vendor-hf-doc-builder.js";import{T as tp}from"../chunks/Tip-hf-doc-builder.js";import{I as _a}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as k}from"../chunks/CodeBlock-hf-doc-builder.js";function lp(va){let f,Y,_,D,N,y,bs,F,A,K,T;return{c(){f=n("p"),Y=t("Feel free to use other data augmentation libraries like "),_=n("a"),D=t("Albumentations"),N=t(", "),y=n("a"),bs=t("Kornia"),F=t(", and "),A=n("a"),K=t("imgaug"),T=t("."),this.h()},l(S){f=o(S,"P",{});var w=r(f);Y=l(w,"Feel free to use other data augmentation libraries like "),_=o(w,"A",{href:!0,rel:!0});var P=r(_);D=l(P,"Albumentations"),P.forEach(a),N=l(w,", "),y=o(w,"A",{href:!0,rel:!0});var O=r(y);bs=l(O,"Kornia"),O.forEach(a),F=l(w,", and "),A=o(w,"A",{href:!0,rel:!0});var Xs=r(A);K=l(Xs,"imgaug"),Xs.forEach(a),T=l(w,"."),w.forEach(a),this.h()},h(){h(_,"href","https://albumentations.ai/docs/"),h(_,"rel","nofollow"),h(y,"href","https://kornia.readthedocs.io/en/latest/"),h(y,"rel","nofollow"),h(A,"href","https://imgaug.readthedocs.io/en/latest/"),h(A,"rel","nofollow")},m(S,w){i(S,f,w),e(f,Y),e(f,_),e(_,D),e(f,N),e(f,y),e(y,bs),e(f,F),e(f,A),e(A,K),e(f,T)},d(S){S&&a(f)}}}function np(va){let f,Y,_,D,N,y,bs,F,A,K,T,S,w,P,O,Xs,Zs,St,Mt,Ut,xs,Jt,se,Ht,Vt,ya,W,Yt,js,Kt,Wt,wa,M,Q,Pe,_s,Qt,Ie,Xt,$a,X,Zt,ee,sl,el,Ea,Z,al,vs,Te,tl,ll,ka,ys,Ca,$,nl,ae,ol,rl,Oe,pl,il,ze,cl,ml,Le,hl,fl,Da,ws,Aa,ss,ul,te,dl,gl,Pa,$s,le,bl,xl,Ia,es,ne,oe,qe,jl,_l,vl,re,pe,Be,yl,wl,Ta,as,$l,ie,El,kl,Oa,U,ts,Ge,Es,Cl,Re,Dl,za,ce,Al,La,J,ls,Ne,ks,Pl,Fe,Il,qa,ns,Tl,Cs,Ol,zl,Ba,os,Ga,rs,Ll,Ds,Se,ql,Bl,Ra,As,Na,ps,Gl,Me,Rl,Nl,Fa,Ps,Sa,is,Fl,me,Sl,Ml,Ma,Is,Ua,cs,Ul,Ue,Jl,Hl,Ja,Ts,Ha,H,he,ko,Vl,fe,Co,Va,V,ms,Je,Os,Yl,He,Kl,Ya,z,Wl,zs,Ql,Xl,Ls,Zl,sn,Ka,L,en,Ve,an,tn,Ye,ln,nn,Wa,qs,Qa,hs,on,Bs,Ke,rn,pn,Xa,ue,cn,Za,Gs,st,de,mn,et,E,ge,We,hn,fn,un,be,Qe,dn,gn,bn,xe,Xe,xn,jn,_n,je,Ze,vn,yn,wn,Rs,sa,$n,En,I,_e,ea,kn,Cn,Dn,ve,aa,An,Pn,In,fs,ta,Tn,On,Ns,zn,Ln,qn,v,la,Bn,Gn,na,Rn,Nn,oa,Fn,Sn,ra,Mn,Un,pa,Jn,Hn,ia,Vn,Yn,at,q,Kn,ca,Wn,Qn,ye,Xn,Zn,tt,Fs,lt,Ss,ma,Do,nt,B,so,ha,eo,ao,fa,to,lo,ot,Ms,ua,no,oo,rt,Us,pt,us,ro,da,po,io,it,Js,ct,Hs,ga,Ao,mt,we,co,ht,Vs,ft,ds,mo,$e,ho,fo,ut,Ys,dt,Ee,uo,gt,Ks,bt,Ws,ba,Po,xt;return y=new _a({}),_s=new _a({}),ys=new k({props:{code:`def transforms(examples):
    examples["pixel_values"] = [image.convert("RGB").resize((100,100)) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [image.convert(<span class="hljs-string">&quot;RGB&quot;</span>).resize((<span class="hljs-number">100</span>,<span class="hljs-number">100</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),ws=new k({props:{code:`dataset = dataset.map(transforms, remove_columns=["image"], batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(transforms, remove_columns=[<span class="hljs-string">&quot;image&quot;</span>], batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">6</span>,
 <span class="hljs-string">&#x27;pixel_values&#x27;</span>: &lt;PIL.PngImagePlugin.PngImageFile image mode=RGB size=100x100 at <span class="hljs-number">0x7F058237BB10</span>&gt;}`}}),Es=new _a({}),ks=new _a({}),os=new tp({props:{$$slots:{default:[lp]},$$scope:{ctx:va}}}),As=new k({props:{code:`from torchvision.transforms import Compose, ColorJitter, ToTensor

jitter = Compose(
    [
         ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.7),
         ToTensor(),
    ]
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, ColorJitter, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>jitter = Compose(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>         ColorJitter(brightness=<span class="hljs-number">0.25</span>, contrast=<span class="hljs-number">0.25</span>, saturation=<span class="hljs-number">0.25</span>, hue=<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>         ToTensor(),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)`}}),Ps=new k({props:{code:`def transforms(examples):
    examples["pixel_values"] = [jitter(image.convert("RGB")) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [jitter(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),Is=new k({props:{code:"dataset.set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_transform(transforms)'}}),Ts=new k({props:{code:`import numpy as np
import matplotlib.pyplot as plt

img = dataset[0]["pixel_values"]
plt.imshow(img.permute(1, 2, 0))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-meta">&gt;&gt;&gt; </span>img = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;pixel_values&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.imshow(img.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))`}}),Os=new _a({}),qs=new k({props:{code:"pip install -U albumentations opencv-python",highlighted:'pip <span class="hljs-keyword">install</span> -U albumentations opencv-python'}}),Gs=new k({props:{code:`
ds = load_dataset("cppe-5")
example = ds['train'][0]
example`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;cppe-5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example = ds[<span class="hljs-string">&#x27;train&#x27;</span>][<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>example
{<span class="hljs-string">&#x27;height&#x27;</span>: <span class="hljs-number">663</span>,
 <span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=943x663 at <span class="hljs-number">0x7FC3DC756250</span>&gt;,
 <span class="hljs-string">&#x27;image_id&#x27;</span>: <span class="hljs-number">15</span>,
 <span class="hljs-string">&#x27;objects&#x27;</span>: {<span class="hljs-string">&#x27;area&#x27;</span>: [<span class="hljs-number">3796</span>, <span class="hljs-number">1596</span>, <span class="hljs-number">152768</span>, <span class="hljs-number">81002</span>],
  <span class="hljs-string">&#x27;bbox&#x27;</span>: [[<span class="hljs-number">302.0</span>, <span class="hljs-number">109.0</span>, <span class="hljs-number">73.0</span>, <span class="hljs-number">52.0</span>],
   [<span class="hljs-number">810.0</span>, <span class="hljs-number">100.0</span>, <span class="hljs-number">57.0</span>, <span class="hljs-number">28.0</span>],
   [<span class="hljs-number">160.0</span>, <span class="hljs-number">31.0</span>, <span class="hljs-number">248.0</span>, <span class="hljs-number">616.0</span>],
   [<span class="hljs-number">741.0</span>, <span class="hljs-number">68.0</span>, <span class="hljs-number">202.0</span>, <span class="hljs-number">401.0</span>]],
  <span class="hljs-string">&#x27;category&#x27;</span>: [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
  <span class="hljs-string">&#x27;id&#x27;</span>: [<span class="hljs-number">114</span>, <span class="hljs-number">115</span>, <span class="hljs-number">116</span>, <span class="hljs-number">117</span>]},
 <span class="hljs-string">&#x27;width&#x27;</span>: <span class="hljs-number">943</span>}`}}),Fs=new k({props:{code:`import torch
from torchvision.ops import box_convert
from torchvision.utils import draw_bounding_boxes
from torchvision.transforms.functional import pil_to_tensor, to_pil_image

categories = ds['train'].features['objects'].feature['category']

boxes_xywh = torch.tensor(example['objects']['bbox'])
boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')
labels = [categories.int2str(x) for x in example['objects']['category']]
to_pil_image(
    draw_bounding_boxes(
        pil_to_tensor(example['image']),
        boxes_xyxy,
        colors="red",
        labels=labels,
    )
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.ops <span class="hljs-keyword">import</span> box_convert
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.utils <span class="hljs-keyword">import</span> draw_bounding_boxes
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms.functional <span class="hljs-keyword">import</span> pil_to_tensor, to_pil_image

<span class="hljs-meta">&gt;&gt;&gt; </span>categories = ds[<span class="hljs-string">&#x27;train&#x27;</span>].features[<span class="hljs-string">&#x27;objects&#x27;</span>].feature[<span class="hljs-string">&#x27;category&#x27;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>boxes_xywh = torch.tensor(example[<span class="hljs-string">&#x27;objects&#x27;</span>][<span class="hljs-string">&#x27;bbox&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes_xyxy = box_convert(boxes_xywh, <span class="hljs-string">&#x27;xywh&#x27;</span>, <span class="hljs-string">&#x27;xyxy&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = [categories.int2str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example[<span class="hljs-string">&#x27;objects&#x27;</span>][<span class="hljs-string">&#x27;category&#x27;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>to_pil_image(
<span class="hljs-meta">... </span>    draw_bounding_boxes(
<span class="hljs-meta">... </span>        pil_to_tensor(example[<span class="hljs-string">&#x27;image&#x27;</span>]),
<span class="hljs-meta">... </span>        boxes_xyxy,
<span class="hljs-meta">... </span>        colors=<span class="hljs-string">&quot;red&quot;</span>,
<span class="hljs-meta">... </span>        labels=labels,
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>)`}}),Us=new k({props:{code:`import albumentations as A
import numpy as np

transform = A.Compose([
    A.Resize(480, 480),
    A.HorizontalFlip(p=1.0),
    A.RandomBrightnessContrast(p=1.0),
], bbox_params=A.BboxParams(format='coco',  label_fields=['category']))

# RGB PIL Image -> BGR Numpy array
image = np.flip(np.array(example['image']), -1)
out = transform(
    image=image,
    bboxes=example['objects']['bbox'],
    category=example['objects']['category'],
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> albumentations <span class="hljs-keyword">as</span> A
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-meta">&gt;&gt;&gt; </span>transform = A.Compose([
<span class="hljs-meta">... </span>    A.Resize(<span class="hljs-number">480</span>, <span class="hljs-number">480</span>),
<span class="hljs-meta">... </span>    A.HorizontalFlip(p=<span class="hljs-number">1.0</span>),
<span class="hljs-meta">... </span>    A.RandomBrightnessContrast(p=<span class="hljs-number">1.0</span>),
<span class="hljs-meta">... </span>], bbox_params=A.BboxParams(<span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;coco&#x27;</span>,  label_fields=[<span class="hljs-string">&#x27;category&#x27;</span>]))

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># RGB PIL Image -&gt; BGR Numpy array</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = np.flip(np.array(example[<span class="hljs-string">&#x27;image&#x27;</span>]), -<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>out = transform(
<span class="hljs-meta">... </span>    image=image,
<span class="hljs-meta">... </span>    bboxes=example[<span class="hljs-string">&#x27;objects&#x27;</span>][<span class="hljs-string">&#x27;bbox&#x27;</span>],
<span class="hljs-meta">... </span>    category=example[<span class="hljs-string">&#x27;objects&#x27;</span>][<span class="hljs-string">&#x27;category&#x27;</span>],
<span class="hljs-meta">... </span>)`}}),Js=new k({props:{code:`image = torch.tensor(out['image']).flip(-1).permute(2, 0, 1)
boxes_xywh = torch.stack([torch.tensor(x) for x in out['bboxes']])
boxes_xyxy = box_convert(boxes_xywh, 'xywh', 'xyxy')
labels = [categories.int2str(x) for x in out['category']]
to_pil_image(
    draw_bounding_boxes(
        image,
        boxes_xyxy,
        colors='red',
        labels=labels
    )
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>image = torch.tensor(out[<span class="hljs-string">&#x27;image&#x27;</span>]).flip(-<span class="hljs-number">1</span>).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes_xywh = torch.stack([torch.tensor(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> out[<span class="hljs-string">&#x27;bboxes&#x27;</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes_xyxy = box_convert(boxes_xywh, <span class="hljs-string">&#x27;xywh&#x27;</span>, <span class="hljs-string">&#x27;xyxy&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = [categories.int2str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> out[<span class="hljs-string">&#x27;category&#x27;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>to_pil_image(
<span class="hljs-meta">... </span>    draw_bounding_boxes(
<span class="hljs-meta">... </span>        image,
<span class="hljs-meta">... </span>        boxes_xyxy,
<span class="hljs-meta">... </span>        colors=<span class="hljs-string">&#x27;red&#x27;</span>,
<span class="hljs-meta">... </span>        labels=labels
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>)`}}),Vs=new k({props:{code:`def transforms(examples):
    images, bboxes, categories = [], [], []
    for image, objects in zip(examples['image'], examples['objects']):
        image = np.array(image.convert("RGB"))[:, :, ::-1]
        out = transform(
            image=image,
            bboxes=objects['bbox'],
            category=objects['category']
        )
        images.append(torch.tensor(out['image']).flip(-1).permute(2, 0, 1))
        bboxes.append(torch.tensor(out['bboxes']))
        categories.append(out['category'])
    return {'image': images, 'bbox': bboxes, 'category': categories}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    images, bboxes, categories = [], [], []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> image, objects <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(examples[<span class="hljs-string">&#x27;image&#x27;</span>], examples[<span class="hljs-string">&#x27;objects&#x27;</span>]):
<span class="hljs-meta">... </span>        image = np.array(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>))[:, :, ::-<span class="hljs-number">1</span>]
<span class="hljs-meta">... </span>        out = transform(
<span class="hljs-meta">... </span>            image=image,
<span class="hljs-meta">... </span>            bboxes=objects[<span class="hljs-string">&#x27;bbox&#x27;</span>],
<span class="hljs-meta">... </span>            category=objects[<span class="hljs-string">&#x27;category&#x27;</span>]
<span class="hljs-meta">... </span>        )
<span class="hljs-meta">... </span>        images.append(torch.tensor(out[<span class="hljs-string">&#x27;image&#x27;</span>]).flip(-<span class="hljs-number">1</span>).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">... </span>        bboxes.append(torch.tensor(out[<span class="hljs-string">&#x27;bboxes&#x27;</span>]))
<span class="hljs-meta">... </span>        categories.append(out[<span class="hljs-string">&#x27;category&#x27;</span>])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&#x27;image&#x27;</span>: images, <span class="hljs-string">&#x27;bbox&#x27;</span>: bboxes, <span class="hljs-string">&#x27;category&#x27;</span>: categories}`}}),Ys=new k({props:{code:"ds['train'].set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>ds[<span class="hljs-string">&#x27;train&#x27;</span>].set_transform(transforms)'}}),Ks=new k({props:{code:`example = ds['train'][10]
to_pil_image(
    draw_bounding_boxes(
        example['image'],
        box_convert(example['bbox'], 'xywh', 'xyxy'),
        colors='red',
        labels=[categories.int2str(x) for x in example['category']]
    )
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>example = ds[<span class="hljs-string">&#x27;train&#x27;</span>][<span class="hljs-number">10</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>to_pil_image(
<span class="hljs-meta">... </span>    draw_bounding_boxes(
<span class="hljs-meta">... </span>        example[<span class="hljs-string">&#x27;image&#x27;</span>],
<span class="hljs-meta">... </span>        box_convert(example[<span class="hljs-string">&#x27;bbox&#x27;</span>], <span class="hljs-string">&#x27;xywh&#x27;</span>, <span class="hljs-string">&#x27;xyxy&#x27;</span>),
<span class="hljs-meta">... </span>        colors=<span class="hljs-string">&#x27;red&#x27;</span>,
<span class="hljs-meta">... </span>        labels=[categories.int2str(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example[<span class="hljs-string">&#x27;category&#x27;</span>]]
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>)`}}),{c(){f=n("meta"),Y=c(),_=n("h1"),D=n("a"),N=n("span"),u(y.$$.fragment),bs=c(),F=n("span"),A=t("Process image data"),K=c(),T=n("p"),S=t("This guide shows specific methods for processing image datasets. Learn how to:"),w=c(),P=n("ul"),O=n("li"),Xs=t("Use "),Zs=n("a"),St=t("map()"),Mt=t(" with image dataset."),Ut=c(),xs=n("li"),Jt=t("Apply data augmentations to your dataset with "),se=n("a"),Ht=t("set_transform()"),Vt=t("."),ya=c(),W=n("p"),Yt=t("For a guide on how to process any type of dataset, take a look at the "),js=n("a"),Kt=t("general process guide"),Wt=t("."),wa=c(),M=n("h2"),Q=n("a"),Pe=n("span"),u(_s.$$.fragment),Qt=c(),Ie=n("span"),Xt=t("Map"),$a=c(),X=n("p"),Zt=t("The "),ee=n("a"),sl=t("map()"),el=t(" function can apply transforms over an entire dataset."),Ea=c(),Z=n("p"),al=t("For example, create a basic "),vs=n("a"),Te=n("code"),tl=t("Resize"),ll=t(" function:"),ka=c(),u(ys.$$.fragment),Ca=c(),$=n("p"),nl=t("Now use the "),ae=n("a"),ol=t("map()"),rl=t(" function to resize the entire dataset, and set "),Oe=n("code"),pl=t("batched=True"),il=t(" to speed up the process by accepting batches of examples. The transform returns "),ze=n("code"),cl=t("pixel_values"),ml=t(" as a cacheable "),Le=n("code"),hl=t("PIL.Image"),fl=t(" object:"),Da=c(),u(ws.$$.fragment),Aa=c(),ss=n("p"),ul=t("The cache file saves time because you don\u2019t have to execute the same transform twice. The "),te=n("a"),dl=t("map()"),gl=t(" function is best for operations you only run once per training - like resizing an image - instead of using it for operations executed for each epoch, like data augmentations."),Pa=c(),$s=n("p"),le=n("a"),bl=t("map()"),xl=t(" takes up some memory, but you can reduce its memory requirements with the following parameters:"),Ia=c(),es=n("ul"),ne=n("li"),oe=n("a"),qe=n("code"),jl=t("batch_size"),_l=t(" determines the number of examples that are processed in one call to the transform function."),vl=c(),re=n("li"),pe=n("a"),Be=n("code"),yl=t("writer_batch_size"),wl=t(" determines the number of processed examples that are kept in memory before they are stored away."),Ta=c(),as=n("p"),$l=t("Both parameter values default to 1000, which can be expensive if you are storing images. Lower these values to use less memory when you use "),ie=n("a"),El=t("map()"),kl=t("."),Oa=c(),U=n("h2"),ts=n("a"),Ge=n("span"),u(Es.$$.fragment),Cl=c(),Re=n("span"),Dl=t("Data augmentation"),za=c(),ce=n("p"),Al=t("\u{1F917} Datasets can apply data augmentations from any library or package to your dataset."),La=c(),J=n("h3"),ls=n("a"),Ne=n("span"),u(ks.$$.fragment),Pl=c(),Fe=n("span"),Il=t("Image Classification"),qa=c(),ns=n("p"),Tl=t("First let\u2019s see how you can transform image classification datasets. This guide will use the transforms from "),Cs=n("a"),Ol=t("torchvision"),zl=t("."),Ba=c(),u(os.$$.fragment),Ga=c(),rs=n("p"),Ll=t("As an example, try to apply a "),Ds=n("a"),Se=n("code"),ql=t("ColorJitter"),Bl=t(" transform to change the color properties of the image randomly:"),Ra=c(),u(As.$$.fragment),Na=c(),ps=n("p"),Gl=t("Create a function to apply the "),Me=n("code"),Rl=t("ColorJitter"),Nl=t(" transform to an image:"),Fa=c(),u(Ps.$$.fragment),Sa=c(),is=n("p"),Fl=t("Use the "),me=n("a"),Sl=t("set_transform()"),Ml=t(" function to apply the transform on-the-fly which consumes less disk space. This function is useful if you only need to access the examples once:"),Ma=c(),u(Is.$$.fragment),Ua=c(),cs=n("p"),Ul=t("Now you can take a look at the augmented image by indexing into the "),Ue=n("code"),Jl=t("pixel_values"),Hl=t(":"),Ja=c(),u(Ts.$$.fragment),Ha=c(),H=n("div"),he=n("img"),Vl=c(),fe=n("img"),Va=c(),V=n("h3"),ms=n("a"),Je=n("span"),u(Os.$$.fragment),Yl=c(),He=n("span"),Kl=t("Object Detection"),Ya=c(),z=n("p"),Wl=t("Object detection models identify something in an image, and object detection datasets are used for applications such as autonomous driving and detecting natural hazards like wildfire. This guide will show you how to apply transformations to an object detection dataset following the "),zs=n("a"),Ql=t("tutorial"),Xl=t(" from "),Ls=n("a"),Zl=t("Albumentations"),sn=t("."),Ka=c(),L=n("p"),en=t("To run these examples, make sure you have up-to-date versions of "),Ve=n("code"),an=t("albumentations"),tn=t(" and "),Ye=n("code"),ln=t("cv2"),nn=t(" installed:"),Wa=c(),u(qs.$$.fragment),Qa=c(),hs=n("p"),on=t("In this example, you\u2019ll use the "),Bs=n("a"),Ke=n("code"),rn=t("cppe-5"),pn=t(" dataset for identifying medical personal protective equipment (PPE) in the context of the COVID-19 pandemic."),Xa=c(),ue=n("p"),cn=t("Load the dataset and take a look at an example:"),Za=c(),u(Gs.$$.fragment),st=c(),de=n("p"),mn=t("The dataset has the following fields:"),et=c(),E=n("ul"),ge=n("li"),We=n("code"),hn=t("image"),fn=t(": PIL.Image.Image object containing the image."),un=c(),be=n("li"),Qe=n("code"),dn=t("image_id"),gn=t(": The image ID."),bn=c(),xe=n("li"),Xe=n("code"),xn=t("height"),jn=t(": The image height."),_n=c(),je=n("li"),Ze=n("code"),vn=t("width"),yn=t(": The image width."),wn=c(),Rs=n("li"),sa=n("code"),$n=t("objects"),En=t(": A dictionary containing bounding box metadata for the objects in the image:"),I=n("ul"),_e=n("li"),ea=n("code"),kn=t("id"),Cn=t(": The annotation id."),Dn=c(),ve=n("li"),aa=n("code"),An=t("area"),Pn=t(": The area of the bounding box."),In=c(),fs=n("li"),ta=n("code"),Tn=t("bbox"),On=t(": The object\u2019s bounding box (in the "),Ns=n("a"),zn=t("coco"),Ln=t(" format)."),qn=c(),v=n("li"),la=n("code"),Bn=t("category"),Gn=t(": The object\u2019s category, with possible values including "),na=n("code"),Rn=t("Coverall (0)"),Nn=t(", "),oa=n("code"),Fn=t("Face_Shield (1)"),Sn=t(", "),ra=n("code"),Mn=t("Gloves (2)"),Un=t(", "),pa=n("code"),Jn=t("Goggles (3)"),Hn=t(" and "),ia=n("code"),Vn=t("Mask (4)"),Yn=t("."),at=c(),q=n("p"),Kn=t("You can visualize the "),ca=n("code"),Wn=t("bboxes"),Qn=t(" on the image using some internal torch utilities. To do that, you will need to reference the "),ye=n("a"),Xn=t("ClassLabel"),Zn=t(" feature associated with the category IDs so you can look up the string labels:"),tt=c(),u(Fs.$$.fragment),lt=c(),Ss=n("div"),ma=n("img"),nt=c(),B=n("p"),so=t("With "),ha=n("code"),eo=t("albumentations"),ao=t(", you can apply transforms that will affect the image while also updating the "),fa=n("code"),to=t("bboxes"),lo=t(" accordingly. In this case, the image is resized to (480, 480), flipped horizontally, and brightened."),ot=c(),Ms=n("p"),ua=n("code"),no=t("albumentations"),oo=t(" expects the image to be in BGR format, not RGB, so you\u2019ll have to convert the image before applying the transform."),rt=c(),u(Us.$$.fragment),pt=c(),us=n("p"),ro=t("Now when you visualize the result, the image should be flipped, but the "),da=n("code"),po=t("bboxes"),io=t(" should still be in the right places."),it=c(),u(Js.$$.fragment),ct=c(),Hs=n("div"),ga=n("img"),mt=c(),we=n("p"),co=t("Create a function to apply the transform to a batch of examples:"),ht=c(),u(Vs.$$.fragment),ft=c(),ds=n("p"),mo=t("Use the "),$e=n("a"),ho=t("set_transform()"),fo=t(" function to apply the transform on-the-fly which consumes less disk space. The randomness of data augmentation may return a different image if you access the same example twice. It is especially useful when training a model for several epochs."),ut=c(),u(Ys.$$.fragment),dt=c(),Ee=n("p"),uo=t("You can verify the transform works by visualizing the 10th example:"),gt=c(),u(Ks.$$.fragment),bt=c(),Ws=n("div"),ba=n("img"),this.h()},l(s){const p=ep('[data-svelte="svelte-1phssyn"]',document.head);f=o(p,"META",{name:!0,content:!0}),p.forEach(a),Y=m(s),_=o(s,"H1",{class:!0});var Qs=r(_);D=o(Qs,"A",{id:!0,class:!0,href:!0});var Io=r(D);N=o(Io,"SPAN",{});var To=r(N);d(y.$$.fragment,To),To.forEach(a),Io.forEach(a),bs=m(Qs),F=o(Qs,"SPAN",{});var Oo=r(F);A=l(Oo,"Process image data"),Oo.forEach(a),Qs.forEach(a),K=m(s),T=o(s,"P",{});var zo=r(T);S=l(zo,"This guide shows specific methods for processing image datasets. Learn how to:"),zo.forEach(a),w=m(s),P=o(s,"UL",{});var jt=r(P);O=o(jt,"LI",{});var _t=r(O);Xs=l(_t,"Use "),Zs=o(_t,"A",{href:!0});var Lo=r(Zs);St=l(Lo,"map()"),Lo.forEach(a),Mt=l(_t," with image dataset."),_t.forEach(a),Ut=m(jt),xs=o(jt,"LI",{});var vt=r(xs);Jt=l(vt,"Apply data augmentations to your dataset with "),se=o(vt,"A",{href:!0});var qo=r(se);Ht=l(qo,"set_transform()"),qo.forEach(a),Vt=l(vt,"."),vt.forEach(a),jt.forEach(a),ya=m(s),W=o(s,"P",{});var yt=r(W);Yt=l(yt,"For a guide on how to process any type of dataset, take a look at the "),js=o(yt,"A",{class:!0,href:!0});var Bo=r(js);Kt=l(Bo,"general process guide"),Bo.forEach(a),Wt=l(yt,"."),yt.forEach(a),wa=m(s),M=o(s,"H2",{class:!0});var wt=r(M);Q=o(wt,"A",{id:!0,class:!0,href:!0});var Go=r(Q);Pe=o(Go,"SPAN",{});var Ro=r(Pe);d(_s.$$.fragment,Ro),Ro.forEach(a),Go.forEach(a),Qt=m(wt),Ie=o(wt,"SPAN",{});var No=r(Ie);Xt=l(No,"Map"),No.forEach(a),wt.forEach(a),$a=m(s),X=o(s,"P",{});var $t=r(X);Zt=l($t,"The "),ee=o($t,"A",{href:!0});var Fo=r(ee);sl=l(Fo,"map()"),Fo.forEach(a),el=l($t," function can apply transforms over an entire dataset."),$t.forEach(a),Ea=m(s),Z=o(s,"P",{});var Et=r(Z);al=l(Et,"For example, create a basic "),vs=o(Et,"A",{href:!0,rel:!0});var So=r(vs);Te=o(So,"CODE",{});var Mo=r(Te);tl=l(Mo,"Resize"),Mo.forEach(a),So.forEach(a),ll=l(Et," function:"),Et.forEach(a),ka=m(s),d(ys.$$.fragment,s),Ca=m(s),$=o(s,"P",{});var G=r($);nl=l(G,"Now use the "),ae=o(G,"A",{href:!0});var Uo=r(ae);ol=l(Uo,"map()"),Uo.forEach(a),rl=l(G," function to resize the entire dataset, and set "),Oe=o(G,"CODE",{});var Jo=r(Oe);pl=l(Jo,"batched=True"),Jo.forEach(a),il=l(G," to speed up the process by accepting batches of examples. The transform returns "),ze=o(G,"CODE",{});var Ho=r(ze);cl=l(Ho,"pixel_values"),Ho.forEach(a),ml=l(G," as a cacheable "),Le=o(G,"CODE",{});var Vo=r(Le);hl=l(Vo,"PIL.Image"),Vo.forEach(a),fl=l(G," object:"),G.forEach(a),Da=m(s),d(ws.$$.fragment,s),Aa=m(s),ss=o(s,"P",{});var kt=r(ss);ul=l(kt,"The cache file saves time because you don\u2019t have to execute the same transform twice. The "),te=o(kt,"A",{href:!0});var Yo=r(te);dl=l(Yo,"map()"),Yo.forEach(a),gl=l(kt," function is best for operations you only run once per training - like resizing an image - instead of using it for operations executed for each epoch, like data augmentations."),kt.forEach(a),Pa=m(s),$s=o(s,"P",{});var go=r($s);le=o(go,"A",{href:!0});var Ko=r(le);bl=l(Ko,"map()"),Ko.forEach(a),xl=l(go," takes up some memory, but you can reduce its memory requirements with the following parameters:"),go.forEach(a),Ia=m(s),es=o(s,"UL",{});var Ct=r(es);ne=o(Ct,"LI",{});var bo=r(ne);oe=o(bo,"A",{href:!0});var Wo=r(oe);qe=o(Wo,"CODE",{});var Qo=r(qe);jl=l(Qo,"batch_size"),Qo.forEach(a),Wo.forEach(a),_l=l(bo," determines the number of examples that are processed in one call to the transform function."),bo.forEach(a),vl=m(Ct),re=o(Ct,"LI",{});var xo=r(re);pe=o(xo,"A",{href:!0});var Xo=r(pe);Be=o(Xo,"CODE",{});var Zo=r(Be);yl=l(Zo,"writer_batch_size"),Zo.forEach(a),Xo.forEach(a),wl=l(xo," determines the number of processed examples that are kept in memory before they are stored away."),xo.forEach(a),Ct.forEach(a),Ta=m(s),as=o(s,"P",{});var Dt=r(as);$l=l(Dt,"Both parameter values default to 1000, which can be expensive if you are storing images. Lower these values to use less memory when you use "),ie=o(Dt,"A",{href:!0});var sr=r(ie);El=l(sr,"map()"),sr.forEach(a),kl=l(Dt,"."),Dt.forEach(a),Oa=m(s),U=o(s,"H2",{class:!0});var At=r(U);ts=o(At,"A",{id:!0,class:!0,href:!0});var er=r(ts);Ge=o(er,"SPAN",{});var ar=r(Ge);d(Es.$$.fragment,ar),ar.forEach(a),er.forEach(a),Cl=m(At),Re=o(At,"SPAN",{});var tr=r(Re);Dl=l(tr,"Data augmentation"),tr.forEach(a),At.forEach(a),za=m(s),ce=o(s,"P",{});var lr=r(ce);Al=l(lr,"\u{1F917} Datasets can apply data augmentations from any library or package to your dataset."),lr.forEach(a),La=m(s),J=o(s,"H3",{class:!0});var Pt=r(J);ls=o(Pt,"A",{id:!0,class:!0,href:!0});var nr=r(ls);Ne=o(nr,"SPAN",{});var or=r(Ne);d(ks.$$.fragment,or),or.forEach(a),nr.forEach(a),Pl=m(Pt),Fe=o(Pt,"SPAN",{});var rr=r(Fe);Il=l(rr,"Image Classification"),rr.forEach(a),Pt.forEach(a),qa=m(s),ns=o(s,"P",{});var It=r(ns);Tl=l(It,"First let\u2019s see how you can transform image classification datasets. This guide will use the transforms from "),Cs=o(It,"A",{href:!0,rel:!0});var pr=r(Cs);Ol=l(pr,"torchvision"),pr.forEach(a),zl=l(It,"."),It.forEach(a),Ba=m(s),d(os.$$.fragment,s),Ga=m(s),rs=o(s,"P",{});var Tt=r(rs);Ll=l(Tt,"As an example, try to apply a "),Ds=o(Tt,"A",{href:!0,rel:!0});var ir=r(Ds);Se=o(ir,"CODE",{});var cr=r(Se);ql=l(cr,"ColorJitter"),cr.forEach(a),ir.forEach(a),Bl=l(Tt," transform to change the color properties of the image randomly:"),Tt.forEach(a),Ra=m(s),d(As.$$.fragment,s),Na=m(s),ps=o(s,"P",{});var Ot=r(ps);Gl=l(Ot,"Create a function to apply the "),Me=o(Ot,"CODE",{});var mr=r(Me);Rl=l(mr,"ColorJitter"),mr.forEach(a),Nl=l(Ot," transform to an image:"),Ot.forEach(a),Fa=m(s),d(Ps.$$.fragment,s),Sa=m(s),is=o(s,"P",{});var zt=r(is);Fl=l(zt,"Use the "),me=o(zt,"A",{href:!0});var hr=r(me);Sl=l(hr,"set_transform()"),hr.forEach(a),Ml=l(zt," function to apply the transform on-the-fly which consumes less disk space. This function is useful if you only need to access the examples once:"),zt.forEach(a),Ma=m(s),d(Is.$$.fragment,s),Ua=m(s),cs=o(s,"P",{});var Lt=r(cs);Ul=l(Lt,"Now you can take a look at the augmented image by indexing into the "),Ue=o(Lt,"CODE",{});var fr=r(Ue);Jl=l(fr,"pixel_values"),fr.forEach(a),Hl=l(Lt,":"),Lt.forEach(a),Ja=m(s),d(Ts.$$.fragment,s),Ha=m(s),H=o(s,"DIV",{class:!0});var qt=r(H);he=o(qt,"IMG",{class:!0,src:!0}),Vl=m(qt),fe=o(qt,"IMG",{class:!0,src:!0}),qt.forEach(a),Va=m(s),V=o(s,"H3",{class:!0});var Bt=r(V);ms=o(Bt,"A",{id:!0,class:!0,href:!0});var ur=r(ms);Je=o(ur,"SPAN",{});var dr=r(Je);d(Os.$$.fragment,dr),dr.forEach(a),ur.forEach(a),Yl=m(Bt),He=o(Bt,"SPAN",{});var gr=r(He);Kl=l(gr,"Object Detection"),gr.forEach(a),Bt.forEach(a),Ya=m(s),z=o(s,"P",{});var ke=r(z);Wl=l(ke,"Object detection models identify something in an image, and object detection datasets are used for applications such as autonomous driving and detecting natural hazards like wildfire. This guide will show you how to apply transformations to an object detection dataset following the "),zs=o(ke,"A",{href:!0,rel:!0});var br=r(zs);Ql=l(br,"tutorial"),br.forEach(a),Xl=l(ke," from "),Ls=o(ke,"A",{href:!0,rel:!0});var xr=r(Ls);Zl=l(xr,"Albumentations"),xr.forEach(a),sn=l(ke,"."),ke.forEach(a),Ka=m(s),L=o(s,"P",{});var Ce=r(L);en=l(Ce,"To run these examples, make sure you have up-to-date versions of "),Ve=o(Ce,"CODE",{});var jr=r(Ve);an=l(jr,"albumentations"),jr.forEach(a),tn=l(Ce," and "),Ye=o(Ce,"CODE",{});var _r=r(Ye);ln=l(_r,"cv2"),_r.forEach(a),nn=l(Ce," installed:"),Ce.forEach(a),Wa=m(s),d(qs.$$.fragment,s),Qa=m(s),hs=o(s,"P",{});var Gt=r(hs);on=l(Gt,"In this example, you\u2019ll use the "),Bs=o(Gt,"A",{href:!0,rel:!0});var vr=r(Bs);Ke=o(vr,"CODE",{});var yr=r(Ke);rn=l(yr,"cppe-5"),yr.forEach(a),vr.forEach(a),pn=l(Gt," dataset for identifying medical personal protective equipment (PPE) in the context of the COVID-19 pandemic."),Gt.forEach(a),Xa=m(s),ue=o(s,"P",{});var wr=r(ue);cn=l(wr,"Load the dataset and take a look at an example:"),wr.forEach(a),Za=m(s),d(Gs.$$.fragment,s),st=m(s),de=o(s,"P",{});var $r=r(de);mn=l($r,"The dataset has the following fields:"),$r.forEach(a),et=m(s),E=o(s,"UL",{});var R=r(E);ge=o(R,"LI",{});var jo=r(ge);We=o(jo,"CODE",{});var Er=r(We);hn=l(Er,"image"),Er.forEach(a),fn=l(jo,": PIL.Image.Image object containing the image."),jo.forEach(a),un=m(R),be=o(R,"LI",{});var _o=r(be);Qe=o(_o,"CODE",{});var kr=r(Qe);dn=l(kr,"image_id"),kr.forEach(a),gn=l(_o,": The image ID."),_o.forEach(a),bn=m(R),xe=o(R,"LI",{});var vo=r(xe);Xe=o(vo,"CODE",{});var Cr=r(Xe);xn=l(Cr,"height"),Cr.forEach(a),jn=l(vo,": The image height."),vo.forEach(a),_n=m(R),je=o(R,"LI",{});var yo=r(je);Ze=o(yo,"CODE",{});var Dr=r(Ze);vn=l(Dr,"width"),Dr.forEach(a),yn=l(yo,": The image width."),yo.forEach(a),wn=m(R),Rs=o(R,"LI",{});var Rt=r(Rs);sa=o(Rt,"CODE",{});var Ar=r(sa);$n=l(Ar,"objects"),Ar.forEach(a),En=l(Rt,": A dictionary containing bounding box metadata for the objects in the image:"),I=o(Rt,"UL",{});var gs=r(I);_e=o(gs,"LI",{});var wo=r(_e);ea=o(wo,"CODE",{});var Pr=r(ea);kn=l(Pr,"id"),Pr.forEach(a),Cn=l(wo,": The annotation id."),wo.forEach(a),Dn=m(gs),ve=o(gs,"LI",{});var $o=r(ve);aa=o($o,"CODE",{});var Ir=r(aa);An=l(Ir,"area"),Ir.forEach(a),Pn=l($o,": The area of the bounding box."),$o.forEach(a),In=m(gs),fs=o(gs,"LI",{});var xa=r(fs);ta=o(xa,"CODE",{});var Tr=r(ta);Tn=l(Tr,"bbox"),Tr.forEach(a),On=l(xa,": The object\u2019s bounding box (in the "),Ns=o(xa,"A",{href:!0,rel:!0});var Or=r(Ns);zn=l(Or,"coco"),Or.forEach(a),Ln=l(xa," format)."),xa.forEach(a),qn=m(gs),v=o(gs,"LI",{});var C=r(v);la=o(C,"CODE",{});var zr=r(la);Bn=l(zr,"category"),zr.forEach(a),Gn=l(C,": The object\u2019s category, with possible values including "),na=o(C,"CODE",{});var Lr=r(na);Rn=l(Lr,"Coverall (0)"),Lr.forEach(a),Nn=l(C,", "),oa=o(C,"CODE",{});var qr=r(oa);Fn=l(qr,"Face_Shield (1)"),qr.forEach(a),Sn=l(C,", "),ra=o(C,"CODE",{});var Br=r(ra);Mn=l(Br,"Gloves (2)"),Br.forEach(a),Un=l(C,", "),pa=o(C,"CODE",{});var Gr=r(pa);Jn=l(Gr,"Goggles (3)"),Gr.forEach(a),Hn=l(C," and "),ia=o(C,"CODE",{});var Rr=r(ia);Vn=l(Rr,"Mask (4)"),Rr.forEach(a),Yn=l(C,"."),C.forEach(a),gs.forEach(a),Rt.forEach(a),R.forEach(a),at=m(s),q=o(s,"P",{});var De=r(q);Kn=l(De,"You can visualize the "),ca=o(De,"CODE",{});var Nr=r(ca);Wn=l(Nr,"bboxes"),Nr.forEach(a),Qn=l(De," on the image using some internal torch utilities. To do that, you will need to reference the "),ye=o(De,"A",{href:!0});var Fr=r(ye);Xn=l(Fr,"ClassLabel"),Fr.forEach(a),Zn=l(De," feature associated with the category IDs so you can look up the string labels:"),De.forEach(a),tt=m(s),d(Fs.$$.fragment,s),lt=m(s),Ss=o(s,"DIV",{class:!0});var Sr=r(Ss);ma=o(Sr,"IMG",{src:!0}),Sr.forEach(a),nt=m(s),B=o(s,"P",{});var Ae=r(B);so=l(Ae,"With "),ha=o(Ae,"CODE",{});var Mr=r(ha);eo=l(Mr,"albumentations"),Mr.forEach(a),ao=l(Ae,", you can apply transforms that will affect the image while also updating the "),fa=o(Ae,"CODE",{});var Ur=r(fa);to=l(Ur,"bboxes"),Ur.forEach(a),lo=l(Ae," accordingly. In this case, the image is resized to (480, 480), flipped horizontally, and brightened."),Ae.forEach(a),ot=m(s),Ms=o(s,"P",{});var Eo=r(Ms);ua=o(Eo,"CODE",{});var Jr=r(ua);no=l(Jr,"albumentations"),Jr.forEach(a),oo=l(Eo," expects the image to be in BGR format, not RGB, so you\u2019ll have to convert the image before applying the transform."),Eo.forEach(a),rt=m(s),d(Us.$$.fragment,s),pt=m(s),us=o(s,"P",{});var Nt=r(us);ro=l(Nt,"Now when you visualize the result, the image should be flipped, but the "),da=o(Nt,"CODE",{});var Hr=r(da);po=l(Hr,"bboxes"),Hr.forEach(a),io=l(Nt," should still be in the right places."),Nt.forEach(a),it=m(s),d(Js.$$.fragment,s),ct=m(s),Hs=o(s,"DIV",{class:!0});var Vr=r(Hs);ga=o(Vr,"IMG",{src:!0}),Vr.forEach(a),mt=m(s),we=o(s,"P",{});var Yr=r(we);co=l(Yr,"Create a function to apply the transform to a batch of examples:"),Yr.forEach(a),ht=m(s),d(Vs.$$.fragment,s),ft=m(s),ds=o(s,"P",{});var Ft=r(ds);mo=l(Ft,"Use the "),$e=o(Ft,"A",{href:!0});var Kr=r($e);ho=l(Kr,"set_transform()"),Kr.forEach(a),fo=l(Ft," function to apply the transform on-the-fly which consumes less disk space. The randomness of data augmentation may return a different image if you access the same example twice. It is especially useful when training a model for several epochs."),Ft.forEach(a),ut=m(s),d(Ys.$$.fragment,s),dt=m(s),Ee=o(s,"P",{});var Wr=r(Ee);uo=l(Wr,"You can verify the transform works by visualizing the 10th example:"),Wr.forEach(a),gt=m(s),d(Ks.$$.fragment,s),bt=m(s),Ws=o(s,"DIV",{class:!0});var Qr=r(Ws);ba=o(Qr,"IMG",{src:!0}),Qr.forEach(a),this.h()},h(){h(f,"name","hf:doc:metadata"),h(f,"content",JSON.stringify(op)),h(D,"id","process-image-data"),h(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(D,"href","#process-image-data"),h(_,"class","relative group"),h(Zs,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.map"),h(se,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.set_transform"),h(js,"class","underline decoration-sky-400 decoration-2 font-semibold"),h(js,"href","./process"),h(Q,"id","map"),h(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Q,"href","#map"),h(M,"class","relative group"),h(ee,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.map"),h(vs,"href","https://pytorch.org/vision/stable/generated/torchvision.transforms.Resize.html"),h(vs,"rel","nofollow"),h(ae,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.map"),h(te,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.map"),h(le,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.map"),h(oe,"href","./package_reference/main_classes#datasets.DatasetDict.map.batch_size"),h(pe,"href","./package_reference/main_classes#datasets.DatasetDict.map.writer_batch_size"),h(ie,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.map"),h(ts,"id","data-augmentation"),h(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ts,"href","#data-augmentation"),h(U,"class","relative group"),h(ls,"id","image-classification"),h(ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ls,"href","#image-classification"),h(J,"class","relative group"),h(Cs,"href","https://pytorch.org/vision/stable/transforms.html"),h(Cs,"rel","nofollow"),h(Ds,"href","https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.ColorJitter"),h(Ds,"rel","nofollow"),h(me,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.set_transform"),h(he,"class","block dark:hidden"),ja(he.src,ko="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/image_process_jitter.png")||h(he,"src",ko),h(fe,"class","hidden dark:block"),ja(fe.src,Co="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/image_process_jitter.png")||h(fe,"src",Co),h(H,"class","flex justify-center"),h(ms,"id","object-detection"),h(ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ms,"href","#object-detection"),h(V,"class","relative group"),h(zs,"href","https://albumentations.ai/docs/examples/example_bboxes/"),h(zs,"rel","nofollow"),h(Ls,"href","https://albumentations.ai/docs/"),h(Ls,"rel","nofollow"),h(Bs,"href","https://huggingface.co/datasets/cppe-5"),h(Bs,"rel","nofollow"),h(Ns,"href","https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco"),h(Ns,"rel","nofollow"),h(ye,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.ClassLabel"),ja(ma.src,Do="https://huggingface.co/datasets/nateraw/documentation-images/resolve/main/visualize_detection_example.png")||h(ma,"src",Do),h(Ss,"class","flex justify-center"),ja(ga.src,Ao="https://huggingface.co/datasets/nateraw/documentation-images/resolve/main/visualize_detection_example_transformed.png")||h(ga,"src",Ao),h(Hs,"class","flex justify-center"),h($e,"href","/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.set_transform"),ja(ba.src,Po="https://huggingface.co/datasets/nateraw/documentation-images/resolve/main/visualize_detection_example_transformed_2.png")||h(ba,"src",Po),h(Ws,"class","flex justify-center")},m(s,p){e(document.head,f),i(s,Y,p),i(s,_,p),e(_,D),e(D,N),g(y,N,null),e(_,bs),e(_,F),e(F,A),i(s,K,p),i(s,T,p),e(T,S),i(s,w,p),i(s,P,p),e(P,O),e(O,Xs),e(O,Zs),e(Zs,St),e(O,Mt),e(P,Ut),e(P,xs),e(xs,Jt),e(xs,se),e(se,Ht),e(xs,Vt),i(s,ya,p),i(s,W,p),e(W,Yt),e(W,js),e(js,Kt),e(W,Wt),i(s,wa,p),i(s,M,p),e(M,Q),e(Q,Pe),g(_s,Pe,null),e(M,Qt),e(M,Ie),e(Ie,Xt),i(s,$a,p),i(s,X,p),e(X,Zt),e(X,ee),e(ee,sl),e(X,el),i(s,Ea,p),i(s,Z,p),e(Z,al),e(Z,vs),e(vs,Te),e(Te,tl),e(Z,ll),i(s,ka,p),g(ys,s,p),i(s,Ca,p),i(s,$,p),e($,nl),e($,ae),e(ae,ol),e($,rl),e($,Oe),e(Oe,pl),e($,il),e($,ze),e(ze,cl),e($,ml),e($,Le),e(Le,hl),e($,fl),i(s,Da,p),g(ws,s,p),i(s,Aa,p),i(s,ss,p),e(ss,ul),e(ss,te),e(te,dl),e(ss,gl),i(s,Pa,p),i(s,$s,p),e($s,le),e(le,bl),e($s,xl),i(s,Ia,p),i(s,es,p),e(es,ne),e(ne,oe),e(oe,qe),e(qe,jl),e(ne,_l),e(es,vl),e(es,re),e(re,pe),e(pe,Be),e(Be,yl),e(re,wl),i(s,Ta,p),i(s,as,p),e(as,$l),e(as,ie),e(ie,El),e(as,kl),i(s,Oa,p),i(s,U,p),e(U,ts),e(ts,Ge),g(Es,Ge,null),e(U,Cl),e(U,Re),e(Re,Dl),i(s,za,p),i(s,ce,p),e(ce,Al),i(s,La,p),i(s,J,p),e(J,ls),e(ls,Ne),g(ks,Ne,null),e(J,Pl),e(J,Fe),e(Fe,Il),i(s,qa,p),i(s,ns,p),e(ns,Tl),e(ns,Cs),e(Cs,Ol),e(ns,zl),i(s,Ba,p),g(os,s,p),i(s,Ga,p),i(s,rs,p),e(rs,Ll),e(rs,Ds),e(Ds,Se),e(Se,ql),e(rs,Bl),i(s,Ra,p),g(As,s,p),i(s,Na,p),i(s,ps,p),e(ps,Gl),e(ps,Me),e(Me,Rl),e(ps,Nl),i(s,Fa,p),g(Ps,s,p),i(s,Sa,p),i(s,is,p),e(is,Fl),e(is,me),e(me,Sl),e(is,Ml),i(s,Ma,p),g(Is,s,p),i(s,Ua,p),i(s,cs,p),e(cs,Ul),e(cs,Ue),e(Ue,Jl),e(cs,Hl),i(s,Ja,p),g(Ts,s,p),i(s,Ha,p),i(s,H,p),e(H,he),e(H,Vl),e(H,fe),i(s,Va,p),i(s,V,p),e(V,ms),e(ms,Je),g(Os,Je,null),e(V,Yl),e(V,He),e(He,Kl),i(s,Ya,p),i(s,z,p),e(z,Wl),e(z,zs),e(zs,Ql),e(z,Xl),e(z,Ls),e(Ls,Zl),e(z,sn),i(s,Ka,p),i(s,L,p),e(L,en),e(L,Ve),e(Ve,an),e(L,tn),e(L,Ye),e(Ye,ln),e(L,nn),i(s,Wa,p),g(qs,s,p),i(s,Qa,p),i(s,hs,p),e(hs,on),e(hs,Bs),e(Bs,Ke),e(Ke,rn),e(hs,pn),i(s,Xa,p),i(s,ue,p),e(ue,cn),i(s,Za,p),g(Gs,s,p),i(s,st,p),i(s,de,p),e(de,mn),i(s,et,p),i(s,E,p),e(E,ge),e(ge,We),e(We,hn),e(ge,fn),e(E,un),e(E,be),e(be,Qe),e(Qe,dn),e(be,gn),e(E,bn),e(E,xe),e(xe,Xe),e(Xe,xn),e(xe,jn),e(E,_n),e(E,je),e(je,Ze),e(Ze,vn),e(je,yn),e(E,wn),e(E,Rs),e(Rs,sa),e(sa,$n),e(Rs,En),e(Rs,I),e(I,_e),e(_e,ea),e(ea,kn),e(_e,Cn),e(I,Dn),e(I,ve),e(ve,aa),e(aa,An),e(ve,Pn),e(I,In),e(I,fs),e(fs,ta),e(ta,Tn),e(fs,On),e(fs,Ns),e(Ns,zn),e(fs,Ln),e(I,qn),e(I,v),e(v,la),e(la,Bn),e(v,Gn),e(v,na),e(na,Rn),e(v,Nn),e(v,oa),e(oa,Fn),e(v,Sn),e(v,ra),e(ra,Mn),e(v,Un),e(v,pa),e(pa,Jn),e(v,Hn),e(v,ia),e(ia,Vn),e(v,Yn),i(s,at,p),i(s,q,p),e(q,Kn),e(q,ca),e(ca,Wn),e(q,Qn),e(q,ye),e(ye,Xn),e(q,Zn),i(s,tt,p),g(Fs,s,p),i(s,lt,p),i(s,Ss,p),e(Ss,ma),i(s,nt,p),i(s,B,p),e(B,so),e(B,ha),e(ha,eo),e(B,ao),e(B,fa),e(fa,to),e(B,lo),i(s,ot,p),i(s,Ms,p),e(Ms,ua),e(ua,no),e(Ms,oo),i(s,rt,p),g(Us,s,p),i(s,pt,p),i(s,us,p),e(us,ro),e(us,da),e(da,po),e(us,io),i(s,it,p),g(Js,s,p),i(s,ct,p),i(s,Hs,p),e(Hs,ga),i(s,mt,p),i(s,we,p),e(we,co),i(s,ht,p),g(Vs,s,p),i(s,ft,p),i(s,ds,p),e(ds,mo),e(ds,$e),e($e,ho),e(ds,fo),i(s,ut,p),g(Ys,s,p),i(s,dt,p),i(s,Ee,p),e(Ee,uo),i(s,gt,p),g(Ks,s,p),i(s,bt,p),i(s,Ws,p),e(Ws,ba),xt=!0},p(s,[p]){const Qs={};p&2&&(Qs.$$scope={dirty:p,ctx:s}),os.$set(Qs)},i(s){xt||(b(y.$$.fragment,s),b(_s.$$.fragment,s),b(ys.$$.fragment,s),b(ws.$$.fragment,s),b(Es.$$.fragment,s),b(ks.$$.fragment,s),b(os.$$.fragment,s),b(As.$$.fragment,s),b(Ps.$$.fragment,s),b(Is.$$.fragment,s),b(Ts.$$.fragment,s),b(Os.$$.fragment,s),b(qs.$$.fragment,s),b(Gs.$$.fragment,s),b(Fs.$$.fragment,s),b(Us.$$.fragment,s),b(Js.$$.fragment,s),b(Vs.$$.fragment,s),b(Ys.$$.fragment,s),b(Ks.$$.fragment,s),xt=!0)},o(s){x(y.$$.fragment,s),x(_s.$$.fragment,s),x(ys.$$.fragment,s),x(ws.$$.fragment,s),x(Es.$$.fragment,s),x(ks.$$.fragment,s),x(os.$$.fragment,s),x(As.$$.fragment,s),x(Ps.$$.fragment,s),x(Is.$$.fragment,s),x(Ts.$$.fragment,s),x(Os.$$.fragment,s),x(qs.$$.fragment,s),x(Gs.$$.fragment,s),x(Fs.$$.fragment,s),x(Us.$$.fragment,s),x(Js.$$.fragment,s),x(Vs.$$.fragment,s),x(Ys.$$.fragment,s),x(Ks.$$.fragment,s),xt=!1},d(s){a(f),s&&a(Y),s&&a(_),j(y),s&&a(K),s&&a(T),s&&a(w),s&&a(P),s&&a(ya),s&&a(W),s&&a(wa),s&&a(M),j(_s),s&&a($a),s&&a(X),s&&a(Ea),s&&a(Z),s&&a(ka),j(ys,s),s&&a(Ca),s&&a($),s&&a(Da),j(ws,s),s&&a(Aa),s&&a(ss),s&&a(Pa),s&&a($s),s&&a(Ia),s&&a(es),s&&a(Ta),s&&a(as),s&&a(Oa),s&&a(U),j(Es),s&&a(za),s&&a(ce),s&&a(La),s&&a(J),j(ks),s&&a(qa),s&&a(ns),s&&a(Ba),j(os,s),s&&a(Ga),s&&a(rs),s&&a(Ra),j(As,s),s&&a(Na),s&&a(ps),s&&a(Fa),j(Ps,s),s&&a(Sa),s&&a(is),s&&a(Ma),j(Is,s),s&&a(Ua),s&&a(cs),s&&a(Ja),j(Ts,s),s&&a(Ha),s&&a(H),s&&a(Va),s&&a(V),j(Os),s&&a(Ya),s&&a(z),s&&a(Ka),s&&a(L),s&&a(Wa),j(qs,s),s&&a(Qa),s&&a(hs),s&&a(Xa),s&&a(ue),s&&a(Za),j(Gs,s),s&&a(st),s&&a(de),s&&a(et),s&&a(E),s&&a(at),s&&a(q),s&&a(tt),j(Fs,s),s&&a(lt),s&&a(Ss),s&&a(nt),s&&a(B),s&&a(ot),s&&a(Ms),s&&a(rt),j(Us,s),s&&a(pt),s&&a(us),s&&a(it),j(Js,s),s&&a(ct),s&&a(Hs),s&&a(mt),s&&a(we),s&&a(ht),j(Vs,s),s&&a(ft),s&&a(ds),s&&a(ut),j(Ys,s),s&&a(dt),s&&a(Ee),s&&a(gt),j(Ks,s),s&&a(bt),s&&a(Ws)}}}const op={local:"process-image-data",sections:[{local:"map",title:"Map"},{local:"data-augmentation",sections:[{local:"image-classification",title:"Image Classification"},{local:"object-detection",title:"Object Detection"}],title:"Data augmentation"}],title:"Process image data"};function rp(va){return ap(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class hp extends Xr{constructor(f){super();Zr(this,f,rp,np,sp,{})}}export{hp as default,op as metadata};
