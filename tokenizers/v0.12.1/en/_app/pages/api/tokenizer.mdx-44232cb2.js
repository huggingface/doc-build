import{S as Zi,i as ed,s as td,e as n,k as i,w as f,t as a,M as nd,c as r,d as t,m as d,a as o,x as k,h as s,b as g,F as e,g as De,y as v,q as _,o as z,B as $,v as rd,L as od}from"../../chunks/vendor-0d3f0756.js";import{D as T}from"../../chunks/Docstring-f99fb2a0.js";import{C as Qi}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as Xi}from"../../chunks/IconCopyLink-9193371d.js";import{T as ad,M as Gs}from"../../chunks/TokenizersLanguageContent-ca787841.js";function sd(I){let m,b,h,u,y,w,q,A,l,P,j,D,E,x,M,vt,J,_t,_r,zr,ee,Ae,$r,B,br,Rt,Tr,Er,Jt,yr,wr,xr,te,Ie,qr,Pe,Dr,zt,Ar,Ir,Pr,ne,je,jr,U,Vr,Bt,Lr,Cr,$t,Nr,Sr,Mr,G,Ve,Or,Ut,Gr,Fr,Y,Yt,Hr,Wr,Kt,Rr,Jr,Qt,Br,Ur,re,Le,Yr,K,Kr,Xt,Qr,Xr,Zt,Zr,eo,to,oe,Ce,no,Q,ro,en,oo,ao,bt,so,io,co,F,Ne,lo,tn,po,ho,X,nn,mo,go,rn,uo,fo,on,ko,vo,V,Se,_o,an,zo,$o,sn,bo,To,dn,Eo,yo,H,Me,wo,cn,xo,qo,ln,Do,Ao,W,Oe,Io,pn,Po,jo,hn,Vo,Lo,ae,Ge,Co,mn,No,So,se,Fe,Mo,gn,Oo,Go,ie,He,Fo,un,Ho,Wo,L,We,Ro,fn,Jo,Bo,kn,Uo,Yo,Re,Ko,C,Je,Qo,vn,Xo,Zo,_n,ea,ta,Be,na,de,Ue,ra,Ye,oa,Tt,aa,sa,ia,ce,Ke,da,Qe,ca,Et,la,pa,ha,le,Xe,ma,Ze,ga,yt,ua,fa,ka,pe,et,va,tt,_a,wt,za,$a,ba,he,nt,Ta,zn,Ea,ya,me,rt,wa,$n,xa,qa,ge,ot,Da,bn,Aa,Ia,ue,at,Pa,Tn,ja,Va,fe,st,La,En,Ca,Na,ke,it,Sa,yn,Ma,Oa,N,dt,Ga,wn,Fa,Ha,xn,Wa,Ra,Z,ct,Ja,qn,Ba,Ua,Ya,xt,Ka,Dn,Qa,Xa,lt,Za,An,es,ts,ns,ve,pt,rs,ht,os,qt,as,ss,is,_e,mt,ds,gt,cs,Dt,ls,ps,hs,ze,ut,ms,In,gs,us,R,ft,fs,Pn,ks,vs,At,_s,jn,zs,$s,S,kt,bs,Vn,Ts,Es,Ln,ys,ws,O,It,xs,Cn,qs,Ds,$e,As,Nn,Is,Ps,Sn,js,Vs,Mn,Ls,Cs,On,Ns,Fn;return u=new Xi({}),P=new T({props:{name:"class tokenizers.Tokenizer",anchor:"tokenizers.Tokenizer",parameters:[{name:"model",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.model",description:`<strong>model</strong> (<a href="/docs/tokenizers/v0.12.1/en/api/models#tokenizers.models.Model">Model</a>) &#x2014;
The core algorithm that this <code>Tokenizer</code> should be using.`,name:"model"}]}}),Ae=new T({props:{name:"decoder",anchor:"tokenizers.Tokenizer.decoder",parameters:[],isGetSetDescriptor:!0}}),Ie=new T({props:{name:"model",anchor:"tokenizers.Tokenizer.model",parameters:[],isGetSetDescriptor:!0}}),je=new T({props:{name:"normalizer",anchor:"tokenizers.Tokenizer.normalizer",parameters:[],isGetSetDescriptor:!0}}),Ve=new T({props:{name:"padding",anchor:"tokenizers.Tokenizer.padding",parameters:[],returnDescription:`
<p>A dict with the current padding parameters if padding is enabled</p>
`,returnType:`
<p>(<code>dict</code>, <em>optional</em>)</p>
`,isGetSetDescriptor:!0}}),Le=new T({props:{name:"post_processor",anchor:"tokenizers.Tokenizer.post_processor",parameters:[],isGetSetDescriptor:!0}}),Ce=new T({props:{name:"pre_tokenizer",anchor:"tokenizers.Tokenizer.pre_tokenizer",parameters:[],isGetSetDescriptor:!0}}),Ne=new T({props:{name:"truncation",anchor:"tokenizers.Tokenizer.truncation",parameters:[],returnDescription:`
<p>A dict with the current truncation parameters if truncation is enabled</p>
`,returnType:`
<p>(<code>dict</code>, <em>optional</em>)</p>
`,isGetSetDescriptor:!0}}),Se=new T({props:{name:"add_special_tokens",anchor:"tokenizers.Tokenizer.add_special_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_special_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/v0.12.1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of special tokens we want to add to the vocabulary. Each token can either
be a string or an instance of <a href="/docs/tokenizers/v0.12.1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more
customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),Me=new T({props:{name:"add_tokens",anchor:"tokenizers.Tokenizer.add_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/v0.12.1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of tokens we want to add to the vocabulary. Each token can be either a
string or an instance of <a href="/docs/tokenizers/v0.12.1/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),Oe=new T({props:{name:"decode",anchor:"tokenizers.Tokenizer.decode",parameters:[{name:"ids",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode.ids",description:`<strong>ids</strong> (A <code>List/Tuple</code> of <code>int</code>) &#x2014;
The list of ids that we want to decode`,name:"ids"},{anchor:"tokenizers.Tokenizer.decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded string`,name:"skip_special_tokens"}],returnDescription:`
<p>The decoded string</p>
`,returnType:`
<p><code>str</code></p>
`}}),Ge=new T({props:{name:"decode_batch",anchor:"tokenizers.Tokenizer.decode_batch",parameters:[{name:"sequences",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode_batch.sequences",description:`<strong>sequences</strong> (<code>List</code> of <code>List[int]</code>) &#x2014;
The batch of sequences we want to decode`,name:"sequences"},{anchor:"tokenizers.Tokenizer.decode_batch.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded strings`,name:"skip_special_tokens"}],returnDescription:`
<p>A list of decoded strings</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),Fe=new T({props:{name:"enable_padding",anchor:"tokenizers.Tokenizer.enable_padding",parameters:[{name:"direction",val:" = 'right'"},{name:"pad_id",val:" = 0"},{name:"pad_type_id",val:" = 0"},{name:"pad_token",val:" = '[PAD]'"},{name:"length",val:" = None"},{name:"pad_to_multiple_of",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_padding.direction",description:`<strong>direction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>right</code>) &#x2014;
The direction in which to pad. Can be either <code>right</code> or <code>left</code>`,name:"direction"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the padding length should always snap to the next multiple of the
given value. For example if we were going to pad witha length of 250 but
<code>pad_to_multiple_of=8</code> then we will pad to 256.`,name:"pad_to_multiple_of"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_id",description:`<strong>pad_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The id to be used when padding`,name:"pad_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_type_id",description:`<strong>pad_type_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The type id to be used when padding`,name:"pad_type_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, defaults to <code>[PAD]</code>) &#x2014;
The pad token to be used when padding`,name:"pad_token"},{anchor:"tokenizers.Tokenizer.enable_padding.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the length at which to pad. If not specified we pad using the size of
the longest sequence in a batch.`,name:"length"}]}}),He=new T({props:{name:"enable_truncation",anchor:"tokenizers.Tokenizer.enable_truncation",parameters:[{name:"max_length",val:""},{name:"stride",val:" = 0"},{name:"strategy",val:" = 'longest_first'"},{name:"direction",val:" = 'right'"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_truncation.max_length",description:`<strong>max_length</strong> (<code>int</code>) &#x2014;
The max length at which to truncate`,name:"max_length"},{anchor:"tokenizers.Tokenizer.enable_truncation.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The length of the previous first sequence to be included in the overflowing
sequence`,name:"stride"},{anchor:"tokenizers.Tokenizer.enable_truncation.strategy",description:`<strong>strategy</strong> (<code>str</code>, <em>optional</em>, defaults to <code>longest_first</code>) &#x2014;
The strategy used to truncation. Can be one of <code>longest_first</code>, <code>only_first</code> or
<code>only_second</code>.`,name:"strategy"},{anchor:"tokenizers.Tokenizer.enable_truncation.direction",description:`<strong>direction</strong> (<code>str</code>, defaults to <code>right</code>) &#x2014;
Truncate direction`,name:"direction"}]}}),We=new T({props:{name:"encode",anchor:"tokenizers.Tokenizer.encode",parameters:[{name:"sequence",val:""},{name:"pair",val:" = None"},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode.sequence",description:`<strong>sequence</strong> (<code>~tokenizers.InputSequence</code>) &#x2014;
The main input sequence we want to encode. This sequence can be either raw
text or pre-tokenized, according to the <code>is_pretokenized</code> argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextInputSequence</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedInputSequence()</code></li>
</ul>`,name:"sequence"},{anchor:"tokenizers.Tokenizer.encode.pair",description:`<strong>pair</strong> (<code>~tokenizers.InputSequence</code>, <em>optional</em>) &#x2014;
An optional input sequence. The expected format is the same that for <code>sequence</code>.`,name:"pair"},{anchor:"tokenizers.Tokenizer.encode.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded result</p>
`,returnType:`
<p><a href="/docs/tokenizers/v0.12.1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),Re=new Qi({props:{code:`encode("A single sequence")*
encode("A sequence", "And its pair")*
encode([ "A", "pre", "tokenized", "sequence" ], is_pretokenized=True)\`
encode(
[ "A", "pre", "tokenized", "sequence" ], [ "And", "its", "pair" ],
is_pretokenized=True
)`,highlighted:`encode(<span class="hljs-string">&quot;A single sequence&quot;</span>)*
encode(<span class="hljs-string">&quot;A sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>)*
encode([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], is_pretokenized=<span class="hljs-literal">True</span>)\`
encode(
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], [ <span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;its&quot;</span>, <span class="hljs-string">&quot;pair&quot;</span> ],
is_pretokenized=<span class="hljs-literal">True</span>
)`}}),Je=new T({props:{name:"encode_batch",anchor:"tokenizers.Tokenizer.encode_batch",parameters:[{name:"input",val:""},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode_batch.input",description:`<strong>input</strong> (A <code>List</code>/\`<code>Tuple</code> of <code>~tokenizers.EncodeInput</code>) &#x2014;
A list of single sequences or pair sequences to encode. Each sequence
can be either raw text or pre-tokenized, according to the <code>is_pretokenized</code>
argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextEncodeInput()</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedEncodeInput()</code></li>
</ul>`,name:"input"},{anchor:"tokenizers.Tokenizer.encode_batch.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode_batch.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded batch</p>
`,returnType:`
<p>A <code>List</code> of [\`~tokenizers.Encoding\u201C]</p>
`}}),Be=new Qi({props:{code:`encode_batch([
"A single sequence",
("A tuple with a sequence", "And its pair"),
[ "A", "pre", "tokenized", "sequence" ],
([ "A", "pre", "tokenized", "sequence" ], "And its pair")
])`,highlighted:`encode_batch([
<span class="hljs-string">&quot;A single sequence&quot;</span>,
(<span class="hljs-string">&quot;A tuple with a sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>),
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ],
([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], <span class="hljs-string">&quot;And its pair&quot;</span>)
])`}}),Ue=new T({props:{name:"from_buffer",anchor:"tokenizers.Tokenizer.from_buffer",parameters:[{name:"buffer",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_buffer.buffer",description:`<strong>buffer</strong> (<code>bytes</code>) &#x2014;
A buffer containing a previously serialized <a href="/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"buffer"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"
>Tokenizer</a></p>
`}}),Ke=new T({props:{name:"from_file",anchor:"tokenizers.Tokenizer.from_file",parameters:[{name:"path",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_file.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a local JSON file representing a previously serialized
<a href="/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"path"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"
>Tokenizer</a></p>
`}}),Xe=new T({props:{name:"from_pretrained",anchor:"tokenizers.Tokenizer.from_pretrained",parameters:[{name:"identifier",val:""},{name:"revision",val:" = 'main'"},{name:"auth_token",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_pretrained.identifier",description:`<strong>identifier</strong> (<code>str</code>) &#x2014;
The identifier of a Model on the Hugging Face Hub, that contains
a tokenizer.json file`,name:"identifier"},{anchor:"tokenizers.Tokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, defaults to <em>main</em>) &#x2014;
A branch or commit id`,name:"revision"},{anchor:"tokenizers.Tokenizer.from_pretrained.auth_token",description:`<strong>auth_token</strong> (<code>str</code>, <em>optional</em>, defaults to <em>None</em>) &#x2014;
An optional auth token used to access private repositories on the
Hugging Face Hub`,name:"auth_token"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"
>Tokenizer</a></p>
`}}),et=new T({props:{name:"from_str",anchor:"tokenizers.Tokenizer.from_str",parameters:[{name:"json",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_str.json",description:`<strong>json</strong> (<code>str</code>) &#x2014;
A valid JSON string representing a previously serialized
<a href="/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"json"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"
>Tokenizer</a></p>
`}}),nt=new T({props:{name:"get_vocab",anchor:"tokenizers.Tokenizer.get_vocab",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The vocabulary</p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),rt=new T({props:{name:"get_vocab_size",anchor:"tokenizers.Tokenizer.get_vocab_size",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab_size.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The size of the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),ot=new T({props:{name:"id_to_token",anchor:"tokenizers.Tokenizer.id_to_token",parameters:[{name:"id",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.id_to_token.id",description:`<strong>id</strong> (<code>int</code>) &#x2014;
The id to convert`,name:"id"}],returnDescription:`
<p>An optional token, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[str]</code></p>
`}}),at=new T({props:{name:"no_padding",anchor:"tokenizers.Tokenizer.no_padding",parameters:[]}}),st=new T({props:{name:"no_truncation",anchor:"tokenizers.Tokenizer.no_truncation",parameters:[]}}),it=new T({props:{name:"num_special_tokens_to_add",anchor:"tokenizers.Tokenizer.num_special_tokens_to_add",parameters:[{name:"is_pair",val:""}]}}),dt=new T({props:{name:"post_process",anchor:"tokenizers.Tokenizer.post_process",parameters:[{name:"encoding",val:""},{name:"pair",val:" = None"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.post_process.encoding",description:`<strong>encoding</strong> (<a href="/docs/tokenizers/v0.12.1/en/api/encoding#tokenizers.Encoding">Encoding</a>) &#x2014;
The <a href="/docs/tokenizers/v0.12.1/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the main sequence.`,name:"encoding"},{anchor:"tokenizers.Tokenizer.post_process.pair",description:`<strong>pair</strong> (<a href="/docs/tokenizers/v0.12.1/en/api/encoding#tokenizers.Encoding">Encoding</a>, <em>optional</em>) &#x2014;
An optional <a href="/docs/tokenizers/v0.12.1/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the pair sequence.`,name:"pair"},{anchor:"tokenizers.Tokenizer.post_process.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The final post-processed encoding</p>
`,returnType:`
<p><a href="/docs/tokenizers/v0.12.1/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),pt=new T({props:{name:"save",anchor:"tokenizers.Tokenizer.save",parameters:[{name:"path",val:""},{name:"pretty",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.save.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a file in which to save the serialized tokenizer.`,name:"path"},{anchor:"tokenizers.Tokenizer.save.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the JSON file should be pretty formatted.`,name:"pretty"}]}}),mt=new T({props:{name:"to_str",anchor:"tokenizers.Tokenizer.to_str",parameters:[{name:"pretty",val:" = False"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.to_str.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the JSON string should be pretty formatted.`,name:"pretty"}],returnDescription:`
<p>A string representing the serialized Tokenizer</p>
`,returnType:`
<p><code>str</code></p>
`}}),ut=new T({props:{name:"token_to_id",anchor:"tokenizers.Tokenizer.token_to_id",parameters:[{name:"token",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.token_to_id.token",description:`<strong>token</strong> (<code>str</code>) &#x2014;
The token to convert`,name:"token"}],returnDescription:`
<p>An optional id, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[int]</code></p>
`}}),ft=new T({props:{name:"train",anchor:"tokenizers.Tokenizer.train",parameters:[{name:"files",val:""},{name:"trainer",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train.files",description:`<strong>files</strong> (<code>List[str]</code>) &#x2014;
A list of path to the files that we should use for training`,name:"files"},{anchor:"tokenizers.Tokenizer.train.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"}]}}),kt=new T({props:{name:"train_from_iterator",anchor:"tokenizers.Tokenizer.train_from_iterator",parameters:[{name:"iterator",val:""},{name:"trainer",val:" = None"},{name:"length",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train_from_iterator.iterator",description:`<strong>iterator</strong> (<code>Iterator</code>) &#x2014;
Any iterator over strings or list of strings`,name:"iterator"},{anchor:"tokenizers.Tokenizer.train_from_iterator.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"},{anchor:"tokenizers.Tokenizer.train_from_iterator.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The total number of sequences in the iterator. This is used to
provide meaningful progress tracking`,name:"length"}]}}),{c(){m=n("h2"),b=n("a"),h=n("span"),f(u.$$.fragment),y=i(),w=n("span"),q=a("Tokenizer"),A=i(),l=n("div"),f(P.$$.fragment),j=i(),D=n("p"),E=a("A "),x=n("code"),M=a("Tokenizer"),vt=a(` works as a pipeline. It processes some raw text as input
and outputs an `),J=n("a"),_t=a("Encoding"),_r=a("."),zr=i(),ee=n("div"),f(Ae.$$.fragment),$r=i(),B=n("p"),br=a("The "),Rt=n("em"),Tr=a("optional"),Er=i(),Jt=n("code"),yr=a("Decoder"),wr=a(" in use by the Tokenizer"),xr=i(),te=n("div"),f(Ie.$$.fragment),qr=i(),Pe=n("p"),Dr=a("The "),zt=n("a"),Ar=a("Model"),Ir=a(" in use by the Tokenizer"),Pr=i(),ne=n("div"),f(je.$$.fragment),jr=i(),U=n("p"),Vr=a("The "),Bt=n("em"),Lr=a("optional"),Cr=i(),$t=n("a"),Nr=a("Normalizer"),Sr=a(" in use by the Tokenizer"),Mr=i(),G=n("div"),f(Ve.$$.fragment),Or=i(),Ut=n("p"),Gr=a("Get the current padding parameters"),Fr=i(),Y=n("p"),Yt=n("em"),Hr=a("Cannot be set, use"),Wr=i(),Kt=n("code"),Rr=a("enable_padding()"),Jr=i(),Qt=n("em"),Br=a("instead"),Ur=i(),re=n("div"),f(Le.$$.fragment),Yr=i(),K=n("p"),Kr=a("The "),Xt=n("em"),Qr=a("optional"),Xr=i(),Zt=n("code"),Zr=a("PostProcessor"),eo=a(" in use by the Tokenizer"),to=i(),oe=n("div"),f(Ce.$$.fragment),no=i(),Q=n("p"),ro=a("The "),en=n("em"),oo=a("optional"),ao=i(),bt=n("a"),so=a("PreTokenizer"),io=a(" in use by the Tokenizer"),co=i(),F=n("div"),f(Ne.$$.fragment),lo=i(),tn=n("p"),po=a("Get the currently set truncation parameters"),ho=i(),X=n("p"),nn=n("em"),mo=a("Cannot set, use"),go=i(),rn=n("code"),uo=a("enable_truncation()"),fo=i(),on=n("em"),ko=a("instead"),vo=i(),V=n("div"),f(Se.$$.fragment),_o=i(),an=n("p"),zo=a("Add the given special tokens to the Tokenizer."),$o=i(),sn=n("p"),bo=a(`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),To=i(),dn=n("p"),Eo=a(`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),yo=i(),H=n("div"),f(Me.$$.fragment),wo=i(),cn=n("p"),xo=a("Add the given tokens to the vocabulary"),qo=i(),ln=n("p"),Do=a(`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),Ao=i(),W=n("div"),f(Oe.$$.fragment),Io=i(),pn=n("p"),Po=a("Decode the given list of ids back to a string"),jo=i(),hn=n("p"),Vo=a("This is used to decode anything coming back from a Language Model"),Lo=i(),ae=n("div"),f(Ge.$$.fragment),Co=i(),mn=n("p"),No=a("Decode a batch of ids back to their corresponding string"),So=i(),se=n("div"),f(Fe.$$.fragment),Mo=i(),gn=n("p"),Oo=a("Enable the padding"),Go=i(),ie=n("div"),f(He.$$.fragment),Fo=i(),un=n("p"),Ho=a("Enable truncation"),Wo=i(),L=n("div"),f(We.$$.fragment),Ro=i(),fn=n("p"),Jo=a(`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),Bo=i(),kn=n("p"),Uo=a(`Example:
Here are some examples of the inputs that are accepted:`),Yo=i(),f(Re.$$.fragment),Ko=i(),C=n("div"),f(Je.$$.fragment),Qo=i(),vn=n("p"),Xo=a(`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),Zo=i(),_n=n("p"),ea=a(`Example:
Here are some examples of the inputs that are accepted:`),ta=i(),f(Be.$$.fragment),na=i(),de=n("div"),f(Ue.$$.fragment),ra=i(),Ye=n("p"),oa=a("Instantiate a new "),Tt=n("a"),aa=a("Tokenizer"),sa=a(" from the given buffer."),ia=i(),ce=n("div"),f(Ke.$$.fragment),da=i(),Qe=n("p"),ca=a("Instantiate a new "),Et=n("a"),la=a("Tokenizer"),pa=a(" from the file at the given path."),ha=i(),le=n("div"),f(Xe.$$.fragment),ma=i(),Ze=n("p"),ga=a("Instantiate a new "),yt=n("a"),ua=a("Tokenizer"),fa=a(` from an existing file on the
Hugging Face Hub.`),ka=i(),pe=n("div"),f(et.$$.fragment),va=i(),tt=n("p"),_a=a("Instantiate a new "),wt=n("a"),za=a("Tokenizer"),$a=a(" from the given JSON string."),ba=i(),he=n("div"),f(nt.$$.fragment),Ta=i(),zn=n("p"),Ea=a("Get the underlying vocabulary"),ya=i(),me=n("div"),f(rt.$$.fragment),wa=i(),$n=n("p"),xa=a("Get the size of the underlying vocabulary"),qa=i(),ge=n("div"),f(ot.$$.fragment),Da=i(),bn=n("p"),Aa=a("Convert the given id to its corresponding token if it exists"),Ia=i(),ue=n("div"),f(at.$$.fragment),Pa=i(),Tn=n("p"),ja=a("Disable padding"),Va=i(),fe=n("div"),f(st.$$.fragment),La=i(),En=n("p"),Ca=a("Disable truncation"),Na=i(),ke=n("div"),f(it.$$.fragment),Sa=i(),yn=n("p"),Ma=a(`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),Oa=i(),N=n("div"),f(dt.$$.fragment),Ga=i(),wn=n("p"),Fa=a("Apply all the post-processing steps to the given encodings."),Ha=i(),xn=n("p"),Wa=a("The various steps are:"),Ra=i(),Z=n("ol"),ct=n("li"),Ja=a(`Truncate according to the set truncation params (provided with
`),qn=n("code"),Ba=a("enable_truncation()"),Ua=a(")"),Ya=i(),xt=n("li"),Ka=a("Apply the "),Dn=n("code"),Qa=a("PostProcessor"),Xa=i(),lt=n("li"),Za=a(`Pad according to the set padding params (provided with
`),An=n("code"),es=a("enable_padding()"),ts=a(")"),ns=i(),ve=n("div"),f(pt.$$.fragment),rs=i(),ht=n("p"),os=a("Save the "),qt=n("a"),as=a("Tokenizer"),ss=a(" to the file at the given path."),is=i(),_e=n("div"),f(mt.$$.fragment),ds=i(),gt=n("p"),cs=a("Gets a serialized string representing this "),Dt=n("a"),ls=a("Tokenizer"),ps=a("."),hs=i(),ze=n("div"),f(ut.$$.fragment),ms=i(),In=n("p"),gs=a("Convert the given token to its corresponding id if it exists"),us=i(),R=n("div"),f(ft.$$.fragment),fs=i(),Pn=n("p"),ks=a("Train the Tokenizer using the given files."),vs=i(),At=n("p"),_s=a(`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),jn=n("code"),zs=a("train_from_iterator()"),$s=i(),S=n("div"),f(kt.$$.fragment),bs=i(),Vn=n("p"),Ts=a("Train the Tokenizer using the provided iterator."),Es=i(),Ln=n("p"),ys=a("You can provide anything that is a Python Iterator"),ws=i(),O=n("ul"),It=n("li"),xs=a("A list of sequences "),Cn=n("code"),qs=a("List[str]"),Ds=i(),$e=n("li"),As=a("A generator that yields "),Nn=n("code"),Is=a("str"),Ps=a(" or "),Sn=n("code"),js=a("List[str]"),Vs=i(),Mn=n("li"),Ls=a("A Numpy array of strings"),Cs=i(),On=n("li"),Ns=a("\u2026"),this.h()},l(c){m=r(c,"H2",{class:!0});var be=o(m);b=r(be,"A",{id:!0,class:!0,href:!0});var Fs=o(b);h=r(Fs,"SPAN",{});var Hs=o(h);k(u.$$.fragment,Hs),Hs.forEach(t),Fs.forEach(t),y=d(be),w=r(be,"SPAN",{});var Ws=o(w);q=s(Ws,"Tokenizer"),Ws.forEach(t),be.forEach(t),A=d(c),l=r(c,"DIV",{class:!0});var p=o(l);k(P.$$.fragment,p),j=d(p),D=r(p,"P",{});var Pt=o(D);E=s(Pt,"A "),x=r(Pt,"CODE",{});var Rs=o(x);M=s(Rs,"Tokenizer"),Rs.forEach(t),vt=s(Pt,` works as a pipeline. It processes some raw text as input
and outputs an `),J=r(Pt,"A",{href:!0});var Js=o(J);_t=s(Js,"Encoding"),Js.forEach(t),_r=s(Pt,"."),Pt.forEach(t),zr=d(p),ee=r(p,"DIV",{class:!0});var Hn=o(ee);k(Ae.$$.fragment,Hn),$r=d(Hn),B=r(Hn,"P",{});var jt=o(B);br=s(jt,"The "),Rt=r(jt,"EM",{});var Bs=o(Rt);Tr=s(Bs,"optional"),Bs.forEach(t),Er=d(jt),Jt=r(jt,"CODE",{});var Us=o(Jt);yr=s(Us,"Decoder"),Us.forEach(t),wr=s(jt," in use by the Tokenizer"),jt.forEach(t),Hn.forEach(t),xr=d(p),te=r(p,"DIV",{class:!0});var Wn=o(te);k(Ie.$$.fragment,Wn),qr=d(Wn),Pe=r(Wn,"P",{});var Rn=o(Pe);Dr=s(Rn,"The "),zt=r(Rn,"A",{href:!0});var Ys=o(zt);Ar=s(Ys,"Model"),Ys.forEach(t),Ir=s(Rn," in use by the Tokenizer"),Rn.forEach(t),Wn.forEach(t),Pr=d(p),ne=r(p,"DIV",{class:!0});var Jn=o(ne);k(je.$$.fragment,Jn),jr=d(Jn),U=r(Jn,"P",{});var Vt=o(U);Vr=s(Vt,"The "),Bt=r(Vt,"EM",{});var Ks=o(Bt);Lr=s(Ks,"optional"),Ks.forEach(t),Cr=d(Vt),$t=r(Vt,"A",{href:!0});var Qs=o($t);Nr=s(Qs,"Normalizer"),Qs.forEach(t),Sr=s(Vt," in use by the Tokenizer"),Vt.forEach(t),Jn.forEach(t),Mr=d(p),G=r(p,"DIV",{class:!0});var Lt=o(G);k(Ve.$$.fragment,Lt),Or=d(Lt),Ut=r(Lt,"P",{});var Xs=o(Ut);Gr=s(Xs,"Get the current padding parameters"),Xs.forEach(t),Fr=d(Lt),Y=r(Lt,"P",{});var Ct=o(Y);Yt=r(Ct,"EM",{});var Zs=o(Yt);Hr=s(Zs,"Cannot be set, use"),Zs.forEach(t),Wr=d(Ct),Kt=r(Ct,"CODE",{});var ei=o(Kt);Rr=s(ei,"enable_padding()"),ei.forEach(t),Jr=d(Ct),Qt=r(Ct,"EM",{});var ti=o(Qt);Br=s(ti,"instead"),ti.forEach(t),Ct.forEach(t),Lt.forEach(t),Ur=d(p),re=r(p,"DIV",{class:!0});var Bn=o(re);k(Le.$$.fragment,Bn),Yr=d(Bn),K=r(Bn,"P",{});var Nt=o(K);Kr=s(Nt,"The "),Xt=r(Nt,"EM",{});var ni=o(Xt);Qr=s(ni,"optional"),ni.forEach(t),Xr=d(Nt),Zt=r(Nt,"CODE",{});var ri=o(Zt);Zr=s(ri,"PostProcessor"),ri.forEach(t),eo=s(Nt," in use by the Tokenizer"),Nt.forEach(t),Bn.forEach(t),to=d(p),oe=r(p,"DIV",{class:!0});var Un=o(oe);k(Ce.$$.fragment,Un),no=d(Un),Q=r(Un,"P",{});var St=o(Q);ro=s(St,"The "),en=r(St,"EM",{});var oi=o(en);oo=s(oi,"optional"),oi.forEach(t),ao=d(St),bt=r(St,"A",{href:!0});var ai=o(bt);so=s(ai,"PreTokenizer"),ai.forEach(t),io=s(St," in use by the Tokenizer"),St.forEach(t),Un.forEach(t),co=d(p),F=r(p,"DIV",{class:!0});var Mt=o(F);k(Ne.$$.fragment,Mt),lo=d(Mt),tn=r(Mt,"P",{});var si=o(tn);po=s(si,"Get the currently set truncation parameters"),si.forEach(t),ho=d(Mt),X=r(Mt,"P",{});var Ot=o(X);nn=r(Ot,"EM",{});var ii=o(nn);mo=s(ii,"Cannot set, use"),ii.forEach(t),go=d(Ot),rn=r(Ot,"CODE",{});var di=o(rn);uo=s(di,"enable_truncation()"),di.forEach(t),fo=d(Ot),on=r(Ot,"EM",{});var ci=o(on);ko=s(ci,"instead"),ci.forEach(t),Ot.forEach(t),Mt.forEach(t),vo=d(p),V=r(p,"DIV",{class:!0});var Te=o(V);k(Se.$$.fragment,Te),_o=d(Te),an=r(Te,"P",{});var li=o(an);zo=s(li,"Add the given special tokens to the Tokenizer."),li.forEach(t),$o=d(Te),sn=r(Te,"P",{});var pi=o(sn);bo=s(pi,`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),pi.forEach(t),To=d(Te),dn=r(Te,"P",{});var hi=o(dn);Eo=s(hi,`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),hi.forEach(t),Te.forEach(t),yo=d(p),H=r(p,"DIV",{class:!0});var Gt=o(H);k(Me.$$.fragment,Gt),wo=d(Gt),cn=r(Gt,"P",{});var mi=o(cn);xo=s(mi,"Add the given tokens to the vocabulary"),mi.forEach(t),qo=d(Gt),ln=r(Gt,"P",{});var gi=o(ln);Do=s(gi,`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),gi.forEach(t),Gt.forEach(t),Ao=d(p),W=r(p,"DIV",{class:!0});var Ft=o(W);k(Oe.$$.fragment,Ft),Io=d(Ft),pn=r(Ft,"P",{});var ui=o(pn);Po=s(ui,"Decode the given list of ids back to a string"),ui.forEach(t),jo=d(Ft),hn=r(Ft,"P",{});var fi=o(hn);Vo=s(fi,"This is used to decode anything coming back from a Language Model"),fi.forEach(t),Ft.forEach(t),Lo=d(p),ae=r(p,"DIV",{class:!0});var Yn=o(ae);k(Ge.$$.fragment,Yn),Co=d(Yn),mn=r(Yn,"P",{});var ki=o(mn);No=s(ki,"Decode a batch of ids back to their corresponding string"),ki.forEach(t),Yn.forEach(t),So=d(p),se=r(p,"DIV",{class:!0});var Kn=o(se);k(Fe.$$.fragment,Kn),Mo=d(Kn),gn=r(Kn,"P",{});var vi=o(gn);Oo=s(vi,"Enable the padding"),vi.forEach(t),Kn.forEach(t),Go=d(p),ie=r(p,"DIV",{class:!0});var Qn=o(ie);k(He.$$.fragment,Qn),Fo=d(Qn),un=r(Qn,"P",{});var _i=o(un);Ho=s(_i,"Enable truncation"),_i.forEach(t),Qn.forEach(t),Wo=d(p),L=r(p,"DIV",{class:!0});var Ee=o(L);k(We.$$.fragment,Ee),Ro=d(Ee),fn=r(Ee,"P",{});var zi=o(fn);Jo=s(zi,`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),zi.forEach(t),Bo=d(Ee),kn=r(Ee,"P",{});var $i=o(kn);Uo=s($i,`Example:
Here are some examples of the inputs that are accepted:`),$i.forEach(t),Yo=d(Ee),k(Re.$$.fragment,Ee),Ee.forEach(t),Ko=d(p),C=r(p,"DIV",{class:!0});var ye=o(C);k(Je.$$.fragment,ye),Qo=d(ye),vn=r(ye,"P",{});var bi=o(vn);Xo=s(bi,`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),bi.forEach(t),Zo=d(ye),_n=r(ye,"P",{});var Ti=o(_n);ea=s(Ti,`Example:
Here are some examples of the inputs that are accepted:`),Ti.forEach(t),ta=d(ye),k(Be.$$.fragment,ye),ye.forEach(t),na=d(p),de=r(p,"DIV",{class:!0});var Xn=o(de);k(Ue.$$.fragment,Xn),ra=d(Xn),Ye=r(Xn,"P",{});var Zn=o(Ye);oa=s(Zn,"Instantiate a new "),Tt=r(Zn,"A",{href:!0});var Ei=o(Tt);aa=s(Ei,"Tokenizer"),Ei.forEach(t),sa=s(Zn," from the given buffer."),Zn.forEach(t),Xn.forEach(t),ia=d(p),ce=r(p,"DIV",{class:!0});var er=o(ce);k(Ke.$$.fragment,er),da=d(er),Qe=r(er,"P",{});var tr=o(Qe);ca=s(tr,"Instantiate a new "),Et=r(tr,"A",{href:!0});var yi=o(Et);la=s(yi,"Tokenizer"),yi.forEach(t),pa=s(tr," from the file at the given path."),tr.forEach(t),er.forEach(t),ha=d(p),le=r(p,"DIV",{class:!0});var nr=o(le);k(Xe.$$.fragment,nr),ma=d(nr),Ze=r(nr,"P",{});var rr=o(Ze);ga=s(rr,"Instantiate a new "),yt=r(rr,"A",{href:!0});var wi=o(yt);ua=s(wi,"Tokenizer"),wi.forEach(t),fa=s(rr,` from an existing file on the
Hugging Face Hub.`),rr.forEach(t),nr.forEach(t),ka=d(p),pe=r(p,"DIV",{class:!0});var or=o(pe);k(et.$$.fragment,or),va=d(or),tt=r(or,"P",{});var ar=o(tt);_a=s(ar,"Instantiate a new "),wt=r(ar,"A",{href:!0});var xi=o(wt);za=s(xi,"Tokenizer"),xi.forEach(t),$a=s(ar," from the given JSON string."),ar.forEach(t),or.forEach(t),ba=d(p),he=r(p,"DIV",{class:!0});var sr=o(he);k(nt.$$.fragment,sr),Ta=d(sr),zn=r(sr,"P",{});var qi=o(zn);Ea=s(qi,"Get the underlying vocabulary"),qi.forEach(t),sr.forEach(t),ya=d(p),me=r(p,"DIV",{class:!0});var ir=o(me);k(rt.$$.fragment,ir),wa=d(ir),$n=r(ir,"P",{});var Di=o($n);xa=s(Di,"Get the size of the underlying vocabulary"),Di.forEach(t),ir.forEach(t),qa=d(p),ge=r(p,"DIV",{class:!0});var dr=o(ge);k(ot.$$.fragment,dr),Da=d(dr),bn=r(dr,"P",{});var Ai=o(bn);Aa=s(Ai,"Convert the given id to its corresponding token if it exists"),Ai.forEach(t),dr.forEach(t),Ia=d(p),ue=r(p,"DIV",{class:!0});var cr=o(ue);k(at.$$.fragment,cr),Pa=d(cr),Tn=r(cr,"P",{});var Ii=o(Tn);ja=s(Ii,"Disable padding"),Ii.forEach(t),cr.forEach(t),Va=d(p),fe=r(p,"DIV",{class:!0});var lr=o(fe);k(st.$$.fragment,lr),La=d(lr),En=r(lr,"P",{});var Pi=o(En);Ca=s(Pi,"Disable truncation"),Pi.forEach(t),lr.forEach(t),Na=d(p),ke=r(p,"DIV",{class:!0});var pr=o(ke);k(it.$$.fragment,pr),Sa=d(pr),yn=r(pr,"P",{});var ji=o(yn);Ma=s(ji,`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),ji.forEach(t),pr.forEach(t),Oa=d(p),N=r(p,"DIV",{class:!0});var we=o(N);k(dt.$$.fragment,we),Ga=d(we),wn=r(we,"P",{});var Vi=o(wn);Fa=s(Vi,"Apply all the post-processing steps to the given encodings."),Vi.forEach(t),Ha=d(we),xn=r(we,"P",{});var Li=o(xn);Wa=s(Li,"The various steps are:"),Li.forEach(t),Ra=d(we),Z=r(we,"OL",{});var Ht=o(Z);ct=r(Ht,"LI",{});var hr=o(ct);Ja=s(hr,`Truncate according to the set truncation params (provided with
`),qn=r(hr,"CODE",{});var Ci=o(qn);Ba=s(Ci,"enable_truncation()"),Ci.forEach(t),Ua=s(hr,")"),hr.forEach(t),Ya=d(Ht),xt=r(Ht,"LI",{});var Ss=o(xt);Ka=s(Ss,"Apply the "),Dn=r(Ss,"CODE",{});var Ni=o(Dn);Qa=s(Ni,"PostProcessor"),Ni.forEach(t),Ss.forEach(t),Xa=d(Ht),lt=r(Ht,"LI",{});var mr=o(lt);Za=s(mr,`Pad according to the set padding params (provided with
`),An=r(mr,"CODE",{});var Si=o(An);es=s(Si,"enable_padding()"),Si.forEach(t),ts=s(mr,")"),mr.forEach(t),Ht.forEach(t),we.forEach(t),ns=d(p),ve=r(p,"DIV",{class:!0});var gr=o(ve);k(pt.$$.fragment,gr),rs=d(gr),ht=r(gr,"P",{});var ur=o(ht);os=s(ur,"Save the "),qt=r(ur,"A",{href:!0});var Mi=o(qt);as=s(Mi,"Tokenizer"),Mi.forEach(t),ss=s(ur," to the file at the given path."),ur.forEach(t),gr.forEach(t),is=d(p),_e=r(p,"DIV",{class:!0});var fr=o(_e);k(mt.$$.fragment,fr),ds=d(fr),gt=r(fr,"P",{});var kr=o(gt);cs=s(kr,"Gets a serialized string representing this "),Dt=r(kr,"A",{href:!0});var Oi=o(Dt);ls=s(Oi,"Tokenizer"),Oi.forEach(t),ps=s(kr,"."),kr.forEach(t),fr.forEach(t),hs=d(p),ze=r(p,"DIV",{class:!0});var vr=o(ze);k(ut.$$.fragment,vr),ms=d(vr),In=r(vr,"P",{});var Gi=o(In);gs=s(Gi,"Convert the given token to its corresponding id if it exists"),Gi.forEach(t),vr.forEach(t),us=d(p),R=r(p,"DIV",{class:!0});var Wt=o(R);k(ft.$$.fragment,Wt),fs=d(Wt),Pn=r(Wt,"P",{});var Fi=o(Pn);ks=s(Fi,"Train the Tokenizer using the given files."),Fi.forEach(t),vs=d(Wt),At=r(Wt,"P",{});var Ms=o(At);_s=s(Ms,`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),jn=r(Ms,"CODE",{});var Hi=o(jn);zs=s(Hi,"train_from_iterator()"),Hi.forEach(t),Ms.forEach(t),Wt.forEach(t),$s=d(p),S=r(p,"DIV",{class:!0});var xe=o(S);k(kt.$$.fragment,xe),bs=d(xe),Vn=r(xe,"P",{});var Wi=o(Vn);Ts=s(Wi,"Train the Tokenizer using the provided iterator."),Wi.forEach(t),Es=d(xe),Ln=r(xe,"P",{});var Ri=o(Ln);ys=s(Ri,"You can provide anything that is a Python Iterator"),Ri.forEach(t),ws=d(xe),O=r(xe,"UL",{});var qe=o(O);It=r(qe,"LI",{});var Os=o(It);xs=s(Os,"A list of sequences "),Cn=r(Os,"CODE",{});var Ji=o(Cn);qs=s(Ji,"List[str]"),Ji.forEach(t),Os.forEach(t),Ds=d(qe),$e=r(qe,"LI",{});var Gn=o($e);As=s(Gn,"A generator that yields "),Nn=r(Gn,"CODE",{});var Bi=o(Nn);Is=s(Bi,"str"),Bi.forEach(t),Ps=s(Gn," or "),Sn=r(Gn,"CODE",{});var Ui=o(Sn);js=s(Ui,"List[str]"),Ui.forEach(t),Gn.forEach(t),Vs=d(qe),Mn=r(qe,"LI",{});var Yi=o(Mn);Ls=s(Yi,"A Numpy array of strings"),Yi.forEach(t),Cs=d(qe),On=r(qe,"LI",{});var Ki=o(On);Ns=s(Ki,"\u2026"),Ki.forEach(t),qe.forEach(t),xe.forEach(t),p.forEach(t),this.h()},h(){g(b,"id","tokenizers.Tokenizer"),g(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(b,"href","#tokenizers.Tokenizer"),g(m,"class","relative group"),g(J,"href","/docs/tokenizers/v0.12.1/en/api/encoding#tokenizers.Encoding"),g(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(zt,"href","/docs/tokenizers/v0.12.1/en/api/models#tokenizers.models.Model"),g(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g($t,"href","/docs/tokenizers/v0.12.1/en/api/normalizers#tokenizers.normalizers.Normalizer"),g(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(bt,"href","/docs/tokenizers/v0.12.1/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),g(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(Tt,"href","/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"),g(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(Et,"href","/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"),g(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(yt,"href","/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"),g(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(wt,"href","/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"),g(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(qt,"href","/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"),g(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(Dt,"href","/docs/tokenizers/v0.12.1/en/api/tokenizer#tokenizers.Tokenizer"),g(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(l,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(c,be){De(c,m,be),e(m,b),e(b,h),v(u,h,null),e(m,y),e(m,w),e(w,q),De(c,A,be),De(c,l,be),v(P,l,null),e(l,j),e(l,D),e(D,E),e(D,x),e(x,M),e(D,vt),e(D,J),e(J,_t),e(D,_r),e(l,zr),e(l,ee),v(Ae,ee,null),e(ee,$r),e(ee,B),e(B,br),e(B,Rt),e(Rt,Tr),e(B,Er),e(B,Jt),e(Jt,yr),e(B,wr),e(l,xr),e(l,te),v(Ie,te,null),e(te,qr),e(te,Pe),e(Pe,Dr),e(Pe,zt),e(zt,Ar),e(Pe,Ir),e(l,Pr),e(l,ne),v(je,ne,null),e(ne,jr),e(ne,U),e(U,Vr),e(U,Bt),e(Bt,Lr),e(U,Cr),e(U,$t),e($t,Nr),e(U,Sr),e(l,Mr),e(l,G),v(Ve,G,null),e(G,Or),e(G,Ut),e(Ut,Gr),e(G,Fr),e(G,Y),e(Y,Yt),e(Yt,Hr),e(Y,Wr),e(Y,Kt),e(Kt,Rr),e(Y,Jr),e(Y,Qt),e(Qt,Br),e(l,Ur),e(l,re),v(Le,re,null),e(re,Yr),e(re,K),e(K,Kr),e(K,Xt),e(Xt,Qr),e(K,Xr),e(K,Zt),e(Zt,Zr),e(K,eo),e(l,to),e(l,oe),v(Ce,oe,null),e(oe,no),e(oe,Q),e(Q,ro),e(Q,en),e(en,oo),e(Q,ao),e(Q,bt),e(bt,so),e(Q,io),e(l,co),e(l,F),v(Ne,F,null),e(F,lo),e(F,tn),e(tn,po),e(F,ho),e(F,X),e(X,nn),e(nn,mo),e(X,go),e(X,rn),e(rn,uo),e(X,fo),e(X,on),e(on,ko),e(l,vo),e(l,V),v(Se,V,null),e(V,_o),e(V,an),e(an,zo),e(V,$o),e(V,sn),e(sn,bo),e(V,To),e(V,dn),e(dn,Eo),e(l,yo),e(l,H),v(Me,H,null),e(H,wo),e(H,cn),e(cn,xo),e(H,qo),e(H,ln),e(ln,Do),e(l,Ao),e(l,W),v(Oe,W,null),e(W,Io),e(W,pn),e(pn,Po),e(W,jo),e(W,hn),e(hn,Vo),e(l,Lo),e(l,ae),v(Ge,ae,null),e(ae,Co),e(ae,mn),e(mn,No),e(l,So),e(l,se),v(Fe,se,null),e(se,Mo),e(se,gn),e(gn,Oo),e(l,Go),e(l,ie),v(He,ie,null),e(ie,Fo),e(ie,un),e(un,Ho),e(l,Wo),e(l,L),v(We,L,null),e(L,Ro),e(L,fn),e(fn,Jo),e(L,Bo),e(L,kn),e(kn,Uo),e(L,Yo),v(Re,L,null),e(l,Ko),e(l,C),v(Je,C,null),e(C,Qo),e(C,vn),e(vn,Xo),e(C,Zo),e(C,_n),e(_n,ea),e(C,ta),v(Be,C,null),e(l,na),e(l,de),v(Ue,de,null),e(de,ra),e(de,Ye),e(Ye,oa),e(Ye,Tt),e(Tt,aa),e(Ye,sa),e(l,ia),e(l,ce),v(Ke,ce,null),e(ce,da),e(ce,Qe),e(Qe,ca),e(Qe,Et),e(Et,la),e(Qe,pa),e(l,ha),e(l,le),v(Xe,le,null),e(le,ma),e(le,Ze),e(Ze,ga),e(Ze,yt),e(yt,ua),e(Ze,fa),e(l,ka),e(l,pe),v(et,pe,null),e(pe,va),e(pe,tt),e(tt,_a),e(tt,wt),e(wt,za),e(tt,$a),e(l,ba),e(l,he),v(nt,he,null),e(he,Ta),e(he,zn),e(zn,Ea),e(l,ya),e(l,me),v(rt,me,null),e(me,wa),e(me,$n),e($n,xa),e(l,qa),e(l,ge),v(ot,ge,null),e(ge,Da),e(ge,bn),e(bn,Aa),e(l,Ia),e(l,ue),v(at,ue,null),e(ue,Pa),e(ue,Tn),e(Tn,ja),e(l,Va),e(l,fe),v(st,fe,null),e(fe,La),e(fe,En),e(En,Ca),e(l,Na),e(l,ke),v(it,ke,null),e(ke,Sa),e(ke,yn),e(yn,Ma),e(l,Oa),e(l,N),v(dt,N,null),e(N,Ga),e(N,wn),e(wn,Fa),e(N,Ha),e(N,xn),e(xn,Wa),e(N,Ra),e(N,Z),e(Z,ct),e(ct,Ja),e(ct,qn),e(qn,Ba),e(ct,Ua),e(Z,Ya),e(Z,xt),e(xt,Ka),e(xt,Dn),e(Dn,Qa),e(Z,Xa),e(Z,lt),e(lt,Za),e(lt,An),e(An,es),e(lt,ts),e(l,ns),e(l,ve),v(pt,ve,null),e(ve,rs),e(ve,ht),e(ht,os),e(ht,qt),e(qt,as),e(ht,ss),e(l,is),e(l,_e),v(mt,_e,null),e(_e,ds),e(_e,gt),e(gt,cs),e(gt,Dt),e(Dt,ls),e(gt,ps),e(l,hs),e(l,ze),v(ut,ze,null),e(ze,ms),e(ze,In),e(In,gs),e(l,us),e(l,R),v(ft,R,null),e(R,fs),e(R,Pn),e(Pn,ks),e(R,vs),e(R,At),e(At,_s),e(At,jn),e(jn,zs),e(l,$s),e(l,S),v(kt,S,null),e(S,bs),e(S,Vn),e(Vn,Ts),e(S,Es),e(S,Ln),e(Ln,ys),e(S,ws),e(S,O),e(O,It),e(It,xs),e(It,Cn),e(Cn,qs),e(O,Ds),e(O,$e),e($e,As),e($e,Nn),e(Nn,Is),e($e,Ps),e($e,Sn),e(Sn,js),e(O,Vs),e(O,Mn),e(Mn,Ls),e(O,Cs),e(O,On),e(On,Ns),Fn=!0},p:od,i(c){Fn||(_(u.$$.fragment,c),_(P.$$.fragment,c),_(Ae.$$.fragment,c),_(Ie.$$.fragment,c),_(je.$$.fragment,c),_(Ve.$$.fragment,c),_(Le.$$.fragment,c),_(Ce.$$.fragment,c),_(Ne.$$.fragment,c),_(Se.$$.fragment,c),_(Me.$$.fragment,c),_(Oe.$$.fragment,c),_(Ge.$$.fragment,c),_(Fe.$$.fragment,c),_(He.$$.fragment,c),_(We.$$.fragment,c),_(Re.$$.fragment,c),_(Je.$$.fragment,c),_(Be.$$.fragment,c),_(Ue.$$.fragment,c),_(Ke.$$.fragment,c),_(Xe.$$.fragment,c),_(et.$$.fragment,c),_(nt.$$.fragment,c),_(rt.$$.fragment,c),_(ot.$$.fragment,c),_(at.$$.fragment,c),_(st.$$.fragment,c),_(it.$$.fragment,c),_(dt.$$.fragment,c),_(pt.$$.fragment,c),_(mt.$$.fragment,c),_(ut.$$.fragment,c),_(ft.$$.fragment,c),_(kt.$$.fragment,c),Fn=!0)},o(c){z(u.$$.fragment,c),z(P.$$.fragment,c),z(Ae.$$.fragment,c),z(Ie.$$.fragment,c),z(je.$$.fragment,c),z(Ve.$$.fragment,c),z(Le.$$.fragment,c),z(Ce.$$.fragment,c),z(Ne.$$.fragment,c),z(Se.$$.fragment,c),z(Me.$$.fragment,c),z(Oe.$$.fragment,c),z(Ge.$$.fragment,c),z(Fe.$$.fragment,c),z(He.$$.fragment,c),z(We.$$.fragment,c),z(Re.$$.fragment,c),z(Je.$$.fragment,c),z(Be.$$.fragment,c),z(Ue.$$.fragment,c),z(Ke.$$.fragment,c),z(Xe.$$.fragment,c),z(et.$$.fragment,c),z(nt.$$.fragment,c),z(rt.$$.fragment,c),z(ot.$$.fragment,c),z(at.$$.fragment,c),z(st.$$.fragment,c),z(it.$$.fragment,c),z(dt.$$.fragment,c),z(pt.$$.fragment,c),z(mt.$$.fragment,c),z(ut.$$.fragment,c),z(ft.$$.fragment,c),z(kt.$$.fragment,c),Fn=!1},d(c){c&&t(m),$(u),c&&t(A),c&&t(l),$(P),$(Ae),$(Ie),$(je),$(Ve),$(Le),$(Ce),$(Ne),$(Se),$(Me),$(Oe),$(Ge),$(Fe),$(He),$(We),$(Re),$(Je),$(Be),$(Ue),$(Ke),$(Xe),$(et),$(nt),$(rt),$(ot),$(at),$(st),$(it),$(dt),$(pt),$(mt),$(ut),$(ft),$(kt)}}}function id(I){let m,b;return m=new Gs({props:{$$slots:{default:[sd]},$$scope:{ctx:I}}}),{c(){f(m.$$.fragment)},l(h){k(m.$$.fragment,h)},m(h,u){v(m,h,u),b=!0},p(h,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:h}),m.$set(y)},i(h){b||(_(m.$$.fragment,h),b=!0)},o(h){z(m.$$.fragment,h),b=!1},d(h){$(m,h)}}}function dd(I){let m,b,h,u,y;return{c(){m=n("p"),b=a("The Rust API Reference is available directly on the "),h=n("a"),u=a("Docs.rs"),y=a(" website."),this.h()},l(w){m=r(w,"P",{});var q=o(m);b=s(q,"The Rust API Reference is available directly on the "),h=r(q,"A",{href:!0,rel:!0});var A=o(h);u=s(A,"Docs.rs"),A.forEach(t),y=s(q," website."),q.forEach(t),this.h()},h(){g(h,"href","https://docs.rs/tokenizers/latest/tokenizers/"),g(h,"rel","nofollow")},m(w,q){De(w,m,q),e(m,b),e(m,h),e(h,u),e(m,y)},d(w){w&&t(m)}}}function cd(I){let m,b;return m=new Gs({props:{$$slots:{default:[dd]},$$scope:{ctx:I}}}),{c(){f(m.$$.fragment)},l(h){k(m.$$.fragment,h)},m(h,u){v(m,h,u),b=!0},p(h,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:h}),m.$set(y)},i(h){b||(_(m.$$.fragment,h),b=!0)},o(h){z(m.$$.fragment,h),b=!1},d(h){$(m,h)}}}function ld(I){let m,b;return{c(){m=n("p"),b=a("The node API has not been documented yet.")},l(h){m=r(h,"P",{});var u=o(m);b=s(u,"The node API has not been documented yet."),u.forEach(t)},m(h,u){De(h,m,u),e(m,b)},d(h){h&&t(m)}}}function pd(I){let m,b;return m=new Gs({props:{$$slots:{default:[ld]},$$scope:{ctx:I}}}),{c(){f(m.$$.fragment)},l(h){k(m.$$.fragment,h)},m(h,u){v(m,h,u),b=!0},p(h,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:h}),m.$set(y)},i(h){b||(_(m.$$.fragment,h),b=!0)},o(h){z(m.$$.fragment,h),b=!1},d(h){$(m,h)}}}function hd(I){let m,b,h,u,y,w,q,A,l,P,j,D;return w=new Xi({}),j=new ad({props:{python:!0,rust:!0,node:!0,$$slots:{node:[pd],rust:[cd],python:[id]},$$scope:{ctx:I}}}),{c(){m=n("meta"),b=i(),h=n("h1"),u=n("a"),y=n("span"),f(w.$$.fragment),q=i(),A=n("span"),l=a("Tokenizer"),P=i(),f(j.$$.fragment),this.h()},l(E){const x=nd('[data-svelte="svelte-1phssyn"]',document.head);m=r(x,"META",{name:!0,content:!0}),x.forEach(t),b=d(E),h=r(E,"H1",{class:!0});var M=o(h);u=r(M,"A",{id:!0,class:!0,href:!0});var vt=o(u);y=r(vt,"SPAN",{});var J=o(y);k(w.$$.fragment,J),J.forEach(t),vt.forEach(t),q=d(M),A=r(M,"SPAN",{});var _t=o(A);l=s(_t,"Tokenizer"),_t.forEach(t),M.forEach(t),P=d(E),k(j.$$.fragment,E),this.h()},h(){g(m,"name","hf:doc:metadata"),g(m,"content",JSON.stringify(md)),g(u,"id","tokenizer"),g(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(u,"href","#tokenizer"),g(h,"class","relative group")},m(E,x){e(document.head,m),De(E,b,x),De(E,h,x),e(h,u),e(u,y),v(w,y,null),e(h,q),e(h,A),e(A,l),De(E,P,x),v(j,E,x),D=!0},p(E,[x]){const M={};x&2&&(M.$$scope={dirty:x,ctx:E}),j.$set(M)},i(E){D||(_(w.$$.fragment,E),_(j.$$.fragment,E),D=!0)},o(E){z(w.$$.fragment,E),z(j.$$.fragment,E),D=!1},d(E){t(m),E&&t(b),E&&t(h),$(w),E&&t(P),$(j,E)}}}const md={local:"tokenizer",sections:[{local:"tokenizers.Tokenizer",title:"Tokenizer"}],title:"Tokenizer"};function gd(I){return rd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class zd extends Zi{constructor(m){super();ed(this,m,gd,hd,td,{})}}export{zd as default,md as metadata};
