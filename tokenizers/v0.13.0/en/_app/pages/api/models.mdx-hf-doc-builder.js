import{S as na,i as aa,s as sa,e as r,k as p,w as $,t as i,M as la,c as n,d as t,m as f,a,x as _,h as d,b as u,G as e,g as P,y as b,q as k,o as E,B as y,v as ia,L as ln}from"../../chunks/vendor-hf-doc-builder.js";import{D as B}from"../../chunks/Docstring-hf-doc-builder.js";import{C as dn}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as nt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{T as da,M as cn}from"../../chunks/TokenizersLanguageContent-hf-doc-builder.js";import{E as sn}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function ca(D){let s,h,l,m,v;return m=new dn({props:{code:`vocab, merges = BPE.read_file(vocab_filename, merges_filename)
bpe = BPE(vocab, merges)`,highlighted:`vocab, merges = BPE.read_file(vocab_filename, merges_filename)
bpe = BPE(vocab, merges)`}}),{c(){s=r("p"),h=i("This method is roughly equivalent to doing:"),l=p(),$(m.$$.fragment)},l(c){s=n(c,"P",{});var g=a(s);h=d(g,"This method is roughly equivalent to doing:"),g.forEach(t),l=f(c),_(m.$$.fragment,c)},m(c,g){P(c,s,g),e(s,h),P(c,l,g),b(m,c,g),v=!0},p:ln,i(c){v||(k(m.$$.fragment,c),v=!0)},o(c){E(m.$$.fragment,c),v=!1},d(c){c&&t(s),c&&t(l),y(m,c)}}}function ma(D){let s,h,l,m,v;return m=new dn({props:{code:`vocab = WordLevel.read_file(vocab_filename)
wordlevel = WordLevel(vocab)`,highlighted:`vocab = WordLevel.read_file(vocab_filename)
wordlevel = WordLevel(vocab)`}}),{c(){s=r("p"),h=i("This method is roughly equivalent to doing:"),l=p(),$(m.$$.fragment)},l(c){s=n(c,"P",{});var g=a(s);h=d(g,"This method is roughly equivalent to doing:"),g.forEach(t),l=f(c),_(m.$$.fragment,c)},m(c,g){P(c,s,g),e(s,h),P(c,l,g),b(m,c,g),v=!0},p:ln,i(c){v||(k(m.$$.fragment,c),v=!0)},o(c){E(m.$$.fragment,c),v=!1},d(c){c&&t(s),c&&t(l),y(m,c)}}}function pa(D){let s,h,l,m,v;return m=new dn({props:{code:`vocab = WordPiece.read_file(vocab_filename)
wordpiece = WordPiece(vocab)`,highlighted:`vocab = WordPiece.read_file(vocab_filename)
wordpiece = WordPiece(vocab)`}}),{c(){s=r("p"),h=i("This method is roughly equivalent to doing:"),l=p(),$(m.$$.fragment)},l(c){s=n(c,"P",{});var g=a(s);h=d(g,"This method is roughly equivalent to doing:"),g.forEach(t),l=f(c),_(m.$$.fragment,c)},m(c,g){P(c,s,g),e(s,h),P(c,l,g),b(m,c,g),v=!0},p:ln,i(c){v||(k(m.$$.fragment,c),v=!0)},o(c){E(m.$$.fragment,c),v=!1},d(c){c&&t(s),c&&t(l),y(m,c)}}}function fa(D){let s,h,l,m,v,c,g,M,A,N,S,O,x,L,T,R,je,ae,mo,po,se,fo,U,ho,at,uo,vo,st,go,$o,Ge,_o,bo,j,ke,ko,Y,Eo,lt,yo,wo,it,Po,xo,zo,dt,To,Gt,Z,le,ct,Ee,Do,mt,Wo,Ht,z,ye,Ao,pt,Bo,Io,ft,Lo,Mo,ht,qo,No,G,we,So,He,Co,ut,Vo,Oo,ee,Ro,vt,Uo,jo,Je,Go,Ho,Jo,ie,Pe,Fo,gt,Ko,Qo,H,xe,Xo,$t,Yo,Zo,_t,er,tr,de,ze,or,bt,rr,nr,ce,Te,ar,kt,sr,Jt,te,me,Et,De,lr,yt,ir,Ft,oe,We,dr,wt,cr,Kt,re,pe,Pt,Ae,mr,xt,pr,Qt,I,Be,fr,zt,hr,ur,Tt,vr,gr,C,Ie,$r,Dt,_r,br,fe,kr,J,Er,Wt,yr,wr,At,Pr,xr,Fe,zr,Tr,F,Le,Dr,Ke,Wr,Bt,Ar,Br,It,Ir,Xt,ne,he,Lt,Me,Lr,Mt,Mr,Yt,q,qe,qr,qt,Nr,Sr,V,Ne,Cr,Nt,Vr,Or,ue,Rr,K,Ur,St,jr,Gr,Ct,Hr,Jr,Qe,Fr,Kr,Q,Se,Qr,Ce,Xr,Vt,Yr,Zr,en,Ve,tn,Ot,on,rn,Zt;return m=new nt({}),N=new B({props:{name:"class tokenizers.models.BPE",anchor:"tokenizers.models.BPE",parameters:[{name:"vocab",val:" = None"},{name:"merges",val:" = None"},{name:"cache_capacity",val:" = None"},{name:"dropout",val:" = None"},{name:"unk_token",val:" = None"},{name:"continuing_subword_prefix",val:" = None"},{name:"end_of_word_suffix",val:" = None"},{name:"fuse_unk",val:" = None"}],parametersDescription:[{anchor:"tokenizers.models.BPE.vocab",description:`<strong>vocab</strong> (<code>Dict[str, int]</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.BPE.merges",description:`<strong>merges</strong> (<code>List[Tuple[str, str]]</code>, <em>optional</em>) &#x2014;
A list of pairs of tokens (<code>Tuple[str, str]</code>) <code>[(&quot;a&quot;, &quot;b&quot;),...]</code>`,name:"merges"},{anchor:"tokenizers.models.BPE.cache_capacity",description:`<strong>cache_capacity</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of words that the BPE cache can contain. The cache allows
to speed-up the process by keeping the result of the merge operations
for a number of words.`,name:"cache_capacity"},{anchor:"tokenizers.models.BPE.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>) &#x2014;
A float between 0 and 1 that represents the BPE dropout to use.`,name:"dropout"},{anchor:"tokenizers.models.BPE.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"},{anchor:"tokenizers.models.BPE.continuing_subword_prefix",description:`<strong>continuing_subword_prefix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The prefix to attach to subword units that don&#x2019;t represent a beginning of word.`,name:"continuing_subword_prefix"},{anchor:"tokenizers.models.BPE.end_of_word_suffix",description:`<strong>end_of_word_suffix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The suffix to attach to subword units that represent an end of word.`,name:"end_of_word_suffix"},{anchor:"tokenizers.models.BPE.fuse_unk",description:`<strong>fuse_unk</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to fuse any subsequent unknown tokens into a single one`,name:"fuse_unk"}]}}),R=new B({props:{name:"from_file",anchor:"tokenizers.models.BPE.from_file",parameters:[{name:"vocab",val:""},{name:"merge",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"tokenizers.models.BPE.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"},{anchor:"tokenizers.models.BPE.from_file.merges",description:`<strong>merges</strong> (<code>str</code>) &#x2014;
The path to a <code>merges.txt</code> file`,name:"merges"}],returnDescription:`
<p>An instance of BPE loaded from these files</p>
`,returnType:`
<p><a href="/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.BPE">BPE</a></p>
`}}),se=new sn({props:{anchor:"tokenizers.models.BPE.from_file.example",$$slots:{default:[ca]},$$scope:{ctx:D}}}),ke=new B({props:{name:"read_file",anchor:"tokenizers.models.BPE.read_file",parameters:[{name:"vocab",val:""},{name:"merges",val:""}],parametersDescription:[{anchor:"tokenizers.models.BPE.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"},{anchor:"tokenizers.models.BPE.read_file.merges",description:`<strong>merges</strong> (<code>str</code>) &#x2014;
The path to a <code>merges.txt</code> file`,name:"merges"}],returnDescription:`
<p>The vocabulary and merges loaded into memory</p>
`,returnType:`
<p>A <code>Tuple</code> with the vocab and the merges</p>
`}}),Ee=new nt({}),ye=new B({props:{name:"class tokenizers.models.Model",anchor:"tokenizers.models.Model",parameters:""}}),we=new B({props:{name:"get_trainer",anchor:"tokenizers.models.Model.get_trainer",parameters:"",returnDescription:`
<p>The Trainer used to train this model</p>
`,returnType:`
<p><code>Trainer</code></p>
`}}),Pe=new B({props:{name:"id_to_token",anchor:"tokenizers.models.Model.id_to_token",parameters:[{name:"id",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.id_to_token.id",description:`<strong>id</strong> (<code>int</code>) &#x2014;
An ID to convert to a token`,name:"id"}],returnDescription:`
<p>The token associated to the ID</p>
`,returnType:`
<p><code>str</code></p>
`}}),xe=new B({props:{name:"save",anchor:"tokenizers.models.Model.save",parameters:[{name:"folder",val:""},{name:"prefix",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.save.folder",description:`<strong>folder</strong> (<code>str</code>) &#x2014;
The path to the target folder in which to save the various files`,name:"folder"},{anchor:"tokenizers.models.Model.save.prefix",description:`<strong>prefix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
An optional prefix, used to prefix each file name`,name:"prefix"}],returnDescription:`
<p>The list of saved files</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),ze=new B({props:{name:"token_to_id",anchor:"tokenizers.models.Model.token_to_id",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.token_to_id.token",description:`<strong>token</strong> (<code>str</code>) &#x2014;
A token to convert to an ID`,name:"token"}],returnDescription:`
<p>The ID associated to the token</p>
`,returnType:`
<p><code>int</code></p>
`}}),Te=new B({props:{name:"tokenize",anchor:"tokenizers.models.Model.tokenize",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.tokenize.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A sequence to tokenize`,name:"sequence"}],returnDescription:`
<p>The generated tokens</p>
`,returnType:`
<p>A <code>List</code> of <code>Token</code></p>
`}}),De=new nt({}),We=new B({props:{name:"class tokenizers.models.Unigram",anchor:"tokenizers.models.Unigram",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.Unigram.vocab",description:`<strong>vocab</strong> (<code>List[Tuple[str, float]]</code>, <em>optional</em>) &#x2014;
A list of vocabulary items and their relative score [(&#x201C;am&#x201D;, -0.2442),&#x2026;]`,name:"vocab"}]}}),Ae=new nt({}),Be=new B({props:{name:"class tokenizers.models.WordLevel",anchor:"tokenizers.models.WordLevel",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.vocab",description:`<strong>vocab</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.WordLevel.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"}]}}),Ie=new B({props:{name:"from_file",anchor:"tokenizers.models.WordLevel.from_file",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"}],returnDescription:`
<p>An instance of WordLevel loaded from file</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.WordLevel"
>WordLevel</a></p>
`}}),fe=new sn({props:{anchor:"tokenizers.models.WordLevel.from_file.example",$$slots:{default:[ma]},$$scope:{ctx:D}}}),Le=new B({props:{name:"read_file",anchor:"tokenizers.models.WordLevel.read_file",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"}],returnDescription:`
<p>The vocabulary as a <code>dict</code></p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),Me=new nt({}),qe=new B({props:{name:"class tokenizers.models.WordPiece",anchor:"tokenizers.models.WordPiece",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""},{name:"max_input_chars_per_word",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.vocab",description:`<strong>vocab</strong> (<code>Dict[str, int]</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.WordPiece.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"},{anchor:"tokenizers.models.WordPiece.max_input_chars_per_word",description:`<strong>max_input_chars_per_word</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum number of characters to authorize in a single word.`,name:"max_input_chars_per_word"}]}}),Ne=new B({props:{name:"from_file",anchor:"tokenizers.models.WordPiece.from_file",parameters:[{name:"vocab",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.txt</code> file`,name:"vocab"}],returnDescription:`
<p>An instance of WordPiece loaded from file</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.WordPiece"
>WordPiece</a></p>
`}}),ue=new sn({props:{anchor:"tokenizers.models.WordPiece.from_file.example",$$slots:{default:[pa]},$$scope:{ctx:D}}}),Se=new B({props:{name:"read_file",anchor:"tokenizers.models.WordPiece.read_file",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.txt</code> file`,name:"vocab"}],returnDescription:`
<p>The vocabulary as a <code>dict</code></p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),{c(){s=r("h2"),h=r("a"),l=r("span"),$(m.$$.fragment),v=p(),c=r("span"),g=i("BPE"),M=p(),A=r("div"),$(N.$$.fragment),S=p(),O=r("p"),x=i("An implementation of the BPE (Byte-Pair Encoding) algorithm"),L=p(),T=r("div"),$(R.$$.fragment),je=p(),ae=r("p"),mo=i("Instantiate a BPE model from the given files."),po=p(),$(se.$$.fragment),fo=p(),U=r("p"),ho=i("If you don\u2019t need to keep the "),at=r("code"),uo=i("vocab, merges"),vo=i(` values lying around,
this method is more optimized than manually calling
`),st=r("code"),go=i("read_file()"),$o=i(" to initialize a "),Ge=r("a"),_o=i("BPE"),bo=p(),j=r("div"),$(ke.$$.fragment),ko=p(),Y=r("p"),Eo=i("Read a "),lt=r("code"),yo=i("vocab.json"),wo=i(" and a "),it=r("code"),Po=i("merges.txt"),xo=i(" files"),zo=p(),dt=r("p"),To=i(`This method provides a way to read and parse the content of these files,
returning the relevant data structures. If you want to instantiate some BPE models
from memory, this method gives you the expected input from the standard files.`),Gt=p(),Z=r("h2"),le=r("a"),ct=r("span"),$(Ee.$$.fragment),Do=p(),mt=r("span"),Wo=i("Model"),Ht=p(),z=r("div"),$(ye.$$.fragment),Ao=p(),pt=r("p"),Bo=i("Base class for all models"),Io=p(),ft=r("p"),Lo=i(`The model represents the actual tokenization algorithm. This is the part that
will contain and manage the learned vocabulary.`),Mo=p(),ht=r("p"),qo=i("This class cannot be constructed directly. Please use one of the concrete models."),No=p(),G=r("div"),$(we.$$.fragment),So=p(),He=r("p"),Co=i("Get the associated "),ut=r("code"),Vo=i("Trainer"),Oo=p(),ee=r("p"),Ro=i("Retrieve the "),vt=r("code"),Uo=i("Trainer"),jo=i(` associated to this
`),Je=r("a"),Go=i("Model"),Ho=i("."),Jo=p(),ie=r("div"),$(Pe.$$.fragment),Fo=p(),gt=r("p"),Ko=i("Get the token associated to an ID"),Qo=p(),H=r("div"),$(xe.$$.fragment),Xo=p(),$t=r("p"),Yo=i("Save the current model"),Zo=p(),_t=r("p"),er=i(`Save the current model in the given folder, using the given prefix for the various
files that will get created.
Any file with the same name that already exists in this folder will be overwritten.`),tr=p(),de=r("div"),$(ze.$$.fragment),or=p(),bt=r("p"),rr=i("Get the ID associated to a token"),nr=p(),ce=r("div"),$(Te.$$.fragment),ar=p(),kt=r("p"),sr=i("Tokenize a sequence"),Jt=p(),te=r("h2"),me=r("a"),Et=r("span"),$(De.$$.fragment),lr=p(),yt=r("span"),ir=i("Unigram"),Ft=p(),oe=r("div"),$(We.$$.fragment),dr=p(),wt=r("p"),cr=i("An implementation of the Unigram algorithm"),Kt=p(),re=r("h2"),pe=r("a"),Pt=r("span"),$(Ae.$$.fragment),mr=p(),xt=r("span"),pr=i("WordLevel"),Qt=p(),I=r("div"),$(Be.$$.fragment),fr=p(),zt=r("p"),hr=i("An implementation of the WordLevel algorithm"),ur=p(),Tt=r("p"),vr=i("Most simple tokenizer model based on mapping tokens to their corresponding id."),gr=p(),C=r("div"),$(Ie.$$.fragment),$r=p(),Dt=r("p"),_r=i("Instantiate a WordLevel model from the given file"),br=p(),$(fe.$$.fragment),kr=p(),J=r("p"),Er=i("If you don\u2019t need to keep the "),Wt=r("code"),yr=i("vocab"),wr=i(` values lying around, this method is
more optimized than manually calling `),At=r("code"),Pr=i("read_file()"),xr=i(` to
initialize a `),Fe=r("a"),zr=i("WordLevel"),Tr=p(),F=r("div"),$(Le.$$.fragment),Dr=p(),Ke=r("p"),Wr=i("Read a "),Bt=r("code"),Ar=i("vocab.json"),Br=p(),It=r("p"),Ir=i(`This method provides a way to read and parse the content of a vocabulary file,
returning the relevant data structures. If you want to instantiate some WordLevel models
from memory, this method gives you the expected input from the standard files.`),Xt=p(),ne=r("h2"),he=r("a"),Lt=r("span"),$(Me.$$.fragment),Lr=p(),Mt=r("span"),Mr=i("WordPiece"),Yt=p(),q=r("div"),$(qe.$$.fragment),qr=p(),qt=r("p"),Nr=i("An implementation of the WordPiece algorithm"),Sr=p(),V=r("div"),$(Ne.$$.fragment),Cr=p(),Nt=r("p"),Vr=i("Instantiate a WordPiece model from the given file"),Or=p(),$(ue.$$.fragment),Rr=p(),K=r("p"),Ur=i("If you don\u2019t need to keep the "),St=r("code"),jr=i("vocab"),Gr=i(` values lying around, this method is
more optimized than manually calling `),Ct=r("code"),Hr=i("read_file()"),Jr=i(` to
initialize a `),Qe=r("a"),Fr=i("WordPiece"),Kr=p(),Q=r("div"),$(Se.$$.fragment),Qr=p(),Ce=r("p"),Xr=i("Read a "),Vt=r("code"),Yr=i("vocab.txt"),Zr=i(" file"),en=p(),Ve=r("p"),tn=i("This method provides a way to read and parse the content of a standard "),Ot=r("em"),on=i("vocab.txt"),rn=i(`
file as used by the WordPiece Model, returning the relevant data structures. If you
want to instantiate some WordPiece models from memory, this method gives you the
expected input from the standard files.`),this.h()},l(o){s=n(o,"H2",{class:!0});var w=a(s);h=n(w,"A",{id:!0,class:!0,href:!0});var Rt=a(h);l=n(Rt,"SPAN",{});var Ut=a(l);_(m.$$.fragment,Ut),Ut.forEach(t),Rt.forEach(t),v=f(w),c=n(w,"SPAN",{});var jt=a(c);g=d(jt,"BPE"),jt.forEach(t),w.forEach(t),M=f(o),A=n(o,"DIV",{class:!0});var ve=a(A);_(N.$$.fragment,ve),S=f(ve),O=n(ve,"P",{});var mn=a(O);x=d(mn,"An implementation of the BPE (Byte-Pair Encoding) algorithm"),mn.forEach(t),L=f(ve),T=n(ve,"DIV",{class:!0});var ge=a(T);_(R.$$.fragment,ge),je=f(ge),ae=n(ge,"P",{});var pn=a(ae);mo=d(pn,"Instantiate a BPE model from the given files."),pn.forEach(t),po=f(ge),_(se.$$.fragment,ge),fo=f(ge),U=n(ge,"P",{});var Oe=a(U);ho=d(Oe,"If you don\u2019t need to keep the "),at=n(Oe,"CODE",{});var fn=a(at);uo=d(fn,"vocab, merges"),fn.forEach(t),vo=d(Oe,` values lying around,
this method is more optimized than manually calling
`),st=n(Oe,"CODE",{});var hn=a(st);go=d(hn,"read_file()"),hn.forEach(t),$o=d(Oe," to initialize a "),Ge=n(Oe,"A",{href:!0});var un=a(Ge);_o=d(un,"BPE"),un.forEach(t),Oe.forEach(t),ge.forEach(t),bo=f(ve),j=n(ve,"DIV",{class:!0});var Xe=a(j);_(ke.$$.fragment,Xe),ko=f(Xe),Y=n(Xe,"P",{});var Ye=a(Y);Eo=d(Ye,"Read a "),lt=n(Ye,"CODE",{});var vn=a(lt);yo=d(vn,"vocab.json"),vn.forEach(t),wo=d(Ye," and a "),it=n(Ye,"CODE",{});var gn=a(it);Po=d(gn,"merges.txt"),gn.forEach(t),xo=d(Ye," files"),Ye.forEach(t),zo=f(Xe),dt=n(Xe,"P",{});var $n=a(dt);To=d($n,`This method provides a way to read and parse the content of these files,
returning the relevant data structures. If you want to instantiate some BPE models
from memory, this method gives you the expected input from the standard files.`),$n.forEach(t),Xe.forEach(t),ve.forEach(t),Gt=f(o),Z=n(o,"H2",{class:!0});var eo=a(Z);le=n(eo,"A",{id:!0,class:!0,href:!0});var _n=a(le);ct=n(_n,"SPAN",{});var bn=a(ct);_(Ee.$$.fragment,bn),bn.forEach(t),_n.forEach(t),Do=f(eo),mt=n(eo,"SPAN",{});var kn=a(mt);Wo=d(kn,"Model"),kn.forEach(t),eo.forEach(t),Ht=f(o),z=n(o,"DIV",{class:!0});var W=a(z);_(ye.$$.fragment,W),Ao=f(W),pt=n(W,"P",{});var En=a(pt);Bo=d(En,"Base class for all models"),En.forEach(t),Io=f(W),ft=n(W,"P",{});var yn=a(ft);Lo=d(yn,`The model represents the actual tokenization algorithm. This is the part that
will contain and manage the learned vocabulary.`),yn.forEach(t),Mo=f(W),ht=n(W,"P",{});var wn=a(ht);qo=d(wn,"This class cannot be constructed directly. Please use one of the concrete models."),wn.forEach(t),No=f(W),G=n(W,"DIV",{class:!0});var Ze=a(G);_(we.$$.fragment,Ze),So=f(Ze),He=n(Ze,"P",{});var nn=a(He);Co=d(nn,"Get the associated "),ut=n(nn,"CODE",{});var Pn=a(ut);Vo=d(Pn,"Trainer"),Pn.forEach(t),nn.forEach(t),Oo=f(Ze),ee=n(Ze,"P",{});var et=a(ee);Ro=d(et,"Retrieve the "),vt=n(et,"CODE",{});var xn=a(vt);Uo=d(xn,"Trainer"),xn.forEach(t),jo=d(et,` associated to this
`),Je=n(et,"A",{href:!0});var zn=a(Je);Go=d(zn,"Model"),zn.forEach(t),Ho=d(et,"."),et.forEach(t),Ze.forEach(t),Jo=f(W),ie=n(W,"DIV",{class:!0});var to=a(ie);_(Pe.$$.fragment,to),Fo=f(to),gt=n(to,"P",{});var Tn=a(gt);Ko=d(Tn,"Get the token associated to an ID"),Tn.forEach(t),to.forEach(t),Qo=f(W),H=n(W,"DIV",{class:!0});var tt=a(H);_(xe.$$.fragment,tt),Xo=f(tt),$t=n(tt,"P",{});var Dn=a($t);Yo=d(Dn,"Save the current model"),Dn.forEach(t),Zo=f(tt),_t=n(tt,"P",{});var Wn=a(_t);er=d(Wn,`Save the current model in the given folder, using the given prefix for the various
files that will get created.
Any file with the same name that already exists in this folder will be overwritten.`),Wn.forEach(t),tt.forEach(t),tr=f(W),de=n(W,"DIV",{class:!0});var oo=a(de);_(ze.$$.fragment,oo),or=f(oo),bt=n(oo,"P",{});var An=a(bt);rr=d(An,"Get the ID associated to a token"),An.forEach(t),oo.forEach(t),nr=f(W),ce=n(W,"DIV",{class:!0});var ro=a(ce);_(Te.$$.fragment,ro),ar=f(ro),kt=n(ro,"P",{});var Bn=a(kt);sr=d(Bn,"Tokenize a sequence"),Bn.forEach(t),ro.forEach(t),W.forEach(t),Jt=f(o),te=n(o,"H2",{class:!0});var no=a(te);me=n(no,"A",{id:!0,class:!0,href:!0});var In=a(me);Et=n(In,"SPAN",{});var Ln=a(Et);_(De.$$.fragment,Ln),Ln.forEach(t),In.forEach(t),lr=f(no),yt=n(no,"SPAN",{});var Mn=a(yt);ir=d(Mn,"Unigram"),Mn.forEach(t),no.forEach(t),Ft=f(o),oe=n(o,"DIV",{class:!0});var ao=a(oe);_(We.$$.fragment,ao),dr=f(ao),wt=n(ao,"P",{});var qn=a(wt);cr=d(qn,"An implementation of the Unigram algorithm"),qn.forEach(t),ao.forEach(t),Kt=f(o),re=n(o,"H2",{class:!0});var so=a(re);pe=n(so,"A",{id:!0,class:!0,href:!0});var Nn=a(pe);Pt=n(Nn,"SPAN",{});var Sn=a(Pt);_(Ae.$$.fragment,Sn),Sn.forEach(t),Nn.forEach(t),mr=f(so),xt=n(so,"SPAN",{});var Cn=a(xt);pr=d(Cn,"WordLevel"),Cn.forEach(t),so.forEach(t),Qt=f(o),I=n(o,"DIV",{class:!0});var X=a(I);_(Be.$$.fragment,X),fr=f(X),zt=n(X,"P",{});var Vn=a(zt);hr=d(Vn,"An implementation of the WordLevel algorithm"),Vn.forEach(t),ur=f(X),Tt=n(X,"P",{});var On=a(Tt);vr=d(On,"Most simple tokenizer model based on mapping tokens to their corresponding id."),On.forEach(t),gr=f(X),C=n(X,"DIV",{class:!0});var $e=a(C);_(Ie.$$.fragment,$e),$r=f($e),Dt=n($e,"P",{});var Rn=a(Dt);_r=d(Rn,"Instantiate a WordLevel model from the given file"),Rn.forEach(t),br=f($e),_(fe.$$.fragment,$e),kr=f($e),J=n($e,"P",{});var Re=a(J);Er=d(Re,"If you don\u2019t need to keep the "),Wt=n(Re,"CODE",{});var Un=a(Wt);yr=d(Un,"vocab"),Un.forEach(t),wr=d(Re,` values lying around, this method is
more optimized than manually calling `),At=n(Re,"CODE",{});var jn=a(At);Pr=d(jn,"read_file()"),jn.forEach(t),xr=d(Re,` to
initialize a `),Fe=n(Re,"A",{href:!0});var Gn=a(Fe);zr=d(Gn,"WordLevel"),Gn.forEach(t),Re.forEach(t),$e.forEach(t),Tr=f(X),F=n(X,"DIV",{class:!0});var ot=a(F);_(Le.$$.fragment,ot),Dr=f(ot),Ke=n(ot,"P",{});var an=a(Ke);Wr=d(an,"Read a "),Bt=n(an,"CODE",{});var Hn=a(Bt);Ar=d(Hn,"vocab.json"),Hn.forEach(t),an.forEach(t),Br=f(ot),It=n(ot,"P",{});var Jn=a(It);Ir=d(Jn,`This method provides a way to read and parse the content of a vocabulary file,
returning the relevant data structures. If you want to instantiate some WordLevel models
from memory, this method gives you the expected input from the standard files.`),Jn.forEach(t),ot.forEach(t),X.forEach(t),Xt=f(o),ne=n(o,"H2",{class:!0});var lo=a(ne);he=n(lo,"A",{id:!0,class:!0,href:!0});var Fn=a(he);Lt=n(Fn,"SPAN",{});var Kn=a(Lt);_(Me.$$.fragment,Kn),Kn.forEach(t),Fn.forEach(t),Lr=f(lo),Mt=n(lo,"SPAN",{});var Qn=a(Mt);Mr=d(Qn,"WordPiece"),Qn.forEach(t),lo.forEach(t),Yt=f(o),q=n(o,"DIV",{class:!0});var _e=a(q);_(qe.$$.fragment,_e),qr=f(_e),qt=n(_e,"P",{});var Xn=a(qt);Nr=d(Xn,"An implementation of the WordPiece algorithm"),Xn.forEach(t),Sr=f(_e),V=n(_e,"DIV",{class:!0});var be=a(V);_(Ne.$$.fragment,be),Cr=f(be),Nt=n(be,"P",{});var Yn=a(Nt);Vr=d(Yn,"Instantiate a WordPiece model from the given file"),Yn.forEach(t),Or=f(be),_(ue.$$.fragment,be),Rr=f(be),K=n(be,"P",{});var Ue=a(K);Ur=d(Ue,"If you don\u2019t need to keep the "),St=n(Ue,"CODE",{});var Zn=a(St);jr=d(Zn,"vocab"),Zn.forEach(t),Gr=d(Ue,` values lying around, this method is
more optimized than manually calling `),Ct=n(Ue,"CODE",{});var ea=a(Ct);Hr=d(ea,"read_file()"),ea.forEach(t),Jr=d(Ue,` to
initialize a `),Qe=n(Ue,"A",{href:!0});var ta=a(Qe);Fr=d(ta,"WordPiece"),ta.forEach(t),Ue.forEach(t),be.forEach(t),Kr=f(_e),Q=n(_e,"DIV",{class:!0});var rt=a(Q);_(Se.$$.fragment,rt),Qr=f(rt),Ce=n(rt,"P",{});var io=a(Ce);Xr=d(io,"Read a "),Vt=n(io,"CODE",{});var oa=a(Vt);Yr=d(oa,"vocab.txt"),oa.forEach(t),Zr=d(io," file"),io.forEach(t),en=f(rt),Ve=n(rt,"P",{});var co=a(Ve);tn=d(co,"This method provides a way to read and parse the content of a standard "),Ot=n(co,"EM",{});var ra=a(Ot);on=d(ra,"vocab.txt"),ra.forEach(t),rn=d(co,`
file as used by the WordPiece Model, returning the relevant data structures. If you
want to instantiate some WordPiece models from memory, this method gives you the
expected input from the standard files.`),co.forEach(t),rt.forEach(t),_e.forEach(t),this.h()},h(){u(h,"id","tokenizers.models.BPE"),u(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(h,"href","#tokenizers.models.BPE"),u(s,"class","relative group"),u(Ge,"href","/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.BPE"),u(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(le,"id","tokenizers.models.Model"),u(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(le,"href","#tokenizers.models.Model"),u(Z,"class","relative group"),u(Je,"href","/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.Model"),u(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(me,"id","tokenizers.models.Unigram"),u(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(me,"href","#tokenizers.models.Unigram"),u(te,"class","relative group"),u(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(pe,"id","tokenizers.models.WordLevel"),u(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(pe,"href","#tokenizers.models.WordLevel"),u(re,"class","relative group"),u(Fe,"href","/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.WordLevel"),u(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(he,"id","tokenizers.models.WordPiece"),u(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(he,"href","#tokenizers.models.WordPiece"),u(ne,"class","relative group"),u(Qe,"href","/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.WordPiece"),u(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,w){P(o,s,w),e(s,h),e(h,l),b(m,l,null),e(s,v),e(s,c),e(c,g),P(o,M,w),P(o,A,w),b(N,A,null),e(A,S),e(A,O),e(O,x),e(A,L),e(A,T),b(R,T,null),e(T,je),e(T,ae),e(ae,mo),e(T,po),b(se,T,null),e(T,fo),e(T,U),e(U,ho),e(U,at),e(at,uo),e(U,vo),e(U,st),e(st,go),e(U,$o),e(U,Ge),e(Ge,_o),e(A,bo),e(A,j),b(ke,j,null),e(j,ko),e(j,Y),e(Y,Eo),e(Y,lt),e(lt,yo),e(Y,wo),e(Y,it),e(it,Po),e(Y,xo),e(j,zo),e(j,dt),e(dt,To),P(o,Gt,w),P(o,Z,w),e(Z,le),e(le,ct),b(Ee,ct,null),e(Z,Do),e(Z,mt),e(mt,Wo),P(o,Ht,w),P(o,z,w),b(ye,z,null),e(z,Ao),e(z,pt),e(pt,Bo),e(z,Io),e(z,ft),e(ft,Lo),e(z,Mo),e(z,ht),e(ht,qo),e(z,No),e(z,G),b(we,G,null),e(G,So),e(G,He),e(He,Co),e(He,ut),e(ut,Vo),e(G,Oo),e(G,ee),e(ee,Ro),e(ee,vt),e(vt,Uo),e(ee,jo),e(ee,Je),e(Je,Go),e(ee,Ho),e(z,Jo),e(z,ie),b(Pe,ie,null),e(ie,Fo),e(ie,gt),e(gt,Ko),e(z,Qo),e(z,H),b(xe,H,null),e(H,Xo),e(H,$t),e($t,Yo),e(H,Zo),e(H,_t),e(_t,er),e(z,tr),e(z,de),b(ze,de,null),e(de,or),e(de,bt),e(bt,rr),e(z,nr),e(z,ce),b(Te,ce,null),e(ce,ar),e(ce,kt),e(kt,sr),P(o,Jt,w),P(o,te,w),e(te,me),e(me,Et),b(De,Et,null),e(te,lr),e(te,yt),e(yt,ir),P(o,Ft,w),P(o,oe,w),b(We,oe,null),e(oe,dr),e(oe,wt),e(wt,cr),P(o,Kt,w),P(o,re,w),e(re,pe),e(pe,Pt),b(Ae,Pt,null),e(re,mr),e(re,xt),e(xt,pr),P(o,Qt,w),P(o,I,w),b(Be,I,null),e(I,fr),e(I,zt),e(zt,hr),e(I,ur),e(I,Tt),e(Tt,vr),e(I,gr),e(I,C),b(Ie,C,null),e(C,$r),e(C,Dt),e(Dt,_r),e(C,br),b(fe,C,null),e(C,kr),e(C,J),e(J,Er),e(J,Wt),e(Wt,yr),e(J,wr),e(J,At),e(At,Pr),e(J,xr),e(J,Fe),e(Fe,zr),e(I,Tr),e(I,F),b(Le,F,null),e(F,Dr),e(F,Ke),e(Ke,Wr),e(Ke,Bt),e(Bt,Ar),e(F,Br),e(F,It),e(It,Ir),P(o,Xt,w),P(o,ne,w),e(ne,he),e(he,Lt),b(Me,Lt,null),e(ne,Lr),e(ne,Mt),e(Mt,Mr),P(o,Yt,w),P(o,q,w),b(qe,q,null),e(q,qr),e(q,qt),e(qt,Nr),e(q,Sr),e(q,V),b(Ne,V,null),e(V,Cr),e(V,Nt),e(Nt,Vr),e(V,Or),b(ue,V,null),e(V,Rr),e(V,K),e(K,Ur),e(K,St),e(St,jr),e(K,Gr),e(K,Ct),e(Ct,Hr),e(K,Jr),e(K,Qe),e(Qe,Fr),e(q,Kr),e(q,Q),b(Se,Q,null),e(Q,Qr),e(Q,Ce),e(Ce,Xr),e(Ce,Vt),e(Vt,Yr),e(Ce,Zr),e(Q,en),e(Q,Ve),e(Ve,tn),e(Ve,Ot),e(Ot,on),e(Ve,rn),Zt=!0},p(o,w){const Rt={};w&2&&(Rt.$$scope={dirty:w,ctx:o}),se.$set(Rt);const Ut={};w&2&&(Ut.$$scope={dirty:w,ctx:o}),fe.$set(Ut);const jt={};w&2&&(jt.$$scope={dirty:w,ctx:o}),ue.$set(jt)},i(o){Zt||(k(m.$$.fragment,o),k(N.$$.fragment,o),k(R.$$.fragment,o),k(se.$$.fragment,o),k(ke.$$.fragment,o),k(Ee.$$.fragment,o),k(ye.$$.fragment,o),k(we.$$.fragment,o),k(Pe.$$.fragment,o),k(xe.$$.fragment,o),k(ze.$$.fragment,o),k(Te.$$.fragment,o),k(De.$$.fragment,o),k(We.$$.fragment,o),k(Ae.$$.fragment,o),k(Be.$$.fragment,o),k(Ie.$$.fragment,o),k(fe.$$.fragment,o),k(Le.$$.fragment,o),k(Me.$$.fragment,o),k(qe.$$.fragment,o),k(Ne.$$.fragment,o),k(ue.$$.fragment,o),k(Se.$$.fragment,o),Zt=!0)},o(o){E(m.$$.fragment,o),E(N.$$.fragment,o),E(R.$$.fragment,o),E(se.$$.fragment,o),E(ke.$$.fragment,o),E(Ee.$$.fragment,o),E(ye.$$.fragment,o),E(we.$$.fragment,o),E(Pe.$$.fragment,o),E(xe.$$.fragment,o),E(ze.$$.fragment,o),E(Te.$$.fragment,o),E(De.$$.fragment,o),E(We.$$.fragment,o),E(Ae.$$.fragment,o),E(Be.$$.fragment,o),E(Ie.$$.fragment,o),E(fe.$$.fragment,o),E(Le.$$.fragment,o),E(Me.$$.fragment,o),E(qe.$$.fragment,o),E(Ne.$$.fragment,o),E(ue.$$.fragment,o),E(Se.$$.fragment,o),Zt=!1},d(o){o&&t(s),y(m),o&&t(M),o&&t(A),y(N),y(R),y(se),y(ke),o&&t(Gt),o&&t(Z),y(Ee),o&&t(Ht),o&&t(z),y(ye),y(we),y(Pe),y(xe),y(ze),y(Te),o&&t(Jt),o&&t(te),y(De),o&&t(Ft),o&&t(oe),y(We),o&&t(Kt),o&&t(re),y(Ae),o&&t(Qt),o&&t(I),y(Be),y(Ie),y(fe),y(Le),o&&t(Xt),o&&t(ne),y(Me),o&&t(Yt),o&&t(q),y(qe),y(Ne),y(ue),y(Se)}}}function ha(D){let s,h;return s=new cn({props:{$$slots:{default:[fa]},$$scope:{ctx:D}}}),{c(){$(s.$$.fragment)},l(l){_(s.$$.fragment,l)},m(l,m){b(s,l,m),h=!0},p(l,m){const v={};m&2&&(v.$$scope={dirty:m,ctx:l}),s.$set(v)},i(l){h||(k(s.$$.fragment,l),h=!0)},o(l){E(s.$$.fragment,l),h=!1},d(l){y(s,l)}}}function ua(D){let s,h,l,m,v;return{c(){s=r("p"),h=i("The Rust API Reference is available directly on the "),l=r("a"),m=i("Docs.rs"),v=i(" website."),this.h()},l(c){s=n(c,"P",{});var g=a(s);h=d(g,"The Rust API Reference is available directly on the "),l=n(g,"A",{href:!0,rel:!0});var M=a(l);m=d(M,"Docs.rs"),M.forEach(t),v=d(g," website."),g.forEach(t),this.h()},h(){u(l,"href","https://docs.rs/tokenizers/latest/tokenizers/"),u(l,"rel","nofollow")},m(c,g){P(c,s,g),e(s,h),e(s,l),e(l,m),e(s,v)},d(c){c&&t(s)}}}function va(D){let s,h;return s=new cn({props:{$$slots:{default:[ua]},$$scope:{ctx:D}}}),{c(){$(s.$$.fragment)},l(l){_(s.$$.fragment,l)},m(l,m){b(s,l,m),h=!0},p(l,m){const v={};m&2&&(v.$$scope={dirty:m,ctx:l}),s.$set(v)},i(l){h||(k(s.$$.fragment,l),h=!0)},o(l){E(s.$$.fragment,l),h=!1},d(l){y(s,l)}}}function ga(D){let s,h;return{c(){s=r("p"),h=i("The node API has not been documented yet.")},l(l){s=n(l,"P",{});var m=a(s);h=d(m,"The node API has not been documented yet."),m.forEach(t)},m(l,m){P(l,s,m),e(s,h)},d(l){l&&t(s)}}}function $a(D){let s,h;return s=new cn({props:{$$slots:{default:[ga]},$$scope:{ctx:D}}}),{c(){$(s.$$.fragment)},l(l){_(s.$$.fragment,l)},m(l,m){b(s,l,m),h=!0},p(l,m){const v={};m&2&&(v.$$scope={dirty:m,ctx:l}),s.$set(v)},i(l){h||(k(s.$$.fragment,l),h=!0)},o(l){E(s.$$.fragment,l),h=!1},d(l){y(s,l)}}}function _a(D){let s,h,l,m,v,c,g,M,A,N,S,O;return c=new nt({}),S=new da({props:{python:!0,rust:!0,node:!0,$$slots:{node:[$a],rust:[va],python:[ha]},$$scope:{ctx:D}}}),{c(){s=r("meta"),h=p(),l=r("h1"),m=r("a"),v=r("span"),$(c.$$.fragment),g=p(),M=r("span"),A=i("Models"),N=p(),$(S.$$.fragment),this.h()},l(x){const L=la('[data-svelte="svelte-1phssyn"]',document.head);s=n(L,"META",{name:!0,content:!0}),L.forEach(t),h=f(x),l=n(x,"H1",{class:!0});var T=a(l);m=n(T,"A",{id:!0,class:!0,href:!0});var R=a(m);v=n(R,"SPAN",{});var je=a(v);_(c.$$.fragment,je),je.forEach(t),R.forEach(t),g=f(T),M=n(T,"SPAN",{});var ae=a(M);A=d(ae,"Models"),ae.forEach(t),T.forEach(t),N=f(x),_(S.$$.fragment,x),this.h()},h(){u(s,"name","hf:doc:metadata"),u(s,"content",JSON.stringify(ba)),u(m,"id","models"),u(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(m,"href","#models"),u(l,"class","relative group")},m(x,L){e(document.head,s),P(x,h,L),P(x,l,L),e(l,m),e(m,v),b(c,v,null),e(l,g),e(l,M),e(M,A),P(x,N,L),b(S,x,L),O=!0},p(x,[L]){const T={};L&2&&(T.$$scope={dirty:L,ctx:x}),S.$set(T)},i(x){O||(k(c.$$.fragment,x),k(S.$$.fragment,x),O=!0)},o(x){E(c.$$.fragment,x),E(S.$$.fragment,x),O=!1},d(x){t(s),x&&t(h),x&&t(l),y(c),x&&t(N),y(S,x)}}}const ba={local:"models",sections:[{local:"tokenizers.models.BPE",title:"BPE"},{local:"tokenizers.models.Model",title:"Model"},{local:"tokenizers.models.Unigram",title:"Unigram"},{local:"tokenizers.models.WordLevel",title:"WordLevel"},{local:"tokenizers.models.WordPiece",title:"WordPiece"}],title:"Models"};function ka(D){return ia(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ta extends na{constructor(s){super();aa(this,s,ka,_a,sa,{})}}export{Ta as default,ba as metadata};
