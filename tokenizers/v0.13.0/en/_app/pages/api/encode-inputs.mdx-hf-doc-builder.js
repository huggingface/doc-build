import{S as Nn,i as gn,s as Mn,e as s,k as _,w as W,t as a,M as Hn,c as l,d as n,m as v,a as i,x as F,h as u,b as E,G as t,g as c,y as B,q as G,o as J,B as j,v as Wn}from"../../chunks/vendor-hf-doc-builder.js";import{I as st}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{T as Fn,M as ln}from"../../chunks/TokenizersLanguageContent-hf-doc-builder.js";function Bn(L){let o,d,r,f,$,k,m,P,y,z,T,A,h,I,w,te,K,q,lt,be,ne,it,Ue,b,re,at,oe,ut,pt,se,ft,Q,le,ct,ie,dt,ht,V,Et,ae,_t,vt,Ce,U,$t,me,kt,Tt,Oe,x,C,ze,X,mt,Ie,zt,De,ue,It,Re,pe,wt,Ne,O,fe,Lt,ce,Pt,At,de,qt,Y,he,xt,Ee,St,yt,Z,bt,_e,Ut,Ct,ge,D,Ot,we,Dt,Rt,Me,S,R,Le,ee,Nt,Pe,gt,He,ve,Mt,We,$e,Ht,Fe,N,g,Wt,Ae,Ft,Bt,ke,Gt,Jt,M,jt,qe,Kt,Qt,Te,Vt,Be,H,Xt,xe,Yt,Zt,Ge;return h=new st({}),X=new st({}),ee=new st({}),{c(){o=s("p"),d=a("These types represent all the different kinds of input that a "),r=s("a"),f=a("Tokenizer"),$=a(` accepts
when using `),k=s("code"),m=a("encode_batch()"),P=a("."),y=_(),z=s("h2"),T=s("a"),A=s("span"),W(h.$$.fragment),I=_(),w=s("span"),te=a("TextEncodeInput"),K=_(),q=s("code"),lt=a("tokenizers.TextEncodeInput"),be=_(),ne=s("p"),it=a("Represents a textual input for encoding. Can be either:"),Ue=_(),b=s("ul"),re=s("li"),at=a("A single sequence: "),oe=s("a"),ut=a("TextInputSequence"),pt=_(),se=s("li"),ft=a("A pair of sequences:"),Q=s("ul"),le=s("li"),ct=a("A Tuple of "),ie=s("a"),dt=a("TextInputSequence"),ht=_(),V=s("li"),Et=a("Or a List of "),ae=s("a"),_t=a("TextInputSequence"),vt=a(" of size 2"),Ce=_(),U=s("p"),$t=a("alias of "),me=s("code"),kt=a("Union[str, Tuple[str, str], List[str]]"),Tt=a("."),Oe=_(),x=s("h2"),C=s("a"),ze=s("span"),W(X.$$.fragment),mt=_(),Ie=s("span"),zt=a("PreTokenizedEncodeInput"),De=_(),ue=s("code"),It=a("tokenizers.PreTokenizedEncodeInput"),Re=_(),pe=s("p"),wt=a("Represents a pre-tokenized input for encoding. Can be either:"),Ne=_(),O=s("ul"),fe=s("li"),Lt=a("A single sequence: "),ce=s("a"),Pt=a("PreTokenizedInputSequence"),At=_(),de=s("li"),qt=a("A pair of sequences:"),Y=s("ul"),he=s("li"),xt=a("A Tuple of "),Ee=s("a"),St=a("PreTokenizedInputSequence"),yt=_(),Z=s("li"),bt=a("Or a List of "),_e=s("a"),Ut=a("PreTokenizedInputSequence"),Ct=a(" of size 2"),ge=_(),D=s("p"),Ot=a("alias of "),we=s("code"),Dt=a("Union[List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]"),Rt=a("."),Me=_(),S=s("h2"),R=s("a"),Le=s("span"),W(ee.$$.fragment),Nt=_(),Pe=s("span"),gt=a("EncodeInput"),He=_(),ve=s("code"),Mt=a("tokenizers.EncodeInput"),We=_(),$e=s("p"),Ht=a("Represents all the possible types of input for encoding. Can be:"),Fe=_(),N=s("ul"),g=s("li"),Wt=a("When "),Ae=s("code"),Ft=a("is_pretokenized=False"),Bt=a(": "),ke=s("a"),Gt=a("TextEncodeInput"),Jt=_(),M=s("li"),jt=a("When "),qe=s("code"),Kt=a("is_pretokenized=True"),Qt=a(": "),Te=s("a"),Vt=a("PreTokenizedEncodeInput"),Be=_(),H=s("p"),Xt=a("alias of "),xe=s("code"),Yt=a("Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]"),Zt=a("."),this.h()},l(e){o=l(e,"P",{});var p=i(o);d=u(p,"These types represent all the different kinds of input that a "),r=l(p,"A",{href:!0});var an=i(r);f=u(an,"Tokenizer"),an.forEach(n),$=u(p,` accepts
when using `),k=l(p,"CODE",{});var un=i(k);m=u(un,"encode_batch()"),un.forEach(n),P=u(p,"."),p.forEach(n),y=v(e),z=l(e,"H2",{class:!0});var Je=i(z);T=l(Je,"A",{id:!0,class:!0,href:!0});var pn=i(T);A=l(pn,"SPAN",{});var fn=i(A);F(h.$$.fragment,fn),fn.forEach(n),pn.forEach(n),I=v(Je),w=l(Je,"SPAN",{});var cn=i(w);te=u(cn,"TextEncodeInput"),cn.forEach(n),Je.forEach(n),K=v(e),q=l(e,"CODE",{});var dn=i(q);lt=u(dn,"tokenizers.TextEncodeInput"),dn.forEach(n),be=v(e),ne=l(e,"P",{});var hn=i(ne);it=u(hn,"Represents a textual input for encoding. Can be either:"),hn.forEach(n),Ue=v(e),b=l(e,"UL",{});var je=i(b);re=l(je,"LI",{});var en=i(re);at=u(en,"A single sequence: "),oe=l(en,"A",{href:!0});var En=i(oe);ut=u(En,"TextInputSequence"),En.forEach(n),en.forEach(n),pt=v(je),se=l(je,"LI",{});var tn=i(se);ft=u(tn,"A pair of sequences:"),Q=l(tn,"UL",{});var Ke=i(Q);le=l(Ke,"LI",{});var nn=i(le);ct=u(nn,"A Tuple of "),ie=l(nn,"A",{href:!0});var _n=i(ie);dt=u(_n,"TextInputSequence"),_n.forEach(n),nn.forEach(n),ht=v(Ke),V=l(Ke,"LI",{});var Qe=i(V);Et=u(Qe,"Or a List of "),ae=l(Qe,"A",{href:!0});var vn=i(ae);_t=u(vn,"TextInputSequence"),vn.forEach(n),vt=u(Qe," of size 2"),Qe.forEach(n),Ke.forEach(n),tn.forEach(n),je.forEach(n),Ce=v(e),U=l(e,"P",{});var Ve=i(U);$t=u(Ve,"alias of "),me=l(Ve,"CODE",{});var $n=i(me);kt=u($n,"Union[str, Tuple[str, str], List[str]]"),$n.forEach(n),Tt=u(Ve,"."),Ve.forEach(n),Oe=v(e),x=l(e,"H2",{class:!0});var Xe=i(x);C=l(Xe,"A",{id:!0,class:!0,href:!0});var kn=i(C);ze=l(kn,"SPAN",{});var Tn=i(ze);F(X.$$.fragment,Tn),Tn.forEach(n),kn.forEach(n),mt=v(Xe),Ie=l(Xe,"SPAN",{});var mn=i(Ie);zt=u(mn,"PreTokenizedEncodeInput"),mn.forEach(n),Xe.forEach(n),De=v(e),ue=l(e,"CODE",{});var zn=i(ue);It=u(zn,"tokenizers.PreTokenizedEncodeInput"),zn.forEach(n),Re=v(e),pe=l(e,"P",{});var In=i(pe);wt=u(In,"Represents a pre-tokenized input for encoding. Can be either:"),In.forEach(n),Ne=v(e),O=l(e,"UL",{});var Ye=i(O);fe=l(Ye,"LI",{});var rn=i(fe);Lt=u(rn,"A single sequence: "),ce=l(rn,"A",{href:!0});var wn=i(ce);Pt=u(wn,"PreTokenizedInputSequence"),wn.forEach(n),rn.forEach(n),At=v(Ye),de=l(Ye,"LI",{});var on=i(de);qt=u(on,"A pair of sequences:"),Y=l(on,"UL",{});var Ze=i(Y);he=l(Ze,"LI",{});var sn=i(he);xt=u(sn,"A Tuple of "),Ee=l(sn,"A",{href:!0});var Ln=i(Ee);St=u(Ln,"PreTokenizedInputSequence"),Ln.forEach(n),sn.forEach(n),yt=v(Ze),Z=l(Ze,"LI",{});var et=i(Z);bt=u(et,"Or a List of "),_e=l(et,"A",{href:!0});var Pn=i(_e);Ut=u(Pn,"PreTokenizedInputSequence"),Pn.forEach(n),Ct=u(et," of size 2"),et.forEach(n),Ze.forEach(n),on.forEach(n),Ye.forEach(n),ge=v(e),D=l(e,"P",{});var tt=i(D);Ot=u(tt,"alias of "),we=l(tt,"CODE",{});var An=i(we);Dt=u(An,"Union[List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]"),An.forEach(n),Rt=u(tt,"."),tt.forEach(n),Me=v(e),S=l(e,"H2",{class:!0});var nt=i(S);R=l(nt,"A",{id:!0,class:!0,href:!0});var qn=i(R);Le=l(qn,"SPAN",{});var xn=i(Le);F(ee.$$.fragment,xn),xn.forEach(n),qn.forEach(n),Nt=v(nt),Pe=l(nt,"SPAN",{});var Sn=i(Pe);gt=u(Sn,"EncodeInput"),Sn.forEach(n),nt.forEach(n),He=v(e),ve=l(e,"CODE",{});var yn=i(ve);Mt=u(yn,"tokenizers.EncodeInput"),yn.forEach(n),We=v(e),$e=l(e,"P",{});var bn=i($e);Ht=u(bn,"Represents all the possible types of input for encoding. Can be:"),bn.forEach(n),Fe=v(e),N=l(e,"UL",{});var rt=i(N);g=l(rt,"LI",{});var Se=i(g);Wt=u(Se,"When "),Ae=l(Se,"CODE",{});var Un=i(Ae);Ft=u(Un,"is_pretokenized=False"),Un.forEach(n),Bt=u(Se,": "),ke=l(Se,"A",{href:!0});var Cn=i(ke);Gt=u(Cn,"TextEncodeInput"),Cn.forEach(n),Se.forEach(n),Jt=v(rt),M=l(rt,"LI",{});var ye=i(M);jt=u(ye,"When "),qe=l(ye,"CODE",{});var On=i(qe);Kt=u(On,"is_pretokenized=True"),On.forEach(n),Qt=u(ye,": "),Te=l(ye,"A",{href:!0});var Dn=i(Te);Vt=u(Dn,"PreTokenizedEncodeInput"),Dn.forEach(n),ye.forEach(n),rt.forEach(n),Be=v(e),H=l(e,"P",{});var ot=i(H);Xt=u(ot,"alias of "),xe=l(ot,"CODE",{});var Rn=i(xe);Yt=u(Rn,"Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]"),Rn.forEach(n),Zt=u(ot,"."),ot.forEach(n),this.h()},h(){E(r,"href","/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"),E(T,"id","[[tokenizers.TextEncodeInput]]"),E(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(T,"href","#[[tokenizers.TextEncodeInput]]"),E(z,"class","relative group"),E(oe,"href","/docs/tokenizers/api/input-sequences#tokenizers.TextInputSequence"),E(ie,"href","/docs/tokenizers/api/input-sequences#tokenizers.TextInputSequence"),E(ae,"href","/docs/tokenizers/api/input-sequences#tokenizers.TextInputSequence"),E(C,"id","[[tokenizers.PreTokenizedEncodeInput]]"),E(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(C,"href","#[[tokenizers.PreTokenizedEncodeInput]]"),E(x,"class","relative group"),E(ce,"href","/docs/tokenizers/api/input-sequences#tokenizers.PreTokenizedInputSequence"),E(Ee,"href","/docs/tokenizers/api/input-sequences#tokenizers.PreTokenizedInputSequence"),E(_e,"href","/docs/tokenizers/api/input-sequences#tokenizers.PreTokenizedInputSequence"),E(R,"id","[[tokenizers.EncodeInput]]"),E(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(R,"href","#[[tokenizers.EncodeInput]]"),E(S,"class","relative group"),E(ke,"href","#tokenizers.TextEncodeInput"),E(Te,"href","#tokenizers.PreTokenizedEncodeInput")},m(e,p){c(e,o,p),t(o,d),t(o,r),t(r,f),t(o,$),t(o,k),t(k,m),t(o,P),c(e,y,p),c(e,z,p),t(z,T),t(T,A),B(h,A,null),t(z,I),t(z,w),t(w,te),c(e,K,p),c(e,q,p),t(q,lt),c(e,be,p),c(e,ne,p),t(ne,it),c(e,Ue,p),c(e,b,p),t(b,re),t(re,at),t(re,oe),t(oe,ut),t(b,pt),t(b,se),t(se,ft),t(se,Q),t(Q,le),t(le,ct),t(le,ie),t(ie,dt),t(Q,ht),t(Q,V),t(V,Et),t(V,ae),t(ae,_t),t(V,vt),c(e,Ce,p),c(e,U,p),t(U,$t),t(U,me),t(me,kt),t(U,Tt),c(e,Oe,p),c(e,x,p),t(x,C),t(C,ze),B(X,ze,null),t(x,mt),t(x,Ie),t(Ie,zt),c(e,De,p),c(e,ue,p),t(ue,It),c(e,Re,p),c(e,pe,p),t(pe,wt),c(e,Ne,p),c(e,O,p),t(O,fe),t(fe,Lt),t(fe,ce),t(ce,Pt),t(O,At),t(O,de),t(de,qt),t(de,Y),t(Y,he),t(he,xt),t(he,Ee),t(Ee,St),t(Y,yt),t(Y,Z),t(Z,bt),t(Z,_e),t(_e,Ut),t(Z,Ct),c(e,ge,p),c(e,D,p),t(D,Ot),t(D,we),t(we,Dt),t(D,Rt),c(e,Me,p),c(e,S,p),t(S,R),t(R,Le),B(ee,Le,null),t(S,Nt),t(S,Pe),t(Pe,gt),c(e,He,p),c(e,ve,p),t(ve,Mt),c(e,We,p),c(e,$e,p),t($e,Ht),c(e,Fe,p),c(e,N,p),t(N,g),t(g,Wt),t(g,Ae),t(Ae,Ft),t(g,Bt),t(g,ke),t(ke,Gt),t(N,Jt),t(N,M),t(M,jt),t(M,qe),t(qe,Kt),t(M,Qt),t(M,Te),t(Te,Vt),c(e,Be,p),c(e,H,p),t(H,Xt),t(H,xe),t(xe,Yt),t(H,Zt),Ge=!0},i(e){Ge||(G(h.$$.fragment,e),G(X.$$.fragment,e),G(ee.$$.fragment,e),Ge=!0)},o(e){J(h.$$.fragment,e),J(X.$$.fragment,e),J(ee.$$.fragment,e),Ge=!1},d(e){e&&n(o),e&&n(y),e&&n(z),j(h),e&&n(K),e&&n(q),e&&n(be),e&&n(ne),e&&n(Ue),e&&n(b),e&&n(Ce),e&&n(U),e&&n(Oe),e&&n(x),j(X),e&&n(De),e&&n(ue),e&&n(Re),e&&n(pe),e&&n(Ne),e&&n(O),e&&n(ge),e&&n(D),e&&n(Me),e&&n(S),j(ee),e&&n(He),e&&n(ve),e&&n(We),e&&n($e),e&&n(Fe),e&&n(N),e&&n(Be),e&&n(H)}}}function Gn(L){let o,d;return o=new ln({props:{$$slots:{default:[Bn]},$$scope:{ctx:L}}}),{c(){W(o.$$.fragment)},l(r){F(o.$$.fragment,r)},m(r,f){B(o,r,f),d=!0},p(r,f){const $={};f&2&&($.$$scope={dirty:f,ctx:r}),o.$set($)},i(r){d||(G(o.$$.fragment,r),d=!0)},o(r){J(o.$$.fragment,r),d=!1},d(r){j(o,r)}}}function Jn(L){let o,d,r,f,$;return{c(){o=s("p"),d=a("The Rust API Reference is available directly on the "),r=s("a"),f=a("Docs.rs"),$=a(" website."),this.h()},l(k){o=l(k,"P",{});var m=i(o);d=u(m,"The Rust API Reference is available directly on the "),r=l(m,"A",{href:!0,rel:!0});var P=i(r);f=u(P,"Docs.rs"),P.forEach(n),$=u(m," website."),m.forEach(n),this.h()},h(){E(r,"href","https://docs.rs/tokenizers/latest/tokenizers/"),E(r,"rel","nofollow")},m(k,m){c(k,o,m),t(o,d),t(o,r),t(r,f),t(o,$)},d(k){k&&n(o)}}}function jn(L){let o,d;return o=new ln({props:{$$slots:{default:[Jn]},$$scope:{ctx:L}}}),{c(){W(o.$$.fragment)},l(r){F(o.$$.fragment,r)},m(r,f){B(o,r,f),d=!0},p(r,f){const $={};f&2&&($.$$scope={dirty:f,ctx:r}),o.$set($)},i(r){d||(G(o.$$.fragment,r),d=!0)},o(r){J(o.$$.fragment,r),d=!1},d(r){j(o,r)}}}function Kn(L){let o,d;return{c(){o=s("p"),d=a("The node API has not been documented yet.")},l(r){o=l(r,"P",{});var f=i(o);d=u(f,"The node API has not been documented yet."),f.forEach(n)},m(r,f){c(r,o,f),t(o,d)},d(r){r&&n(o)}}}function Qn(L){let o,d;return o=new ln({props:{$$slots:{default:[Kn]},$$scope:{ctx:L}}}),{c(){W(o.$$.fragment)},l(r){F(o.$$.fragment,r)},m(r,f){B(o,r,f),d=!0},p(r,f){const $={};f&2&&($.$$scope={dirty:f,ctx:r}),o.$set($)},i(r){d||(G(o.$$.fragment,r),d=!0)},o(r){J(o.$$.fragment,r),d=!1},d(r){j(o,r)}}}function Vn(L){let o,d,r,f,$,k,m,P,y,z,T,A;return k=new st({}),T=new Fn({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Qn],rust:[jn],python:[Gn]},$$scope:{ctx:L}}}),{c(){o=s("meta"),d=_(),r=s("h1"),f=s("a"),$=s("span"),W(k.$$.fragment),m=_(),P=s("span"),y=a("Encode Inputs"),z=_(),W(T.$$.fragment),this.h()},l(h){const I=Hn('[data-svelte="svelte-1phssyn"]',document.head);o=l(I,"META",{name:!0,content:!0}),I.forEach(n),d=v(h),r=l(h,"H1",{class:!0});var w=i(r);f=l(w,"A",{id:!0,class:!0,href:!0});var te=i(f);$=l(te,"SPAN",{});var K=i($);F(k.$$.fragment,K),K.forEach(n),te.forEach(n),m=v(w),P=l(w,"SPAN",{});var q=i(P);y=u(q,"Encode Inputs"),q.forEach(n),w.forEach(n),z=v(h),F(T.$$.fragment,h),this.h()},h(){E(o,"name","hf:doc:metadata"),E(o,"content",JSON.stringify(Xn)),E(f,"id","encode-inputs"),E(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(f,"href","#encode-inputs"),E(r,"class","relative group")},m(h,I){t(document.head,o),c(h,d,I),c(h,r,I),t(r,f),t(f,$),B(k,$,null),t(r,m),t(r,P),t(P,y),c(h,z,I),B(T,h,I),A=!0},p(h,[I]){const w={};I&2&&(w.$$scope={dirty:I,ctx:h}),T.$set(w)},i(h){A||(G(k.$$.fragment,h),G(T.$$.fragment,h),A=!0)},o(h){J(k.$$.fragment,h),J(T.$$.fragment,h),A=!1},d(h){n(o),h&&n(d),h&&n(r),j(k),h&&n(z),j(T,h)}}}const Xn={local:"encode-inputs",sections:[{local:"[[tokenizers.TextEncodeInput]]",title:"TextEncodeInput"},{local:"[[tokenizers.PreTokenizedEncodeInput]]",title:"PreTokenizedEncodeInput"},{local:"[[tokenizers.EncodeInput]]",title:"EncodeInput"}],title:"Encode Inputs"};function Yn(L){return Wn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class nr extends Nn{constructor(o){super();gn(this,o,Yn,Vn,Mn,{})}}export{nr as default,Xn as metadata};
