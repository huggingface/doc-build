import{S as td,i as nd,s as rd,e as n,k as i,w as v,t as a,M as od,c as r,d as t,m as d,a as o,x as _,h as s,b as u,G as e,g as O,y as z,q as $,o as b,B as T,v as ad,L as Xi}from"../../chunks/vendor-hf-doc-builder.js";import{D as y}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Zi}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as ed}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{T as sd,M as Fs}from"../../chunks/TokenizersLanguageContent-hf-doc-builder.js";import{E as Qi}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function id(q){let l,k,p,g,E;return g=new Zi({props:{code:`encode("A single sequence")*
encode("A sequence", "And its pair")*
encode([ "A", "pre", "tokenized", "sequence" ], is_pretokenized=True)\`
encode(
[ "A", "pre", "tokenized", "sequence" ], [ "And", "its", "pair" ],
is_pretokenized=True
)`,highlighted:`encode(<span class="hljs-string">&quot;A single sequence&quot;</span>)*
encode(<span class="hljs-string">&quot;A sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>)*
encode([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], is_pretokenized=<span class="hljs-literal">True</span>)\`
encode(
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], [ <span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;its&quot;</span>, <span class="hljs-string">&quot;pair&quot;</span> ],
is_pretokenized=<span class="hljs-literal">True</span>
)`}}),{c(){l=n("p"),k=a("Here are some examples of the inputs that are accepted:"),p=i(),v(g.$$.fragment)},l(f){l=r(f,"P",{});var w=o(l);k=s(w,"Here are some examples of the inputs that are accepted:"),w.forEach(t),p=d(f),_(g.$$.fragment,f)},m(f,w){O(f,l,w),e(l,k),O(f,p,w),z(g,f,w),E=!0},p:Xi,i(f){E||($(g.$$.fragment,f),E=!0)},o(f){b(g.$$.fragment,f),E=!1},d(f){f&&t(l),f&&t(p),T(g,f)}}}function dd(q){let l,k,p,g,E;return g=new Zi({props:{code:`encode_batch([
"A single sequence",
("A tuple with a sequence", "And its pair"),
[ "A", "pre", "tokenized", "sequence" ],
([ "A", "pre", "tokenized", "sequence" ], "And its pair")
])`,highlighted:`encode_batch([
<span class="hljs-string">&quot;A single sequence&quot;</span>,
(<span class="hljs-string">&quot;A tuple with a sequence&quot;</span>, <span class="hljs-string">&quot;And its pair&quot;</span>),
[ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ],
([ <span class="hljs-string">&quot;A&quot;</span>, <span class="hljs-string">&quot;pre&quot;</span>, <span class="hljs-string">&quot;tokenized&quot;</span>, <span class="hljs-string">&quot;sequence&quot;</span> ], <span class="hljs-string">&quot;And its pair&quot;</span>)
])`}}),{c(){l=n("p"),k=a("Here are some examples of the inputs that are accepted:"),p=i(),v(g.$$.fragment)},l(f){l=r(f,"P",{});var w=o(l);k=s(w,"Here are some examples of the inputs that are accepted:"),w.forEach(t),p=d(f),_(g.$$.fragment,f)},m(f,w){O(f,l,w),e(l,k),O(f,p,w),z(g,f,w),E=!0},p:Xi,i(f){E||($(g.$$.fragment,f),E=!0)},o(f){b(g.$$.fragment,f),E=!1},d(f){f&&t(l),f&&t(p),T(g,f)}}}function cd(q){let l,k,p,g,E,f,w,I,h,j,V,A,x,D,G,vt,U,_t,$r,br,ne,Pe,Tr,Y,Er,Rt,yr,wr,Jt,xr,qr,Dr,re,je,Ar,Ve,Ir,zt,Pr,jr,Vr,oe,Ce,Cr,K,Lr,Bt,Nr,Sr,$t,Mr,Or,Gr,F,Le,Hr,Ut,Fr,Wr,Q,Yt,Rr,Jr,Kt,Br,Ur,Qt,Yr,Kr,ae,Ne,Qr,X,Xr,Xt,Zr,eo,Zt,to,no,ro,se,Se,oo,Z,ao,en,so,io,bt,co,lo,po,W,Me,ho,tn,mo,go,ee,nn,uo,fo,rn,ko,vo,on,_o,zo,C,Oe,$o,an,bo,To,sn,Eo,yo,dn,wo,xo,R,Ge,qo,cn,Do,Ao,ln,Io,Po,J,He,jo,pn,Vo,Co,hn,Lo,No,ie,Fe,So,mn,Mo,Oo,de,We,Go,gn,Ho,Fo,ce,Re,Wo,un,Ro,Jo,L,Je,Bo,fn,Uo,Yo,kn,Ko,Qo,le,Xo,N,Be,Zo,vn,ea,ta,_n,na,ra,pe,oa,he,Ue,aa,Ye,sa,Tt,ia,da,ca,me,Ke,la,Qe,pa,Et,ha,ma,ga,ge,Xe,ua,Ze,fa,yt,ka,va,_a,ue,et,za,tt,$a,wt,ba,Ta,Ea,fe,nt,ya,zn,wa,xa,ke,rt,qa,$n,Da,Aa,ve,ot,Ia,bn,Pa,ja,_e,at,Va,Tn,Ca,La,ze,st,Na,En,Sa,Ma,$e,it,Oa,yn,Ga,Ha,S,dt,Fa,wn,Wa,Ra,xn,Ja,Ba,te,ct,Ua,qn,Ya,Ka,Qa,xt,Xa,Dn,Za,es,lt,ts,An,ns,rs,os,be,pt,as,ht,ss,qt,is,ds,cs,Te,mt,ls,gt,ps,Dt,hs,ms,gs,Ee,ut,us,In,fs,ks,B,ft,vs,Pn,_s,zs,At,$s,jn,bs,Ts,M,kt,Es,Vn,ys,ws,Cn,xs,qs,H,It,Ds,Ln,As,Is,ye,Ps,Nn,js,Vs,Sn,Cs,Ls,Mn,Ns,Ss,On,Ms,Wn;return g=new ed({}),j=new y({props:{name:"class tokenizers.Tokenizer",anchor:"tokenizers.Tokenizer",parameters:[{name:"model",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.model",description:`<strong>model</strong> (<a href="/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.Model">Model</a>) &#x2014;
The core algorithm that this <code>Tokenizer</code> should be using.`,name:"model"}]}}),Pe=new y({props:{name:"decoder",anchor:"tokenizers.Tokenizer.decoder",parameters:[],isGetSetDescriptor:!0}}),je=new y({props:{name:"model",anchor:"tokenizers.Tokenizer.model",parameters:[],isGetSetDescriptor:!0}}),Ce=new y({props:{name:"normalizer",anchor:"tokenizers.Tokenizer.normalizer",parameters:[],isGetSetDescriptor:!0}}),Le=new y({props:{name:"padding",anchor:"tokenizers.Tokenizer.padding",parameters:[],returnDescription:`
<p>A dict with the current padding parameters if padding is enabled</p>
`,returnType:`
<p>(<code>dict</code>, <em>optional</em>)</p>
`,isGetSetDescriptor:!0}}),Ne=new y({props:{name:"post_processor",anchor:"tokenizers.Tokenizer.post_processor",parameters:[],isGetSetDescriptor:!0}}),Se=new y({props:{name:"pre_tokenizer",anchor:"tokenizers.Tokenizer.pre_tokenizer",parameters:[],isGetSetDescriptor:!0}}),Me=new y({props:{name:"truncation",anchor:"tokenizers.Tokenizer.truncation",parameters:[],returnDescription:`
<p>A dict with the current truncation parameters if truncation is enabled</p>
`,returnType:`
<p>(<code>dict</code>, <em>optional</em>)</p>
`,isGetSetDescriptor:!0}}),Oe=new y({props:{name:"add_special_tokens",anchor:"tokenizers.Tokenizer.add_special_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_special_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/v0.13.0/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of special tokens we want to add to the vocabulary. Each token can either
be a string or an instance of <a href="/docs/tokenizers/v0.13.0/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more
customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),Ge=new y({props:{name:"add_tokens",anchor:"tokenizers.Tokenizer.add_tokens",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.add_tokens.tokens",description:`<strong>tokens</strong> (A <code>List</code> of <a href="/docs/tokenizers/v0.13.0/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> or <code>str</code>) &#x2014;
The list of tokens we want to add to the vocabulary. Each token can be either a
string or an instance of <a href="/docs/tokenizers/v0.13.0/en/api/added-tokens#tokenizers.AddedToken">AddedToken</a> for more customization.`,name:"tokens"}],returnDescription:`
<p>The number of tokens that were created in the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),He=new y({props:{name:"decode",anchor:"tokenizers.Tokenizer.decode",parameters:[{name:"ids",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode.ids",description:`<strong>ids</strong> (A <code>List/Tuple</code> of <code>int</code>) &#x2014;
The list of ids that we want to decode`,name:"ids"},{anchor:"tokenizers.Tokenizer.decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded string`,name:"skip_special_tokens"}],returnDescription:`
<p>The decoded string</p>
`,returnType:`
<p><code>str</code></p>
`}}),Fe=new y({props:{name:"decode_batch",anchor:"tokenizers.Tokenizer.decode_batch",parameters:[{name:"sequences",val:""},{name:"skip_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.decode_batch.sequences",description:`<strong>sequences</strong> (<code>List</code> of <code>List[int]</code>) &#x2014;
The batch of sequences we want to decode`,name:"sequences"},{anchor:"tokenizers.Tokenizer.decode_batch.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the special tokens should be removed from the decoded strings`,name:"skip_special_tokens"}],returnDescription:`
<p>A list of decoded strings</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),We=new y({props:{name:"enable_padding",anchor:"tokenizers.Tokenizer.enable_padding",parameters:[{name:"direction",val:" = 'right'"},{name:"pad_id",val:" = 0"},{name:"pad_type_id",val:" = 0"},{name:"pad_token",val:" = '[PAD]'"},{name:"length",val:" = None"},{name:"pad_to_multiple_of",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_padding.direction",description:`<strong>direction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>right</code>) &#x2014;
The direction in which to pad. Can be either <code>right</code> or <code>left</code>`,name:"direction"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the padding length should always snap to the next multiple of the
given value. For example if we were going to pad witha length of 250 but
<code>pad_to_multiple_of=8</code> then we will pad to 256.`,name:"pad_to_multiple_of"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_id",description:`<strong>pad_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The id to be used when padding`,name:"pad_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_type_id",description:`<strong>pad_type_id</strong> (<code>int</code>, defaults to 0) &#x2014;
The type id to be used when padding`,name:"pad_type_id"},{anchor:"tokenizers.Tokenizer.enable_padding.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, defaults to <code>[PAD]</code>) &#x2014;
The pad token to be used when padding`,name:"pad_token"},{anchor:"tokenizers.Tokenizer.enable_padding.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If specified, the length at which to pad. If not specified we pad using the size of
the longest sequence in a batch.`,name:"length"}]}}),Re=new y({props:{name:"enable_truncation",anchor:"tokenizers.Tokenizer.enable_truncation",parameters:[{name:"max_length",val:""},{name:"stride",val:" = 0"},{name:"strategy",val:" = 'longest_first'"},{name:"direction",val:" = 'right'"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.enable_truncation.max_length",description:`<strong>max_length</strong> (<code>int</code>) &#x2014;
The max length at which to truncate`,name:"max_length"},{anchor:"tokenizers.Tokenizer.enable_truncation.stride",description:`<strong>stride</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The length of the previous first sequence to be included in the overflowing
sequence`,name:"stride"},{anchor:"tokenizers.Tokenizer.enable_truncation.strategy",description:`<strong>strategy</strong> (<code>str</code>, <em>optional</em>, defaults to <code>longest_first</code>) &#x2014;
The strategy used to truncation. Can be one of <code>longest_first</code>, <code>only_first</code> or
<code>only_second</code>.`,name:"strategy"},{anchor:"tokenizers.Tokenizer.enable_truncation.direction",description:`<strong>direction</strong> (<code>str</code>, defaults to <code>right</code>) &#x2014;
Truncate direction`,name:"direction"}]}}),Je=new y({props:{name:"encode",anchor:"tokenizers.Tokenizer.encode",parameters:[{name:"sequence",val:""},{name:"pair",val:" = None"},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode.sequence",description:`<strong>sequence</strong> (<code>~tokenizers.InputSequence</code>) &#x2014;
The main input sequence we want to encode. This sequence can be either raw
text or pre-tokenized, according to the <code>is_pretokenized</code> argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextInputSequence</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedInputSequence()</code></li>
</ul>`,name:"sequence"},{anchor:"tokenizers.Tokenizer.encode.pair",description:`<strong>pair</strong> (<code>~tokenizers.InputSequence</code>, <em>optional</em>) &#x2014;
An optional input sequence. The expected format is the same that for <code>sequence</code>.`,name:"pair"},{anchor:"tokenizers.Tokenizer.encode.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded result</p>
`,returnType:`
<p><a href="/docs/tokenizers/v0.13.0/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),le=new Qi({props:{anchor:"tokenizers.Tokenizer.encode.example",$$slots:{default:[id]},$$scope:{ctx:q}}}),Be=new y({props:{name:"encode_batch",anchor:"tokenizers.Tokenizer.encode_batch",parameters:[{name:"input",val:""},{name:"is_pretokenized",val:" = False"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.encode_batch.input",description:`<strong>input</strong> (A <code>List</code>/\`<code>Tuple</code> of <code>~tokenizers.EncodeInput</code>) &#x2014;
A list of single sequences or pair sequences to encode. Each sequence
can be either raw text or pre-tokenized, according to the <code>is_pretokenized</code>
argument:</p>
<ul>
<li>If <code>is_pretokenized=False</code>: <code>TextEncodeInput()</code></li>
<li>If <code>is_pretokenized=True</code>: <code>PreTokenizedEncodeInput()</code></li>
</ul>`,name:"input"},{anchor:"tokenizers.Tokenizer.encode_batch.is_pretokenized",description:`<strong>is_pretokenized</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the input is already pre-tokenized`,name:"is_pretokenized"},{anchor:"tokenizers.Tokenizer.encode_batch.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The encoded batch</p>
`,returnType:`
<p>A <code>List</code> of [\`~tokenizers.Encoding\u201C]</p>
`}}),pe=new Qi({props:{anchor:"tokenizers.Tokenizer.encode_batch.example",$$slots:{default:[dd]},$$scope:{ctx:q}}}),Ue=new y({props:{name:"from_buffer",anchor:"tokenizers.Tokenizer.from_buffer",parameters:[{name:"buffer",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_buffer.buffer",description:`<strong>buffer</strong> (<code>bytes</code>) &#x2014;
A buffer containing a previously serialized <a href="/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"buffer"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"
>Tokenizer</a></p>
`}}),Ke=new y({props:{name:"from_file",anchor:"tokenizers.Tokenizer.from_file",parameters:[{name:"path",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_file.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a local JSON file representing a previously serialized
<a href="/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"path"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"
>Tokenizer</a></p>
`}}),Xe=new y({props:{name:"from_pretrained",anchor:"tokenizers.Tokenizer.from_pretrained",parameters:[{name:"identifier",val:""},{name:"revision",val:" = 'main'"},{name:"auth_token",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_pretrained.identifier",description:`<strong>identifier</strong> (<code>str</code>) &#x2014;
The identifier of a Model on the Hugging Face Hub, that contains
a tokenizer.json file`,name:"identifier"},{anchor:"tokenizers.Tokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, defaults to <em>main</em>) &#x2014;
A branch or commit id`,name:"revision"},{anchor:"tokenizers.Tokenizer.from_pretrained.auth_token",description:`<strong>auth_token</strong> (<code>str</code>, <em>optional</em>, defaults to <em>None</em>) &#x2014;
An optional auth token used to access private repositories on the
Hugging Face Hub`,name:"auth_token"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"
>Tokenizer</a></p>
`}}),et=new y({props:{name:"from_str",anchor:"tokenizers.Tokenizer.from_str",parameters:[{name:"json",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.from_str.json",description:`<strong>json</strong> (<code>str</code>) &#x2014;
A valid JSON string representing a previously serialized
<a href="/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer">Tokenizer</a>`,name:"json"}],returnDescription:`
<p>The new tokenizer</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"
>Tokenizer</a></p>
`}}),nt=new y({props:{name:"get_vocab",anchor:"tokenizers.Tokenizer.get_vocab",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The vocabulary</p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),rt=new y({props:{name:"get_vocab_size",anchor:"tokenizers.Tokenizer.get_vocab_size",parameters:[{name:"with_added_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.get_vocab_size.with_added_tokens",description:`<strong>with_added_tokens</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to include the added tokens`,name:"with_added_tokens"}],returnDescription:`
<p>The size of the vocabulary</p>
`,returnType:`
<p><code>int</code></p>
`}}),ot=new y({props:{name:"id_to_token",anchor:"tokenizers.Tokenizer.id_to_token",parameters:[{name:"id",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.id_to_token.id",description:`<strong>id</strong> (<code>int</code>) &#x2014;
The id to convert`,name:"id"}],returnDescription:`
<p>An optional token, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[str]</code></p>
`}}),at=new y({props:{name:"no_padding",anchor:"tokenizers.Tokenizer.no_padding",parameters:[]}}),st=new y({props:{name:"no_truncation",anchor:"tokenizers.Tokenizer.no_truncation",parameters:[]}}),it=new y({props:{name:"num_special_tokens_to_add",anchor:"tokenizers.Tokenizer.num_special_tokens_to_add",parameters:[{name:"is_pair",val:""}]}}),dt=new y({props:{name:"post_process",anchor:"tokenizers.Tokenizer.post_process",parameters:[{name:"encoding",val:""},{name:"pair",val:" = None"},{name:"add_special_tokens",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.post_process.encoding",description:`<strong>encoding</strong> (<a href="/docs/tokenizers/v0.13.0/en/api/encoding#tokenizers.Encoding">Encoding</a>) &#x2014;
The <a href="/docs/tokenizers/v0.13.0/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the main sequence.`,name:"encoding"},{anchor:"tokenizers.Tokenizer.post_process.pair",description:`<strong>pair</strong> (<a href="/docs/tokenizers/v0.13.0/en/api/encoding#tokenizers.Encoding">Encoding</a>, <em>optional</em>) &#x2014;
An optional <a href="/docs/tokenizers/v0.13.0/en/api/encoding#tokenizers.Encoding">Encoding</a> corresponding to the pair sequence.`,name:"pair"},{anchor:"tokenizers.Tokenizer.post_process.add_special_tokens",description:`<strong>add_special_tokens</strong> (<code>bool</code>) &#x2014;
Whether to add the special tokens`,name:"add_special_tokens"}],returnDescription:`
<p>The final post-processed encoding</p>
`,returnType:`
<p><a href="/docs/tokenizers/v0.13.0/en/api/encoding#tokenizers.Encoding">Encoding</a></p>
`}}),pt=new y({props:{name:"save",anchor:"tokenizers.Tokenizer.save",parameters:[{name:"path",val:""},{name:"pretty",val:" = True"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.save.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
A path to a file in which to save the serialized tokenizer.`,name:"path"},{anchor:"tokenizers.Tokenizer.save.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the JSON file should be pretty formatted.`,name:"pretty"}]}}),mt=new y({props:{name:"to_str",anchor:"tokenizers.Tokenizer.to_str",parameters:[{name:"pretty",val:" = False"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.to_str.pretty",description:`<strong>pretty</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether the JSON string should be pretty formatted.`,name:"pretty"}],returnDescription:`
<p>A string representing the serialized Tokenizer</p>
`,returnType:`
<p><code>str</code></p>
`}}),ut=new y({props:{name:"token_to_id",anchor:"tokenizers.Tokenizer.token_to_id",parameters:[{name:"token",val:""}],parametersDescription:[{anchor:"tokenizers.Tokenizer.token_to_id.token",description:`<strong>token</strong> (<code>str</code>) &#x2014;
The token to convert`,name:"token"}],returnDescription:`
<p>An optional id, <code>None</code> if out of vocabulary</p>
`,returnType:`
<p><code>Optional[int]</code></p>
`}}),ft=new y({props:{name:"train",anchor:"tokenizers.Tokenizer.train",parameters:[{name:"files",val:""},{name:"trainer",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train.files",description:`<strong>files</strong> (<code>List[str]</code>) &#x2014;
A list of path to the files that we should use for training`,name:"files"},{anchor:"tokenizers.Tokenizer.train.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"}]}}),kt=new y({props:{name:"train_from_iterator",anchor:"tokenizers.Tokenizer.train_from_iterator",parameters:[{name:"iterator",val:""},{name:"trainer",val:" = None"},{name:"length",val:" = None"}],parametersDescription:[{anchor:"tokenizers.Tokenizer.train_from_iterator.iterator",description:`<strong>iterator</strong> (<code>Iterator</code>) &#x2014;
Any iterator over strings or list of strings`,name:"iterator"},{anchor:"tokenizers.Tokenizer.train_from_iterator.trainer",description:`<strong>trainer</strong> (<code>~tokenizers.trainers.Trainer</code>, <em>optional</em>) &#x2014;
An optional trainer that should be used to train our Model`,name:"trainer"},{anchor:"tokenizers.Tokenizer.train_from_iterator.length",description:`<strong>length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The total number of sequences in the iterator. This is used to
provide meaningful progress tracking`,name:"length"}]}}),{c(){l=n("h2"),k=n("a"),p=n("span"),v(g.$$.fragment),E=i(),f=n("span"),w=a("Tokenizer"),I=i(),h=n("div"),v(j.$$.fragment),V=i(),A=n("p"),x=a("A "),D=n("code"),G=a("Tokenizer"),vt=a(` works as a pipeline. It processes some raw text as input
and outputs an `),U=n("a"),_t=a("Encoding"),$r=a("."),br=i(),ne=n("div"),v(Pe.$$.fragment),Tr=i(),Y=n("p"),Er=a("The "),Rt=n("em"),yr=a("optional"),wr=i(),Jt=n("code"),xr=a("Decoder"),qr=a(" in use by the Tokenizer"),Dr=i(),re=n("div"),v(je.$$.fragment),Ar=i(),Ve=n("p"),Ir=a("The "),zt=n("a"),Pr=a("Model"),jr=a(" in use by the Tokenizer"),Vr=i(),oe=n("div"),v(Ce.$$.fragment),Cr=i(),K=n("p"),Lr=a("The "),Bt=n("em"),Nr=a("optional"),Sr=i(),$t=n("a"),Mr=a("Normalizer"),Or=a(" in use by the Tokenizer"),Gr=i(),F=n("div"),v(Le.$$.fragment),Hr=i(),Ut=n("p"),Fr=a("Get the current padding parameters"),Wr=i(),Q=n("p"),Yt=n("em"),Rr=a("Cannot be set, use"),Jr=i(),Kt=n("code"),Br=a("enable_padding()"),Ur=i(),Qt=n("em"),Yr=a("instead"),Kr=i(),ae=n("div"),v(Ne.$$.fragment),Qr=i(),X=n("p"),Xr=a("The "),Xt=n("em"),Zr=a("optional"),eo=i(),Zt=n("code"),to=a("PostProcessor"),no=a(" in use by the Tokenizer"),ro=i(),se=n("div"),v(Se.$$.fragment),oo=i(),Z=n("p"),ao=a("The "),en=n("em"),so=a("optional"),io=i(),bt=n("a"),co=a("PreTokenizer"),lo=a(" in use by the Tokenizer"),po=i(),W=n("div"),v(Me.$$.fragment),ho=i(),tn=n("p"),mo=a("Get the currently set truncation parameters"),go=i(),ee=n("p"),nn=n("em"),uo=a("Cannot set, use"),fo=i(),rn=n("code"),ko=a("enable_truncation()"),vo=i(),on=n("em"),_o=a("instead"),zo=i(),C=n("div"),v(Oe.$$.fragment),$o=i(),an=n("p"),bo=a("Add the given special tokens to the Tokenizer."),To=i(),sn=n("p"),Eo=a(`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),yo=i(),dn=n("p"),wo=a(`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),xo=i(),R=n("div"),v(Ge.$$.fragment),qo=i(),cn=n("p"),Do=a("Add the given tokens to the vocabulary"),Ao=i(),ln=n("p"),Io=a(`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),Po=i(),J=n("div"),v(He.$$.fragment),jo=i(),pn=n("p"),Vo=a("Decode the given list of ids back to a string"),Co=i(),hn=n("p"),Lo=a("This is used to decode anything coming back from a Language Model"),No=i(),ie=n("div"),v(Fe.$$.fragment),So=i(),mn=n("p"),Mo=a("Decode a batch of ids back to their corresponding string"),Oo=i(),de=n("div"),v(We.$$.fragment),Go=i(),gn=n("p"),Ho=a("Enable the padding"),Fo=i(),ce=n("div"),v(Re.$$.fragment),Wo=i(),un=n("p"),Ro=a("Enable truncation"),Jo=i(),L=n("div"),v(Je.$$.fragment),Bo=i(),fn=n("p"),Uo=a(`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),Yo=i(),kn=n("p"),Ko=a("Example:"),Qo=i(),v(le.$$.fragment),Xo=i(),N=n("div"),v(Be.$$.fragment),Zo=i(),vn=n("p"),ea=a(`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),ta=i(),_n=n("p"),na=a("Example:"),ra=i(),v(pe.$$.fragment),oa=i(),he=n("div"),v(Ue.$$.fragment),aa=i(),Ye=n("p"),sa=a("Instantiate a new "),Tt=n("a"),ia=a("Tokenizer"),da=a(" from the given buffer."),ca=i(),me=n("div"),v(Ke.$$.fragment),la=i(),Qe=n("p"),pa=a("Instantiate a new "),Et=n("a"),ha=a("Tokenizer"),ma=a(" from the file at the given path."),ga=i(),ge=n("div"),v(Xe.$$.fragment),ua=i(),Ze=n("p"),fa=a("Instantiate a new "),yt=n("a"),ka=a("Tokenizer"),va=a(` from an existing file on the
Hugging Face Hub.`),_a=i(),ue=n("div"),v(et.$$.fragment),za=i(),tt=n("p"),$a=a("Instantiate a new "),wt=n("a"),ba=a("Tokenizer"),Ta=a(" from the given JSON string."),Ea=i(),fe=n("div"),v(nt.$$.fragment),ya=i(),zn=n("p"),wa=a("Get the underlying vocabulary"),xa=i(),ke=n("div"),v(rt.$$.fragment),qa=i(),$n=n("p"),Da=a("Get the size of the underlying vocabulary"),Aa=i(),ve=n("div"),v(ot.$$.fragment),Ia=i(),bn=n("p"),Pa=a("Convert the given id to its corresponding token if it exists"),ja=i(),_e=n("div"),v(at.$$.fragment),Va=i(),Tn=n("p"),Ca=a("Disable padding"),La=i(),ze=n("div"),v(st.$$.fragment),Na=i(),En=n("p"),Sa=a("Disable truncation"),Ma=i(),$e=n("div"),v(it.$$.fragment),Oa=i(),yn=n("p"),Ga=a(`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),Ha=i(),S=n("div"),v(dt.$$.fragment),Fa=i(),wn=n("p"),Wa=a("Apply all the post-processing steps to the given encodings."),Ra=i(),xn=n("p"),Ja=a("The various steps are:"),Ba=i(),te=n("ol"),ct=n("li"),Ua=a(`Truncate according to the set truncation params (provided with
`),qn=n("code"),Ya=a("enable_truncation()"),Ka=a(")"),Qa=i(),xt=n("li"),Xa=a("Apply the "),Dn=n("code"),Za=a("PostProcessor"),es=i(),lt=n("li"),ts=a(`Pad according to the set padding params (provided with
`),An=n("code"),ns=a("enable_padding()"),rs=a(")"),os=i(),be=n("div"),v(pt.$$.fragment),as=i(),ht=n("p"),ss=a("Save the "),qt=n("a"),is=a("Tokenizer"),ds=a(" to the file at the given path."),cs=i(),Te=n("div"),v(mt.$$.fragment),ls=i(),gt=n("p"),ps=a("Gets a serialized string representing this "),Dt=n("a"),hs=a("Tokenizer"),ms=a("."),gs=i(),Ee=n("div"),v(ut.$$.fragment),us=i(),In=n("p"),fs=a("Convert the given token to its corresponding id if it exists"),ks=i(),B=n("div"),v(ft.$$.fragment),vs=i(),Pn=n("p"),_s=a("Train the Tokenizer using the given files."),zs=i(),At=n("p"),$s=a(`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),jn=n("code"),bs=a("train_from_iterator()"),Ts=i(),M=n("div"),v(kt.$$.fragment),Es=i(),Vn=n("p"),ys=a("Train the Tokenizer using the provided iterator."),ws=i(),Cn=n("p"),xs=a("You can provide anything that is a Python Iterator"),qs=i(),H=n("ul"),It=n("li"),Ds=a("A list of sequences "),Ln=n("code"),As=a("List[str]"),Is=i(),ye=n("li"),Ps=a("A generator that yields "),Nn=n("code"),js=a("str"),Vs=a(" or "),Sn=n("code"),Cs=a("List[str]"),Ls=i(),Mn=n("li"),Ns=a("A Numpy array of strings"),Ss=i(),On=n("li"),Ms=a("\u2026"),this.h()},l(c){l=r(c,"H2",{class:!0});var P=o(l);k=r(P,"A",{id:!0,class:!0,href:!0});var Gn=o(k);p=r(Gn,"SPAN",{});var Hn=o(p);_(g.$$.fragment,Hn),Hn.forEach(t),Gn.forEach(t),E=d(P),f=r(P,"SPAN",{});var Ws=o(f);w=s(Ws,"Tokenizer"),Ws.forEach(t),P.forEach(t),I=d(c),h=r(c,"DIV",{class:!0});var m=o(h);_(j.$$.fragment,m),V=d(m),A=r(m,"P",{});var Pt=o(A);x=s(Pt,"A "),D=r(Pt,"CODE",{});var Rs=o(D);G=s(Rs,"Tokenizer"),Rs.forEach(t),vt=s(Pt,` works as a pipeline. It processes some raw text as input
and outputs an `),U=r(Pt,"A",{href:!0});var Js=o(U);_t=s(Js,"Encoding"),Js.forEach(t),$r=s(Pt,"."),Pt.forEach(t),br=d(m),ne=r(m,"DIV",{class:!0});var Rn=o(ne);_(Pe.$$.fragment,Rn),Tr=d(Rn),Y=r(Rn,"P",{});var jt=o(Y);Er=s(jt,"The "),Rt=r(jt,"EM",{});var Bs=o(Rt);yr=s(Bs,"optional"),Bs.forEach(t),wr=d(jt),Jt=r(jt,"CODE",{});var Us=o(Jt);xr=s(Us,"Decoder"),Us.forEach(t),qr=s(jt," in use by the Tokenizer"),jt.forEach(t),Rn.forEach(t),Dr=d(m),re=r(m,"DIV",{class:!0});var Jn=o(re);_(je.$$.fragment,Jn),Ar=d(Jn),Ve=r(Jn,"P",{});var Bn=o(Ve);Ir=s(Bn,"The "),zt=r(Bn,"A",{href:!0});var Ys=o(zt);Pr=s(Ys,"Model"),Ys.forEach(t),jr=s(Bn," in use by the Tokenizer"),Bn.forEach(t),Jn.forEach(t),Vr=d(m),oe=r(m,"DIV",{class:!0});var Un=o(oe);_(Ce.$$.fragment,Un),Cr=d(Un),K=r(Un,"P",{});var Vt=o(K);Lr=s(Vt,"The "),Bt=r(Vt,"EM",{});var Ks=o(Bt);Nr=s(Ks,"optional"),Ks.forEach(t),Sr=d(Vt),$t=r(Vt,"A",{href:!0});var Qs=o($t);Mr=s(Qs,"Normalizer"),Qs.forEach(t),Or=s(Vt," in use by the Tokenizer"),Vt.forEach(t),Un.forEach(t),Gr=d(m),F=r(m,"DIV",{class:!0});var Ct=o(F);_(Le.$$.fragment,Ct),Hr=d(Ct),Ut=r(Ct,"P",{});var Xs=o(Ut);Fr=s(Xs,"Get the current padding parameters"),Xs.forEach(t),Wr=d(Ct),Q=r(Ct,"P",{});var Lt=o(Q);Yt=r(Lt,"EM",{});var Zs=o(Yt);Rr=s(Zs,"Cannot be set, use"),Zs.forEach(t),Jr=d(Lt),Kt=r(Lt,"CODE",{});var ei=o(Kt);Br=s(ei,"enable_padding()"),ei.forEach(t),Ur=d(Lt),Qt=r(Lt,"EM",{});var ti=o(Qt);Yr=s(ti,"instead"),ti.forEach(t),Lt.forEach(t),Ct.forEach(t),Kr=d(m),ae=r(m,"DIV",{class:!0});var Yn=o(ae);_(Ne.$$.fragment,Yn),Qr=d(Yn),X=r(Yn,"P",{});var Nt=o(X);Xr=s(Nt,"The "),Xt=r(Nt,"EM",{});var ni=o(Xt);Zr=s(ni,"optional"),ni.forEach(t),eo=d(Nt),Zt=r(Nt,"CODE",{});var ri=o(Zt);to=s(ri,"PostProcessor"),ri.forEach(t),no=s(Nt," in use by the Tokenizer"),Nt.forEach(t),Yn.forEach(t),ro=d(m),se=r(m,"DIV",{class:!0});var Kn=o(se);_(Se.$$.fragment,Kn),oo=d(Kn),Z=r(Kn,"P",{});var St=o(Z);ao=s(St,"The "),en=r(St,"EM",{});var oi=o(en);so=s(oi,"optional"),oi.forEach(t),io=d(St),bt=r(St,"A",{href:!0});var ai=o(bt);co=s(ai,"PreTokenizer"),ai.forEach(t),lo=s(St," in use by the Tokenizer"),St.forEach(t),Kn.forEach(t),po=d(m),W=r(m,"DIV",{class:!0});var Mt=o(W);_(Me.$$.fragment,Mt),ho=d(Mt),tn=r(Mt,"P",{});var si=o(tn);mo=s(si,"Get the currently set truncation parameters"),si.forEach(t),go=d(Mt),ee=r(Mt,"P",{});var Ot=o(ee);nn=r(Ot,"EM",{});var ii=o(nn);uo=s(ii,"Cannot set, use"),ii.forEach(t),fo=d(Ot),rn=r(Ot,"CODE",{});var di=o(rn);ko=s(di,"enable_truncation()"),di.forEach(t),vo=d(Ot),on=r(Ot,"EM",{});var ci=o(on);_o=s(ci,"instead"),ci.forEach(t),Ot.forEach(t),Mt.forEach(t),zo=d(m),C=r(m,"DIV",{class:!0});var we=o(C);_(Oe.$$.fragment,we),$o=d(we),an=r(we,"P",{});var li=o(an);bo=s(li,"Add the given special tokens to the Tokenizer."),li.forEach(t),To=d(we),sn=r(we,"P",{});var pi=o(sn);Eo=s(pi,`If these tokens are already part of the vocabulary, it just let the Tokenizer know about
them. If they don\u2019t exist, the Tokenizer creates them, giving them a new id.`),pi.forEach(t),yo=d(we),dn=r(we,"P",{});var hi=o(dn);wo=s(hi,`These special tokens will never be processed by the model (ie won\u2019t be split into
multiple tokens), and they can be removed from the output when decoding.`),hi.forEach(t),we.forEach(t),xo=d(m),R=r(m,"DIV",{class:!0});var Gt=o(R);_(Ge.$$.fragment,Gt),qo=d(Gt),cn=r(Gt,"P",{});var mi=o(cn);Do=s(mi,"Add the given tokens to the vocabulary"),mi.forEach(t),Ao=d(Gt),ln=r(Gt,"P",{});var gi=o(ln);Io=s(gi,`The given tokens are added only if they don\u2019t already exist in the vocabulary.
Each token then gets a new attributed id.`),gi.forEach(t),Gt.forEach(t),Po=d(m),J=r(m,"DIV",{class:!0});var Ht=o(J);_(He.$$.fragment,Ht),jo=d(Ht),pn=r(Ht,"P",{});var ui=o(pn);Vo=s(ui,"Decode the given list of ids back to a string"),ui.forEach(t),Co=d(Ht),hn=r(Ht,"P",{});var fi=o(hn);Lo=s(fi,"This is used to decode anything coming back from a Language Model"),fi.forEach(t),Ht.forEach(t),No=d(m),ie=r(m,"DIV",{class:!0});var Qn=o(ie);_(Fe.$$.fragment,Qn),So=d(Qn),mn=r(Qn,"P",{});var ki=o(mn);Mo=s(ki,"Decode a batch of ids back to their corresponding string"),ki.forEach(t),Qn.forEach(t),Oo=d(m),de=r(m,"DIV",{class:!0});var Xn=o(de);_(We.$$.fragment,Xn),Go=d(Xn),gn=r(Xn,"P",{});var vi=o(gn);Ho=s(vi,"Enable the padding"),vi.forEach(t),Xn.forEach(t),Fo=d(m),ce=r(m,"DIV",{class:!0});var Zn=o(ce);_(Re.$$.fragment,Zn),Wo=d(Zn),un=r(Zn,"P",{});var _i=o(un);Ro=s(_i,"Enable truncation"),_i.forEach(t),Zn.forEach(t),Jo=d(m),L=r(m,"DIV",{class:!0});var xe=o(L);_(Je.$$.fragment,xe),Bo=d(xe),fn=r(xe,"P",{});var zi=o(fn);Uo=s(zi,`Encode the given sequence and pair. This method can process raw text sequences
as well as already pre-tokenized sequences.`),zi.forEach(t),Yo=d(xe),kn=r(xe,"P",{});var $i=o(kn);Ko=s($i,"Example:"),$i.forEach(t),Qo=d(xe),_(le.$$.fragment,xe),xe.forEach(t),Xo=d(m),N=r(m,"DIV",{class:!0});var qe=o(N);_(Be.$$.fragment,qe),Zo=d(qe),vn=r(qe,"P",{});var bi=o(vn);ea=s(bi,`Encode the given batch of inputs. This method accept both raw text sequences
as well as already pre-tokenized sequences.`),bi.forEach(t),ta=d(qe),_n=r(qe,"P",{});var Ti=o(_n);na=s(Ti,"Example:"),Ti.forEach(t),ra=d(qe),_(pe.$$.fragment,qe),qe.forEach(t),oa=d(m),he=r(m,"DIV",{class:!0});var er=o(he);_(Ue.$$.fragment,er),aa=d(er),Ye=r(er,"P",{});var tr=o(Ye);sa=s(tr,"Instantiate a new "),Tt=r(tr,"A",{href:!0});var Ei=o(Tt);ia=s(Ei,"Tokenizer"),Ei.forEach(t),da=s(tr," from the given buffer."),tr.forEach(t),er.forEach(t),ca=d(m),me=r(m,"DIV",{class:!0});var nr=o(me);_(Ke.$$.fragment,nr),la=d(nr),Qe=r(nr,"P",{});var rr=o(Qe);pa=s(rr,"Instantiate a new "),Et=r(rr,"A",{href:!0});var yi=o(Et);ha=s(yi,"Tokenizer"),yi.forEach(t),ma=s(rr," from the file at the given path."),rr.forEach(t),nr.forEach(t),ga=d(m),ge=r(m,"DIV",{class:!0});var or=o(ge);_(Xe.$$.fragment,or),ua=d(or),Ze=r(or,"P",{});var ar=o(Ze);fa=s(ar,"Instantiate a new "),yt=r(ar,"A",{href:!0});var wi=o(yt);ka=s(wi,"Tokenizer"),wi.forEach(t),va=s(ar,` from an existing file on the
Hugging Face Hub.`),ar.forEach(t),or.forEach(t),_a=d(m),ue=r(m,"DIV",{class:!0});var sr=o(ue);_(et.$$.fragment,sr),za=d(sr),tt=r(sr,"P",{});var ir=o(tt);$a=s(ir,"Instantiate a new "),wt=r(ir,"A",{href:!0});var xi=o(wt);ba=s(xi,"Tokenizer"),xi.forEach(t),Ta=s(ir," from the given JSON string."),ir.forEach(t),sr.forEach(t),Ea=d(m),fe=r(m,"DIV",{class:!0});var dr=o(fe);_(nt.$$.fragment,dr),ya=d(dr),zn=r(dr,"P",{});var qi=o(zn);wa=s(qi,"Get the underlying vocabulary"),qi.forEach(t),dr.forEach(t),xa=d(m),ke=r(m,"DIV",{class:!0});var cr=o(ke);_(rt.$$.fragment,cr),qa=d(cr),$n=r(cr,"P",{});var Di=o($n);Da=s(Di,"Get the size of the underlying vocabulary"),Di.forEach(t),cr.forEach(t),Aa=d(m),ve=r(m,"DIV",{class:!0});var lr=o(ve);_(ot.$$.fragment,lr),Ia=d(lr),bn=r(lr,"P",{});var Ai=o(bn);Pa=s(Ai,"Convert the given id to its corresponding token if it exists"),Ai.forEach(t),lr.forEach(t),ja=d(m),_e=r(m,"DIV",{class:!0});var pr=o(_e);_(at.$$.fragment,pr),Va=d(pr),Tn=r(pr,"P",{});var Ii=o(Tn);Ca=s(Ii,"Disable padding"),Ii.forEach(t),pr.forEach(t),La=d(m),ze=r(m,"DIV",{class:!0});var hr=o(ze);_(st.$$.fragment,hr),Na=d(hr),En=r(hr,"P",{});var Pi=o(En);Sa=s(Pi,"Disable truncation"),Pi.forEach(t),hr.forEach(t),Ma=d(m),$e=r(m,"DIV",{class:!0});var mr=o($e);_(it.$$.fragment,mr),Oa=d(mr),yn=r(mr,"P",{});var ji=o(yn);Ga=s(ji,`Return the number of special tokens that would be added for single/pair sentences.
:param is_pair: Boolean indicating if the input would be a single sentence or a pair
:return:`),ji.forEach(t),mr.forEach(t),Ha=d(m),S=r(m,"DIV",{class:!0});var De=o(S);_(dt.$$.fragment,De),Fa=d(De),wn=r(De,"P",{});var Vi=o(wn);Wa=s(Vi,"Apply all the post-processing steps to the given encodings."),Vi.forEach(t),Ra=d(De),xn=r(De,"P",{});var Ci=o(xn);Ja=s(Ci,"The various steps are:"),Ci.forEach(t),Ba=d(De),te=r(De,"OL",{});var Ft=o(te);ct=r(Ft,"LI",{});var gr=o(ct);Ua=s(gr,`Truncate according to the set truncation params (provided with
`),qn=r(gr,"CODE",{});var Li=o(qn);Ya=s(Li,"enable_truncation()"),Li.forEach(t),Ka=s(gr,")"),gr.forEach(t),Qa=d(Ft),xt=r(Ft,"LI",{});var Os=o(xt);Xa=s(Os,"Apply the "),Dn=r(Os,"CODE",{});var Ni=o(Dn);Za=s(Ni,"PostProcessor"),Ni.forEach(t),Os.forEach(t),es=d(Ft),lt=r(Ft,"LI",{});var ur=o(lt);ts=s(ur,`Pad according to the set padding params (provided with
`),An=r(ur,"CODE",{});var Si=o(An);ns=s(Si,"enable_padding()"),Si.forEach(t),rs=s(ur,")"),ur.forEach(t),Ft.forEach(t),De.forEach(t),os=d(m),be=r(m,"DIV",{class:!0});var fr=o(be);_(pt.$$.fragment,fr),as=d(fr),ht=r(fr,"P",{});var kr=o(ht);ss=s(kr,"Save the "),qt=r(kr,"A",{href:!0});var Mi=o(qt);is=s(Mi,"Tokenizer"),Mi.forEach(t),ds=s(kr," to the file at the given path."),kr.forEach(t),fr.forEach(t),cs=d(m),Te=r(m,"DIV",{class:!0});var vr=o(Te);_(mt.$$.fragment,vr),ls=d(vr),gt=r(vr,"P",{});var _r=o(gt);ps=s(_r,"Gets a serialized string representing this "),Dt=r(_r,"A",{href:!0});var Oi=o(Dt);hs=s(Oi,"Tokenizer"),Oi.forEach(t),ms=s(_r,"."),_r.forEach(t),vr.forEach(t),gs=d(m),Ee=r(m,"DIV",{class:!0});var zr=o(Ee);_(ut.$$.fragment,zr),us=d(zr),In=r(zr,"P",{});var Gi=o(In);fs=s(Gi,"Convert the given token to its corresponding id if it exists"),Gi.forEach(t),zr.forEach(t),ks=d(m),B=r(m,"DIV",{class:!0});var Wt=o(B);_(ft.$$.fragment,Wt),vs=d(Wt),Pn=r(Wt,"P",{});var Hi=o(Pn);_s=s(Hi,"Train the Tokenizer using the given files."),Hi.forEach(t),zs=d(Wt),At=r(Wt,"P",{});var Gs=o(At);$s=s(Gs,`Reads the files line by line, while keeping all the whitespace, even new lines.
If you want to train from data store in-memory, you can check
`),jn=r(Gs,"CODE",{});var Fi=o(jn);bs=s(Fi,"train_from_iterator()"),Fi.forEach(t),Gs.forEach(t),Wt.forEach(t),Ts=d(m),M=r(m,"DIV",{class:!0});var Ae=o(M);_(kt.$$.fragment,Ae),Es=d(Ae),Vn=r(Ae,"P",{});var Wi=o(Vn);ys=s(Wi,"Train the Tokenizer using the provided iterator."),Wi.forEach(t),ws=d(Ae),Cn=r(Ae,"P",{});var Ri=o(Cn);xs=s(Ri,"You can provide anything that is a Python Iterator"),Ri.forEach(t),qs=d(Ae),H=r(Ae,"UL",{});var Ie=o(H);It=r(Ie,"LI",{});var Hs=o(It);Ds=s(Hs,"A list of sequences "),Ln=r(Hs,"CODE",{});var Ji=o(Ln);As=s(Ji,"List[str]"),Ji.forEach(t),Hs.forEach(t),Is=d(Ie),ye=r(Ie,"LI",{});var Fn=o(ye);Ps=s(Fn,"A generator that yields "),Nn=r(Fn,"CODE",{});var Bi=o(Nn);js=s(Bi,"str"),Bi.forEach(t),Vs=s(Fn," or "),Sn=r(Fn,"CODE",{});var Ui=o(Sn);Cs=s(Ui,"List[str]"),Ui.forEach(t),Fn.forEach(t),Ls=d(Ie),Mn=r(Ie,"LI",{});var Yi=o(Mn);Ns=s(Yi,"A Numpy array of strings"),Yi.forEach(t),Ss=d(Ie),On=r(Ie,"LI",{});var Ki=o(On);Ms=s(Ki,"\u2026"),Ki.forEach(t),Ie.forEach(t),Ae.forEach(t),m.forEach(t),this.h()},h(){u(k,"id","tokenizers.Tokenizer"),u(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(k,"href","#tokenizers.Tokenizer"),u(l,"class","relative group"),u(U,"href","/docs/tokenizers/v0.13.0/en/api/encoding#tokenizers.Encoding"),u(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(zt,"href","/docs/tokenizers/v0.13.0/en/api/models#tokenizers.models.Model"),u(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u($t,"href","/docs/tokenizers/v0.13.0/en/api/normalizers#tokenizers.normalizers.Normalizer"),u(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(bt,"href","/docs/tokenizers/v0.13.0/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),u(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Tt,"href","/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"),u(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Et,"href","/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"),u(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(yt,"href","/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"),u(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(wt,"href","/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"),u(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(qt,"href","/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"),u(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Dt,"href","/docs/tokenizers/v0.13.0/en/api/tokenizer#tokenizers.Tokenizer"),u(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(c,P){O(c,l,P),e(l,k),e(k,p),z(g,p,null),e(l,E),e(l,f),e(f,w),O(c,I,P),O(c,h,P),z(j,h,null),e(h,V),e(h,A),e(A,x),e(A,D),e(D,G),e(A,vt),e(A,U),e(U,_t),e(A,$r),e(h,br),e(h,ne),z(Pe,ne,null),e(ne,Tr),e(ne,Y),e(Y,Er),e(Y,Rt),e(Rt,yr),e(Y,wr),e(Y,Jt),e(Jt,xr),e(Y,qr),e(h,Dr),e(h,re),z(je,re,null),e(re,Ar),e(re,Ve),e(Ve,Ir),e(Ve,zt),e(zt,Pr),e(Ve,jr),e(h,Vr),e(h,oe),z(Ce,oe,null),e(oe,Cr),e(oe,K),e(K,Lr),e(K,Bt),e(Bt,Nr),e(K,Sr),e(K,$t),e($t,Mr),e(K,Or),e(h,Gr),e(h,F),z(Le,F,null),e(F,Hr),e(F,Ut),e(Ut,Fr),e(F,Wr),e(F,Q),e(Q,Yt),e(Yt,Rr),e(Q,Jr),e(Q,Kt),e(Kt,Br),e(Q,Ur),e(Q,Qt),e(Qt,Yr),e(h,Kr),e(h,ae),z(Ne,ae,null),e(ae,Qr),e(ae,X),e(X,Xr),e(X,Xt),e(Xt,Zr),e(X,eo),e(X,Zt),e(Zt,to),e(X,no),e(h,ro),e(h,se),z(Se,se,null),e(se,oo),e(se,Z),e(Z,ao),e(Z,en),e(en,so),e(Z,io),e(Z,bt),e(bt,co),e(Z,lo),e(h,po),e(h,W),z(Me,W,null),e(W,ho),e(W,tn),e(tn,mo),e(W,go),e(W,ee),e(ee,nn),e(nn,uo),e(ee,fo),e(ee,rn),e(rn,ko),e(ee,vo),e(ee,on),e(on,_o),e(h,zo),e(h,C),z(Oe,C,null),e(C,$o),e(C,an),e(an,bo),e(C,To),e(C,sn),e(sn,Eo),e(C,yo),e(C,dn),e(dn,wo),e(h,xo),e(h,R),z(Ge,R,null),e(R,qo),e(R,cn),e(cn,Do),e(R,Ao),e(R,ln),e(ln,Io),e(h,Po),e(h,J),z(He,J,null),e(J,jo),e(J,pn),e(pn,Vo),e(J,Co),e(J,hn),e(hn,Lo),e(h,No),e(h,ie),z(Fe,ie,null),e(ie,So),e(ie,mn),e(mn,Mo),e(h,Oo),e(h,de),z(We,de,null),e(de,Go),e(de,gn),e(gn,Ho),e(h,Fo),e(h,ce),z(Re,ce,null),e(ce,Wo),e(ce,un),e(un,Ro),e(h,Jo),e(h,L),z(Je,L,null),e(L,Bo),e(L,fn),e(fn,Uo),e(L,Yo),e(L,kn),e(kn,Ko),e(L,Qo),z(le,L,null),e(h,Xo),e(h,N),z(Be,N,null),e(N,Zo),e(N,vn),e(vn,ea),e(N,ta),e(N,_n),e(_n,na),e(N,ra),z(pe,N,null),e(h,oa),e(h,he),z(Ue,he,null),e(he,aa),e(he,Ye),e(Ye,sa),e(Ye,Tt),e(Tt,ia),e(Ye,da),e(h,ca),e(h,me),z(Ke,me,null),e(me,la),e(me,Qe),e(Qe,pa),e(Qe,Et),e(Et,ha),e(Qe,ma),e(h,ga),e(h,ge),z(Xe,ge,null),e(ge,ua),e(ge,Ze),e(Ze,fa),e(Ze,yt),e(yt,ka),e(Ze,va),e(h,_a),e(h,ue),z(et,ue,null),e(ue,za),e(ue,tt),e(tt,$a),e(tt,wt),e(wt,ba),e(tt,Ta),e(h,Ea),e(h,fe),z(nt,fe,null),e(fe,ya),e(fe,zn),e(zn,wa),e(h,xa),e(h,ke),z(rt,ke,null),e(ke,qa),e(ke,$n),e($n,Da),e(h,Aa),e(h,ve),z(ot,ve,null),e(ve,Ia),e(ve,bn),e(bn,Pa),e(h,ja),e(h,_e),z(at,_e,null),e(_e,Va),e(_e,Tn),e(Tn,Ca),e(h,La),e(h,ze),z(st,ze,null),e(ze,Na),e(ze,En),e(En,Sa),e(h,Ma),e(h,$e),z(it,$e,null),e($e,Oa),e($e,yn),e(yn,Ga),e(h,Ha),e(h,S),z(dt,S,null),e(S,Fa),e(S,wn),e(wn,Wa),e(S,Ra),e(S,xn),e(xn,Ja),e(S,Ba),e(S,te),e(te,ct),e(ct,Ua),e(ct,qn),e(qn,Ya),e(ct,Ka),e(te,Qa),e(te,xt),e(xt,Xa),e(xt,Dn),e(Dn,Za),e(te,es),e(te,lt),e(lt,ts),e(lt,An),e(An,ns),e(lt,rs),e(h,os),e(h,be),z(pt,be,null),e(be,as),e(be,ht),e(ht,ss),e(ht,qt),e(qt,is),e(ht,ds),e(h,cs),e(h,Te),z(mt,Te,null),e(Te,ls),e(Te,gt),e(gt,ps),e(gt,Dt),e(Dt,hs),e(gt,ms),e(h,gs),e(h,Ee),z(ut,Ee,null),e(Ee,us),e(Ee,In),e(In,fs),e(h,ks),e(h,B),z(ft,B,null),e(B,vs),e(B,Pn),e(Pn,_s),e(B,zs),e(B,At),e(At,$s),e(At,jn),e(jn,bs),e(h,Ts),e(h,M),z(kt,M,null),e(M,Es),e(M,Vn),e(Vn,ys),e(M,ws),e(M,Cn),e(Cn,xs),e(M,qs),e(M,H),e(H,It),e(It,Ds),e(It,Ln),e(Ln,As),e(H,Is),e(H,ye),e(ye,Ps),e(ye,Nn),e(Nn,js),e(ye,Vs),e(ye,Sn),e(Sn,Cs),e(H,Ls),e(H,Mn),e(Mn,Ns),e(H,Ss),e(H,On),e(On,Ms),Wn=!0},p(c,P){const Gn={};P&2&&(Gn.$$scope={dirty:P,ctx:c}),le.$set(Gn);const Hn={};P&2&&(Hn.$$scope={dirty:P,ctx:c}),pe.$set(Hn)},i(c){Wn||($(g.$$.fragment,c),$(j.$$.fragment,c),$(Pe.$$.fragment,c),$(je.$$.fragment,c),$(Ce.$$.fragment,c),$(Le.$$.fragment,c),$(Ne.$$.fragment,c),$(Se.$$.fragment,c),$(Me.$$.fragment,c),$(Oe.$$.fragment,c),$(Ge.$$.fragment,c),$(He.$$.fragment,c),$(Fe.$$.fragment,c),$(We.$$.fragment,c),$(Re.$$.fragment,c),$(Je.$$.fragment,c),$(le.$$.fragment,c),$(Be.$$.fragment,c),$(pe.$$.fragment,c),$(Ue.$$.fragment,c),$(Ke.$$.fragment,c),$(Xe.$$.fragment,c),$(et.$$.fragment,c),$(nt.$$.fragment,c),$(rt.$$.fragment,c),$(ot.$$.fragment,c),$(at.$$.fragment,c),$(st.$$.fragment,c),$(it.$$.fragment,c),$(dt.$$.fragment,c),$(pt.$$.fragment,c),$(mt.$$.fragment,c),$(ut.$$.fragment,c),$(ft.$$.fragment,c),$(kt.$$.fragment,c),Wn=!0)},o(c){b(g.$$.fragment,c),b(j.$$.fragment,c),b(Pe.$$.fragment,c),b(je.$$.fragment,c),b(Ce.$$.fragment,c),b(Le.$$.fragment,c),b(Ne.$$.fragment,c),b(Se.$$.fragment,c),b(Me.$$.fragment,c),b(Oe.$$.fragment,c),b(Ge.$$.fragment,c),b(He.$$.fragment,c),b(Fe.$$.fragment,c),b(We.$$.fragment,c),b(Re.$$.fragment,c),b(Je.$$.fragment,c),b(le.$$.fragment,c),b(Be.$$.fragment,c),b(pe.$$.fragment,c),b(Ue.$$.fragment,c),b(Ke.$$.fragment,c),b(Xe.$$.fragment,c),b(et.$$.fragment,c),b(nt.$$.fragment,c),b(rt.$$.fragment,c),b(ot.$$.fragment,c),b(at.$$.fragment,c),b(st.$$.fragment,c),b(it.$$.fragment,c),b(dt.$$.fragment,c),b(pt.$$.fragment,c),b(mt.$$.fragment,c),b(ut.$$.fragment,c),b(ft.$$.fragment,c),b(kt.$$.fragment,c),Wn=!1},d(c){c&&t(l),T(g),c&&t(I),c&&t(h),T(j),T(Pe),T(je),T(Ce),T(Le),T(Ne),T(Se),T(Me),T(Oe),T(Ge),T(He),T(Fe),T(We),T(Re),T(Je),T(le),T(Be),T(pe),T(Ue),T(Ke),T(Xe),T(et),T(nt),T(rt),T(ot),T(at),T(st),T(it),T(dt),T(pt),T(mt),T(ut),T(ft),T(kt)}}}function ld(q){let l,k;return l=new Fs({props:{$$slots:{default:[cd]},$$scope:{ctx:q}}}),{c(){v(l.$$.fragment)},l(p){_(l.$$.fragment,p)},m(p,g){z(l,p,g),k=!0},p(p,g){const E={};g&2&&(E.$$scope={dirty:g,ctx:p}),l.$set(E)},i(p){k||($(l.$$.fragment,p),k=!0)},o(p){b(l.$$.fragment,p),k=!1},d(p){T(l,p)}}}function pd(q){let l,k,p,g,E;return{c(){l=n("p"),k=a("The Rust API Reference is available directly on the "),p=n("a"),g=a("Docs.rs"),E=a(" website."),this.h()},l(f){l=r(f,"P",{});var w=o(l);k=s(w,"The Rust API Reference is available directly on the "),p=r(w,"A",{href:!0,rel:!0});var I=o(p);g=s(I,"Docs.rs"),I.forEach(t),E=s(w," website."),w.forEach(t),this.h()},h(){u(p,"href","https://docs.rs/tokenizers/latest/tokenizers/"),u(p,"rel","nofollow")},m(f,w){O(f,l,w),e(l,k),e(l,p),e(p,g),e(l,E)},d(f){f&&t(l)}}}function hd(q){let l,k;return l=new Fs({props:{$$slots:{default:[pd]},$$scope:{ctx:q}}}),{c(){v(l.$$.fragment)},l(p){_(l.$$.fragment,p)},m(p,g){z(l,p,g),k=!0},p(p,g){const E={};g&2&&(E.$$scope={dirty:g,ctx:p}),l.$set(E)},i(p){k||($(l.$$.fragment,p),k=!0)},o(p){b(l.$$.fragment,p),k=!1},d(p){T(l,p)}}}function md(q){let l,k;return{c(){l=n("p"),k=a("The node API has not been documented yet.")},l(p){l=r(p,"P",{});var g=o(l);k=s(g,"The node API has not been documented yet."),g.forEach(t)},m(p,g){O(p,l,g),e(l,k)},d(p){p&&t(l)}}}function gd(q){let l,k;return l=new Fs({props:{$$slots:{default:[md]},$$scope:{ctx:q}}}),{c(){v(l.$$.fragment)},l(p){_(l.$$.fragment,p)},m(p,g){z(l,p,g),k=!0},p(p,g){const E={};g&2&&(E.$$scope={dirty:g,ctx:p}),l.$set(E)},i(p){k||($(l.$$.fragment,p),k=!0)},o(p){b(l.$$.fragment,p),k=!1},d(p){T(l,p)}}}function ud(q){let l,k,p,g,E,f,w,I,h,j,V,A;return f=new ed({}),V=new sd({props:{python:!0,rust:!0,node:!0,$$slots:{node:[gd],rust:[hd],python:[ld]},$$scope:{ctx:q}}}),{c(){l=n("meta"),k=i(),p=n("h1"),g=n("a"),E=n("span"),v(f.$$.fragment),w=i(),I=n("span"),h=a("Tokenizer"),j=i(),v(V.$$.fragment),this.h()},l(x){const D=od('[data-svelte="svelte-1phssyn"]',document.head);l=r(D,"META",{name:!0,content:!0}),D.forEach(t),k=d(x),p=r(x,"H1",{class:!0});var G=o(p);g=r(G,"A",{id:!0,class:!0,href:!0});var vt=o(g);E=r(vt,"SPAN",{});var U=o(E);_(f.$$.fragment,U),U.forEach(t),vt.forEach(t),w=d(G),I=r(G,"SPAN",{});var _t=o(I);h=s(_t,"Tokenizer"),_t.forEach(t),G.forEach(t),j=d(x),_(V.$$.fragment,x),this.h()},h(){u(l,"name","hf:doc:metadata"),u(l,"content",JSON.stringify(fd)),u(g,"id","tokenizer"),u(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(g,"href","#tokenizer"),u(p,"class","relative group")},m(x,D){e(document.head,l),O(x,k,D),O(x,p,D),e(p,g),e(g,E),z(f,E,null),e(p,w),e(p,I),e(I,h),O(x,j,D),z(V,x,D),A=!0},p(x,[D]){const G={};D&2&&(G.$$scope={dirty:D,ctx:x}),V.$set(G)},i(x){A||($(f.$$.fragment,x),$(V.$$.fragment,x),A=!0)},o(x){b(f.$$.fragment,x),b(V.$$.fragment,x),A=!1},d(x){t(l),x&&t(k),x&&t(p),T(f),x&&t(j),T(V,x)}}}const fd={local:"tokenizer",sections:[{local:"tokenizers.Tokenizer",title:"Tokenizer"}],title:"Tokenizer"};function kd(q){return ad(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ed extends td{constructor(l){super();nd(this,l,kd,ud,rd,{})}}export{Ed as default,fd as metadata};
