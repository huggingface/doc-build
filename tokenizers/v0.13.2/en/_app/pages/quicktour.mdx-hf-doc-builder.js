import{S as ii,i as ui,s as pi,e as _,k as j,w as p,t as d,M as ci,c as k,d as r,m as v,a as w,x as c,h as g,b as P,G as a,g as q,y as f,q as $,o as h,B as m,v as fi,L as y}from"../chunks/vendor-hf-doc-builder.js";import{T as $i}from"../chunks/Tip-hf-doc-builder.js";import{I as Ee}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as b}from"../chunks/CodeBlock-hf-doc-builder.js";import{T as S,M as E}from"../chunks/TokenizersLanguageContent-hf-doc-builder.js";function hi(l){let t,n;return t=new b({props:{code:`from tokenizers import Tokenizer
from tokenizers.models import BPE
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function mi(l){let t,n;return t=new E({props:{$$slots:{default:[hi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function di(l){let t,n;return t=new b({props:{code:`use tokenizers::models::bpe::BPE;
let mut tokenizer: TokenizerImpl<
    BPE,
    NormalizerWrapper,
    PreTokenizerWrapper,
    PostProcessorWrapper,
    DecoderWrapper,
> = TokenizerImpl::new(
    BPE::builder()
        .unk_token("[UNK]".to_string())
        .build()
        .unwrap(),
);`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::models::bpe::BPE;
<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">tokenizer</span>: TokenizerImpl&lt;
    BPE,
    NormalizerWrapper,
    PreTokenizerWrapper,
    PostProcessorWrapper,
    DecoderWrapper,
&gt; = TokenizerImpl::<span class="hljs-title function_ invoke__">new</span>(
    BPE::<span class="hljs-title function_ invoke__">builder</span>()
        .<span class="hljs-title function_ invoke__">unk_token</span>(<span class="hljs-string">&quot;[UNK]&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>())
        .<span class="hljs-title function_ invoke__">build</span>()
        .<span class="hljs-title function_ invoke__">unwrap</span>(),
);`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function gi(l){let t,n;return t=new E({props:{$$slots:{default:[di]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function _i(l){let t,n;return t=new b({props:{code:`let { Tokenizer } = require("tokenizers/bindings/tokenizer");
let { BPE } = require("tokenizers/bindings/models");
let tokenizer = new Tokenizer(BPE.init({}, [], { unkToken: "[UNK]" }));`,highlighted:`<span class="hljs-keyword">let</span> { <span class="hljs-title class_">Tokenizer</span> } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/tokenizer&quot;</span>);
<span class="hljs-keyword">let</span> { <span class="hljs-variable constant_">BPE</span> } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/models&quot;</span>);
<span class="hljs-keyword">let</span> tokenizer = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Tokenizer</span>(<span class="hljs-variable constant_">BPE</span>.<span class="hljs-title function_">init</span>({}, [], { <span class="hljs-attr">unkToken</span>: <span class="hljs-string">&quot;[UNK]&quot;</span> }));`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ki(l){let t,n;return t=new E({props:{$$slots:{default:[_i]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function wi(l){let t,n;return t=new b({props:{code:`from tokenizers.trainers import BpeTrainer
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function qi(l){let t,n;return t=new E({props:{$$slots:{default:[wi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ji(l){let t,n;return t=new b({props:{code:`use tokenizers::models::bpe::BpeTrainer;
let mut trainer = BpeTrainer::builder()
    .special_tokens(vec![
        AddedToken::from("[UNK]", true),
        AddedToken::from("[CLS]", true),
        AddedToken::from("[SEP]", true),
        AddedToken::from("[PAD]", true),
        AddedToken::from("[MASK]", true),
    ])
    .build();`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::models::bpe::BpeTrainer;
<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">trainer</span> = BpeTrainer::<span class="hljs-title function_ invoke__">builder</span>()
    .<span class="hljs-title function_ invoke__">special_tokens</span>(<span class="hljs-built_in">vec!</span>[
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-literal">true</span>),
        AddedToken::<span class="hljs-title function_ invoke__">from</span>(<span class="hljs-string">&quot;[MASK]&quot;</span>, <span class="hljs-literal">true</span>),
    ])
    .<span class="hljs-title function_ invoke__">build</span>();`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function vi(l){let t,n;return t=new E({props:{$$slots:{default:[ji]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zi(l){let t,n;return t=new b({props:{code:`let { bpeTrainer } = require("tokenizers/bindings/trainers");
let trainer = bpeTrainer({
    specialTokens: ["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"]
});`,highlighted:`<span class="hljs-keyword">let</span> { bpeTrainer } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/trainers&quot;</span>);
<span class="hljs-keyword">let</span> trainer = <span class="hljs-title function_">bpeTrainer</span>({
    <span class="hljs-attr">specialTokens</span>: [<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>]
});`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bi(l){let t,n;return t=new E({props:{$$slots:{default:[zi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yi(l){let t,n,e,o,i,x,Z,U;return{c(){t=_("p"),n=d("The order in which you write the special tokens list matters: here "),e=_("code"),o=d('"[UNK]"'),i=d(` will get the ID 0,
`),x=_("code"),Z=d('"[CLS]"'),U=d(" will get the ID 1 and so forth.")},l(C){t=k(C,"P",{});var D=w(t);n=g(D,"The order in which you write the special tokens list matters: here "),e=k(D,"CODE",{});var I=w(e);o=g(I,'"[UNK]"'),I.forEach(r),i=g(D,` will get the ID 0,
`),x=k(D,"CODE",{});var re=w(x);Z=g(re,'"[CLS]"'),re.forEach(r),U=g(D," will get the ID 1 and so forth."),D.forEach(r)},m(C,D){q(C,t,D),a(t,n),a(t,e),a(e,o),a(t,i),a(t,x),a(x,Z),a(t,U)},d(C){C&&r(t)}}}function Ei(l){let t,n;return t=new b({props:{code:`from tokenizers.pre_tokenizers import Whitespace
tokenizer.pre_tokenizer = Whitespace()`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace
tokenizer.pre_tokenizer = Whitespace()`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pi(l){let t,n;return t=new E({props:{$$slots:{default:[Ei]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ti(l){let t,n;return t=new b({props:{code:`use tokenizers::pre_tokenizers::whitespace::Whitespace;
tokenizer.with_pre_tokenizer(Whitespace::default());`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::pre_tokenizers::whitespace::Whitespace;
tokenizer.<span class="hljs-title function_ invoke__">with_pre_tokenizer</span>(Whitespace::<span class="hljs-title function_ invoke__">default</span>());`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Si(l){let t,n;return t=new E({props:{$$slots:{default:[Ti]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hi(l){let t,n;return t=new b({props:{code:`let { whitespacePreTokenizer } = require("tokenizers/bindings/pre-tokenizers");
tokenizer.setPreTokenizer(whitespacePreTokenizer());`,highlighted:`<span class="hljs-keyword">let</span> { whitespacePreTokenizer } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/pre-tokenizers&quot;</span>);
tokenizer.<span class="hljs-title function_">setPreTokenizer</span>(<span class="hljs-title function_">whitespacePreTokenizer</span>());`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xi(l){let t,n;return t=new E({props:{$$slots:{default:[Hi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ci(l){let t,n;return t=new b({props:{code:`files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)`,highlighted:`files = [<span class="hljs-string">f&quot;data/wikitext-103-raw/wiki.<span class="hljs-subst">{split}</span>.raw&quot;</span> <span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;valid&quot;</span>]]
tokenizer.train(files, trainer)`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ai(l){let t,n;return t=new E({props:{$$slots:{default:[Ci]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Di(l){let t,n;return t=new b({props:{code:`let files = vec![
    "data/wikitext-103-raw/wiki.train.raw".into(),
    "data/wikitext-103-raw/wiki.test.raw".into(),
    "data/wikitext-103-raw/wiki.valid.raw".into(),
];
tokenizer.train_from_files(&mut trainer, files)?;`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">files</span> = <span class="hljs-built_in">vec!</span>[
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.train.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.test.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
    <span class="hljs-string">&quot;data/wikitext-103-raw/wiki.valid.raw&quot;</span>.<span class="hljs-title function_ invoke__">into</span>(),
];
tokenizer.<span class="hljs-title function_ invoke__">train_from_files</span>(&amp;<span class="hljs-keyword">mut</span> trainer, files)?;`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Li(l){let t,n;return t=new E({props:{$$slots:{default:[Di]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Bi(l){let t,n;return t=new b({props:{code:'let files = ["test", "train", "valid"].map(split => `data/wikitext-103-raw/wiki.${split}.raw`);\ntokenizer.train(files, trainer);',highlighted:'<span class="hljs-keyword">let</span> files = [<span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;valid&quot;</span>].<span class="hljs-title function_">map</span>(<span class="hljs-function"><span class="hljs-params">split</span> =&gt;</span> <span class="hljs-string">`data/wikitext-103-raw/wiki.<span class="hljs-subst">${split}</span>.raw`</span>);\ntokenizer.<span class="hljs-title function_">train</span>(files, trainer);'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ni(l){let t,n;return t=new E({props:{$$slots:{default:[Bi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ii(l){let t,n;return t=new b({props:{code:'tokenizer.save("data/tokenizer-wiki.json")',highlighted:'tokenizer.save(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Oi(l){let t,n;return t=new E({props:{$$slots:{default:[Ii]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ui(l){let t,n;return t=new b({props:{code:'tokenizer.save("data/tokenizer-wiki.json", false)?;',highlighted:'tokenizer.<span class="hljs-title function_ invoke__">save</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>, <span class="hljs-literal">false</span>)?;'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ki(l){let t,n;return t=new E({props:{$$slots:{default:[Ui]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Wi(l){let t,n;return t=new b({props:{code:'tokenizer.save("data/tokenizer-wiki.json");',highlighted:'tokenizer.<span class="hljs-title function_">save</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>);'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Mi(l){let t,n;return t=new E({props:{$$slots:{default:[Wi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Fi(l){let t,n;return t=new b({props:{code:'tokenizer = Tokenizer.from_file("data/tokenizer-wiki.json")',highlighted:'tokenizer = Tokenizer.from_file(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Yi(l){let t,n;return t=new E({props:{$$slots:{default:[Fi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ri(l){let t,n;return t=new b({props:{code:'let mut tokenizer = Tokenizer::from_file("data/tokenizer-wiki.json")?;',highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-keyword">mut </span><span class="hljs-variable">tokenizer</span> = Tokenizer::<span class="hljs-title function_ invoke__">from_file</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>)?;'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Qi(l){let t,n;return t=new E({props:{$$slots:{default:[Ri]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Gi(l){let t,n;return t=new b({props:{code:'let tokenizer = Tokenizer.fromFile("data/tokenizer-wiki.json");',highlighted:'<span class="hljs-keyword">let</span> tokenizer = <span class="hljs-title class_">Tokenizer</span>.<span class="hljs-title function_">fromFile</span>(<span class="hljs-string">&quot;data/tokenizer-wiki.json&quot;</span>);'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ji(l){let t,n;return t=new E({props:{$$slots:{default:[Gi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Vi(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?")`,highlighted:'output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>)'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Xi(l){let t,n;return t=new E({props:{$$slots:{default:[Vi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Zi(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?", true)?;`,highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>, <span class="hljs-literal">true</span>)?;'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function eu(l){let t,n;return t=new E({props:{$$slots:{default:[Zi]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function tu(l){let t,n;return t=new b({props:{code:`let { promisify } = require('util');
let encode = promisify(tokenizer.encode.bind(tokenizer));
var output = await encode("Hello, y'all! How are you \u{1F601} ?");`,highlighted:`<span class="hljs-keyword">let</span> { promisify } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&#x27;util&#x27;</span>);
<span class="hljs-keyword">let</span> encode = <span class="hljs-title function_">promisify</span>(tokenizer.<span class="hljs-property">encode</span>.<span class="hljs-title function_">bind</span>(tokenizer));
<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>);`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function nu(l){let t,n;return t=new E({props:{$$slots:{default:[tu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function su(l){let t,n;return t=new b({props:{code:`print(output.tokens)
# ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]`,highlighted:`<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ou(l){let t,n;return t=new E({props:{$$slots:{default:[su]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ru(l){let t,n;return t=new b({props:{code:`println!("{:?}", output.get_tokens());
// ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?",]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;,]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function au(l){let t,n;return t=new E({props:{$$slots:{default:[ru]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function lu(l){let t,n;return t=new b({props:{code:`console.log(output.getTokens());
// ["Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?"]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function iu(l){let t,n;return t=new E({props:{$$slots:{default:[lu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function uu(l){let t,n;return t=new b({props:{code:`print(output.ids)
# [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-built_in">print</span>(output.ids)
<span class="hljs-comment"># [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function pu(l){let t,n;return t=new E({props:{$$slots:{default:[uu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function cu(l){let t,n;return t=new b({props:{code:`println!("{:?}", output.get_ids());
// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_ids</span>());
<span class="hljs-comment">// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function fu(l){let t,n;return t=new E({props:{$$slots:{default:[cu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function $u(l){let t,n;return t=new b({props:{code:`console.log(output.getIds());
// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getIds</span>());
<span class="hljs-comment">// [27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function hu(l){let t,n;return t=new E({props:{$$slots:{default:[$u]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function mu(l){let t,n;return t=new b({props:{code:`print(output.offsets[9])
# (26, 27)`,highlighted:`<span class="hljs-built_in">print</span>(output.offsets[<span class="hljs-number">9</span>])
<span class="hljs-comment"># (26, 27)</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function du(l){let t,n;return t=new E({props:{$$slots:{default:[mu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function gu(l){let t,n;return t=new b({props:{code:`println!("{:?}", output.get_offsets()[9]);
// (26, 30)`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_offsets</span>()[<span class="hljs-number">9</span>]);
<span class="hljs-comment">// (26, 30)</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function _u(l){let t,n;return t=new E({props:{$$slots:{default:[gu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ku(l){let t,n;return t=new b({props:{code:`let offsets = output.getOffsets();
console.log(offsets[9]);
// (26, 27)`,highlighted:`<span class="hljs-keyword">let</span> offsets = output.<span class="hljs-title function_">getOffsets</span>();
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(offsets[<span class="hljs-number">9</span>]);
<span class="hljs-comment">// (26, 27)</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function wu(l){let t,n;return t=new E({props:{$$slots:{default:[ku]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function qu(l){let t,n;return t=new b({props:{code:`sentence = "Hello, y'all! How are you \u{1F601} ?"
sentence[26:27]
# "\u{1F601}"`,highlighted:`sentence = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>
sentence[<span class="hljs-number">26</span>:<span class="hljs-number">27</span>]
<span class="hljs-comment"># &quot;\u{1F601}&quot;</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ju(l){let t,n;return t=new E({props:{$$slots:{default:[qu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function vu(l){let t,n;return t=new b({props:{code:`let sentence = "Hello, y'all! How are you \u{1F601} ?";
println!("{}", &sentence[26..30]);
// "\u{1F601}"`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">sentence</span> = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{}&quot;</span>, &amp;sentence[<span class="hljs-number">26</span>..<span class="hljs-number">30</span>]);
<span class="hljs-comment">// &quot;\u{1F601}&quot;</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zu(l){let t,n;return t=new E({props:{$$slots:{default:[vu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bu(l){let t,n;return t=new b({props:{code:`let { slice } = require("tokenizers/bindings/utils");
let sentence = "Hello, y'all! How are you \u{1F601} ?"
let [start, end] = offsets[9];
console.log(slice(sentence, start, end));
// "\u{1F601}"`,highlighted:`<span class="hljs-keyword">let</span> { slice } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/utils&quot;</span>);
<span class="hljs-keyword">let</span> sentence = <span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>
<span class="hljs-keyword">let</span> [start, end] = offsets[<span class="hljs-number">9</span>];
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title function_">slice</span>(sentence, start, end));
<span class="hljs-comment">// &quot;\u{1F601}&quot;</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yu(l){let t,n;return t=new E({props:{$$slots:{default:[bu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Eu(l){let t,n;return t=new b({props:{code:`tokenizer.token_to_id("[SEP]")
# 2`,highlighted:`tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)
<span class="hljs-comment"># 2</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pu(l){let t,n;return t=new E({props:{$$slots:{default:[Eu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Tu(l){let t,n;return t=new b({props:{code:`println!("{}", tokenizer.token_to_id("[SEP]").unwrap());
// 2`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{}&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>());
<span class="hljs-comment">// 2</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Su(l){let t,n;return t=new E({props:{$$slots:{default:[Tu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hu(l){let t,n;return t=new b({props:{code:`console.log(tokenizer.tokenToId("[SEP]"));
// 2`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>));
<span class="hljs-comment">// 2</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xu(l){let t,n;return t=new E({props:{$$slots:{default:[Hu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Cu(l){let t,n;return t=new b({props:{code:`from tokenizers.processors import TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers.processors <span class="hljs-keyword">import</span> TemplateProcessing
tokenizer.post_processor = TemplateProcessing(
    single=<span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>,
    pair=<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    special_tokens=[
        (<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.token_to_id(<span class="hljs-string">&quot;[CLS]&quot;</span>)),
        (<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.token_to_id(<span class="hljs-string">&quot;[SEP]&quot;</span>)),
    ],
)`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Au(l){let t,n;return t=new E({props:{$$slots:{default:[Cu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Du(l){let t,n;return t=new b({props:{code:`use tokenizers::processors::template::TemplateProcessing;
let special_tokens = vec![
    ("[CLS]", tokenizer.token_to_id("[CLS]").unwrap()),
    ("[SEP]", tokenizer.token_to_id("[SEP]").unwrap()),
];
tokenizer.with_post_processor(
    TemplateProcessing::builder()
        .try_single("[CLS] $A [SEP]")
        .unwrap()
        .try_pair("[CLS] $A [SEP] $B:1 [SEP]:1")
        .unwrap()
        .special_tokens(special_tokens)
        .build()?,
);`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::processors::template::TemplateProcessing;
<span class="hljs-keyword">let</span> <span class="hljs-variable">special_tokens</span> = <span class="hljs-built_in">vec!</span>[
    (<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>()),
    (<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.<span class="hljs-title function_ invoke__">token_to_id</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>).<span class="hljs-title function_ invoke__">unwrap</span>()),
];
tokenizer.<span class="hljs-title function_ invoke__">with_post_processor</span>(
    TemplateProcessing::<span class="hljs-title function_ invoke__">builder</span>()
        .<span class="hljs-title function_ invoke__">try_single</span>(<span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>)
        .<span class="hljs-title function_ invoke__">unwrap</span>()
        .<span class="hljs-title function_ invoke__">try_pair</span>(<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>)
        .<span class="hljs-title function_ invoke__">unwrap</span>()
        .<span class="hljs-title function_ invoke__">special_tokens</span>(special_tokens)
        .<span class="hljs-title function_ invoke__">build</span>()?,
);`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Lu(l){let t,n;return t=new E({props:{$$slots:{default:[Du]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Bu(l){let t,n;return t=new b({props:{code:`let { templateProcessing } = require("tokenizers/bindings/post-processors");
tokenizer.setPostProcessor(templateProcessing(
    "[CLS] $A [SEP]",
    "[CLS] $A [SEP] $B:1 [SEP]:1",
    [
        ["[CLS]", tokenizer.tokenToId("[CLS]")],
        ["[SEP]", tokenizer.tokenToId("[SEP]")],
    ],
));`,highlighted:`<span class="hljs-keyword">let</span> { templateProcessing } = <span class="hljs-built_in">require</span>(<span class="hljs-string">&quot;tokenizers/bindings/post-processors&quot;</span>);
tokenizer.<span class="hljs-title function_">setPostProcessor</span>(<span class="hljs-title function_">templateProcessing</span>(
    <span class="hljs-string">&quot;[CLS] $A [SEP]&quot;</span>,
    <span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    [
        [<span class="hljs-string">&quot;[CLS]&quot;</span>, tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[CLS]&quot;</span>)],
        [<span class="hljs-string">&quot;[SEP]&quot;</span>, tokenizer.<span class="hljs-title function_">tokenToId</span>(<span class="hljs-string">&quot;[SEP]&quot;</span>)],
    ],
));`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Nu(l){let t,n;return t=new E({props:{$$slots:{default:[Bu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Iu(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>)
<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ou(l){let t,n;return t=new E({props:{$$slots:{default:[Iu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Uu(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode("Hello, y'all! How are you \u{1F601} ?", true)?;
println!("{:?}", output.get_tokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>, <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ku(l){let t,n;return t=new E({props:{$$slots:{default:[Uu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Wu(l){let t,n;return t=new b({props:{code:`var output = await encode("Hello, y'all! How are you \u{1F601} ?");
console.log(output.getTokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all! How are you \u{1F601} ?&quot;</span>);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Mu(l){let t,n;return t=new E({props:{$$slots:{default:[Wu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Fu(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode("Hello, y'all!", "How are you \u{1F601} ?")
print(output.tokens)
# ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`output = tokenizer.encode(<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>)
<span class="hljs-built_in">print</span>(output.tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Yu(l){let t,n;return t=new E({props:{$$slots:{default:[Fu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ru(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode(("Hello, y'all!", "How are you \u{1F601} ?"), true)?;
println!("{:?}", output.get_tokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode</span>((<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>), <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Qu(l){let t,n;return t=new E({props:{$$slots:{default:[Ru]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Gu(l){let t,n;return t=new b({props:{code:`var output = await encode("Hello, y'all!", "How are you \u{1F601} ?");
console.log(output.getTokens());
// ["[CLS]", "Hello", ",", "y", "'", "all", "!", "[SEP]", "How", "are", "you", "[UNK]", "?", "[SEP]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encode</span>(<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;Hello&quot;, &quot;,&quot;, &quot;y&quot;, &quot;&#x27;&quot;, &quot;all&quot;, &quot;!&quot;, &quot;[SEP]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ju(l){let t,n;return t=new E({props:{$$slots:{default:[Gu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Vu(l){let t,n;return t=new b({props:{code:`print(output.type_ids)
# [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-built_in">print</span>(output.type_ids)
<span class="hljs-comment"># [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Xu(l){let t,n;return t=new E({props:{$$slots:{default:[Vu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Zu(l){let t,n;return t=new b({props:{code:`println!("{:?}", output.get_type_ids());
// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output.<span class="hljs-title function_ invoke__">get_type_ids</span>());
<span class="hljs-comment">// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ep(l){let t,n;return t=new E({props:{$$slots:{default:[Zu]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function tp(l){let t,n;return t=new b({props:{code:`console.log(output.getTypeIds());
// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output.<span class="hljs-title function_">getTypeIds</span>());
<span class="hljs-comment">// [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function np(l){let t,n;return t=new E({props:{$$slots:{default:[tp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function sp(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode_batch(["Hello, y'all!", "How are you \u{1F601} ?"])`,highlighted:'output = tokenizer.encode_batch([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>])'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function op(l){let t,n;return t=new E({props:{$$slots:{default:[sp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function rp(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode_batch(vec!["Hello, y'all!", "How are you \u{1F601} ?"], true)?;`,highlighted:'<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], <span class="hljs-literal">true</span>)?;'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ap(l){let t,n;return t=new E({props:{$$slots:{default:[rp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function lp(l){let t,n;return t=new b({props:{code:`let encodeBatch = promisify(tokenizer.encodeBatch.bind(tokenizer));
var output = await encodeBatch(["Hello, y'all!", "How are you \u{1F601} ?"]);`,highlighted:`<span class="hljs-keyword">let</span> encodeBatch = <span class="hljs-title function_">promisify</span>(tokenizer.<span class="hljs-property">encodeBatch</span>.<span class="hljs-title function_">bind</span>(tokenizer));
<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>]);`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function ip(l){let t,n;return t=new E({props:{$$slots:{default:[lp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function up(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode_batch(
    [["Hello, y'all!", "How are you \u{1F601} ?"], ["Hello to you too!", "I'm fine, thank you!"]]
)`,highlighted:`output = tokenizer.encode_batch(
    [[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], [<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>]]
)`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function pp(l){let t,n;return t=new E({props:{$$slots:{default:[up]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function cp(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode_batch(
    vec![
        ("Hello, y'all!", "How are you \u{1F601} ?"),
        ("Hello to you too!", "I'm fine, thank you!"),
    ],
    true,
)?;`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(
    <span class="hljs-built_in">vec!</span>[
        (<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>),
        (<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>),
    ],
    <span class="hljs-literal">true</span>,
)?;`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function fp(l){let t,n;return t=new E({props:{$$slots:{default:[cp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function $p(l){let t,n;return t=new b({props:{code:`var output = await encodeBatch(
    [["Hello, y'all!", "How are you \u{1F601} ?"], ["Hello to you too!", "I'm fine, thank you!"]]
);`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>(
    [[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], [<span class="hljs-string">&quot;Hello to you too!&quot;</span>, <span class="hljs-string">&quot;I&#x27;m fine, thank you!&quot;</span>]]
);`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function hp(l){let t,n;return t=new E({props:{$$slots:{default:[$p]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function mp(l){let t,n;return t=new b({props:{code:'tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")',highlighted:'tokenizer.enable_padding(pad_id=<span class="hljs-number">3</span>, pad_token=<span class="hljs-string">&quot;[PAD]&quot;</span>)'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function dp(l){let t,n;return t=new E({props:{$$slots:{default:[mp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function gp(l){let t,n;return t=new b({props:{code:`use tokenizers::PaddingParams;
tokenizer.with_padding(Some(PaddingParams {
    pad_id: 3,
    pad_token: "[PAD]".to_string(),
    ..PaddingParams::default()
}));`,highlighted:`<span class="hljs-keyword">use</span> tokenizers::PaddingParams;
tokenizer.<span class="hljs-title function_ invoke__">with_padding</span>(<span class="hljs-title function_ invoke__">Some</span>(PaddingParams {
    pad_id: <span class="hljs-number">3</span>,
    pad_token: <span class="hljs-string">&quot;[PAD]&quot;</span>.<span class="hljs-title function_ invoke__">to_string</span>(),
    ..PaddingParams::<span class="hljs-title function_ invoke__">default</span>()
}));`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function _p(l){let t,n;return t=new E({props:{$$slots:{default:[gp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function kp(l){let t,n;return t=new b({props:{code:'tokenizer.setPadding({ padId: 3, padToken: "[PAD]" });',highlighted:'tokenizer.<span class="hljs-title function_">setPadding</span>({ <span class="hljs-attr">padId</span>: <span class="hljs-number">3</span>, <span class="hljs-attr">padToken</span>: <span class="hljs-string">&quot;[PAD]&quot;</span> });'}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function wp(l){let t,n;return t=new E({props:{$$slots:{default:[kp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function qp(l){let t,n;return t=new b({props:{code:`output = tokenizer.encode_batch(["Hello, y'all!", "How are you \u{1F601} ?"])
print(output[1].tokens)
# ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`output = tokenizer.encode_batch([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>])
<span class="hljs-built_in">print</span>(output[<span class="hljs-number">1</span>].tokens)
<span class="hljs-comment"># [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function jp(l){let t,n;return t=new E({props:{$$slots:{default:[qp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function vp(l){let t,n;return t=new b({props:{code:`let output = tokenizer.encode_batch(vec!["Hello, y'all!", "How are you \u{1F601} ?"], true)?;
println!("{:?}", output[1].get_tokens());
// ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`<span class="hljs-keyword">let</span> <span class="hljs-variable">output</span> = tokenizer.<span class="hljs-title function_ invoke__">encode_batch</span>(<span class="hljs-built_in">vec!</span>[<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>], <span class="hljs-literal">true</span>)?;
<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output[<span class="hljs-number">1</span>].<span class="hljs-title function_ invoke__">get_tokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function zp(l){let t,n;return t=new E({props:{$$slots:{default:[vp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function bp(l){let t,n;return t=new b({props:{code:`var output = await encodeBatch(["Hello, y'all!", "How are you \u{1F601} ?"]);
console.log(output[1].getTokens());
// ["[CLS]", "How", "are", "you", "[UNK]", "?", "[SEP]", "[PAD]"]`,highlighted:`<span class="hljs-keyword">var</span> output = <span class="hljs-keyword">await</span> <span class="hljs-title function_">encodeBatch</span>([<span class="hljs-string">&quot;Hello, y&#x27;all!&quot;</span>, <span class="hljs-string">&quot;How are you \u{1F601} ?&quot;</span>]);
<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output[<span class="hljs-number">1</span>].<span class="hljs-title function_">getTokens</span>());
<span class="hljs-comment">// [&quot;[CLS]&quot;, &quot;How&quot;, &quot;are&quot;, &quot;you&quot;, &quot;[UNK]&quot;, &quot;?&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function yp(l){let t,n;return t=new E({props:{$$slots:{default:[bp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Ep(l){let t,n;return t=new b({props:{code:`print(output[1].attention_mask)
# [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-built_in">print</span>(output[<span class="hljs-number">1</span>].attention_mask)
<span class="hljs-comment"># [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Pp(l){let t,n;return t=new E({props:{$$slots:{default:[Ep]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Tp(l){let t,n;return t=new b({props:{code:`println!("{:?}", output[1].get_attention_mask());
// [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-built_in">println!</span>(<span class="hljs-string">&quot;{:?}&quot;</span>, output[<span class="hljs-number">1</span>].<span class="hljs-title function_ invoke__">get_attention_mask</span>());
<span class="hljs-comment">// [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Sp(l){let t,n;return t=new E({props:{$$slots:{default:[Tp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Hp(l){let t,n;return t=new b({props:{code:`console.log(output[1].getAttentionMask());
// [1, 1, 1, 1, 1, 1, 1, 0]`,highlighted:`<span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(output[<span class="hljs-number">1</span>].<span class="hljs-title function_">getAttentionMask</span>());
<span class="hljs-comment">// [1, 1, 1, 1, 1, 1, 1, 0]</span>`}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p:y,i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function xp(l){let t,n;return t=new E({props:{$$slots:{default:[Hp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Cp(l){let t,n,e,o,i,x,Z,U,C,D,I,re,Pe,K,B,ae,L,F,ee,te,Te,W,jt,Y,le,vt,ge,O,_e,A,R,ne,se,zt,ie,oe,ke;return o=new Ee({}),B=new b({props:{code:`from tokenizers import Tokenizer

tokenizer = Tokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer

tokenizer = Tokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`}}),te=new Ee({}),O=new b({props:{code:`from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)`,highlighted:`<span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer(<span class="hljs-string">&quot;bert-base-uncased-vocab.txt&quot;</span>, lowercase=<span class="hljs-literal">True</span>)`}}),oe=new b({props:{code:"wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt",highlighted:"wget https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"}}),{c(){t=_("h3"),n=_("a"),e=_("span"),p(o.$$.fragment),i=j(),x=_("span"),Z=d("Using a pretrained tokenizer"),U=j(),C=_("p"),D=d(`You can load any tokenizer from the Hugging Face Hub as long as a
`),I=_("code"),re=d("tokenizer.json"),Pe=d(" file is available in the repository."),K=j(),p(B.$$.fragment),ae=j(),L=_("h3"),F=_("a"),ee=_("span"),p(te.$$.fragment),Te=j(),W=_("span"),jt=d("Importing a pretrained tokenizer from legacy vocabulary files"),Y=j(),le=_("p"),vt=d(`You can also import a pretrained tokenizer directly in, as long as you
have its vocabulary file. For instance, here is how to import the
classic pretrained BERT tokenizer:`),ge=j(),p(O.$$.fragment),_e=j(),A=_("p"),R=d("as long as you have downloaded the file "),ne=_("code"),se=d("bert-base-uncased-vocab.txt"),zt=d(" with"),ie=j(),p(oe.$$.fragment),this.h()},l(z){t=k(z,"H3",{class:!0});var T=w(t);n=k(T,"A",{id:!0,class:!0,href:!0});var ue=w(n);e=k(ue,"SPAN",{});var Dt=w(e);c(o.$$.fragment,Dt),Dt.forEach(r),ue.forEach(r),i=v(T),x=k(T,"SPAN",{});var Lt=w(x);Z=g(Lt,"Using a pretrained tokenizer"),Lt.forEach(r),T.forEach(r),U=v(z),C=k(z,"P",{});var we=w(C);D=g(we,`You can load any tokenizer from the Hugging Face Hub as long as a
`),I=k(we,"CODE",{});var M=w(I);re=g(M,"tokenizer.json"),M.forEach(r),Pe=g(we," file is available in the repository."),we.forEach(r),K=v(z),c(B.$$.fragment,z),ae=v(z),L=k(z,"H3",{class:!0});var pe=w(L);F=k(pe,"A",{id:!0,class:!0,href:!0});var Bt=w(F);ee=k(Bt,"SPAN",{});var Nt=w(ee);c(te.$$.fragment,Nt),Nt.forEach(r),Bt.forEach(r),Te=v(pe),W=k(pe,"SPAN",{});var Se=w(W);jt=g(Se,"Importing a pretrained tokenizer from legacy vocabulary files"),Se.forEach(r),pe.forEach(r),Y=v(z),le=k(z,"P",{});var It=w(le);vt=g(It,`You can also import a pretrained tokenizer directly in, as long as you
have its vocabulary file. For instance, here is how to import the
classic pretrained BERT tokenizer:`),It.forEach(r),ge=v(z),c(O.$$.fragment,z),_e=v(z),A=k(z,"P",{});var He=w(A);R=g(He,"as long as you have downloaded the file "),ne=k(He,"CODE",{});var xe=w(ne);se=g(xe,"bert-base-uncased-vocab.txt"),xe.forEach(r),zt=g(He," with"),He.forEach(r),ie=v(z),c(oe.$$.fragment,z),this.h()},h(){P(n,"id","using-a-pretrained-tokenizer"),P(n,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(n,"href","#using-a-pretrained-tokenizer"),P(t,"class","relative group"),P(F,"id","importing-a-pretrained-tokenizer-from-legacy-vocabulary-files"),P(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(F,"href","#importing-a-pretrained-tokenizer-from-legacy-vocabulary-files"),P(L,"class","relative group")},m(z,T){q(z,t,T),a(t,n),a(n,e),f(o,e,null),a(t,i),a(t,x),a(x,Z),q(z,U,T),q(z,C,T),a(C,D),a(C,I),a(I,re),a(C,Pe),q(z,K,T),f(B,z,T),q(z,ae,T),q(z,L,T),a(L,F),a(F,ee),f(te,ee,null),a(L,Te),a(L,W),a(W,jt),q(z,Y,T),q(z,le,T),a(le,vt),q(z,ge,T),f(O,z,T),q(z,_e,T),q(z,A,T),a(A,R),a(A,ne),a(ne,se),a(A,zt),q(z,ie,T),f(oe,z,T),ke=!0},p:y,i(z){ke||($(o.$$.fragment,z),$(B.$$.fragment,z),$(te.$$.fragment,z),$(O.$$.fragment,z),$(oe.$$.fragment,z),ke=!0)},o(z){h(o.$$.fragment,z),h(B.$$.fragment,z),h(te.$$.fragment,z),h(O.$$.fragment,z),h(oe.$$.fragment,z),ke=!1},d(z){z&&r(t),m(o),z&&r(U),z&&r(C),z&&r(K),m(B,z),z&&r(ae),z&&r(L),m(te),z&&r(Y),z&&r(le),z&&r(ge),m(O,z),z&&r(_e),z&&r(A),z&&r(ie),m(oe,z)}}}function Ap(l){let t,n;return t=new E({props:{$$slots:{default:[Cp]},$$scope:{ctx:l}}}),{c(){p(t.$$.fragment)},l(e){c(t.$$.fragment,e)},m(e,o){f(t,e,o),n=!0},p(e,o){const i={};o&2&&(i.$$scope={dirty:o,ctx:e}),t.$set(i)},i(e){n||($(t.$$.fragment,e),n=!0)},o(e){h(t.$$.fragment,e),n=!1},d(e){m(t,e)}}}function Dp(l){let t,n,e,o,i,x,Z,U,C,D,I,re,Pe,K,B,ae,L,F,ee,te,Te,W,jt,Y,le,vt,ge,O,_e,A,R,ne,se,zt,ie,oe,ke,z,T,ue,Dt,Lt,we,M,pe,Bt,Nt,Se,It,He,xe,xo,$s,ce,Co,Jt,Ao,Do,Vt,Lo,Bo,hs,Ce,ms,bt,No,Xt,Io,ds,Ae,gs,Q,Oo,Zt,Uo,Ko,en,Wo,Mo,tn,Fo,Yo,_s,De,ks,Le,Ro,nn,Qo,Go,ws,Be,qs,Ne,Jo,sn,Vo,Xo,js,Ie,vs,Oe,Zo,on,er,tr,zs,Ue,bs,fe,nr,rn,sr,or,an,rr,ar,ys,Ke,Es,qe,We,ln,yt,lr,un,ir,Ps,Me,ur,pn,pr,cr,Ts,Fe,Ss,$e,fr,cn,$r,hr,fn,mr,dr,Hs,he,gr,$n,_r,kr,hn,wr,qr,xs,Ye,Cs,Re,jr,mn,vr,zr,As,Qe,Ds,G,br,dn,yr,Er,gn,Pr,Tr,_n,Sr,Hr,Ls,Ge,Bs,Ot,xr,Ns,Je,Is,je,Ve,kn,Et,Cr,wn,Ar,Os,J,Dr,qn,Lr,Br,jn,Nr,Ir,vn,Or,Ur,Us,V,Kr,zn,Wr,Mr,bn,Fr,Yr,yn,Rr,Qr,Ks,Xe,Ws,Ut,Gr,Ms,Ze,Fs,me,Jr,En,Vr,Xr,Pn,Zr,ea,Ys,H,ta,Tn,na,sa,Sn,oa,ra,Hn,aa,la,xn,ia,ua,Cn,pa,ca,An,fa,$a,Dn,ha,ma,Rs,Kt,da,Qs,Wt,ga,Gs,et,Js,tt,_a,Ln,ka,wa,Vs,nt,Xs,Mt,qa,Zs,st,eo,ot,ja,Bn,va,za,to,ve,rt,Nn,Pt,ba,In,ya,no,at,Ea,On,Pa,Ta,so,lt,oo,it,Sa,Un,Ha,xa,ro,ut,Ca,Kn,Aa,Da,ao,pt,lo,X,La,Wn,Ba,Na,Mn,Ia,Oa,Fn,Ua,Ka,io,ct,uo,de,Wa,Yn,Ma,Fa,Rn,Ya,Ra,po,ft,co,$t,Qa,Qn,Ga,Ja,fo,ht,$o,ze,mt,Gn,Tt,Va,Jn,Xa,ho,dt,mo;return x=new Ee({}),L=new Ee({}),O=new b({props:{code:`wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
unzip wikitext-103-raw-v1.zip`,highlighted:`wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip
unzip wikitext-103-raw-v1.zip`}}),se=new Ee({}),Ce=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ki],rust:[gi],python:[mi]},$$scope:{ctx:l}}}),Ae=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[bi],rust:[vi],python:[qi]},$$scope:{ctx:l}}}),De=new $i({props:{$$slots:{default:[yi]},$$scope:{ctx:l}}}),Be=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[xi],rust:[Si],python:[Pi]},$$scope:{ctx:l}}}),Ie=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ni],rust:[Li],python:[Ai]},$$scope:{ctx:l}}}),Ue=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Mi],rust:[Ki],python:[Oi]},$$scope:{ctx:l}}}),Ke=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ji],rust:[Qi],python:[Yi]},$$scope:{ctx:l}}}),yt=new Ee({}),Fe=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[nu],rust:[eu],python:[Xi]},$$scope:{ctx:l}}}),Ye=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[iu],rust:[au],python:[ou]},$$scope:{ctx:l}}}),Qe=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[hu],rust:[fu],python:[pu]},$$scope:{ctx:l}}}),Ge=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[wu],rust:[_u],python:[du]},$$scope:{ctx:l}}}),Je=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[yu],rust:[zu],python:[ju]},$$scope:{ctx:l}}}),Et=new Ee({}),Xe=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[xu],rust:[Su],python:[Pu]},$$scope:{ctx:l}}}),Ze=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Nu],rust:[Lu],python:[Au]},$$scope:{ctx:l}}}),et=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Mu],rust:[Ku],python:[Ou]},$$scope:{ctx:l}}}),nt=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Ju],rust:[Qu],python:[Yu]},$$scope:{ctx:l}}}),st=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[np],rust:[ep],python:[Xu]},$$scope:{ctx:l}}}),Pt=new Ee({}),lt=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ip],rust:[ap],python:[op]},$$scope:{ctx:l}}}),pt=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[hp],rust:[fp],python:[pp]},$$scope:{ctx:l}}}),ct=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[wp],rust:[_p],python:[dp]},$$scope:{ctx:l}}}),ft=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[yp],rust:[zp],python:[jp]},$$scope:{ctx:l}}}),ht=new S({props:{python:!0,rust:!0,node:!0,$$slots:{node:[xp],rust:[Sp],python:[Pp]},$$scope:{ctx:l}}}),Tt=new Ee({}),dt=new S({props:{python:!0,rust:!0,node:!0,$$slots:{python:[Ap]},$$scope:{ctx:l}}}),{c(){t=_("meta"),n=j(),e=_("h1"),o=_("a"),i=_("span"),p(x.$$.fragment),Z=j(),U=_("span"),C=d("Quicktour"),D=j(),I=_("p"),re=d(`Let\u2019s have a quick look at the \u{1F917} Tokenizers library features. The
library provides an implementation of today\u2019s most used tokenizers that
is both easy to use and blazing fast.`),Pe=j(),K=_("h2"),B=_("a"),ae=_("span"),p(L.$$.fragment),F=j(),ee=_("span"),te=d("Build a tokenizer from scratch"),Te=j(),W=_("p"),jt=d(`To illustrate how fast the \u{1F917} Tokenizers library is, let\u2019s train a new
tokenizer on `),Y=_("a"),le=d("wikitext-103"),vt=d(`
(516M of text) in just a few seconds. First things first, you will need
to download this dataset and unzip it with:`),ge=j(),p(O.$$.fragment),_e=j(),A=_("h3"),R=_("a"),ne=_("span"),p(se.$$.fragment),zt=j(),ie=_("span"),oe=d("Training the tokenizer"),ke=j(),z=_("p"),T=d(`In this tour, we will build and train a Byte-Pair Encoding (BPE)
tokenizer. For more information about the different type of tokenizers,
check out this `),ue=_("a"),Dt=d("guide"),Lt=d(` in
the \u{1F917} Transformers documentation. Here, training the tokenizer means it
will learn merge rules by:`),we=j(),M=_("ul"),pe=_("li"),Bt=d(`Start with all the characters present in the training corpus as
tokens.`),Nt=j(),Se=_("li"),It=d("Identify the most common pair of tokens and merge it into one token."),He=j(),xe=_("li"),xo=d(`Repeat until the vocabulary (e.g., the number of tokens) has reached
the size we want.`),$s=j(),ce=_("p"),Co=d("The main API of the library is the "),Jt=_("code"),Ao=d("class"),Do=j(),Vt=_("code"),Lo=d("Tokenizer"),Bo=d(`, here is how
we instantiate one with a BPE model:`),hs=j(),p(Ce.$$.fragment),ms=j(),bt=_("p"),No=d(`To train our tokenizer on the wikitext files, we will need to
instantiate a [trainer]{.title-ref}, in this case a
`),Xt=_("code"),Io=d("BpeTrainer"),ds=j(),p(Ae.$$.fragment),gs=j(),Q=_("p"),Oo=d("We can set the training arguments like "),Zt=_("code"),Uo=d("vocab_size"),Ko=d(" or "),en=_("code"),Wo=d("min_frequency"),Mo=d(` (here
left at their default values of 30,000 and 0) but the most important
part is to give the `),tn=_("code"),Fo=d("special_tokens"),Yo=d(` we
plan to use later on (they are not used at all during training) so that
they get inserted in the vocabulary.`),_s=j(),p(De.$$.fragment),ks=j(),Le=_("p"),Ro=d(`We could train our tokenizer right now, but it wouldn\u2019t be optimal.
Without a pre-tokenizer that will split our inputs into words, we might
get tokens that overlap several words: for instance we could get an
`),nn=_("code"),Qo=d('"it is"'),Go=d(` token since those two words
often appear next to each other. Using a pre-tokenizer will ensure no
token is bigger than a word returned by the pre-tokenizer. Here we want
to train a subword BPE tokenizer, and we will use the easiest
pre-tokenizer possible by splitting on whitespace.`),ws=j(),p(Be.$$.fragment),qs=j(),Ne=_("p"),Jo=d("Now, we can just call the "),sn=_("code"),Vo=d("Tokenizer.train"),Xo=d(" method with any list of files we want to use:"),js=j(),p(Ie.$$.fragment),vs=j(),Oe=_("p"),Zo=d(`This should only take a few seconds to train our tokenizer on the full
wikitext dataset! To save the tokenizer in one file that contains all
its configuration and vocabulary, just use the
`),on=_("code"),er=d("Tokenizer.save"),tr=d(" method:"),zs=j(),p(Ue.$$.fragment),bs=j(),fe=_("p"),nr=d(`and you can reload your tokenizer from that file with the
`),rn=_("code"),sr=d("Tokenizer.from_file"),or=j(),an=_("code"),rr=d("classmethod"),ar=d(":"),ys=j(),p(Ke.$$.fragment),Es=j(),qe=_("h3"),We=_("a"),ln=_("span"),p(yt.$$.fragment),lr=j(),un=_("span"),ir=d("Using the tokenizer"),Ps=j(),Me=_("p"),ur=d(`Now that we have trained a tokenizer, we can use it on any text we want
with the `),pn=_("code"),pr=d("Tokenizer.encode"),cr=d(" method:"),Ts=j(),p(Fe.$$.fragment),Ss=j(),$e=_("p"),fr=d(`This applied the full pipeline of the tokenizer on the text, returning
an `),cn=_("code"),$r=d("Encoding"),hr=d(` object. To learn more
about this pipeline, and how to apply (or customize) parts of it, check out `),fn=_("code"),mr=d("this page <pipeline>"),dr=d("."),Hs=j(),he=_("p"),gr=d("This "),$n=_("code"),_r=d("Encoding"),kr=d(` object then has all the
attributes you need for your deep learning model (or other). The
`),hn=_("code"),wr=d("tokens"),qr=d(` attribute contains the
segmentation of your text in tokens:`),xs=j(),p(Ye.$$.fragment),Cs=j(),Re=_("p"),jr=d("Similarly, the "),mn=_("code"),vr=d("ids"),zr=d(` attribute will
contain the index of each of those tokens in the tokenizer\u2019s
vocabulary:`),As=j(),p(Qe.$$.fragment),Ds=j(),G=_("p"),br=d(`An important feature of the \u{1F917} Tokenizers library is that it comes with
full alignment tracking, meaning you can always get the part of your
original sentence that corresponds to a given token. Those are stored in
the `),dn=_("code"),yr=d("offsets"),Er=d(` attribute of our
`),gn=_("code"),Pr=d("Encoding"),Tr=d(` object. For instance, let\u2019s
assume we would want to find back what caused the
`),_n=_("code"),Sr=d('"[UNK]"'),Hr=d(` token to appear, which is the
token at index 9 in the list, we can just ask for the offset at the
index:`),Ls=j(),p(Ge.$$.fragment),Bs=j(),Ot=_("p"),xr=d(`and those are the indices that correspond to the emoji in the original
sentence:`),Ns=j(),p(Je.$$.fragment),Is=j(),je=_("h3"),Ve=_("a"),kn=_("span"),p(Et.$$.fragment),Cr=j(),wn=_("span"),Ar=d("Post-processing"),Os=j(),J=_("p"),Dr=d(`We might want our tokenizer to automatically add special tokens, like
`),qn=_("code"),Lr=d('"[CLS]"'),Br=d(" or "),jn=_("code"),Nr=d('"[SEP]"'),Ir=d(`. To do this, we use a post-processor.
`),vn=_("code"),Or=d("TemplateProcessing"),Ur=d(` is the most
commonly used, you just have to specify a template for the processing of
single sentences and pairs of sentences, along with the special tokens
and their IDs.`),Us=j(),V=_("p"),Kr=d("When we built our tokenizer, we set "),zn=_("code"),Wr=d('"[CLS]"'),Mr=d(" and "),bn=_("code"),Fr=d('"[SEP]"'),Yr=d(` in positions 1
and 2 of our list of special tokens, so this should be their IDs. To
double-check, we can use the `),yn=_("code"),Rr=d("Tokenizer.token_to_id"),Qr=d(" method:"),Ks=j(),p(Xe.$$.fragment),Ws=j(),Ut=_("p"),Gr=d(`Here is how we can set the post-processing to give us the traditional
BERT inputs:`),Ms=j(),p(Ze.$$.fragment),Fs=j(),me=_("p"),Jr=d(`Let\u2019s go over this snippet of code in more details. First we specify
the template for single sentences: those should have the form
`),En=_("code"),Vr=d('"[CLS] $A [SEP]"'),Xr=d(` where
`),Pn=_("code"),Zr=d("$A"),ea=d(" represents our sentence."),Ys=j(),H=_("p"),ta=d(`Then, we specify the template for sentence pairs, which should have the
form `),Tn=_("code"),na=d('"[CLS] $A [SEP] $B [SEP]"'),sa=d(` where
`),Sn=_("code"),oa=d("$A"),ra=d(` represents the first sentence and
`),Hn=_("code"),aa=d("$B"),la=d(` the second one. The
`),xn=_("code"),ia=d(":1"),ua=d(" added in the template represent the "),Cn=_("code"),pa=d("type IDs"),ca=d(` we want for each part of our input: it defaults
to 0 for everything (which is why we don\u2019t have
`),An=_("code"),fa=d("$A:0"),$a=d(`) and here we set it to 1 for the
tokens of the second sentence and the last `),Dn=_("code"),ha=d('"[SEP]"'),ma=d(" token."),Rs=j(),Kt=_("p"),da=d(`Lastly, we specify the special tokens we used and their IDs in our
tokenizer\u2019s vocabulary.`),Qs=j(),Wt=_("p"),ga=d(`To check out this worked properly, let\u2019s try to encode the same
sentence as before:`),Gs=j(),p(et.$$.fragment),Js=j(),tt=_("p"),_a=d(`To check the results on a pair of sentences, we just pass the two
sentences to `),Ln=_("code"),ka=d("Tokenizer.encode"),wa=d(":"),Vs=j(),p(nt.$$.fragment),Xs=j(),Mt=_("p"),qa=d("You can then check the type IDs attributed to each token is correct with"),Zs=j(),p(st.$$.fragment),eo=j(),ot=_("p"),ja=d("If you save your tokenizer with "),Bn=_("code"),va=d("Tokenizer.save"),za=d(", the post-processor will be saved along."),to=j(),ve=_("h3"),rt=_("a"),Nn=_("span"),p(Pt.$$.fragment),ba=j(),In=_("span"),ya=d("Encoding multiple sentences in a batch"),no=j(),at=_("p"),Ea=d(`To get the full speed of the \u{1F917} Tokenizers library, it\u2019s best to
process your texts by batches by using the
`),On=_("code"),Pa=d("Tokenizer.encode_batch"),Ta=d(" method:"),so=j(),p(lt.$$.fragment),oo=j(),it=_("p"),Sa=d("The output is then a list of "),Un=_("code"),Ha=d("Encoding"),xa=d(`
objects like the ones we saw before. You can process together as many
texts as you like, as long as it fits in memory.`),ro=j(),ut=_("p"),Ca=d(`To process a batch of sentences pairs, pass two lists to the
`),Kn=_("code"),Aa=d("Tokenizer.encode_batch"),Da=d(` method: the
list of sentences A and the list of sentences B:`),ao=j(),p(pt.$$.fragment),lo=j(),X=_("p"),La=d(`When encoding multiple sentences, you can automatically pad the outputs
to the longest sentence present by using
`),Wn=_("code"),Ba=d("Tokenizer.enable_padding"),Na=d(`, with the
`),Mn=_("code"),Ia=d("pad_token"),Oa=d(` and its ID (which we can
double-check the id for the padding token with
`),Fn=_("code"),Ua=d("Tokenizer.token_to_id"),Ka=d(" like before):"),io=j(),p(ct.$$.fragment),uo=j(),de=_("p"),Wa=d("We can set the "),Yn=_("code"),Ma=d("direction"),Fa=d(` of the padding
(defaults to the right) or a given `),Rn=_("code"),Ya=d("length"),Ra=d(` if we want to pad every sample to that specific number (here
we leave it unset to pad to the size of the longest text).`),po=j(),p(ft.$$.fragment),co=j(),$t=_("p"),Qa=d("In this case, the "),Qn=_("code"),Ga=d("attention mask"),Ja=d(` generated by the
tokenizer takes the padding into account:`),fo=j(),p(ht.$$.fragment),$o=j(),ze=_("h2"),mt=_("a"),Gn=_("span"),p(Tt.$$.fragment),Va=j(),Jn=_("span"),Xa=d("Pretrained"),ho=j(),p(dt.$$.fragment),this.h()},l(s){const u=ci('[data-svelte="svelte-1phssyn"]',document.head);t=k(u,"META",{name:!0,content:!0}),u.forEach(r),n=v(s),e=k(s,"H1",{class:!0});var St=w(e);o=k(St,"A",{id:!0,class:!0,href:!0});var Vn=w(o);i=k(Vn,"SPAN",{});var Xn=w(i);c(x.$$.fragment,Xn),Xn.forEach(r),Vn.forEach(r),Z=v(St),U=k(St,"SPAN",{});var Zn=w(U);C=g(Zn,"Quicktour"),Zn.forEach(r),St.forEach(r),D=v(s),I=k(s,"P",{});var es=w(I);re=g(es,`Let\u2019s have a quick look at the \u{1F917} Tokenizers library features. The
library provides an implementation of today\u2019s most used tokenizers that
is both easy to use and blazing fast.`),es.forEach(r),Pe=v(s),K=k(s,"H2",{class:!0});var Ht=w(K);B=k(Ht,"A",{id:!0,class:!0,href:!0});var ts=w(B);ae=k(ts,"SPAN",{});var ns=w(ae);c(L.$$.fragment,ns),ns.forEach(r),ts.forEach(r),F=v(Ht),ee=k(Ht,"SPAN",{});var ss=w(ee);te=g(ss,"Build a tokenizer from scratch"),ss.forEach(r),Ht.forEach(r),Te=v(s),W=k(s,"P",{});var xt=w(W);jt=g(xt,`To illustrate how fast the \u{1F917} Tokenizers library is, let\u2019s train a new
tokenizer on `),Y=k(xt,"A",{href:!0,rel:!0});var os=w(Y);le=g(os,"wikitext-103"),os.forEach(r),vt=g(xt,`
(516M of text) in just a few seconds. First things first, you will need
to download this dataset and unzip it with:`),xt.forEach(r),ge=v(s),c(O.$$.fragment,s),_e=v(s),A=k(s,"H3",{class:!0});var Ct=w(A);R=k(Ct,"A",{id:!0,class:!0,href:!0});var rs=w(R);ne=k(rs,"SPAN",{});var as=w(ne);c(se.$$.fragment,as),as.forEach(r),rs.forEach(r),zt=v(Ct),ie=k(Ct,"SPAN",{});var ls=w(ie);oe=g(ls,"Training the tokenizer"),ls.forEach(r),Ct.forEach(r),ke=v(s),z=k(s,"P",{});var At=w(z);T=g(At,`In this tour, we will build and train a Byte-Pair Encoding (BPE)
tokenizer. For more information about the different type of tokenizers,
check out this `),ue=k(At,"A",{href:!0,rel:!0});var is=w(ue);Dt=g(is,"guide"),is.forEach(r),Lt=g(At,` in
the \u{1F917} Transformers documentation. Here, training the tokenizer means it
will learn merge rules by:`),At.forEach(r),we=v(s),M=k(s,"UL",{});var be=w(M);pe=k(be,"LI",{});var us=w(pe);Bt=g(us,`Start with all the characters present in the training corpus as
tokens.`),us.forEach(r),Nt=v(be),Se=k(be,"LI",{});var ps=w(Se);It=g(ps,"Identify the most common pair of tokens and merge it into one token."),ps.forEach(r),He=v(be),xe=k(be,"LI",{});var cs=w(xe);xo=g(cs,`Repeat until the vocabulary (e.g., the number of tokens) has reached
the size we want.`),cs.forEach(r),be.forEach(r),$s=v(s),ce=k(s,"P",{});var ye=w(ce);Co=g(ye,"The main API of the library is the "),Jt=k(ye,"CODE",{});var fs=w(Jt);Ao=g(fs,"class"),fs.forEach(r),Do=v(ye),Vt=k(ye,"CODE",{});var el=w(Vt);Lo=g(el,"Tokenizer"),el.forEach(r),Bo=g(ye,`, here is how
we instantiate one with a BPE model:`),ye.forEach(r),hs=v(s),c(Ce.$$.fragment,s),ms=v(s),bt=k(s,"P",{});var Za=w(bt);No=g(Za,`To train our tokenizer on the wikitext files, we will need to
instantiate a [trainer]{.title-ref}, in this case a
`),Xt=k(Za,"CODE",{});var tl=w(Xt);Io=g(tl,"BpeTrainer"),tl.forEach(r),Za.forEach(r),ds=v(s),c(Ae.$$.fragment,s),gs=v(s),Q=k(s,"P",{});var gt=w(Q);Oo=g(gt,"We can set the training arguments like "),Zt=k(gt,"CODE",{});var nl=w(Zt);Uo=g(nl,"vocab_size"),nl.forEach(r),Ko=g(gt," or "),en=k(gt,"CODE",{});var sl=w(en);Wo=g(sl,"min_frequency"),sl.forEach(r),Mo=g(gt,` (here
left at their default values of 30,000 and 0) but the most important
part is to give the `),tn=k(gt,"CODE",{});var ol=w(tn);Fo=g(ol,"special_tokens"),ol.forEach(r),Yo=g(gt,` we
plan to use later on (they are not used at all during training) so that
they get inserted in the vocabulary.`),gt.forEach(r),_s=v(s),c(De.$$.fragment,s),ks=v(s),Le=k(s,"P",{});var go=w(Le);Ro=g(go,`We could train our tokenizer right now, but it wouldn\u2019t be optimal.
Without a pre-tokenizer that will split our inputs into words, we might
get tokens that overlap several words: for instance we could get an
`),nn=k(go,"CODE",{});var rl=w(nn);Qo=g(rl,'"it is"'),rl.forEach(r),Go=g(go,` token since those two words
often appear next to each other. Using a pre-tokenizer will ensure no
token is bigger than a word returned by the pre-tokenizer. Here we want
to train a subword BPE tokenizer, and we will use the easiest
pre-tokenizer possible by splitting on whitespace.`),go.forEach(r),ws=v(s),c(Be.$$.fragment,s),qs=v(s),Ne=k(s,"P",{});var _o=w(Ne);Jo=g(_o,"Now, we can just call the "),sn=k(_o,"CODE",{});var al=w(sn);Vo=g(al,"Tokenizer.train"),al.forEach(r),Xo=g(_o," method with any list of files we want to use:"),_o.forEach(r),js=v(s),c(Ie.$$.fragment,s),vs=v(s),Oe=k(s,"P",{});var ko=w(Oe);Zo=g(ko,`This should only take a few seconds to train our tokenizer on the full
wikitext dataset! To save the tokenizer in one file that contains all
its configuration and vocabulary, just use the
`),on=k(ko,"CODE",{});var ll=w(on);er=g(ll,"Tokenizer.save"),ll.forEach(r),tr=g(ko," method:"),ko.forEach(r),zs=v(s),c(Ue.$$.fragment,s),bs=v(s),fe=k(s,"P",{});var Ft=w(fe);nr=g(Ft,`and you can reload your tokenizer from that file with the
`),rn=k(Ft,"CODE",{});var il=w(rn);sr=g(il,"Tokenizer.from_file"),il.forEach(r),or=v(Ft),an=k(Ft,"CODE",{});var ul=w(an);rr=g(ul,"classmethod"),ul.forEach(r),ar=g(Ft,":"),Ft.forEach(r),ys=v(s),c(Ke.$$.fragment,s),Es=v(s),qe=k(s,"H3",{class:!0});var wo=w(qe);We=k(wo,"A",{id:!0,class:!0,href:!0});var pl=w(We);ln=k(pl,"SPAN",{});var cl=w(ln);c(yt.$$.fragment,cl),cl.forEach(r),pl.forEach(r),lr=v(wo),un=k(wo,"SPAN",{});var fl=w(un);ir=g(fl,"Using the tokenizer"),fl.forEach(r),wo.forEach(r),Ps=v(s),Me=k(s,"P",{});var qo=w(Me);ur=g(qo,`Now that we have trained a tokenizer, we can use it on any text we want
with the `),pn=k(qo,"CODE",{});var $l=w(pn);pr=g($l,"Tokenizer.encode"),$l.forEach(r),cr=g(qo," method:"),qo.forEach(r),Ts=v(s),c(Fe.$$.fragment,s),Ss=v(s),$e=k(s,"P",{});var Yt=w($e);fr=g(Yt,`This applied the full pipeline of the tokenizer on the text, returning
an `),cn=k(Yt,"CODE",{});var hl=w(cn);$r=g(hl,"Encoding"),hl.forEach(r),hr=g(Yt,` object. To learn more
about this pipeline, and how to apply (or customize) parts of it, check out `),fn=k(Yt,"CODE",{});var ml=w(fn);mr=g(ml,"this page <pipeline>"),ml.forEach(r),dr=g(Yt,"."),Yt.forEach(r),Hs=v(s),he=k(s,"P",{});var Rt=w(he);gr=g(Rt,"This "),$n=k(Rt,"CODE",{});var dl=w($n);_r=g(dl,"Encoding"),dl.forEach(r),kr=g(Rt,` object then has all the
attributes you need for your deep learning model (or other). The
`),hn=k(Rt,"CODE",{});var gl=w(hn);wr=g(gl,"tokens"),gl.forEach(r),qr=g(Rt,` attribute contains the
segmentation of your text in tokens:`),Rt.forEach(r),xs=v(s),c(Ye.$$.fragment,s),Cs=v(s),Re=k(s,"P",{});var jo=w(Re);jr=g(jo,"Similarly, the "),mn=k(jo,"CODE",{});var _l=w(mn);vr=g(_l,"ids"),_l.forEach(r),zr=g(jo,` attribute will
contain the index of each of those tokens in the tokenizer\u2019s
vocabulary:`),jo.forEach(r),As=v(s),c(Qe.$$.fragment,s),Ds=v(s),G=k(s,"P",{});var _t=w(G);br=g(_t,`An important feature of the \u{1F917} Tokenizers library is that it comes with
full alignment tracking, meaning you can always get the part of your
original sentence that corresponds to a given token. Those are stored in
the `),dn=k(_t,"CODE",{});var kl=w(dn);yr=g(kl,"offsets"),kl.forEach(r),Er=g(_t,` attribute of our
`),gn=k(_t,"CODE",{});var wl=w(gn);Pr=g(wl,"Encoding"),wl.forEach(r),Tr=g(_t,` object. For instance, let\u2019s
assume we would want to find back what caused the
`),_n=k(_t,"CODE",{});var ql=w(_n);Sr=g(ql,'"[UNK]"'),ql.forEach(r),Hr=g(_t,` token to appear, which is the
token at index 9 in the list, we can just ask for the offset at the
index:`),_t.forEach(r),Ls=v(s),c(Ge.$$.fragment,s),Bs=v(s),Ot=k(s,"P",{});var jl=w(Ot);xr=g(jl,`and those are the indices that correspond to the emoji in the original
sentence:`),jl.forEach(r),Ns=v(s),c(Je.$$.fragment,s),Is=v(s),je=k(s,"H3",{class:!0});var vo=w(je);Ve=k(vo,"A",{id:!0,class:!0,href:!0});var vl=w(Ve);kn=k(vl,"SPAN",{});var zl=w(kn);c(Et.$$.fragment,zl),zl.forEach(r),vl.forEach(r),Cr=v(vo),wn=k(vo,"SPAN",{});var bl=w(wn);Ar=g(bl,"Post-processing"),bl.forEach(r),vo.forEach(r),Os=v(s),J=k(s,"P",{});var kt=w(J);Dr=g(kt,`We might want our tokenizer to automatically add special tokens, like
`),qn=k(kt,"CODE",{});var yl=w(qn);Lr=g(yl,'"[CLS]"'),yl.forEach(r),Br=g(kt," or "),jn=k(kt,"CODE",{});var El=w(jn);Nr=g(El,'"[SEP]"'),El.forEach(r),Ir=g(kt,`. To do this, we use a post-processor.
`),vn=k(kt,"CODE",{});var Pl=w(vn);Or=g(Pl,"TemplateProcessing"),Pl.forEach(r),Ur=g(kt,` is the most
commonly used, you just have to specify a template for the processing of
single sentences and pairs of sentences, along with the special tokens
and their IDs.`),kt.forEach(r),Us=v(s),V=k(s,"P",{});var wt=w(V);Kr=g(wt,"When we built our tokenizer, we set "),zn=k(wt,"CODE",{});var Tl=w(zn);Wr=g(Tl,'"[CLS]"'),Tl.forEach(r),Mr=g(wt," and "),bn=k(wt,"CODE",{});var Sl=w(bn);Fr=g(Sl,'"[SEP]"'),Sl.forEach(r),Yr=g(wt,` in positions 1
and 2 of our list of special tokens, so this should be their IDs. To
double-check, we can use the `),yn=k(wt,"CODE",{});var Hl=w(yn);Rr=g(Hl,"Tokenizer.token_to_id"),Hl.forEach(r),Qr=g(wt," method:"),wt.forEach(r),Ks=v(s),c(Xe.$$.fragment,s),Ws=v(s),Ut=k(s,"P",{});var xl=w(Ut);Gr=g(xl,`Here is how we can set the post-processing to give us the traditional
BERT inputs:`),xl.forEach(r),Ms=v(s),c(Ze.$$.fragment,s),Fs=v(s),me=k(s,"P",{});var Qt=w(me);Jr=g(Qt,`Let\u2019s go over this snippet of code in more details. First we specify
the template for single sentences: those should have the form
`),En=k(Qt,"CODE",{});var Cl=w(En);Vr=g(Cl,'"[CLS] $A [SEP]"'),Cl.forEach(r),Xr=g(Qt,` where
`),Pn=k(Qt,"CODE",{});var Al=w(Pn);Zr=g(Al,"$A"),Al.forEach(r),ea=g(Qt," represents our sentence."),Qt.forEach(r),Ys=v(s),H=k(s,"P",{});var N=w(H);ta=g(N,`Then, we specify the template for sentence pairs, which should have the
form `),Tn=k(N,"CODE",{});var Dl=w(Tn);na=g(Dl,'"[CLS] $A [SEP] $B [SEP]"'),Dl.forEach(r),sa=g(N,` where
`),Sn=k(N,"CODE",{});var Ll=w(Sn);oa=g(Ll,"$A"),Ll.forEach(r),ra=g(N,` represents the first sentence and
`),Hn=k(N,"CODE",{});var Bl=w(Hn);aa=g(Bl,"$B"),Bl.forEach(r),la=g(N,` the second one. The
`),xn=k(N,"CODE",{});var Nl=w(xn);ia=g(Nl,":1"),Nl.forEach(r),ua=g(N," added in the template represent the "),Cn=k(N,"CODE",{});var Il=w(Cn);pa=g(Il,"type IDs"),Il.forEach(r),ca=g(N,` we want for each part of our input: it defaults
to 0 for everything (which is why we don\u2019t have
`),An=k(N,"CODE",{});var Ol=w(An);fa=g(Ol,"$A:0"),Ol.forEach(r),$a=g(N,`) and here we set it to 1 for the
tokens of the second sentence and the last `),Dn=k(N,"CODE",{});var Ul=w(Dn);ha=g(Ul,'"[SEP]"'),Ul.forEach(r),ma=g(N," token."),N.forEach(r),Rs=v(s),Kt=k(s,"P",{});var Kl=w(Kt);da=g(Kl,`Lastly, we specify the special tokens we used and their IDs in our
tokenizer\u2019s vocabulary.`),Kl.forEach(r),Qs=v(s),Wt=k(s,"P",{});var Wl=w(Wt);ga=g(Wl,`To check out this worked properly, let\u2019s try to encode the same
sentence as before:`),Wl.forEach(r),Gs=v(s),c(et.$$.fragment,s),Js=v(s),tt=k(s,"P",{});var zo=w(tt);_a=g(zo,`To check the results on a pair of sentences, we just pass the two
sentences to `),Ln=k(zo,"CODE",{});var Ml=w(Ln);ka=g(Ml,"Tokenizer.encode"),Ml.forEach(r),wa=g(zo,":"),zo.forEach(r),Vs=v(s),c(nt.$$.fragment,s),Xs=v(s),Mt=k(s,"P",{});var Fl=w(Mt);qa=g(Fl,"You can then check the type IDs attributed to each token is correct with"),Fl.forEach(r),Zs=v(s),c(st.$$.fragment,s),eo=v(s),ot=k(s,"P",{});var bo=w(ot);ja=g(bo,"If you save your tokenizer with "),Bn=k(bo,"CODE",{});var Yl=w(Bn);va=g(Yl,"Tokenizer.save"),Yl.forEach(r),za=g(bo,", the post-processor will be saved along."),bo.forEach(r),to=v(s),ve=k(s,"H3",{class:!0});var yo=w(ve);rt=k(yo,"A",{id:!0,class:!0,href:!0});var Rl=w(rt);Nn=k(Rl,"SPAN",{});var Ql=w(Nn);c(Pt.$$.fragment,Ql),Ql.forEach(r),Rl.forEach(r),ba=v(yo),In=k(yo,"SPAN",{});var Gl=w(In);ya=g(Gl,"Encoding multiple sentences in a batch"),Gl.forEach(r),yo.forEach(r),no=v(s),at=k(s,"P",{});var Eo=w(at);Ea=g(Eo,`To get the full speed of the \u{1F917} Tokenizers library, it\u2019s best to
process your texts by batches by using the
`),On=k(Eo,"CODE",{});var Jl=w(On);Pa=g(Jl,"Tokenizer.encode_batch"),Jl.forEach(r),Ta=g(Eo," method:"),Eo.forEach(r),so=v(s),c(lt.$$.fragment,s),oo=v(s),it=k(s,"P",{});var Po=w(it);Sa=g(Po,"The output is then a list of "),Un=k(Po,"CODE",{});var Vl=w(Un);Ha=g(Vl,"Encoding"),Vl.forEach(r),xa=g(Po,`
objects like the ones we saw before. You can process together as many
texts as you like, as long as it fits in memory.`),Po.forEach(r),ro=v(s),ut=k(s,"P",{});var To=w(ut);Ca=g(To,`To process a batch of sentences pairs, pass two lists to the
`),Kn=k(To,"CODE",{});var Xl=w(Kn);Aa=g(Xl,"Tokenizer.encode_batch"),Xl.forEach(r),Da=g(To,` method: the
list of sentences A and the list of sentences B:`),To.forEach(r),ao=v(s),c(pt.$$.fragment,s),lo=v(s),X=k(s,"P",{});var qt=w(X);La=g(qt,`When encoding multiple sentences, you can automatically pad the outputs
to the longest sentence present by using
`),Wn=k(qt,"CODE",{});var Zl=w(Wn);Ba=g(Zl,"Tokenizer.enable_padding"),Zl.forEach(r),Na=g(qt,`, with the
`),Mn=k(qt,"CODE",{});var ei=w(Mn);Ia=g(ei,"pad_token"),ei.forEach(r),Oa=g(qt,` and its ID (which we can
double-check the id for the padding token with
`),Fn=k(qt,"CODE",{});var ti=w(Fn);Ua=g(ti,"Tokenizer.token_to_id"),ti.forEach(r),Ka=g(qt," like before):"),qt.forEach(r),io=v(s),c(ct.$$.fragment,s),uo=v(s),de=k(s,"P",{});var Gt=w(de);Wa=g(Gt,"We can set the "),Yn=k(Gt,"CODE",{});var ni=w(Yn);Ma=g(ni,"direction"),ni.forEach(r),Fa=g(Gt,` of the padding
(defaults to the right) or a given `),Rn=k(Gt,"CODE",{});var si=w(Rn);Ya=g(si,"length"),si.forEach(r),Ra=g(Gt,` if we want to pad every sample to that specific number (here
we leave it unset to pad to the size of the longest text).`),Gt.forEach(r),po=v(s),c(ft.$$.fragment,s),co=v(s),$t=k(s,"P",{});var So=w($t);Qa=g(So,"In this case, the "),Qn=k(So,"CODE",{});var oi=w(Qn);Ga=g(oi,"attention mask"),oi.forEach(r),Ja=g(So,` generated by the
tokenizer takes the padding into account:`),So.forEach(r),fo=v(s),c(ht.$$.fragment,s),$o=v(s),ze=k(s,"H2",{class:!0});var Ho=w(ze);mt=k(Ho,"A",{id:!0,class:!0,href:!0});var ri=w(mt);Gn=k(ri,"SPAN",{});var ai=w(Gn);c(Tt.$$.fragment,ai),ai.forEach(r),ri.forEach(r),Va=v(Ho),Jn=k(Ho,"SPAN",{});var li=w(Jn);Xa=g(li,"Pretrained"),li.forEach(r),Ho.forEach(r),ho=v(s),c(dt.$$.fragment,s),this.h()},h(){P(t,"name","hf:doc:metadata"),P(t,"content",JSON.stringify(Lp)),P(o,"id","quicktour"),P(o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(o,"href","#quicktour"),P(e,"class","relative group"),P(B,"id","build-a-tokenizer-from-scratch"),P(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(B,"href","#build-a-tokenizer-from-scratch"),P(K,"class","relative group"),P(Y,"href","https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/"),P(Y,"rel","nofollow"),P(R,"id","training-the-tokenizer"),P(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(R,"href","#training-the-tokenizer"),P(A,"class","relative group"),P(ue,"href","https://huggingface.co/transformers/tokenizer_summary.html"),P(ue,"rel","nofollow"),P(We,"id","using-the-tokenizer"),P(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(We,"href","#using-the-tokenizer"),P(qe,"class","relative group"),P(Ve,"id","postprocessing"),P(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(Ve,"href","#postprocessing"),P(je,"class","relative group"),P(rt,"id","encoding-multiple-sentences-in-a-batch"),P(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(rt,"href","#encoding-multiple-sentences-in-a-batch"),P(ve,"class","relative group"),P(mt,"id","pretrained"),P(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),P(mt,"href","#pretrained"),P(ze,"class","relative group")},m(s,u){a(document.head,t),q(s,n,u),q(s,e,u),a(e,o),a(o,i),f(x,i,null),a(e,Z),a(e,U),a(U,C),q(s,D,u),q(s,I,u),a(I,re),q(s,Pe,u),q(s,K,u),a(K,B),a(B,ae),f(L,ae,null),a(K,F),a(K,ee),a(ee,te),q(s,Te,u),q(s,W,u),a(W,jt),a(W,Y),a(Y,le),a(W,vt),q(s,ge,u),f(O,s,u),q(s,_e,u),q(s,A,u),a(A,R),a(R,ne),f(se,ne,null),a(A,zt),a(A,ie),a(ie,oe),q(s,ke,u),q(s,z,u),a(z,T),a(z,ue),a(ue,Dt),a(z,Lt),q(s,we,u),q(s,M,u),a(M,pe),a(pe,Bt),a(M,Nt),a(M,Se),a(Se,It),a(M,He),a(M,xe),a(xe,xo),q(s,$s,u),q(s,ce,u),a(ce,Co),a(ce,Jt),a(Jt,Ao),a(ce,Do),a(ce,Vt),a(Vt,Lo),a(ce,Bo),q(s,hs,u),f(Ce,s,u),q(s,ms,u),q(s,bt,u),a(bt,No),a(bt,Xt),a(Xt,Io),q(s,ds,u),f(Ae,s,u),q(s,gs,u),q(s,Q,u),a(Q,Oo),a(Q,Zt),a(Zt,Uo),a(Q,Ko),a(Q,en),a(en,Wo),a(Q,Mo),a(Q,tn),a(tn,Fo),a(Q,Yo),q(s,_s,u),f(De,s,u),q(s,ks,u),q(s,Le,u),a(Le,Ro),a(Le,nn),a(nn,Qo),a(Le,Go),q(s,ws,u),f(Be,s,u),q(s,qs,u),q(s,Ne,u),a(Ne,Jo),a(Ne,sn),a(sn,Vo),a(Ne,Xo),q(s,js,u),f(Ie,s,u),q(s,vs,u),q(s,Oe,u),a(Oe,Zo),a(Oe,on),a(on,er),a(Oe,tr),q(s,zs,u),f(Ue,s,u),q(s,bs,u),q(s,fe,u),a(fe,nr),a(fe,rn),a(rn,sr),a(fe,or),a(fe,an),a(an,rr),a(fe,ar),q(s,ys,u),f(Ke,s,u),q(s,Es,u),q(s,qe,u),a(qe,We),a(We,ln),f(yt,ln,null),a(qe,lr),a(qe,un),a(un,ir),q(s,Ps,u),q(s,Me,u),a(Me,ur),a(Me,pn),a(pn,pr),a(Me,cr),q(s,Ts,u),f(Fe,s,u),q(s,Ss,u),q(s,$e,u),a($e,fr),a($e,cn),a(cn,$r),a($e,hr),a($e,fn),a(fn,mr),a($e,dr),q(s,Hs,u),q(s,he,u),a(he,gr),a(he,$n),a($n,_r),a(he,kr),a(he,hn),a(hn,wr),a(he,qr),q(s,xs,u),f(Ye,s,u),q(s,Cs,u),q(s,Re,u),a(Re,jr),a(Re,mn),a(mn,vr),a(Re,zr),q(s,As,u),f(Qe,s,u),q(s,Ds,u),q(s,G,u),a(G,br),a(G,dn),a(dn,yr),a(G,Er),a(G,gn),a(gn,Pr),a(G,Tr),a(G,_n),a(_n,Sr),a(G,Hr),q(s,Ls,u),f(Ge,s,u),q(s,Bs,u),q(s,Ot,u),a(Ot,xr),q(s,Ns,u),f(Je,s,u),q(s,Is,u),q(s,je,u),a(je,Ve),a(Ve,kn),f(Et,kn,null),a(je,Cr),a(je,wn),a(wn,Ar),q(s,Os,u),q(s,J,u),a(J,Dr),a(J,qn),a(qn,Lr),a(J,Br),a(J,jn),a(jn,Nr),a(J,Ir),a(J,vn),a(vn,Or),a(J,Ur),q(s,Us,u),q(s,V,u),a(V,Kr),a(V,zn),a(zn,Wr),a(V,Mr),a(V,bn),a(bn,Fr),a(V,Yr),a(V,yn),a(yn,Rr),a(V,Qr),q(s,Ks,u),f(Xe,s,u),q(s,Ws,u),q(s,Ut,u),a(Ut,Gr),q(s,Ms,u),f(Ze,s,u),q(s,Fs,u),q(s,me,u),a(me,Jr),a(me,En),a(En,Vr),a(me,Xr),a(me,Pn),a(Pn,Zr),a(me,ea),q(s,Ys,u),q(s,H,u),a(H,ta),a(H,Tn),a(Tn,na),a(H,sa),a(H,Sn),a(Sn,oa),a(H,ra),a(H,Hn),a(Hn,aa),a(H,la),a(H,xn),a(xn,ia),a(H,ua),a(H,Cn),a(Cn,pa),a(H,ca),a(H,An),a(An,fa),a(H,$a),a(H,Dn),a(Dn,ha),a(H,ma),q(s,Rs,u),q(s,Kt,u),a(Kt,da),q(s,Qs,u),q(s,Wt,u),a(Wt,ga),q(s,Gs,u),f(et,s,u),q(s,Js,u),q(s,tt,u),a(tt,_a),a(tt,Ln),a(Ln,ka),a(tt,wa),q(s,Vs,u),f(nt,s,u),q(s,Xs,u),q(s,Mt,u),a(Mt,qa),q(s,Zs,u),f(st,s,u),q(s,eo,u),q(s,ot,u),a(ot,ja),a(ot,Bn),a(Bn,va),a(ot,za),q(s,to,u),q(s,ve,u),a(ve,rt),a(rt,Nn),f(Pt,Nn,null),a(ve,ba),a(ve,In),a(In,ya),q(s,no,u),q(s,at,u),a(at,Ea),a(at,On),a(On,Pa),a(at,Ta),q(s,so,u),f(lt,s,u),q(s,oo,u),q(s,it,u),a(it,Sa),a(it,Un),a(Un,Ha),a(it,xa),q(s,ro,u),q(s,ut,u),a(ut,Ca),a(ut,Kn),a(Kn,Aa),a(ut,Da),q(s,ao,u),f(pt,s,u),q(s,lo,u),q(s,X,u),a(X,La),a(X,Wn),a(Wn,Ba),a(X,Na),a(X,Mn),a(Mn,Ia),a(X,Oa),a(X,Fn),a(Fn,Ua),a(X,Ka),q(s,io,u),f(ct,s,u),q(s,uo,u),q(s,de,u),a(de,Wa),a(de,Yn),a(Yn,Ma),a(de,Fa),a(de,Rn),a(Rn,Ya),a(de,Ra),q(s,po,u),f(ft,s,u),q(s,co,u),q(s,$t,u),a($t,Qa),a($t,Qn),a(Qn,Ga),a($t,Ja),q(s,fo,u),f(ht,s,u),q(s,$o,u),q(s,ze,u),a(ze,mt),a(mt,Gn),f(Tt,Gn,null),a(ze,Va),a(ze,Jn),a(Jn,Xa),q(s,ho,u),f(dt,s,u),mo=!0},p(s,[u]){const St={};u&2&&(St.$$scope={dirty:u,ctx:s}),Ce.$set(St);const Vn={};u&2&&(Vn.$$scope={dirty:u,ctx:s}),Ae.$set(Vn);const Xn={};u&2&&(Xn.$$scope={dirty:u,ctx:s}),De.$set(Xn);const Zn={};u&2&&(Zn.$$scope={dirty:u,ctx:s}),Be.$set(Zn);const es={};u&2&&(es.$$scope={dirty:u,ctx:s}),Ie.$set(es);const Ht={};u&2&&(Ht.$$scope={dirty:u,ctx:s}),Ue.$set(Ht);const ts={};u&2&&(ts.$$scope={dirty:u,ctx:s}),Ke.$set(ts);const ns={};u&2&&(ns.$$scope={dirty:u,ctx:s}),Fe.$set(ns);const ss={};u&2&&(ss.$$scope={dirty:u,ctx:s}),Ye.$set(ss);const xt={};u&2&&(xt.$$scope={dirty:u,ctx:s}),Qe.$set(xt);const os={};u&2&&(os.$$scope={dirty:u,ctx:s}),Ge.$set(os);const Ct={};u&2&&(Ct.$$scope={dirty:u,ctx:s}),Je.$set(Ct);const rs={};u&2&&(rs.$$scope={dirty:u,ctx:s}),Xe.$set(rs);const as={};u&2&&(as.$$scope={dirty:u,ctx:s}),Ze.$set(as);const ls={};u&2&&(ls.$$scope={dirty:u,ctx:s}),et.$set(ls);const At={};u&2&&(At.$$scope={dirty:u,ctx:s}),nt.$set(At);const is={};u&2&&(is.$$scope={dirty:u,ctx:s}),st.$set(is);const be={};u&2&&(be.$$scope={dirty:u,ctx:s}),lt.$set(be);const us={};u&2&&(us.$$scope={dirty:u,ctx:s}),pt.$set(us);const ps={};u&2&&(ps.$$scope={dirty:u,ctx:s}),ct.$set(ps);const cs={};u&2&&(cs.$$scope={dirty:u,ctx:s}),ft.$set(cs);const ye={};u&2&&(ye.$$scope={dirty:u,ctx:s}),ht.$set(ye);const fs={};u&2&&(fs.$$scope={dirty:u,ctx:s}),dt.$set(fs)},i(s){mo||($(x.$$.fragment,s),$(L.$$.fragment,s),$(O.$$.fragment,s),$(se.$$.fragment,s),$(Ce.$$.fragment,s),$(Ae.$$.fragment,s),$(De.$$.fragment,s),$(Be.$$.fragment,s),$(Ie.$$.fragment,s),$(Ue.$$.fragment,s),$(Ke.$$.fragment,s),$(yt.$$.fragment,s),$(Fe.$$.fragment,s),$(Ye.$$.fragment,s),$(Qe.$$.fragment,s),$(Ge.$$.fragment,s),$(Je.$$.fragment,s),$(Et.$$.fragment,s),$(Xe.$$.fragment,s),$(Ze.$$.fragment,s),$(et.$$.fragment,s),$(nt.$$.fragment,s),$(st.$$.fragment,s),$(Pt.$$.fragment,s),$(lt.$$.fragment,s),$(pt.$$.fragment,s),$(ct.$$.fragment,s),$(ft.$$.fragment,s),$(ht.$$.fragment,s),$(Tt.$$.fragment,s),$(dt.$$.fragment,s),mo=!0)},o(s){h(x.$$.fragment,s),h(L.$$.fragment,s),h(O.$$.fragment,s),h(se.$$.fragment,s),h(Ce.$$.fragment,s),h(Ae.$$.fragment,s),h(De.$$.fragment,s),h(Be.$$.fragment,s),h(Ie.$$.fragment,s),h(Ue.$$.fragment,s),h(Ke.$$.fragment,s),h(yt.$$.fragment,s),h(Fe.$$.fragment,s),h(Ye.$$.fragment,s),h(Qe.$$.fragment,s),h(Ge.$$.fragment,s),h(Je.$$.fragment,s),h(Et.$$.fragment,s),h(Xe.$$.fragment,s),h(Ze.$$.fragment,s),h(et.$$.fragment,s),h(nt.$$.fragment,s),h(st.$$.fragment,s),h(Pt.$$.fragment,s),h(lt.$$.fragment,s),h(pt.$$.fragment,s),h(ct.$$.fragment,s),h(ft.$$.fragment,s),h(ht.$$.fragment,s),h(Tt.$$.fragment,s),h(dt.$$.fragment,s),mo=!1},d(s){r(t),s&&r(n),s&&r(e),m(x),s&&r(D),s&&r(I),s&&r(Pe),s&&r(K),m(L),s&&r(Te),s&&r(W),s&&r(ge),m(O,s),s&&r(_e),s&&r(A),m(se),s&&r(ke),s&&r(z),s&&r(we),s&&r(M),s&&r($s),s&&r(ce),s&&r(hs),m(Ce,s),s&&r(ms),s&&r(bt),s&&r(ds),m(Ae,s),s&&r(gs),s&&r(Q),s&&r(_s),m(De,s),s&&r(ks),s&&r(Le),s&&r(ws),m(Be,s),s&&r(qs),s&&r(Ne),s&&r(js),m(Ie,s),s&&r(vs),s&&r(Oe),s&&r(zs),m(Ue,s),s&&r(bs),s&&r(fe),s&&r(ys),m(Ke,s),s&&r(Es),s&&r(qe),m(yt),s&&r(Ps),s&&r(Me),s&&r(Ts),m(Fe,s),s&&r(Ss),s&&r($e),s&&r(Hs),s&&r(he),s&&r(xs),m(Ye,s),s&&r(Cs),s&&r(Re),s&&r(As),m(Qe,s),s&&r(Ds),s&&r(G),s&&r(Ls),m(Ge,s),s&&r(Bs),s&&r(Ot),s&&r(Ns),m(Je,s),s&&r(Is),s&&r(je),m(Et),s&&r(Os),s&&r(J),s&&r(Us),s&&r(V),s&&r(Ks),m(Xe,s),s&&r(Ws),s&&r(Ut),s&&r(Ms),m(Ze,s),s&&r(Fs),s&&r(me),s&&r(Ys),s&&r(H),s&&r(Rs),s&&r(Kt),s&&r(Qs),s&&r(Wt),s&&r(Gs),m(et,s),s&&r(Js),s&&r(tt),s&&r(Vs),m(nt,s),s&&r(Xs),s&&r(Mt),s&&r(Zs),m(st,s),s&&r(eo),s&&r(ot),s&&r(to),s&&r(ve),m(Pt),s&&r(no),s&&r(at),s&&r(so),m(lt,s),s&&r(oo),s&&r(it),s&&r(ro),s&&r(ut),s&&r(ao),m(pt,s),s&&r(lo),s&&r(X),s&&r(io),m(ct,s),s&&r(uo),s&&r(de),s&&r(po),m(ft,s),s&&r(co),s&&r($t),s&&r(fo),m(ht,s),s&&r($o),s&&r(ze),m(Tt),s&&r(ho),m(dt,s)}}}const Lp={local:"quicktour",sections:[{local:"build-a-tokenizer-from-scratch",sections:[{local:"training-the-tokenizer",title:"Training the tokenizer"},{local:"using-the-tokenizer",title:"Using the tokenizer"},{local:"postprocessing",title:"Post-processing"},{local:"encoding-multiple-sentences-in-a-batch",title:"Encoding multiple sentences in a batch"}],title:"Build a tokenizer from scratch"},{local:"pretrained",sections:[{local:"using-a-pretrained-tokenizer",title:"Using a pretrained tokenizer"},{local:"importing-a-pretrained-tokenizer-from-legacy-vocabulary-files",title:"Importing a pretrained tokenizer from legacy vocabulary files"}],title:"Pretrained"}],title:"Quicktour"};function Bp(l){return fi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Wp extends ii{constructor(t){super();ui(this,t,Bp,Dp,pi,{})}}export{Wp as default,Lp as metadata};
