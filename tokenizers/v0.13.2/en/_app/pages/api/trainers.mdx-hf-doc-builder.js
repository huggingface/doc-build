import{S as Fe,i as Ke,s as Qe,e as s,k as g,w as T,t as z,M as Xe,c as l,d as t,m as _,a as c,x,h as y,b as d,G as o,g as f,y as P,q as A,o as E,B as W,v as Ye,L as Ze}from"../../chunks/vendor-hf-doc-builder.js";import{D as ve}from"../../chunks/Docstring-hf-doc-builder.js";import{I as oe}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{T as er,M as Le}from"../../chunks/TokenizersLanguageContent-hf-doc-builder.js";function rr(L){let n,i,e,a,h,u,k,b,w,U,B,N,p,$,v,q,V,S,$e,Q,ke,ae,I,J,be,X,we,ie,D,C,Y,O,Te,Z,ze,se,M,j,xe,ee,ye,le,H,G,re,F,Pe,ne,Ae,ce,R,K,Ee,te,We,de;return a=new oe({}),U=new ve({props:{name:"class tokenizers.trainers.BpeTrainer",anchor:"tokenizers.trainers.BpeTrainer",parameters:"",parametersDescription:[{anchor:"tokenizers.trainers.BpeTrainer.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The size of the final vocabulary, including all tokens and alphabet.`,name:"vocab_size"},{anchor:"tokenizers.trainers.BpeTrainer.min_frequency",description:`<strong>min_frequency</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The minimum frequency a pair should have in order to be merged.`,name:"min_frequency"},{anchor:"tokenizers.trainers.BpeTrainer.show_progress",description:`<strong>show_progress</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to show progress bars while training.`,name:"show_progress"},{anchor:"tokenizers.trainers.BpeTrainer.special_tokens",description:`<strong>special_tokens</strong> (<code>List[Union[str, AddedToken]]</code>, <em>optional</em>) &#x2014;
A list of special tokens the model should know of.`,name:"special_tokens"},{anchor:"tokenizers.trainers.BpeTrainer.limit_alphabet",description:`<strong>limit_alphabet</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum different characters to keep in the alphabet.`,name:"limit_alphabet"},{anchor:"tokenizers.trainers.BpeTrainer.initial_alphabet",description:`<strong>initial_alphabet</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of characters to include in the initial alphabet, even
if not seen in the training dataset.
If the strings contain more than one character, only the first one
is kept.`,name:"initial_alphabet"},{anchor:"tokenizers.trainers.BpeTrainer.continuing_subword_prefix",description:`<strong>continuing_subword_prefix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A prefix to be used for every subword that is not a beginning-of-word.`,name:"continuing_subword_prefix"},{anchor:"tokenizers.trainers.BpeTrainer.end_of_word_suffix",description:`<strong>end_of_word_suffix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A suffix to be used for every subword that is a end-of-word.`,name:"end_of_word_suffix"}]}}),S=new oe({}),J=new ve({props:{name:"class tokenizers.trainers.UnigramTrainer",anchor:"tokenizers.trainers.UnigramTrainer",parameters:[{name:"vocab_size",val:" = 8000"},{name:"show_progress",val:" = True"},{name:"special_tokens",val:" = []"},{name:"shrinking_factor",val:" = 0.75"},{name:"unk_token",val:" = None"},{name:"max_piece_length",val:" = 16"},{name:"n_sub_iterations",val:" = 2"}],parametersDescription:[{anchor:"tokenizers.trainers.UnigramTrainer.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>) &#x2014;
The size of the final vocabulary, including all tokens and alphabet.`,name:"vocab_size"},{anchor:"tokenizers.trainers.UnigramTrainer.show_progress",description:`<strong>show_progress</strong> (<code>bool</code>) &#x2014;
Whether to show progress bars while training.`,name:"show_progress"},{anchor:"tokenizers.trainers.UnigramTrainer.special_tokens",description:`<strong>special_tokens</strong> (<code>List[Union[str, AddedToken]]</code>) &#x2014;
A list of special tokens the model should know of.`,name:"special_tokens"},{anchor:"tokenizers.trainers.UnigramTrainer.initial_alphabet",description:`<strong>initial_alphabet</strong> (<code>List[str]</code>) &#x2014;
A list of characters to include in the initial alphabet, even
if not seen in the training dataset.
If the strings contain more than one character, only the first one
is kept.`,name:"initial_alphabet"},{anchor:"tokenizers.trainers.UnigramTrainer.shrinking_factor",description:`<strong>shrinking_factor</strong> (<code>float</code>) &#x2014;
The shrinking factor used at each step of the training to prune the
vocabulary.`,name:"shrinking_factor"},{anchor:"tokenizers.trainers.UnigramTrainer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>) &#x2014;
The token used for out-of-vocabulary tokens.`,name:"unk_token"},{anchor:"tokenizers.trainers.UnigramTrainer.max_piece_length",description:`<strong>max_piece_length</strong> (<code>int</code>) &#x2014;
The maximum length of a given token.`,name:"max_piece_length"},{anchor:"tokenizers.trainers.UnigramTrainer.n_sub_iterations",description:`<strong>n_sub_iterations</strong> (<code>int</code>) &#x2014;
The number of iterations of the EM algorithm to perform before
pruning the vocabulary.`,name:"n_sub_iterations"}]}}),O=new oe({}),j=new ve({props:{name:"class tokenizers.trainers.WordLevelTrainer",anchor:"tokenizers.trainers.WordLevelTrainer",parameters:"",parametersDescription:[{anchor:"tokenizers.trainers.WordLevelTrainer.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The size of the final vocabulary, including all tokens and alphabet.`,name:"vocab_size"},{anchor:"tokenizers.trainers.WordLevelTrainer.min_frequency",description:`<strong>min_frequency</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The minimum frequency a pair should have in order to be merged.`,name:"min_frequency"},{anchor:"tokenizers.trainers.WordLevelTrainer.show_progress",description:`<strong>show_progress</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to show progress bars while training.`,name:"show_progress"},{anchor:"tokenizers.trainers.WordLevelTrainer.special_tokens",description:`<strong>special_tokens</strong> (<code>List[Union[str, AddedToken]]</code>) &#x2014;
A list of special tokens the model should know of.`,name:"special_tokens"}]}}),F=new oe({}),K=new ve({props:{name:"class tokenizers.trainers.WordPieceTrainer",anchor:"tokenizers.trainers.WordPieceTrainer",parameters:[{name:"vocab_size",val:" = 30000"},{name:"min_frequency",val:" = 0"},{name:"show_progress",val:" = True"},{name:"special_tokens",val:" = []"},{name:"limit_alphabet",val:" = None"},{name:"initial_alphabet",val:" = []"},{name:"continuing_subword_prefix",val:" = '##'"},{name:"end_of_word_suffix",val:" = None"}],parametersDescription:[{anchor:"tokenizers.trainers.WordPieceTrainer.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The size of the final vocabulary, including all tokens and alphabet.`,name:"vocab_size"},{anchor:"tokenizers.trainers.WordPieceTrainer.min_frequency",description:`<strong>min_frequency</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The minimum frequency a pair should have in order to be merged.`,name:"min_frequency"},{anchor:"tokenizers.trainers.WordPieceTrainer.show_progress",description:`<strong>show_progress</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to show progress bars while training.`,name:"show_progress"},{anchor:"tokenizers.trainers.WordPieceTrainer.special_tokens",description:`<strong>special_tokens</strong> (<code>List[Union[str, AddedToken]]</code>, <em>optional</em>) &#x2014;
A list of special tokens the model should know of.`,name:"special_tokens"},{anchor:"tokenizers.trainers.WordPieceTrainer.limit_alphabet",description:`<strong>limit_alphabet</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum different characters to keep in the alphabet.`,name:"limit_alphabet"},{anchor:"tokenizers.trainers.WordPieceTrainer.initial_alphabet",description:`<strong>initial_alphabet</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of characters to include in the initial alphabet, even
if not seen in the training dataset.
If the strings contain more than one character, only the first one
is kept.`,name:"initial_alphabet"},{anchor:"tokenizers.trainers.WordPieceTrainer.continuing_subword_prefix",description:`<strong>continuing_subword_prefix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A prefix to be used for every subword that is not a beginning-of-word.`,name:"continuing_subword_prefix"},{anchor:"tokenizers.trainers.WordPieceTrainer.end_of_word_suffix",description:`<strong>end_of_word_suffix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A suffix to be used for every subword that is a end-of-word.`,name:"end_of_word_suffix"}]}}),{c(){n=s("h2"),i=s("a"),e=s("span"),T(a.$$.fragment),h=g(),u=s("span"),k=z("BpeTrainer"),b=g(),w=s("div"),T(U.$$.fragment),B=g(),N=s("p"),p=z("Trainer capable of training a BPE model"),$=g(),v=s("h2"),q=s("a"),V=s("span"),T(S.$$.fragment),$e=g(),Q=s("span"),ke=z("UnigramTrainer"),ae=g(),I=s("div"),T(J.$$.fragment),be=g(),X=s("p"),we=z("Trainer capable of training a Unigram model"),ie=g(),D=s("h2"),C=s("a"),Y=s("span"),T(O.$$.fragment),Te=g(),Z=s("span"),ze=z("WordLevelTrainer"),se=g(),M=s("div"),T(j.$$.fragment),xe=g(),ee=s("p"),ye=z("Trainer capable of training a WorldLevel model"),le=g(),H=s("h2"),G=s("a"),re=s("span"),T(F.$$.fragment),Pe=g(),ne=s("span"),Ae=z("WordPieceTrainer"),ce=g(),R=s("div"),T(K.$$.fragment),Ee=g(),te=s("p"),We=z("Trainer capable of training a WordPiece model"),this.h()},l(r){n=l(r,"H2",{class:!0});var m=c(n);i=l(m,"A",{id:!0,class:!0,href:!0});var Ue=c(i);e=l(Ue,"SPAN",{});var Be=c(e);x(a.$$.fragment,Be),Be.forEach(t),Ue.forEach(t),h=_(m),u=l(m,"SPAN",{});var qe=c(u);k=y(qe,"BpeTrainer"),qe.forEach(t),m.forEach(t),b=_(r),w=l(r,"DIV",{class:!0});var pe=c(w);x(U.$$.fragment,pe),B=_(pe),N=l(pe,"P",{});var Ne=c(N);p=y(Ne,"Trainer capable of training a BPE model"),Ne.forEach(t),pe.forEach(t),$=_(r),v=l(r,"H2",{class:!0});var he=c(v);q=l(he,"A",{id:!0,class:!0,href:!0});var Se=c(q);V=l(Se,"SPAN",{});var Ie=c(V);x(S.$$.fragment,Ie),Ie.forEach(t),Se.forEach(t),$e=_(he),Q=l(he,"SPAN",{});var De=c(Q);ke=y(De,"UnigramTrainer"),De.forEach(t),he.forEach(t),ae=_(r),I=l(r,"DIV",{class:!0});var me=c(I);x(J.$$.fragment,me),be=_(me),X=l(me,"P",{});var Me=c(X);we=y(Me,"Trainer capable of training a Unigram model"),Me.forEach(t),me.forEach(t),ie=_(r),D=l(r,"H2",{class:!0});var fe=c(D);C=l(fe,"A",{id:!0,class:!0,href:!0});var He=c(C);Y=l(He,"SPAN",{});var Re=c(Y);x(O.$$.fragment,Re),Re.forEach(t),He.forEach(t),Te=_(fe),Z=l(fe,"SPAN",{});var Ve=c(Z);ze=y(Ve,"WordLevelTrainer"),Ve.forEach(t),fe.forEach(t),se=_(r),M=l(r,"DIV",{class:!0});var ue=c(M);x(j.$$.fragment,ue),xe=_(ue),ee=l(ue,"P",{});var Ce=c(ee);ye=y(Ce,"Trainer capable of training a WorldLevel model"),Ce.forEach(t),ue.forEach(t),le=_(r),H=l(r,"H2",{class:!0});var ge=c(H);G=l(ge,"A",{id:!0,class:!0,href:!0});var Ge=c(G);re=l(Ge,"SPAN",{});var Je=c(re);x(F.$$.fragment,Je),Je.forEach(t),Ge.forEach(t),Pe=_(ge),ne=l(ge,"SPAN",{});var Oe=c(ne);Ae=y(Oe,"WordPieceTrainer"),Oe.forEach(t),ge.forEach(t),ce=_(r),R=l(r,"DIV",{class:!0});var _e=c(R);x(K.$$.fragment,_e),Ee=_(_e),te=l(_e,"P",{});var je=c(te);We=y(je,"Trainer capable of training a WordPiece model"),je.forEach(t),_e.forEach(t),this.h()},h(){d(i,"id","tokenizers.trainers.BpeTrainer"),d(i,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(i,"href","#tokenizers.trainers.BpeTrainer"),d(n,"class","relative group"),d(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"id","tokenizers.trainers.UnigramTrainer"),d(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(q,"href","#tokenizers.trainers.UnigramTrainer"),d(v,"class","relative group"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"id","tokenizers.trainers.WordLevelTrainer"),d(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(C,"href","#tokenizers.trainers.WordLevelTrainer"),d(D,"class","relative group"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(G,"id","tokenizers.trainers.WordPieceTrainer"),d(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(G,"href","#tokenizers.trainers.WordPieceTrainer"),d(H,"class","relative group"),d(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,m){f(r,n,m),o(n,i),o(i,e),P(a,e,null),o(n,h),o(n,u),o(u,k),f(r,b,m),f(r,w,m),P(U,w,null),o(w,B),o(w,N),o(N,p),f(r,$,m),f(r,v,m),o(v,q),o(q,V),P(S,V,null),o(v,$e),o(v,Q),o(Q,ke),f(r,ae,m),f(r,I,m),P(J,I,null),o(I,be),o(I,X),o(X,we),f(r,ie,m),f(r,D,m),o(D,C),o(C,Y),P(O,Y,null),o(D,Te),o(D,Z),o(Z,ze),f(r,se,m),f(r,M,m),P(j,M,null),o(M,xe),o(M,ee),o(ee,ye),f(r,le,m),f(r,H,m),o(H,G),o(G,re),P(F,re,null),o(H,Pe),o(H,ne),o(ne,Ae),f(r,ce,m),f(r,R,m),P(K,R,null),o(R,Ee),o(R,te),o(te,We),de=!0},p:Ze,i(r){de||(A(a.$$.fragment,r),A(U.$$.fragment,r),A(S.$$.fragment,r),A(J.$$.fragment,r),A(O.$$.fragment,r),A(j.$$.fragment,r),A(F.$$.fragment,r),A(K.$$.fragment,r),de=!0)},o(r){E(a.$$.fragment,r),E(U.$$.fragment,r),E(S.$$.fragment,r),E(J.$$.fragment,r),E(O.$$.fragment,r),E(j.$$.fragment,r),E(F.$$.fragment,r),E(K.$$.fragment,r),de=!1},d(r){r&&t(n),W(a),r&&t(b),r&&t(w),W(U),r&&t($),r&&t(v),W(S),r&&t(ae),r&&t(I),W(J),r&&t(ie),r&&t(D),W(O),r&&t(se),r&&t(M),W(j),r&&t(le),r&&t(H),W(F),r&&t(ce),r&&t(R),W(K)}}}function nr(L){let n,i;return n=new Le({props:{$$slots:{default:[rr]},$$scope:{ctx:L}}}),{c(){T(n.$$.fragment)},l(e){x(n.$$.fragment,e)},m(e,a){P(n,e,a),i=!0},p(e,a){const h={};a&2&&(h.$$scope={dirty:a,ctx:e}),n.$set(h)},i(e){i||(A(n.$$.fragment,e),i=!0)},o(e){E(n.$$.fragment,e),i=!1},d(e){W(n,e)}}}function tr(L){let n,i,e,a,h;return{c(){n=s("p"),i=z("The Rust API Reference is available directly on the "),e=s("a"),a=z("Docs.rs"),h=z(" website."),this.h()},l(u){n=l(u,"P",{});var k=c(n);i=y(k,"The Rust API Reference is available directly on the "),e=l(k,"A",{href:!0,rel:!0});var b=c(e);a=y(b,"Docs.rs"),b.forEach(t),h=y(k," website."),k.forEach(t),this.h()},h(){d(e,"href","https://docs.rs/tokenizers/latest/tokenizers/"),d(e,"rel","nofollow")},m(u,k){f(u,n,k),o(n,i),o(n,e),o(e,a),o(n,h)},d(u){u&&t(n)}}}function or(L){let n,i;return n=new Le({props:{$$slots:{default:[tr]},$$scope:{ctx:L}}}),{c(){T(n.$$.fragment)},l(e){x(n.$$.fragment,e)},m(e,a){P(n,e,a),i=!0},p(e,a){const h={};a&2&&(h.$$scope={dirty:a,ctx:e}),n.$set(h)},i(e){i||(A(n.$$.fragment,e),i=!0)},o(e){E(n.$$.fragment,e),i=!1},d(e){W(n,e)}}}function ar(L){let n,i;return{c(){n=s("p"),i=z("The node API has not been documented yet.")},l(e){n=l(e,"P",{});var a=c(n);i=y(a,"The node API has not been documented yet."),a.forEach(t)},m(e,a){f(e,n,a),o(n,i)},d(e){e&&t(n)}}}function ir(L){let n,i;return n=new Le({props:{$$slots:{default:[ar]},$$scope:{ctx:L}}}),{c(){T(n.$$.fragment)},l(e){x(n.$$.fragment,e)},m(e,a){P(n,e,a),i=!0},p(e,a){const h={};a&2&&(h.$$scope={dirty:a,ctx:e}),n.$set(h)},i(e){i||(A(n.$$.fragment,e),i=!0)},o(e){E(n.$$.fragment,e),i=!1},d(e){W(n,e)}}}function sr(L){let n,i,e,a,h,u,k,b,w,U,B,N;return u=new oe({}),B=new er({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ir],rust:[or],python:[nr]},$$scope:{ctx:L}}}),{c(){n=s("meta"),i=g(),e=s("h1"),a=s("a"),h=s("span"),T(u.$$.fragment),k=g(),b=s("span"),w=z("Trainers"),U=g(),T(B.$$.fragment),this.h()},l(p){const $=Xe('[data-svelte="svelte-1phssyn"]',document.head);n=l($,"META",{name:!0,content:!0}),$.forEach(t),i=_(p),e=l(p,"H1",{class:!0});var v=c(e);a=l(v,"A",{id:!0,class:!0,href:!0});var q=c(a);h=l(q,"SPAN",{});var V=c(h);x(u.$$.fragment,V),V.forEach(t),q.forEach(t),k=_(v),b=l(v,"SPAN",{});var S=c(b);w=y(S,"Trainers"),S.forEach(t),v.forEach(t),U=_(p),x(B.$$.fragment,p),this.h()},h(){d(n,"name","hf:doc:metadata"),d(n,"content",JSON.stringify(lr)),d(a,"id","trainers"),d(a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(a,"href","#trainers"),d(e,"class","relative group")},m(p,$){o(document.head,n),f(p,i,$),f(p,e,$),o(e,a),o(a,h),P(u,h,null),o(e,k),o(e,b),o(b,w),f(p,U,$),P(B,p,$),N=!0},p(p,[$]){const v={};$&2&&(v.$$scope={dirty:$,ctx:p}),B.$set(v)},i(p){N||(A(u.$$.fragment,p),A(B.$$.fragment,p),N=!0)},o(p){E(u.$$.fragment,p),E(B.$$.fragment,p),N=!1},d(p){t(n),p&&t(i),p&&t(e),W(u),p&&t(U),W(B,p)}}}const lr={local:"trainers",sections:[{local:"tokenizers.trainers.BpeTrainer",title:"BpeTrainer"},{local:"tokenizers.trainers.UnigramTrainer",title:"UnigramTrainer"},{local:"tokenizers.trainers.WordLevelTrainer",title:"WordLevelTrainer"},{local:"tokenizers.trainers.WordPieceTrainer",title:"WordPieceTrainer"}],title:"Trainers"};function cr(L){return Ye(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fr extends Fe{constructor(n){super();Ke(this,n,cr,sr,Qe,{})}}export{fr as default,lr as metadata};
