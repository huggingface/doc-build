import{S as qn,i as On,s as Rn,e as r,k as h,w,t as s,M as Nn,c as n,d as t,m as f,a,x as y,h as o,b as _,G as e,g as P,y as b,q as T,o as L,B as S,v as jn,L as Bn}from"../../chunks/vendor-hf-doc-builder.js";import{D as ds}from"../../chunks/Docstring-hf-doc-builder.js";import{C as An}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Rt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{T as Wn,M as Ir}from"../../chunks/TokenizersLanguageContent-hf-doc-builder.js";import{E as zn}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Mn(C){let l,u,i,d,g;return d=new An({props:{code:`[CLS]   ...   [SEP]   ...   [SEP]
0      0      0      1      1`,highlighted:`[CLS]   ...   [SEP]   ...   [SEP]
<span class="hljs-number">0</span>      <span class="hljs-number">0</span>      <span class="hljs-number">0</span>      <span class="hljs-number">1</span>      <span class="hljs-number">1</span>`}}),{c(){l=r("p"),u=s("With the type ids as following:"),i=h(),w(d.$$.fragment)},l(p){l=n(p,"P",{});var $=a(l);u=o($,"With the type ids as following:"),$.forEach(t),i=f(p),y(d.$$.fragment,p)},m(p,$){P(p,l,$),e(l,u),P(p,i,$),b(d,p,$),g=!0},p:Bn,i(p){g||(T(d.$$.fragment,p),g=!0)},o(p){L(d.$$.fragment,p),g=!1},d(p){p&&t(l),p&&t(i),S(d,p)}}}function Un(C){let l,u,i,d,g;return d=new An({props:{code:`TemplateProcessing(
    single="[CLS] $0 [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[("[CLS]", 1), ("[SEP]", 0)],
)`,highlighted:`TemplateProcessing(
    single=<span class="hljs-string">&quot;[CLS] $0 [SEP]&quot;</span>,
    pair=<span class="hljs-string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,
    special_tokens=[(<span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-number">1</span>), (<span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-number">0</span>)],
)`}}),{c(){l=r("p"),u=s("You can achieve such behavior using a TemplateProcessing:"),i=h(),w(d.$$.fragment)},l(p){l=n(p,"P",{});var $=a(l);u=o($,"You can achieve such behavior using a TemplateProcessing:"),$.forEach(t),i=f(p),y(d.$$.fragment,p)},m(p,$){P(p,l,$),e(l,u),P(p,i,$),b(d,p,$),g=!0},p:Bn,i(p){g||(T(d.$$.fragment,p),g=!0)},o(p){L(d.$$.fragment,p),g=!1},d(p){p&&t(l),p&&t(i),S(d,p)}}}function Hn(C){let l,u,i,d,g,p,$,B,x,q,O,W,E,z,I,Z,Ce,xe,Me,ps,Nt,H,ee,Ue,le,hs,He,fs,jt,N,ie,us,Ve,ms,gs,Ye,vs,Wt,V,te,Ge,ce,$s,Je,_s,Mt,A,de,Es,Fe,ks,Ps,pe,Ke,ws,ys,Qe,bs,Ts,Ie,Ls,Xe,Ss,Ut,Y,se,Ze,he,Cs,et,xs,Ht,m,fe,Is,tt,Ds,zs,j,Bs,st,As,qs,ot,Os,Rs,rt,Ns,js,Ws,ue,De,Ms,nt,Us,Hs,ze,Vs,at,Ys,Gs,oe,Js,re,Fs,me,Ks,lt,Qs,Xs,Zs,G,M,eo,it,to,so,ct,oo,ro,dt,no,ao,D,lo,pt,io,co,ht,po,ho,ft,fo,uo,ut,mo,go,mt,vo,$o,_o,J,Eo,gt,ko,Po,vt,wo,yo,bo,ge,To,$t,Lo,So,Co,U,_t,xo,Io,Et,Do,zo,kt,Bo,Ao,qo,Pt,Oo,Ro,F,No,wt,jo,Wo,yt,Mo,Uo,Ho,ve,$e,Vo,bt,Yo,Go,Jo,_e,Fo,Tt,Ko,Qo,Xo,Ee,Zo,Lt,er,tr,sr,ke,St,Pe,or,Ct,rr,nr,ar,K,we,lr,xt,ir,cr,dr,Q,ye,pr,It,hr,fr,ur,be,mr,Dt,gr,vr,$r,Te,_r,zt,Er,kr,Pr,X,wr,Bt,yr,br,At,Tr,Lr,Vt;return d=new Rt({}),q=new ds({props:{name:"class tokenizers.processors.BertProcessing",anchor:"tokenizers.processors.BertProcessing",parameters:[{name:"sep",val:""},{name:"cls",val:""}],parametersDescription:[{anchor:"tokenizers.processors.BertProcessing.sep",description:`<strong>sep</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the SEP token, and its id`,name:"sep"},{anchor:"tokenizers.processors.BertProcessing.cls",description:`<strong>cls</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the CLS token, and its id`,name:"cls"}]}}),le=new Rt({}),ie=new ds({props:{name:"class tokenizers.processors.ByteLevel",anchor:"tokenizers.processors.ByteLevel",parameters:[{name:"trim_offsets",val:" = True"}],parametersDescription:[{anchor:"tokenizers.processors.ByteLevel.trim_offsets",description:`<strong>trim_offsets</strong> (<code>bool</code>) &#x2014;
Whether to trim the whitespaces from the produced offsets.`,name:"trim_offsets"}]}}),ce=new Rt({}),de=new ds({props:{name:"class tokenizers.processors.RobertaProcessing",anchor:"tokenizers.processors.RobertaProcessing",parameters:[{name:"sep",val:""},{name:"cls",val:""},{name:"trim_offsets",val:" = True"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.processors.RobertaProcessing.sep",description:`<strong>sep</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the SEP token, and its id`,name:"sep"},{anchor:"tokenizers.processors.RobertaProcessing.cls",description:`<strong>cls</strong> (<code>Tuple[str, int]</code>) &#x2014;
A tuple with the string representation of the CLS token, and its id`,name:"cls"},{anchor:"tokenizers.processors.RobertaProcessing.trim_offsets",description:`<strong>trim_offsets</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to trim the whitespaces from the produced offsets.`,name:"trim_offsets"},{anchor:"tokenizers.processors.RobertaProcessing.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the add_prefix_space option was enabled during pre-tokenization. This
is relevant because it defines the way the offsets are trimmed out.`,name:"add_prefix_space"}]}}),he=new Rt({}),fe=new ds({props:{name:"class tokenizers.processors.TemplateProcessing",anchor:"tokenizers.processors.TemplateProcessing",parameters:[{name:"single",val:""},{name:"pair",val:""},{name:"special_tokens",val:""}],parametersDescription:[{anchor:"tokenizers.processors.TemplateProcessing.single",description:`<strong>single</strong> (<code>Template</code>) &#x2014;
The template used for single sequences`,name:"single"},{anchor:"tokenizers.processors.TemplateProcessing.pair",description:`<strong>pair</strong> (<code>Template</code>) &#x2014;
The template used when both sequences are specified`,name:"pair"},{anchor:"tokenizers.processors.TemplateProcessing.special_tokens",description:`<strong>special_tokens</strong> (<code>Tokens</code>) &#x2014;
The list of special tokens used in each sequences`,name:"special_tokens"}]}}),oe=new zn({props:{anchor:"tokenizers.processors.TemplateProcessing.example",$$slots:{default:[Mn]},$$scope:{ctx:C}}}),re=new zn({props:{anchor:"tokenizers.processors.TemplateProcessing.example-2",$$slots:{default:[Un]},$$scope:{ctx:C}}}),{c(){l=r("h2"),u=r("a"),i=r("span"),w(d.$$.fragment),g=h(),p=r("span"),$=s("BertProcessing"),B=h(),x=r("div"),w(q.$$.fragment),O=h(),W=r("p"),E=s(`This post-processor takes care of adding the special tokens needed by
a Bert model:`),z=h(),I=r("ul"),Z=r("li"),Ce=s("a SEP token"),xe=h(),Me=r("li"),ps=s("a CLS token"),Nt=h(),H=r("h2"),ee=r("a"),Ue=r("span"),w(le.$$.fragment),hs=h(),He=r("span"),fs=s("ByteLevel"),jt=h(),N=r("div"),w(ie.$$.fragment),us=h(),Ve=r("p"),ms=s("This post-processor takes care of trimming the offsets."),gs=h(),Ye=r("p"),vs=s(`By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor must be used.`),Wt=h(),V=r("h2"),te=r("a"),Ge=r("span"),w(ce.$$.fragment),$s=h(),Je=r("span"),_s=s("RobertaProcessing"),Mt=h(),A=r("div"),w(de.$$.fragment),Es=h(),Fe=r("p"),ks=s(`This post-processor takes care of adding the special tokens needed by
a Roberta model:`),Ps=h(),pe=r("ul"),Ke=r("li"),ws=s("a SEP token"),ys=h(),Qe=r("li"),bs=s("a CLS token"),Ts=h(),Ie=r("p"),Ls=s(`It also takes care of trimming the offsets.
By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor should be initialized
with `),Xe=r("code"),Ss=s("trim_offsets=True"),Ut=h(),Y=r("h2"),se=r("a"),Ze=r("span"),w(he.$$.fragment),Cs=h(),et=r("span"),xs=s("TemplateProcessing"),Ht=h(),m=r("div"),w(fe.$$.fragment),Is=h(),tt=r("p"),Ds=s(`Provides a way to specify templates in order to add the special tokens to each
input sequence as relevant.`),zs=h(),j=r("p"),Bs=s("Let\u2019s take "),st=r("code"),As=s("BERT"),qs=s(` tokenizer as an example. It uses two special tokens, used to
delimitate each sequence. `),ot=r("code"),Os=s("[CLS]"),Rs=s(` is always used at the beginning of the first
sequence, and `),rt=r("code"),Ns=s("[SEP]"),js=s(` is added at the end of both the first, and the pair
sequences. The final result looks like this:`),Ws=h(),ue=r("ul"),De=r("li"),Ms=s("Single sequence: "),nt=r("code"),Us=s("[CLS] Hello there [SEP]"),Hs=h(),ze=r("li"),Vs=s("Pair sequences: "),at=r("code"),Ys=s("[CLS] My name is Anthony [SEP] What is my name? [SEP]"),Gs=h(),w(oe.$$.fragment),Js=h(),w(re.$$.fragment),Fs=h(),me=r("p"),Ks=s("In this example, each input sequence is identified using a "),lt=r("code"),Qs=s("$"),Xs=s(` construct. This identifier
lets us specify each input sequence, and the type_id to use. When nothing is specified,
it uses the default values. Here are the different ways to specify it:`),Zs=h(),G=r("ul"),M=r("li"),eo=s("Specifying the sequence, with default "),it=r("code"),to=s("type_id == 0"),so=s(": "),ct=r("code"),oo=s("$A"),ro=s(" or "),dt=r("code"),no=s("$B"),ao=h(),D=r("li"),lo=s("Specifying the "),pt=r("em"),io=s("type_id"),co=s(" with default "),ht=r("code"),po=s("sequence == A"),ho=s(": "),ft=r("code"),fo=s("$0"),uo=s(", "),ut=r("code"),mo=s("$1"),go=s(", "),mt=r("code"),vo=s("$2"),$o=s(", \u2026"),_o=h(),J=r("li"),Eo=s("Specifying both: "),gt=r("code"),ko=s("$A:0"),Po=s(", "),vt=r("code"),wo=s("$B:1"),yo=s(", \u2026"),bo=h(),ge=r("p"),To=s("The same construct is used for special tokens: "),$t=r("code"),Lo=s("<identifier>(:<type_id>)?"),So=s("."),Co=h(),U=r("p"),_t=r("strong"),xo=s("Warning"),Io=s(`: You must ensure that you are giving the correct tokens/ids as these
will be added to the Encoding without any further check. If the given ids correspond
to something totally different in a `),Et=r("em"),Do=s("Tokenizer"),zo=s(" using this "),kt=r("em"),Bo=s("PostProcessor"),Ao=s(`, it
might lead to unexpected results.`),qo=h(),Pt=r("p"),Oo=s("Types:"),Ro=h(),F=r("p"),No=s("Template ("),wt=r("code"),jo=s("str"),Wo=s(" or "),yt=r("code"),Mo=s("List"),Uo=s("):"),Ho=h(),ve=r("ul"),$e=r("li"),Vo=s("If a "),bt=r("code"),Yo=s("str"),Go=s(" is provided, the whitespace is used as delimiter between tokens"),Jo=h(),_e=r("li"),Fo=s("If a "),Tt=r("code"),Ko=s("List[str]"),Qo=s(" is provided, a list of tokens"),Xo=h(),Ee=r("p"),Zo=s("Tokens ("),Lt=r("code"),er=s("List[Union[Tuple[int, str], Tuple[str, int], dict]]"),tr=s("):"),sr=h(),ke=r("ul"),St=r("li"),Pe=r("p"),or=s("A "),Ct=r("code"),rr=s("Tuple"),nr=s(" with both a token and its associated ID, in any order"),ar=h(),K=r("li"),we=r("p"),lr=s("A "),xt=r("code"),ir=s("dict"),cr=s(" with the following keys:"),dr=h(),Q=r("ul"),ye=r("li"),pr=s("\u201Cid\u201D: "),It=r("code"),hr=s("str"),fr=s(" => The special token id, as specified in the Template"),ur=h(),be=r("li"),mr=s("\u201Cids\u201D: "),Dt=r("code"),gr=s("List[int]"),vr=s(" => The associated IDs"),$r=h(),Te=r("li"),_r=s("\u201Ctokens\u201D: "),zt=r("code"),Er=s("List[str]"),kr=s(" => The associated tokens"),Pr=h(),X=r("p"),wr=s("The given dict expects the provided "),Bt=r("code"),yr=s("ids"),br=s(" and "),At=r("code"),Tr=s("tokens"),Lr=s(` lists to have
the same length.`),this.h()},l(c){l=n(c,"H2",{class:!0});var k=a(l);u=n(k,"A",{id:!0,class:!0,href:!0});var qt=a(u);i=n(qt,"SPAN",{});var Ot=a(i);y(d.$$.fragment,Ot),Ot.forEach(t),qt.forEach(t),g=f(k),p=n(k,"SPAN",{});var Dr=a(p);$=o(Dr,"BertProcessing"),Dr.forEach(t),k.forEach(t),B=f(c),x=n(c,"DIV",{class:!0});var Be=a(x);y(q.$$.fragment,Be),O=f(Be),W=n(Be,"P",{});var zr=a(W);E=o(zr,`This post-processor takes care of adding the special tokens needed by
a Bert model:`),zr.forEach(t),z=f(Be),I=n(Be,"UL",{});var Yt=a(I);Z=n(Yt,"LI",{});var Br=a(Z);Ce=o(Br,"a SEP token"),Br.forEach(t),xe=f(Yt),Me=n(Yt,"LI",{});var Ar=a(Me);ps=o(Ar,"a CLS token"),Ar.forEach(t),Yt.forEach(t),Be.forEach(t),Nt=f(c),H=n(c,"H2",{class:!0});var Gt=a(H);ee=n(Gt,"A",{id:!0,class:!0,href:!0});var qr=a(ee);Ue=n(qr,"SPAN",{});var Or=a(Ue);y(le.$$.fragment,Or),Or.forEach(t),qr.forEach(t),hs=f(Gt),He=n(Gt,"SPAN",{});var Rr=a(He);fs=o(Rr,"ByteLevel"),Rr.forEach(t),Gt.forEach(t),jt=f(c),N=n(c,"DIV",{class:!0});var Ae=a(N);y(ie.$$.fragment,Ae),us=f(Ae),Ve=n(Ae,"P",{});var Nr=a(Ve);ms=o(Nr,"This post-processor takes care of trimming the offsets."),Nr.forEach(t),gs=f(Ae),Ye=n(Ae,"P",{});var jr=a(Ye);vs=o(jr,`By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor must be used.`),jr.forEach(t),Ae.forEach(t),Wt=f(c),V=n(c,"H2",{class:!0});var Jt=a(V);te=n(Jt,"A",{id:!0,class:!0,href:!0});var Wr=a(te);Ge=n(Wr,"SPAN",{});var Mr=a(Ge);y(ce.$$.fragment,Mr),Mr.forEach(t),Wr.forEach(t),$s=f(Jt),Je=n(Jt,"SPAN",{});var Ur=a(Je);_s=o(Ur,"RobertaProcessing"),Ur.forEach(t),Jt.forEach(t),Mt=f(c),A=n(c,"DIV",{class:!0});var ne=a(A);y(de.$$.fragment,ne),Es=f(ne),Fe=n(ne,"P",{});var Hr=a(Fe);ks=o(Hr,`This post-processor takes care of adding the special tokens needed by
a Roberta model:`),Hr.forEach(t),Ps=f(ne),pe=n(ne,"UL",{});var Ft=a(pe);Ke=n(Ft,"LI",{});var Vr=a(Ke);ws=o(Vr,"a SEP token"),Vr.forEach(t),ys=f(Ft),Qe=n(Ft,"LI",{});var Yr=a(Qe);bs=o(Yr,"a CLS token"),Yr.forEach(t),Ft.forEach(t),Ts=f(ne),Ie=n(ne,"P",{});var Sr=a(Ie);Ls=o(Sr,`It also takes care of trimming the offsets.
By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don\u2019t
want the offsets to include these whitespaces, then this PostProcessor should be initialized
with `),Xe=n(Sr,"CODE",{});var Gr=a(Xe);Ss=o(Gr,"trim_offsets=True"),Gr.forEach(t),Sr.forEach(t),ne.forEach(t),Ut=f(c),Y=n(c,"H2",{class:!0});var Kt=a(Y);se=n(Kt,"A",{id:!0,class:!0,href:!0});var Jr=a(se);Ze=n(Jr,"SPAN",{});var Fr=a(Ze);y(he.$$.fragment,Fr),Fr.forEach(t),Jr.forEach(t),Cs=f(Kt),et=n(Kt,"SPAN",{});var Kr=a(et);xs=o(Kr,"TemplateProcessing"),Kr.forEach(t),Kt.forEach(t),Ht=f(c),m=n(c,"DIV",{class:!0});var v=a(m);y(fe.$$.fragment,v),Is=f(v),tt=n(v,"P",{});var Qr=a(tt);Ds=o(Qr,`Provides a way to specify templates in order to add the special tokens to each
input sequence as relevant.`),Qr.forEach(t),zs=f(v),j=n(v,"P",{});var ae=a(j);Bs=o(ae,"Let\u2019s take "),st=n(ae,"CODE",{});var Xr=a(st);As=o(Xr,"BERT"),Xr.forEach(t),qs=o(ae,` tokenizer as an example. It uses two special tokens, used to
delimitate each sequence. `),ot=n(ae,"CODE",{});var Zr=a(ot);Os=o(Zr,"[CLS]"),Zr.forEach(t),Rs=o(ae,` is always used at the beginning of the first
sequence, and `),rt=n(ae,"CODE",{});var en=a(rt);Ns=o(en,"[SEP]"),en.forEach(t),js=o(ae,` is added at the end of both the first, and the pair
sequences. The final result looks like this:`),ae.forEach(t),Ws=f(v),ue=n(v,"UL",{});var Qt=a(ue);De=n(Qt,"LI",{});var Cr=a(De);Ms=o(Cr,"Single sequence: "),nt=n(Cr,"CODE",{});var tn=a(nt);Us=o(tn,"[CLS] Hello there [SEP]"),tn.forEach(t),Cr.forEach(t),Hs=f(Qt),ze=n(Qt,"LI",{});var xr=a(ze);Vs=o(xr,"Pair sequences: "),at=n(xr,"CODE",{});var sn=a(at);Ys=o(sn,"[CLS] My name is Anthony [SEP] What is my name? [SEP]"),sn.forEach(t),xr.forEach(t),Qt.forEach(t),Gs=f(v),y(oe.$$.fragment,v),Js=f(v),y(re.$$.fragment,v),Fs=f(v),me=n(v,"P",{});var Xt=a(me);Ks=o(Xt,"In this example, each input sequence is identified using a "),lt=n(Xt,"CODE",{});var on=a(lt);Qs=o(on,"$"),on.forEach(t),Xs=o(Xt,` construct. This identifier
lets us specify each input sequence, and the type_id to use. When nothing is specified,
it uses the default values. Here are the different ways to specify it:`),Xt.forEach(t),Zs=f(v),G=n(v,"UL",{});var qe=a(G);M=n(qe,"LI",{});var Le=a(M);eo=o(Le,"Specifying the sequence, with default "),it=n(Le,"CODE",{});var rn=a(it);to=o(rn,"type_id == 0"),rn.forEach(t),so=o(Le,": "),ct=n(Le,"CODE",{});var nn=a(ct);oo=o(nn,"$A"),nn.forEach(t),ro=o(Le," or "),dt=n(Le,"CODE",{});var an=a(dt);no=o(an,"$B"),an.forEach(t),Le.forEach(t),ao=f(qe),D=n(qe,"LI",{});var R=a(D);lo=o(R,"Specifying the "),pt=n(R,"EM",{});var ln=a(pt);io=o(ln,"type_id"),ln.forEach(t),co=o(R," with default "),ht=n(R,"CODE",{});var cn=a(ht);po=o(cn,"sequence == A"),cn.forEach(t),ho=o(R,": "),ft=n(R,"CODE",{});var dn=a(ft);fo=o(dn,"$0"),dn.forEach(t),uo=o(R,", "),ut=n(R,"CODE",{});var pn=a(ut);mo=o(pn,"$1"),pn.forEach(t),go=o(R,", "),mt=n(R,"CODE",{});var hn=a(mt);vo=o(hn,"$2"),hn.forEach(t),$o=o(R,", \u2026"),R.forEach(t),_o=f(qe),J=n(qe,"LI",{});var Oe=a(J);Eo=o(Oe,"Specifying both: "),gt=n(Oe,"CODE",{});var fn=a(gt);ko=o(fn,"$A:0"),fn.forEach(t),Po=o(Oe,", "),vt=n(Oe,"CODE",{});var un=a(vt);wo=o(un,"$B:1"),un.forEach(t),yo=o(Oe,", \u2026"),Oe.forEach(t),qe.forEach(t),bo=f(v),ge=n(v,"P",{});var Zt=a(ge);To=o(Zt,"The same construct is used for special tokens: "),$t=n(Zt,"CODE",{});var mn=a($t);Lo=o(mn,"<identifier>(:<type_id>)?"),mn.forEach(t),So=o(Zt,"."),Zt.forEach(t),Co=f(v),U=n(v,"P",{});var Se=a(U);_t=n(Se,"STRONG",{});var gn=a(_t);xo=o(gn,"Warning"),gn.forEach(t),Io=o(Se,`: You must ensure that you are giving the correct tokens/ids as these
will be added to the Encoding without any further check. If the given ids correspond
to something totally different in a `),Et=n(Se,"EM",{});var vn=a(Et);Do=o(vn,"Tokenizer"),vn.forEach(t),zo=o(Se," using this "),kt=n(Se,"EM",{});var $n=a(kt);Bo=o($n,"PostProcessor"),$n.forEach(t),Ao=o(Se,`, it
might lead to unexpected results.`),Se.forEach(t),qo=f(v),Pt=n(v,"P",{});var _n=a(Pt);Oo=o(_n,"Types:"),_n.forEach(t),Ro=f(v),F=n(v,"P",{});var Re=a(F);No=o(Re,"Template ("),wt=n(Re,"CODE",{});var En=a(wt);jo=o(En,"str"),En.forEach(t),Wo=o(Re," or "),yt=n(Re,"CODE",{});var kn=a(yt);Mo=o(kn,"List"),kn.forEach(t),Uo=o(Re,"):"),Re.forEach(t),Ho=f(v),ve=n(v,"UL",{});var es=a(ve);$e=n(es,"LI",{});var ts=a($e);Vo=o(ts,"If a "),bt=n(ts,"CODE",{});var Pn=a(bt);Yo=o(Pn,"str"),Pn.forEach(t),Go=o(ts," is provided, the whitespace is used as delimiter between tokens"),ts.forEach(t),Jo=f(es),_e=n(es,"LI",{});var ss=a(_e);Fo=o(ss,"If a "),Tt=n(ss,"CODE",{});var wn=a(Tt);Ko=o(wn,"List[str]"),wn.forEach(t),Qo=o(ss," is provided, a list of tokens"),ss.forEach(t),es.forEach(t),Xo=f(v),Ee=n(v,"P",{});var os=a(Ee);Zo=o(os,"Tokens ("),Lt=n(os,"CODE",{});var yn=a(Lt);er=o(yn,"List[Union[Tuple[int, str], Tuple[str, int], dict]]"),yn.forEach(t),tr=o(os,"):"),os.forEach(t),sr=f(v),ke=n(v,"UL",{});var rs=a(ke);St=n(rs,"LI",{});var bn=a(St);Pe=n(bn,"P",{});var ns=a(Pe);or=o(ns,"A "),Ct=n(ns,"CODE",{});var Tn=a(Ct);rr=o(Tn,"Tuple"),Tn.forEach(t),nr=o(ns," with both a token and its associated ID, in any order"),ns.forEach(t),bn.forEach(t),ar=f(rs),K=n(rs,"LI",{});var Ne=a(K);we=n(Ne,"P",{});var as=a(we);lr=o(as,"A "),xt=n(as,"CODE",{});var Ln=a(xt);ir=o(Ln,"dict"),Ln.forEach(t),cr=o(as," with the following keys:"),as.forEach(t),dr=f(Ne),Q=n(Ne,"UL",{});var je=a(Q);ye=n(je,"LI",{});var ls=a(ye);pr=o(ls,"\u201Cid\u201D: "),It=n(ls,"CODE",{});var Sn=a(It);hr=o(Sn,"str"),Sn.forEach(t),fr=o(ls," => The special token id, as specified in the Template"),ls.forEach(t),ur=f(je),be=n(je,"LI",{});var is=a(be);mr=o(is,"\u201Cids\u201D: "),Dt=n(is,"CODE",{});var Cn=a(Dt);gr=o(Cn,"List[int]"),Cn.forEach(t),vr=o(is," => The associated IDs"),is.forEach(t),$r=f(je),Te=n(je,"LI",{});var cs=a(Te);_r=o(cs,"\u201Ctokens\u201D: "),zt=n(cs,"CODE",{});var xn=a(zt);Er=o(xn,"List[str]"),xn.forEach(t),kr=o(cs," => The associated tokens"),cs.forEach(t),je.forEach(t),Pr=f(Ne),X=n(Ne,"P",{});var We=a(X);wr=o(We,"The given dict expects the provided "),Bt=n(We,"CODE",{});var In=a(Bt);yr=o(In,"ids"),In.forEach(t),br=o(We," and "),At=n(We,"CODE",{});var Dn=a(At);Tr=o(Dn,"tokens"),Dn.forEach(t),Lr=o(We,` lists to have
the same length.`),We.forEach(t),Ne.forEach(t),rs.forEach(t),v.forEach(t),this.h()},h(){_(u,"id","tokenizers.processors.BertProcessing"),_(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(u,"href","#tokenizers.processors.BertProcessing"),_(l,"class","relative group"),_(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(ee,"id","tokenizers.processors.ByteLevel"),_(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(ee,"href","#tokenizers.processors.ByteLevel"),_(H,"class","relative group"),_(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(te,"id","tokenizers.processors.RobertaProcessing"),_(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(te,"href","#tokenizers.processors.RobertaProcessing"),_(V,"class","relative group"),_(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),_(se,"id","tokenizers.processors.TemplateProcessing"),_(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(se,"href","#tokenizers.processors.TemplateProcessing"),_(Y,"class","relative group"),_(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(c,k){P(c,l,k),e(l,u),e(u,i),b(d,i,null),e(l,g),e(l,p),e(p,$),P(c,B,k),P(c,x,k),b(q,x,null),e(x,O),e(x,W),e(W,E),e(x,z),e(x,I),e(I,Z),e(Z,Ce),e(I,xe),e(I,Me),e(Me,ps),P(c,Nt,k),P(c,H,k),e(H,ee),e(ee,Ue),b(le,Ue,null),e(H,hs),e(H,He),e(He,fs),P(c,jt,k),P(c,N,k),b(ie,N,null),e(N,us),e(N,Ve),e(Ve,ms),e(N,gs),e(N,Ye),e(Ye,vs),P(c,Wt,k),P(c,V,k),e(V,te),e(te,Ge),b(ce,Ge,null),e(V,$s),e(V,Je),e(Je,_s),P(c,Mt,k),P(c,A,k),b(de,A,null),e(A,Es),e(A,Fe),e(Fe,ks),e(A,Ps),e(A,pe),e(pe,Ke),e(Ke,ws),e(pe,ys),e(pe,Qe),e(Qe,bs),e(A,Ts),e(A,Ie),e(Ie,Ls),e(Ie,Xe),e(Xe,Ss),P(c,Ut,k),P(c,Y,k),e(Y,se),e(se,Ze),b(he,Ze,null),e(Y,Cs),e(Y,et),e(et,xs),P(c,Ht,k),P(c,m,k),b(fe,m,null),e(m,Is),e(m,tt),e(tt,Ds),e(m,zs),e(m,j),e(j,Bs),e(j,st),e(st,As),e(j,qs),e(j,ot),e(ot,Os),e(j,Rs),e(j,rt),e(rt,Ns),e(j,js),e(m,Ws),e(m,ue),e(ue,De),e(De,Ms),e(De,nt),e(nt,Us),e(ue,Hs),e(ue,ze),e(ze,Vs),e(ze,at),e(at,Ys),e(m,Gs),b(oe,m,null),e(m,Js),b(re,m,null),e(m,Fs),e(m,me),e(me,Ks),e(me,lt),e(lt,Qs),e(me,Xs),e(m,Zs),e(m,G),e(G,M),e(M,eo),e(M,it),e(it,to),e(M,so),e(M,ct),e(ct,oo),e(M,ro),e(M,dt),e(dt,no),e(G,ao),e(G,D),e(D,lo),e(D,pt),e(pt,io),e(D,co),e(D,ht),e(ht,po),e(D,ho),e(D,ft),e(ft,fo),e(D,uo),e(D,ut),e(ut,mo),e(D,go),e(D,mt),e(mt,vo),e(D,$o),e(G,_o),e(G,J),e(J,Eo),e(J,gt),e(gt,ko),e(J,Po),e(J,vt),e(vt,wo),e(J,yo),e(m,bo),e(m,ge),e(ge,To),e(ge,$t),e($t,Lo),e(ge,So),e(m,Co),e(m,U),e(U,_t),e(_t,xo),e(U,Io),e(U,Et),e(Et,Do),e(U,zo),e(U,kt),e(kt,Bo),e(U,Ao),e(m,qo),e(m,Pt),e(Pt,Oo),e(m,Ro),e(m,F),e(F,No),e(F,wt),e(wt,jo),e(F,Wo),e(F,yt),e(yt,Mo),e(F,Uo),e(m,Ho),e(m,ve),e(ve,$e),e($e,Vo),e($e,bt),e(bt,Yo),e($e,Go),e(ve,Jo),e(ve,_e),e(_e,Fo),e(_e,Tt),e(Tt,Ko),e(_e,Qo),e(m,Xo),e(m,Ee),e(Ee,Zo),e(Ee,Lt),e(Lt,er),e(Ee,tr),e(m,sr),e(m,ke),e(ke,St),e(St,Pe),e(Pe,or),e(Pe,Ct),e(Ct,rr),e(Pe,nr),e(ke,ar),e(ke,K),e(K,we),e(we,lr),e(we,xt),e(xt,ir),e(we,cr),e(K,dr),e(K,Q),e(Q,ye),e(ye,pr),e(ye,It),e(It,hr),e(ye,fr),e(Q,ur),e(Q,be),e(be,mr),e(be,Dt),e(Dt,gr),e(be,vr),e(Q,$r),e(Q,Te),e(Te,_r),e(Te,zt),e(zt,Er),e(Te,kr),e(K,Pr),e(K,X),e(X,wr),e(X,Bt),e(Bt,yr),e(X,br),e(X,At),e(At,Tr),e(X,Lr),Vt=!0},p(c,k){const qt={};k&2&&(qt.$$scope={dirty:k,ctx:c}),oe.$set(qt);const Ot={};k&2&&(Ot.$$scope={dirty:k,ctx:c}),re.$set(Ot)},i(c){Vt||(T(d.$$.fragment,c),T(q.$$.fragment,c),T(le.$$.fragment,c),T(ie.$$.fragment,c),T(ce.$$.fragment,c),T(de.$$.fragment,c),T(he.$$.fragment,c),T(fe.$$.fragment,c),T(oe.$$.fragment,c),T(re.$$.fragment,c),Vt=!0)},o(c){L(d.$$.fragment,c),L(q.$$.fragment,c),L(le.$$.fragment,c),L(ie.$$.fragment,c),L(ce.$$.fragment,c),L(de.$$.fragment,c),L(he.$$.fragment,c),L(fe.$$.fragment,c),L(oe.$$.fragment,c),L(re.$$.fragment,c),Vt=!1},d(c){c&&t(l),S(d),c&&t(B),c&&t(x),S(q),c&&t(Nt),c&&t(H),S(le),c&&t(jt),c&&t(N),S(ie),c&&t(Wt),c&&t(V),S(ce),c&&t(Mt),c&&t(A),S(de),c&&t(Ut),c&&t(Y),S(he),c&&t(Ht),c&&t(m),S(fe),S(oe),S(re)}}}function Vn(C){let l,u;return l=new Ir({props:{$$slots:{default:[Hn]},$$scope:{ctx:C}}}),{c(){w(l.$$.fragment)},l(i){y(l.$$.fragment,i)},m(i,d){b(l,i,d),u=!0},p(i,d){const g={};d&2&&(g.$$scope={dirty:d,ctx:i}),l.$set(g)},i(i){u||(T(l.$$.fragment,i),u=!0)},o(i){L(l.$$.fragment,i),u=!1},d(i){S(l,i)}}}function Yn(C){let l,u,i,d,g;return{c(){l=r("p"),u=s("The Rust API Reference is available directly on the "),i=r("a"),d=s("Docs.rs"),g=s(" website."),this.h()},l(p){l=n(p,"P",{});var $=a(l);u=o($,"The Rust API Reference is available directly on the "),i=n($,"A",{href:!0,rel:!0});var B=a(i);d=o(B,"Docs.rs"),B.forEach(t),g=o($," website."),$.forEach(t),this.h()},h(){_(i,"href","https://docs.rs/tokenizers/latest/tokenizers/"),_(i,"rel","nofollow")},m(p,$){P(p,l,$),e(l,u),e(l,i),e(i,d),e(l,g)},d(p){p&&t(l)}}}function Gn(C){let l,u;return l=new Ir({props:{$$slots:{default:[Yn]},$$scope:{ctx:C}}}),{c(){w(l.$$.fragment)},l(i){y(l.$$.fragment,i)},m(i,d){b(l,i,d),u=!0},p(i,d){const g={};d&2&&(g.$$scope={dirty:d,ctx:i}),l.$set(g)},i(i){u||(T(l.$$.fragment,i),u=!0)},o(i){L(l.$$.fragment,i),u=!1},d(i){S(l,i)}}}function Jn(C){let l,u;return{c(){l=r("p"),u=s("The node API has not been documented yet.")},l(i){l=n(i,"P",{});var d=a(l);u=o(d,"The node API has not been documented yet."),d.forEach(t)},m(i,d){P(i,l,d),e(l,u)},d(i){i&&t(l)}}}function Fn(C){let l,u;return l=new Ir({props:{$$slots:{default:[Jn]},$$scope:{ctx:C}}}),{c(){w(l.$$.fragment)},l(i){y(l.$$.fragment,i)},m(i,d){b(l,i,d),u=!0},p(i,d){const g={};d&2&&(g.$$scope={dirty:d,ctx:i}),l.$set(g)},i(i){u||(T(l.$$.fragment,i),u=!0)},o(i){L(l.$$.fragment,i),u=!1},d(i){S(l,i)}}}function Kn(C){let l,u,i,d,g,p,$,B,x,q,O,W;return p=new Rt({}),O=new Wn({props:{python:!0,rust:!0,node:!0,$$slots:{node:[Fn],rust:[Gn],python:[Vn]},$$scope:{ctx:C}}}),{c(){l=r("meta"),u=h(),i=r("h1"),d=r("a"),g=r("span"),w(p.$$.fragment),$=h(),B=r("span"),x=s("Post-processors"),q=h(),w(O.$$.fragment),this.h()},l(E){const z=Nn('[data-svelte="svelte-1phssyn"]',document.head);l=n(z,"META",{name:!0,content:!0}),z.forEach(t),u=f(E),i=n(E,"H1",{class:!0});var I=a(i);d=n(I,"A",{id:!0,class:!0,href:!0});var Z=a(d);g=n(Z,"SPAN",{});var Ce=a(g);y(p.$$.fragment,Ce),Ce.forEach(t),Z.forEach(t),$=f(I),B=n(I,"SPAN",{});var xe=a(B);x=o(xe,"Post-processors"),xe.forEach(t),I.forEach(t),q=f(E),y(O.$$.fragment,E),this.h()},h(){_(l,"name","hf:doc:metadata"),_(l,"content",JSON.stringify(Qn)),_(d,"id","postprocessors"),_(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(d,"href","#postprocessors"),_(i,"class","relative group")},m(E,z){e(document.head,l),P(E,u,z),P(E,i,z),e(i,d),e(d,g),b(p,g,null),e(i,$),e(i,B),e(B,x),P(E,q,z),b(O,E,z),W=!0},p(E,[z]){const I={};z&2&&(I.$$scope={dirty:z,ctx:E}),O.$set(I)},i(E){W||(T(p.$$.fragment,E),T(O.$$.fragment,E),W=!0)},o(E){L(p.$$.fragment,E),L(O.$$.fragment,E),W=!1},d(E){t(l),E&&t(u),E&&t(i),S(p),E&&t(q),S(O,E)}}}const Qn={local:"postprocessors",sections:[{local:"tokenizers.processors.BertProcessing",title:"BertProcessing"},{local:"tokenizers.processors.ByteLevel",title:"ByteLevel"},{local:"tokenizers.processors.RobertaProcessing",title:"RobertaProcessing"},{local:"tokenizers.processors.TemplateProcessing",title:"TemplateProcessing"}],title:"Post-processors"};function Xn(C){return jn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class na extends qn{constructor(l){super();On(this,l,Xn,Kn,Rn,{})}}export{na as default,Qn as metadata};
