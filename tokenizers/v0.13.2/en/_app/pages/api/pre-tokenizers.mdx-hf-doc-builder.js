import{S as Un,i as jn,s as On,e as s,k as p,w as $,t as h,M as Fn,c as o,d as r,m as c,a as n,x as g,h as d,b as l,G as t,g as m,y as _,q as z,o as w,B as b,v as Rn,L as Mn}from"../../chunks/vendor-hf-doc-builder.js";import{D}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Vn}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as N}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{T as Gn,M as Do}from"../../chunks/TokenizersLanguageContent-hf-doc-builder.js";import{E as Hn}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Kn(x){let a,v,i,u,y;return u=new Vn({props:{code:'"Call 123 please" -> "Call ", "1", "2", "3", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>, <span class="hljs-string">&quot;3&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),{c(){a=s("p"),v=h("If set to True, digits will each be separated as follows:"),i=p(),$(u.$$.fragment)},l(k){a=o(k,"P",{});var P=n(a);v=d(P,"If set to True, digits will each be separated as follows:"),P.forEach(r),i=c(k),g(u.$$.fragment,k)},m(k,P){m(k,a,P),t(a,v),m(k,i,P),_(u,k,P),y=!0},p:Mn,i(k){y||(z(u.$$.fragment,k),y=!0)},o(k){w(u.$$.fragment,k),y=!1},d(k){k&&r(a),k&&r(i),b(u,k)}}}function Jn(x){let a,v,i,u,y;return u=new Vn({props:{code:'"Call 123 please" -> "Call ", "123", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;123&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),{c(){a=s("p"),v=h("If set to False, digits will grouped as follows:"),i=p(),$(u.$$.fragment)},l(k){a=o(k,"P",{});var P=n(a);v=d(P,"If set to False, digits will grouped as follows:"),P.forEach(r),i=c(k),g(u.$$.fragment,k)},m(k,P){m(k,a,P),t(a,v),m(k,i,P),_(u,k,P),y=!0},p:Mn,i(k){y||(z(u.$$.fragment,k),y=!0)},o(k){w(u.$$.fragment,k),y=!1},d(k){k&&r(a),k&&r(i),b(u,k)}}}function Qn(x){let a,v,i,u,y,k,P,C,T,W,L,U,E,A,q,st,Ee,H,he,ut,xe,Kr,mt,Jr,ir,I,Te,Qr,kt,Xr,Yr,vt,Zr,es,j,Se,ts,$t,rs,ss,gt,os,lr,J,de,_t,De,ns,zt,as,pr,Q,Ae,is,ot,ls,wt,ps,cr,X,fe,bt,Ce,cs,yt,hs,hr,B,qe,ds,Pt,fs,us,ue,ms,me,dr,Y,ke,Et,Ie,ks,xt,vs,fr,M,Be,$s,Tt,gs,_s,St,zs,ur,Z,ve,Dt,Ne,ws,At,bs,mr,S,We,ys,Ct,Ps,Es,qt,xs,Ts,O,Le,Ss,He,Ds,It,As,Cs,qs,F,Is,Bt,Bs,Ns,Nt,Ws,Ls,Wt,Hs,Ms,R,Me,Vs,Lt,Us,js,G,Os,nt,Fs,Rs,Ht,Gs,Ks,Mt,Js,kr,ee,$e,Vt,Ve,Qs,Ut,Xs,vr,te,Ue,Ys,jt,Zs,$r,re,ge,Ot,je,eo,Ft,to,gr,se,Oe,ro,Rt,so,_r,oe,_e,Gt,Fe,oo,Kt,no,zr,V,Re,ao,Jt,io,lo,Qt,po,wr,ne,ze,Xt,Ge,co,Yt,ho,br,ae,Ke,fo,Je,uo,Qe,mo,ko,yr,ie,we,Zt,Xe,vo,er,$o,Pr,le,Ye,go,at,_o,tr,zo,Er,pe,be,rr,Ze,wo,sr,bo,xr,ce,et,yo,it,Po,or,Eo,Tr;return u=new N({}),W=new D({props:{name:"class tokenizers.pre_tokenizers.BertPreTokenizer",anchor:"tokenizers.pre_tokenizers.BertPreTokenizer",parameters:[]}}),xe=new N({}),Te=new D({props:{name:"class tokenizers.pre_tokenizers.ByteLevel",anchor:"tokenizers.pre_tokenizers.ByteLevel",parameters:[{name:"add_prefix_space",val:" = True"},{name:"use_regex",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.ByteLevel.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"},{anchor:"tokenizers.pre_tokenizers.ByteLevel.use_regex",description:`<strong>use_regex</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Set this to <code>False</code> to prevent this <em>pre_tokenizer</em> from using
the GPT2 specific regexp for spliting on whitespace.`,name:"use_regex"}]}}),Se=new D({props:{name:"alphabet",anchor:"tokenizers.pre_tokenizers.ByteLevel.alphabet",parameters:[],returnDescription:`
<p>A list of characters that compose the alphabet</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),De=new N({}),Ae=new D({props:{name:"class tokenizers.pre_tokenizers.CharDelimiterSplit",anchor:"tokenizers.pre_tokenizers.CharDelimiterSplit",parameters:""}}),Ce=new N({}),qe=new D({props:{name:"class tokenizers.pre_tokenizers.Digits",anchor:"tokenizers.pre_tokenizers.Digits",parameters:[{name:"individual_digits",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Digits.individual_digits",description:"<strong>individual_digits</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;",name:"individual_digits"}]}}),ue=new Hn({props:{anchor:"tokenizers.pre_tokenizers.Digits.example",$$slots:{default:[Kn]},$$scope:{ctx:x}}}),me=new Hn({props:{anchor:"tokenizers.pre_tokenizers.Digits.example-2",$$slots:{default:[Jn]},$$scope:{ctx:x}}}),Ie=new N({}),Be=new D({props:{name:"class tokenizers.pre_tokenizers.Metaspace",anchor:"tokenizers.pre_tokenizers.Metaspace",parameters:[{name:"replacement",val:" = '_'"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.pre_tokenizers.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),Ne=new N({}),We=new D({props:{name:"class tokenizers.pre_tokenizers.PreTokenizer",anchor:"tokenizers.pre_tokenizers.PreTokenizer",parameters:""}}),Le=new D({props:{name:"pre_tokenize",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize",parameters:[{name:"pretok",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize.pretok",description:"<strong>pretok</strong> (<code>~tokenizers.PreTokenizedString) -- The pre-tokenized string on which to apply this :class:</code>~tokenizers.pre_tokenizers.PreTokenizer`",name:"pretok"}]}}),Me=new D({props:{name:"pre_tokenize_str",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A string to pre-tokeize`,name:"sequence"}],returnDescription:`
<p>A list of tuple with the pre-tokenized parts and their offsets</p>
`,returnType:`
<p><code>List[Tuple[str, Offsets]]</code></p>
`}}),Ve=new N({}),Ue=new D({props:{name:"class tokenizers.pre_tokenizers.Punctuation",anchor:"tokenizers.pre_tokenizers.Punctuation",parameters:[{name:"behavior",val:" = 'isolated'"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Punctuation.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D; (default), &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"}]}}),je=new N({}),Oe=new D({props:{name:"class tokenizers.pre_tokenizers.Sequence",anchor:"tokenizers.pre_tokenizers.Sequence",parameters:[{name:"pretokenizers",val:""}]}}),Fe=new N({}),Re=new D({props:{name:"class tokenizers.pre_tokenizers.Split",anchor:"tokenizers.pre_tokenizers.Split",parameters:[{name:"pattern",val:""},{name:"behavior",val:""},{name:"invert",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Split.pattern",description:`<strong>pattern</strong> (<code>str</code> or <code>Regex</code>) &#x2014;
A pattern used to split the string. Usually a string or a Regex`,name:"pattern"},{anchor:"tokenizers.pre_tokenizers.Split.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D;, &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"},{anchor:"tokenizers.pre_tokenizers.Split.invert",description:`<strong>invert</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to invert the pattern.`,name:"invert"}]}}),Ge=new N({}),Ke=new D({props:{name:"class tokenizers.pre_tokenizers.UnicodeScripts",anchor:"tokenizers.pre_tokenizers.UnicodeScripts",parameters:[]}}),Xe=new N({}),Ye=new D({props:{name:"class tokenizers.pre_tokenizers.Whitespace",anchor:"tokenizers.pre_tokenizers.Whitespace",parameters:[]}}),Ze=new N({}),et=new D({props:{name:"class tokenizers.pre_tokenizers.WhitespaceSplit",anchor:"tokenizers.pre_tokenizers.WhitespaceSplit",parameters:[]}}),{c(){a=s("h2"),v=s("a"),i=s("span"),$(u.$$.fragment),y=p(),k=s("span"),P=h("BertPreTokenizer"),C=p(),T=s("div"),$(W.$$.fragment),L=p(),U=s("p"),E=h("BertPreTokenizer"),A=p(),q=s("p"),st=h(`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),Ee=p(),H=s("h2"),he=s("a"),ut=s("span"),$(xe.$$.fragment),Kr=p(),mt=s("span"),Jr=h("ByteLevel"),ir=p(),I=s("div"),$(Te.$$.fragment),Qr=p(),kt=s("p"),Xr=h("ByteLevel PreTokenizer"),Yr=p(),vt=s("p"),Zr=h(`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),es=p(),j=s("div"),$(Se.$$.fragment),ts=p(),$t=s("p"),rs=h("Returns the alphabet used by this PreTokenizer."),ss=p(),gt=s("p"),os=h(`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),lr=p(),J=s("h2"),de=s("a"),_t=s("span"),$(De.$$.fragment),ns=p(),zt=s("span"),as=h("CharDelimiterSplit"),pr=p(),Q=s("div"),$(Ae.$$.fragment),is=p(),ot=s("p"),ls=h("This pre-tokenizer simply splits on the provided char. Works like "),wt=s("code"),ps=h(".split(delimiter)"),cr=p(),X=s("h2"),fe=s("a"),bt=s("span"),$(Ce.$$.fragment),cs=p(),yt=s("span"),hs=h("Digits"),hr=p(),B=s("div"),$(qe.$$.fragment),ds=p(),Pt=s("p"),fs=h("This pre-tokenizer simply splits using the digits in separate tokens"),us=p(),$(ue.$$.fragment),ms=p(),$(me.$$.fragment),dr=p(),Y=s("h2"),ke=s("a"),Et=s("span"),$(Ie.$$.fragment),ks=p(),xt=s("span"),vs=h("Metaspace"),fr=p(),M=s("div"),$(Be.$$.fragment),$s=p(),Tt=s("p"),gs=h("Metaspace pre-tokenizer"),_s=p(),St=s("p"),zs=h(`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),ur=p(),Z=s("h2"),ve=s("a"),Dt=s("span"),$(Ne.$$.fragment),ws=p(),At=s("span"),bs=h("PreTokenizer"),mr=p(),S=s("div"),$(We.$$.fragment),ys=p(),Ct=s("p"),Ps=h("Base class for all pre-tokenizers"),Es=p(),qt=s("p"),xs=h(`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),Ts=p(),O=s("div"),$(Le.$$.fragment),Ss=p(),He=s("p"),Ds=h("Pre-tokenize a "),It=s("code"),As=h("~tokenizers.PyPreTokenizedString"),Cs=h(" in-place"),qs=p(),F=s("p"),Is=h("This method allows to modify a "),Bt=s("code"),Bs=h("PreTokenizedString"),Ns=h(` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Nt=s("code"),Ws=h("PreTokenizedString"),Ls=h(`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Wt=s("code"),Hs=h("pre_tokenize_str()"),Ms=p(),R=s("div"),$(Me.$$.fragment),Vs=p(),Lt=s("p"),Us=h("Pre tokenize the given string"),js=p(),G=s("p"),Os=h(`This method provides a way to visualize the effect of a
`),nt=s("a"),Fs=h("PreTokenizer"),Rs=h(` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Ht=s("code"),Gs=h("PreTokenizedString"),Ks=h(`. If you need some of these, you can use
`),Mt=s("code"),Js=h("pre_tokenize()"),kr=p(),ee=s("h2"),$e=s("a"),Vt=s("span"),$(Ve.$$.fragment),Qs=p(),Ut=s("span"),Xs=h("Punctuation"),vr=p(),te=s("div"),$(Ue.$$.fragment),Ys=p(),jt=s("p"),Zs=h("This pre-tokenizer simply splits on punctuation as individual characters."),$r=p(),re=s("h2"),ge=s("a"),Ot=s("span"),$(je.$$.fragment),eo=p(),Ft=s("span"),to=h("Sequence"),gr=p(),se=s("div"),$(Oe.$$.fragment),ro=p(),Rt=s("p"),so=h("This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),_r=p(),oe=s("h2"),_e=s("a"),Gt=s("span"),$(Fe.$$.fragment),oo=p(),Kt=s("span"),no=h("Split"),zr=p(),V=s("div"),$(Re.$$.fragment),ao=p(),Jt=s("p"),io=h("Split PreTokenizer"),lo=p(),Qt=s("p"),po=h(`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),wr=p(),ne=s("h2"),ze=s("a"),Xt=s("span"),$(Ge.$$.fragment),co=p(),Yt=s("span"),ho=h("UnicodeScripts"),br=p(),ae=s("div"),$(Ke.$$.fragment),fo=p(),Je=s("p"),uo=h(`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=s("a"),mo=h("https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),ko=h(`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),yr=p(),ie=s("h2"),we=s("a"),Zt=s("span"),$(Xe.$$.fragment),vo=p(),er=s("span"),$o=h("Whitespace"),Pr=p(),le=s("div"),$(Ye.$$.fragment),go=p(),at=s("p"),_o=h("This pre-tokenizer simply splits using the following regex: "),tr=s("code"),zo=h("\\w+|[^\\w\\s]+"),Er=p(),pe=s("h2"),be=s("a"),rr=s("span"),$(Ze.$$.fragment),wo=p(),sr=s("span"),bo=h("WhitespaceSplit"),xr=p(),ce=s("div"),$(et.$$.fragment),yo=p(),it=s("p"),Po=h("This pre-tokenizer simply splits on the whitespace. Works like "),or=s("code"),Eo=h(".split()"),this.h()},l(e){a=o(e,"H2",{class:!0});var f=n(a);v=o(f,"A",{id:!0,class:!0,href:!0});var nr=n(v);i=o(nr,"SPAN",{});var ar=n(i);g(u.$$.fragment,ar),ar.forEach(r),nr.forEach(r),y=c(f),k=o(f,"SPAN",{});var Ao=n(k);P=d(Ao,"BertPreTokenizer"),Ao.forEach(r),f.forEach(r),C=c(e),T=o(e,"DIV",{class:!0});var lt=n(T);g(W.$$.fragment,lt),L=c(lt),U=o(lt,"P",{});var Co=n(U);E=d(Co,"BertPreTokenizer"),Co.forEach(r),A=c(lt),q=o(lt,"P",{});var qo=n(q);st=d(qo,`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),qo.forEach(r),lt.forEach(r),Ee=c(e),H=o(e,"H2",{class:!0});var Sr=n(H);he=o(Sr,"A",{id:!0,class:!0,href:!0});var Io=n(he);ut=o(Io,"SPAN",{});var Bo=n(ut);g(xe.$$.fragment,Bo),Bo.forEach(r),Io.forEach(r),Kr=c(Sr),mt=o(Sr,"SPAN",{});var No=n(mt);Jr=d(No,"ByteLevel"),No.forEach(r),Sr.forEach(r),ir=c(e),I=o(e,"DIV",{class:!0});var ye=n(I);g(Te.$$.fragment,ye),Qr=c(ye),kt=o(ye,"P",{});var Wo=n(kt);Xr=d(Wo,"ByteLevel PreTokenizer"),Wo.forEach(r),Yr=c(ye),vt=o(ye,"P",{});var Lo=n(vt);Zr=d(Lo,`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),Lo.forEach(r),es=c(ye),j=o(ye,"DIV",{class:!0});var pt=n(j);g(Se.$$.fragment,pt),ts=c(pt),$t=o(pt,"P",{});var Ho=n($t);rs=d(Ho,"Returns the alphabet used by this PreTokenizer."),Ho.forEach(r),ss=c(pt),gt=o(pt,"P",{});var Mo=n(gt);os=d(Mo,`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),Mo.forEach(r),pt.forEach(r),ye.forEach(r),lr=c(e),J=o(e,"H2",{class:!0});var Dr=n(J);de=o(Dr,"A",{id:!0,class:!0,href:!0});var Vo=n(de);_t=o(Vo,"SPAN",{});var Uo=n(_t);g(De.$$.fragment,Uo),Uo.forEach(r),Vo.forEach(r),ns=c(Dr),zt=o(Dr,"SPAN",{});var jo=n(zt);as=d(jo,"CharDelimiterSplit"),jo.forEach(r),Dr.forEach(r),pr=c(e),Q=o(e,"DIV",{class:!0});var Ar=n(Q);g(Ae.$$.fragment,Ar),is=c(Ar),ot=o(Ar,"P",{});var xo=n(ot);ls=d(xo,"This pre-tokenizer simply splits on the provided char. Works like "),wt=o(xo,"CODE",{});var Oo=n(wt);ps=d(Oo,".split(delimiter)"),Oo.forEach(r),xo.forEach(r),Ar.forEach(r),cr=c(e),X=o(e,"H2",{class:!0});var Cr=n(X);fe=o(Cr,"A",{id:!0,class:!0,href:!0});var Fo=n(fe);bt=o(Fo,"SPAN",{});var Ro=n(bt);g(Ce.$$.fragment,Ro),Ro.forEach(r),Fo.forEach(r),cs=c(Cr),yt=o(Cr,"SPAN",{});var Go=n(yt);hs=d(Go,"Digits"),Go.forEach(r),Cr.forEach(r),hr=c(e),B=o(e,"DIV",{class:!0});var Pe=n(B);g(qe.$$.fragment,Pe),ds=c(Pe),Pt=o(Pe,"P",{});var Ko=n(Pt);fs=d(Ko,"This pre-tokenizer simply splits using the digits in separate tokens"),Ko.forEach(r),us=c(Pe),g(ue.$$.fragment,Pe),ms=c(Pe),g(me.$$.fragment,Pe),Pe.forEach(r),dr=c(e),Y=o(e,"H2",{class:!0});var qr=n(Y);ke=o(qr,"A",{id:!0,class:!0,href:!0});var Jo=n(ke);Et=o(Jo,"SPAN",{});var Qo=n(Et);g(Ie.$$.fragment,Qo),Qo.forEach(r),Jo.forEach(r),ks=c(qr),xt=o(qr,"SPAN",{});var Xo=n(xt);vs=d(Xo,"Metaspace"),Xo.forEach(r),qr.forEach(r),fr=c(e),M=o(e,"DIV",{class:!0});var ct=n(M);g(Be.$$.fragment,ct),$s=c(ct),Tt=o(ct,"P",{});var Yo=n(Tt);gs=d(Yo,"Metaspace pre-tokenizer"),Yo.forEach(r),_s=c(ct),St=o(ct,"P",{});var Zo=n(St);zs=d(Zo,`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),Zo.forEach(r),ct.forEach(r),ur=c(e),Z=o(e,"H2",{class:!0});var Ir=n(Z);ve=o(Ir,"A",{id:!0,class:!0,href:!0});var en=n(ve);Dt=o(en,"SPAN",{});var tn=n(Dt);g(Ne.$$.fragment,tn),tn.forEach(r),en.forEach(r),ws=c(Ir),At=o(Ir,"SPAN",{});var rn=n(At);bs=d(rn,"PreTokenizer"),rn.forEach(r),Ir.forEach(r),mr=c(e),S=o(e,"DIV",{class:!0});var K=n(S);g(We.$$.fragment,K),ys=c(K),Ct=o(K,"P",{});var sn=n(Ct);Ps=d(sn,"Base class for all pre-tokenizers"),sn.forEach(r),Es=c(K),qt=o(K,"P",{});var on=n(qt);xs=d(on,`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),on.forEach(r),Ts=c(K),O=o(K,"DIV",{class:!0});var ht=n(O);g(Le.$$.fragment,ht),Ss=c(ht),He=o(ht,"P",{});var Br=n(He);Ds=d(Br,"Pre-tokenize a "),It=o(Br,"CODE",{});var nn=n(It);As=d(nn,"~tokenizers.PyPreTokenizedString"),nn.forEach(r),Cs=d(Br," in-place"),Br.forEach(r),qs=c(ht),F=o(ht,"P",{});var tt=n(F);Is=d(tt,"This method allows to modify a "),Bt=o(tt,"CODE",{});var an=n(Bt);Bs=d(an,"PreTokenizedString"),an.forEach(r),Ns=d(tt,` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Nt=o(tt,"CODE",{});var ln=n(Nt);Ws=d(ln,"PreTokenizedString"),ln.forEach(r),Ls=d(tt,`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Wt=o(tt,"CODE",{});var pn=n(Wt);Hs=d(pn,"pre_tokenize_str()"),pn.forEach(r),tt.forEach(r),ht.forEach(r),Ms=c(K),R=o(K,"DIV",{class:!0});var dt=n(R);g(Me.$$.fragment,dt),Vs=c(dt),Lt=o(dt,"P",{});var cn=n(Lt);Us=d(cn,"Pre tokenize the given string"),cn.forEach(r),js=c(dt),G=o(dt,"P",{});var rt=n(G);Os=d(rt,`This method provides a way to visualize the effect of a
`),nt=o(rt,"A",{href:!0});var hn=n(nt);Fs=d(hn,"PreTokenizer"),hn.forEach(r),Rs=d(rt,` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Ht=o(rt,"CODE",{});var dn=n(Ht);Gs=d(dn,"PreTokenizedString"),dn.forEach(r),Ks=d(rt,`. If you need some of these, you can use
`),Mt=o(rt,"CODE",{});var fn=n(Mt);Js=d(fn,"pre_tokenize()"),fn.forEach(r),rt.forEach(r),dt.forEach(r),K.forEach(r),kr=c(e),ee=o(e,"H2",{class:!0});var Nr=n(ee);$e=o(Nr,"A",{id:!0,class:!0,href:!0});var un=n($e);Vt=o(un,"SPAN",{});var mn=n(Vt);g(Ve.$$.fragment,mn),mn.forEach(r),un.forEach(r),Qs=c(Nr),Ut=o(Nr,"SPAN",{});var kn=n(Ut);Xs=d(kn,"Punctuation"),kn.forEach(r),Nr.forEach(r),vr=c(e),te=o(e,"DIV",{class:!0});var Wr=n(te);g(Ue.$$.fragment,Wr),Ys=c(Wr),jt=o(Wr,"P",{});var vn=n(jt);Zs=d(vn,"This pre-tokenizer simply splits on punctuation as individual characters."),vn.forEach(r),Wr.forEach(r),$r=c(e),re=o(e,"H2",{class:!0});var Lr=n(re);ge=o(Lr,"A",{id:!0,class:!0,href:!0});var $n=n(ge);Ot=o($n,"SPAN",{});var gn=n(Ot);g(je.$$.fragment,gn),gn.forEach(r),$n.forEach(r),eo=c(Lr),Ft=o(Lr,"SPAN",{});var _n=n(Ft);to=d(_n,"Sequence"),_n.forEach(r),Lr.forEach(r),gr=c(e),se=o(e,"DIV",{class:!0});var Hr=n(se);g(Oe.$$.fragment,Hr),ro=c(Hr),Rt=o(Hr,"P",{});var zn=n(Rt);so=d(zn,"This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),zn.forEach(r),Hr.forEach(r),_r=c(e),oe=o(e,"H2",{class:!0});var Mr=n(oe);_e=o(Mr,"A",{id:!0,class:!0,href:!0});var wn=n(_e);Gt=o(wn,"SPAN",{});var bn=n(Gt);g(Fe.$$.fragment,bn),bn.forEach(r),wn.forEach(r),oo=c(Mr),Kt=o(Mr,"SPAN",{});var yn=n(Kt);no=d(yn,"Split"),yn.forEach(r),Mr.forEach(r),zr=c(e),V=o(e,"DIV",{class:!0});var ft=n(V);g(Re.$$.fragment,ft),ao=c(ft),Jt=o(ft,"P",{});var Pn=n(Jt);io=d(Pn,"Split PreTokenizer"),Pn.forEach(r),lo=c(ft),Qt=o(ft,"P",{});var En=n(Qt);po=d(En,`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),En.forEach(r),ft.forEach(r),wr=c(e),ne=o(e,"H2",{class:!0});var Vr=n(ne);ze=o(Vr,"A",{id:!0,class:!0,href:!0});var xn=n(ze);Xt=o(xn,"SPAN",{});var Tn=n(Xt);g(Ge.$$.fragment,Tn),Tn.forEach(r),xn.forEach(r),co=c(Vr),Yt=o(Vr,"SPAN",{});var Sn=n(Yt);ho=d(Sn,"UnicodeScripts"),Sn.forEach(r),Vr.forEach(r),br=c(e),ae=o(e,"DIV",{class:!0});var Ur=n(ae);g(Ke.$$.fragment,Ur),fo=c(Ur),Je=o(Ur,"P",{});var jr=n(Je);uo=d(jr,`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=o(jr,"A",{href:!0,rel:!0});var Dn=n(Qe);mo=d(Dn,"https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),Dn.forEach(r),ko=d(jr,`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),jr.forEach(r),Ur.forEach(r),yr=c(e),ie=o(e,"H2",{class:!0});var Or=n(ie);we=o(Or,"A",{id:!0,class:!0,href:!0});var An=n(we);Zt=o(An,"SPAN",{});var Cn=n(Zt);g(Xe.$$.fragment,Cn),Cn.forEach(r),An.forEach(r),vo=c(Or),er=o(Or,"SPAN",{});var qn=n(er);$o=d(qn,"Whitespace"),qn.forEach(r),Or.forEach(r),Pr=c(e),le=o(e,"DIV",{class:!0});var Fr=n(le);g(Ye.$$.fragment,Fr),go=c(Fr),at=o(Fr,"P",{});var To=n(at);_o=d(To,"This pre-tokenizer simply splits using the following regex: "),tr=o(To,"CODE",{});var In=n(tr);zo=d(In,"\\w+|[^\\w\\s]+"),In.forEach(r),To.forEach(r),Fr.forEach(r),Er=c(e),pe=o(e,"H2",{class:!0});var Rr=n(pe);be=o(Rr,"A",{id:!0,class:!0,href:!0});var Bn=n(be);rr=o(Bn,"SPAN",{});var Nn=n(rr);g(Ze.$$.fragment,Nn),Nn.forEach(r),Bn.forEach(r),wo=c(Rr),sr=o(Rr,"SPAN",{});var Wn=n(sr);bo=d(Wn,"WhitespaceSplit"),Wn.forEach(r),Rr.forEach(r),xr=c(e),ce=o(e,"DIV",{class:!0});var Gr=n(ce);g(et.$$.fragment,Gr),yo=c(Gr),it=o(Gr,"P",{});var So=n(it);Po=d(So,"This pre-tokenizer simply splits on the whitespace. Works like "),or=o(So,"CODE",{});var Ln=n(or);Eo=d(Ln,".split()"),Ln.forEach(r),So.forEach(r),Gr.forEach(r),this.h()},h(){l(v,"id","tokenizers.pre_tokenizers.BertPreTokenizer"),l(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(v,"href","#tokenizers.pre_tokenizers.BertPreTokenizer"),l(a,"class","relative group"),l(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(he,"id","tokenizers.pre_tokenizers.ByteLevel"),l(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(he,"href","#tokenizers.pre_tokenizers.ByteLevel"),l(H,"class","relative group"),l(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(de,"id","tokenizers.pre_tokenizers.CharDelimiterSplit"),l(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(de,"href","#tokenizers.pre_tokenizers.CharDelimiterSplit"),l(J,"class","relative group"),l(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fe,"id","tokenizers.pre_tokenizers.Digits"),l(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(fe,"href","#tokenizers.pre_tokenizers.Digits"),l(X,"class","relative group"),l(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ke,"id","tokenizers.pre_tokenizers.Metaspace"),l(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ke,"href","#tokenizers.pre_tokenizers.Metaspace"),l(Y,"class","relative group"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ve,"id","tokenizers.pre_tokenizers.PreTokenizer"),l(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ve,"href","#tokenizers.pre_tokenizers.PreTokenizer"),l(Z,"class","relative group"),l(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(nt,"href","/docs/tokenizers/v0.13.2/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),l(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l($e,"id","tokenizers.pre_tokenizers.Punctuation"),l($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($e,"href","#tokenizers.pre_tokenizers.Punctuation"),l(ee,"class","relative group"),l(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ge,"id","tokenizers.pre_tokenizers.Sequence"),l(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ge,"href","#tokenizers.pre_tokenizers.Sequence"),l(re,"class","relative group"),l(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(_e,"id","tokenizers.pre_tokenizers.Split"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#tokenizers.pre_tokenizers.Split"),l(oe,"class","relative group"),l(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ze,"id","tokenizers.pre_tokenizers.UnicodeScripts"),l(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ze,"href","#tokenizers.pre_tokenizers.UnicodeScripts"),l(ne,"class","relative group"),l(Qe,"href","https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),l(Qe,"rel","nofollow"),l(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(we,"id","tokenizers.pre_tokenizers.Whitespace"),l(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(we,"href","#tokenizers.pre_tokenizers.Whitespace"),l(ie,"class","relative group"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(be,"id","tokenizers.pre_tokenizers.WhitespaceSplit"),l(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(be,"href","#tokenizers.pre_tokenizers.WhitespaceSplit"),l(pe,"class","relative group"),l(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,f){m(e,a,f),t(a,v),t(v,i),_(u,i,null),t(a,y),t(a,k),t(k,P),m(e,C,f),m(e,T,f),_(W,T,null),t(T,L),t(T,U),t(U,E),t(T,A),t(T,q),t(q,st),m(e,Ee,f),m(e,H,f),t(H,he),t(he,ut),_(xe,ut,null),t(H,Kr),t(H,mt),t(mt,Jr),m(e,ir,f),m(e,I,f),_(Te,I,null),t(I,Qr),t(I,kt),t(kt,Xr),t(I,Yr),t(I,vt),t(vt,Zr),t(I,es),t(I,j),_(Se,j,null),t(j,ts),t(j,$t),t($t,rs),t(j,ss),t(j,gt),t(gt,os),m(e,lr,f),m(e,J,f),t(J,de),t(de,_t),_(De,_t,null),t(J,ns),t(J,zt),t(zt,as),m(e,pr,f),m(e,Q,f),_(Ae,Q,null),t(Q,is),t(Q,ot),t(ot,ls),t(ot,wt),t(wt,ps),m(e,cr,f),m(e,X,f),t(X,fe),t(fe,bt),_(Ce,bt,null),t(X,cs),t(X,yt),t(yt,hs),m(e,hr,f),m(e,B,f),_(qe,B,null),t(B,ds),t(B,Pt),t(Pt,fs),t(B,us),_(ue,B,null),t(B,ms),_(me,B,null),m(e,dr,f),m(e,Y,f),t(Y,ke),t(ke,Et),_(Ie,Et,null),t(Y,ks),t(Y,xt),t(xt,vs),m(e,fr,f),m(e,M,f),_(Be,M,null),t(M,$s),t(M,Tt),t(Tt,gs),t(M,_s),t(M,St),t(St,zs),m(e,ur,f),m(e,Z,f),t(Z,ve),t(ve,Dt),_(Ne,Dt,null),t(Z,ws),t(Z,At),t(At,bs),m(e,mr,f),m(e,S,f),_(We,S,null),t(S,ys),t(S,Ct),t(Ct,Ps),t(S,Es),t(S,qt),t(qt,xs),t(S,Ts),t(S,O),_(Le,O,null),t(O,Ss),t(O,He),t(He,Ds),t(He,It),t(It,As),t(He,Cs),t(O,qs),t(O,F),t(F,Is),t(F,Bt),t(Bt,Bs),t(F,Ns),t(F,Nt),t(Nt,Ws),t(F,Ls),t(F,Wt),t(Wt,Hs),t(S,Ms),t(S,R),_(Me,R,null),t(R,Vs),t(R,Lt),t(Lt,Us),t(R,js),t(R,G),t(G,Os),t(G,nt),t(nt,Fs),t(G,Rs),t(G,Ht),t(Ht,Gs),t(G,Ks),t(G,Mt),t(Mt,Js),m(e,kr,f),m(e,ee,f),t(ee,$e),t($e,Vt),_(Ve,Vt,null),t(ee,Qs),t(ee,Ut),t(Ut,Xs),m(e,vr,f),m(e,te,f),_(Ue,te,null),t(te,Ys),t(te,jt),t(jt,Zs),m(e,$r,f),m(e,re,f),t(re,ge),t(ge,Ot),_(je,Ot,null),t(re,eo),t(re,Ft),t(Ft,to),m(e,gr,f),m(e,se,f),_(Oe,se,null),t(se,ro),t(se,Rt),t(Rt,so),m(e,_r,f),m(e,oe,f),t(oe,_e),t(_e,Gt),_(Fe,Gt,null),t(oe,oo),t(oe,Kt),t(Kt,no),m(e,zr,f),m(e,V,f),_(Re,V,null),t(V,ao),t(V,Jt),t(Jt,io),t(V,lo),t(V,Qt),t(Qt,po),m(e,wr,f),m(e,ne,f),t(ne,ze),t(ze,Xt),_(Ge,Xt,null),t(ne,co),t(ne,Yt),t(Yt,ho),m(e,br,f),m(e,ae,f),_(Ke,ae,null),t(ae,fo),t(ae,Je),t(Je,uo),t(Je,Qe),t(Qe,mo),t(Je,ko),m(e,yr,f),m(e,ie,f),t(ie,we),t(we,Zt),_(Xe,Zt,null),t(ie,vo),t(ie,er),t(er,$o),m(e,Pr,f),m(e,le,f),_(Ye,le,null),t(le,go),t(le,at),t(at,_o),t(at,tr),t(tr,zo),m(e,Er,f),m(e,pe,f),t(pe,be),t(be,rr),_(Ze,rr,null),t(pe,wo),t(pe,sr),t(sr,bo),m(e,xr,f),m(e,ce,f),_(et,ce,null),t(ce,yo),t(ce,it),t(it,Po),t(it,or),t(or,Eo),Tr=!0},p(e,f){const nr={};f&2&&(nr.$$scope={dirty:f,ctx:e}),ue.$set(nr);const ar={};f&2&&(ar.$$scope={dirty:f,ctx:e}),me.$set(ar)},i(e){Tr||(z(u.$$.fragment,e),z(W.$$.fragment,e),z(xe.$$.fragment,e),z(Te.$$.fragment,e),z(Se.$$.fragment,e),z(De.$$.fragment,e),z(Ae.$$.fragment,e),z(Ce.$$.fragment,e),z(qe.$$.fragment,e),z(ue.$$.fragment,e),z(me.$$.fragment,e),z(Ie.$$.fragment,e),z(Be.$$.fragment,e),z(Ne.$$.fragment,e),z(We.$$.fragment,e),z(Le.$$.fragment,e),z(Me.$$.fragment,e),z(Ve.$$.fragment,e),z(Ue.$$.fragment,e),z(je.$$.fragment,e),z(Oe.$$.fragment,e),z(Fe.$$.fragment,e),z(Re.$$.fragment,e),z(Ge.$$.fragment,e),z(Ke.$$.fragment,e),z(Xe.$$.fragment,e),z(Ye.$$.fragment,e),z(Ze.$$.fragment,e),z(et.$$.fragment,e),Tr=!0)},o(e){w(u.$$.fragment,e),w(W.$$.fragment,e),w(xe.$$.fragment,e),w(Te.$$.fragment,e),w(Se.$$.fragment,e),w(De.$$.fragment,e),w(Ae.$$.fragment,e),w(Ce.$$.fragment,e),w(qe.$$.fragment,e),w(ue.$$.fragment,e),w(me.$$.fragment,e),w(Ie.$$.fragment,e),w(Be.$$.fragment,e),w(Ne.$$.fragment,e),w(We.$$.fragment,e),w(Le.$$.fragment,e),w(Me.$$.fragment,e),w(Ve.$$.fragment,e),w(Ue.$$.fragment,e),w(je.$$.fragment,e),w(Oe.$$.fragment,e),w(Fe.$$.fragment,e),w(Re.$$.fragment,e),w(Ge.$$.fragment,e),w(Ke.$$.fragment,e),w(Xe.$$.fragment,e),w(Ye.$$.fragment,e),w(Ze.$$.fragment,e),w(et.$$.fragment,e),Tr=!1},d(e){e&&r(a),b(u),e&&r(C),e&&r(T),b(W),e&&r(Ee),e&&r(H),b(xe),e&&r(ir),e&&r(I),b(Te),b(Se),e&&r(lr),e&&r(J),b(De),e&&r(pr),e&&r(Q),b(Ae),e&&r(cr),e&&r(X),b(Ce),e&&r(hr),e&&r(B),b(qe),b(ue),b(me),e&&r(dr),e&&r(Y),b(Ie),e&&r(fr),e&&r(M),b(Be),e&&r(ur),e&&r(Z),b(Ne),e&&r(mr),e&&r(S),b(We),b(Le),b(Me),e&&r(kr),e&&r(ee),b(Ve),e&&r(vr),e&&r(te),b(Ue),e&&r($r),e&&r(re),b(je),e&&r(gr),e&&r(se),b(Oe),e&&r(_r),e&&r(oe),b(Fe),e&&r(zr),e&&r(V),b(Re),e&&r(wr),e&&r(ne),b(Ge),e&&r(br),e&&r(ae),b(Ke),e&&r(yr),e&&r(ie),b(Xe),e&&r(Pr),e&&r(le),b(Ye),e&&r(Er),e&&r(pe),b(Ze),e&&r(xr),e&&r(ce),b(et)}}}function Xn(x){let a,v;return a=new Do({props:{$$slots:{default:[Qn]},$$scope:{ctx:x}}}),{c(){$(a.$$.fragment)},l(i){g(a.$$.fragment,i)},m(i,u){_(a,i,u),v=!0},p(i,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:i}),a.$set(y)},i(i){v||(z(a.$$.fragment,i),v=!0)},o(i){w(a.$$.fragment,i),v=!1},d(i){b(a,i)}}}function Yn(x){let a,v,i,u,y;return{c(){a=s("p"),v=h("The Rust API Reference is available directly on the "),i=s("a"),u=h("Docs.rs"),y=h(" website."),this.h()},l(k){a=o(k,"P",{});var P=n(a);v=d(P,"The Rust API Reference is available directly on the "),i=o(P,"A",{href:!0,rel:!0});var C=n(i);u=d(C,"Docs.rs"),C.forEach(r),y=d(P," website."),P.forEach(r),this.h()},h(){l(i,"href","https://docs.rs/tokenizers/latest/tokenizers/"),l(i,"rel","nofollow")},m(k,P){m(k,a,P),t(a,v),t(a,i),t(i,u),t(a,y)},d(k){k&&r(a)}}}function Zn(x){let a,v;return a=new Do({props:{$$slots:{default:[Yn]},$$scope:{ctx:x}}}),{c(){$(a.$$.fragment)},l(i){g(a.$$.fragment,i)},m(i,u){_(a,i,u),v=!0},p(i,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:i}),a.$set(y)},i(i){v||(z(a.$$.fragment,i),v=!0)},o(i){w(a.$$.fragment,i),v=!1},d(i){b(a,i)}}}function ea(x){let a,v;return{c(){a=s("p"),v=h("The node API has not been documented yet.")},l(i){a=o(i,"P",{});var u=n(a);v=d(u,"The node API has not been documented yet."),u.forEach(r)},m(i,u){m(i,a,u),t(a,v)},d(i){i&&r(a)}}}function ta(x){let a,v;return a=new Do({props:{$$slots:{default:[ea]},$$scope:{ctx:x}}}),{c(){$(a.$$.fragment)},l(i){g(a.$$.fragment,i)},m(i,u){_(a,i,u),v=!0},p(i,u){const y={};u&2&&(y.$$scope={dirty:u,ctx:i}),a.$set(y)},i(i){v||(z(a.$$.fragment,i),v=!0)},o(i){w(a.$$.fragment,i),v=!1},d(i){b(a,i)}}}function ra(x){let a,v,i,u,y,k,P,C,T,W,L,U;return k=new N({}),L=new Gn({props:{python:!0,rust:!0,node:!0,$$slots:{node:[ta],rust:[Zn],python:[Xn]},$$scope:{ctx:x}}}),{c(){a=s("meta"),v=p(),i=s("h1"),u=s("a"),y=s("span"),$(k.$$.fragment),P=p(),C=s("span"),T=h("Pre-tokenizers"),W=p(),$(L.$$.fragment),this.h()},l(E){const A=Fn('[data-svelte="svelte-1phssyn"]',document.head);a=o(A,"META",{name:!0,content:!0}),A.forEach(r),v=c(E),i=o(E,"H1",{class:!0});var q=n(i);u=o(q,"A",{id:!0,class:!0,href:!0});var st=n(u);y=o(st,"SPAN",{});var Ee=n(y);g(k.$$.fragment,Ee),Ee.forEach(r),st.forEach(r),P=c(q),C=o(q,"SPAN",{});var H=n(C);T=d(H,"Pre-tokenizers"),H.forEach(r),q.forEach(r),W=c(E),g(L.$$.fragment,E),this.h()},h(){l(a,"name","hf:doc:metadata"),l(a,"content",JSON.stringify(sa)),l(u,"id","pretokenizers"),l(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(u,"href","#pretokenizers"),l(i,"class","relative group")},m(E,A){t(document.head,a),m(E,v,A),m(E,i,A),t(i,u),t(u,y),_(k,y,null),t(i,P),t(i,C),t(C,T),m(E,W,A),_(L,E,A),U=!0},p(E,[A]){const q={};A&2&&(q.$$scope={dirty:A,ctx:E}),L.$set(q)},i(E){U||(z(k.$$.fragment,E),z(L.$$.fragment,E),U=!0)},o(E){w(k.$$.fragment,E),w(L.$$.fragment,E),U=!1},d(E){r(a),E&&r(v),E&&r(i),b(k),E&&r(W),b(L,E)}}}const sa={local:"pretokenizers",sections:[{local:"tokenizers.pre_tokenizers.BertPreTokenizer",title:"BertPreTokenizer"},{local:"tokenizers.pre_tokenizers.ByteLevel",title:"ByteLevel"},{local:"tokenizers.pre_tokenizers.CharDelimiterSplit",title:"CharDelimiterSplit"},{local:"tokenizers.pre_tokenizers.Digits",title:"Digits"},{local:"tokenizers.pre_tokenizers.Metaspace",title:"Metaspace"},{local:"tokenizers.pre_tokenizers.PreTokenizer",title:"PreTokenizer"},{local:"tokenizers.pre_tokenizers.Punctuation",title:"Punctuation"},{local:"tokenizers.pre_tokenizers.Sequence",title:"Sequence"},{local:"tokenizers.pre_tokenizers.Split",title:"Split"},{local:"tokenizers.pre_tokenizers.UnicodeScripts",title:"UnicodeScripts"},{local:"tokenizers.pre_tokenizers.Whitespace",title:"Whitespace"},{local:"tokenizers.pre_tokenizers.WhitespaceSplit",title:"WhitespaceSplit"}],title:"Pre-tokenizers"};function oa(x){return Rn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ha extends Un{constructor(a){super();jn(this,a,oa,ra,On,{})}}export{ha as default,sa as metadata};
