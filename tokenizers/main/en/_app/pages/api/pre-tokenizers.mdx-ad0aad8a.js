import{S as Gn,i as Jn,s as Qn,e as s,k as i,w as k,t as h,M as Xn,c as o,d as r,m as l,a as n,x as g,h as d,b as a,F as t,g as u,y as $,q as _,o as z,B as w,v as Yn,L as Zn}from"../../chunks/vendor-0d3f0756.js";import{D as S}from"../../chunks/Docstring-f99fb2a0.js";import{C as Kn}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as B}from"../../chunks/IconCopyLink-9193371d.js";import{T as ea,M as Io}from"../../chunks/TokenizersLanguageContent-ca787841.js";function ta(N){let c,v,p,m,y,P,D,C,T,W,L,j,b,A,q,st,ye,H,de,ut,Pe,Gr,mt,Jr,ir,I,Ee,Qr,vt,Xr,Yr,kt,Zr,es,O,Te,ts,gt,rs,ss,$t,os,lr,Q,fe,_t,xe,ns,zt,as,pr,X,Se,is,ot,ls,wt,ps,cr,Y,ue,bt,De,cs,yt,hs,hr,E,Ae,ds,Pt,fs,us,Et,ms,vs,Ce,ks,Tt,gs,$s,qe,dr,Z,me,xt,Ie,_s,St,zs,fr,V,Be,ws,Dt,bs,ys,At,Ps,ur,ee,ve,Ct,Ne,Es,qt,Ts,mr,x,We,xs,It,Ss,Ds,Bt,As,Cs,F,Le,qs,He,Is,Nt,Bs,Ns,Ws,R,Ls,Wt,Hs,Ms,Lt,Vs,Us,Ht,js,Os,K,Me,Fs,Mt,Rs,Ks,G,Gs,nt,Js,Qs,Vt,Xs,Ys,Ut,Zs,vr,te,ke,jt,Ve,eo,Ot,to,kr,re,Ue,ro,Ft,so,gr,se,ge,Rt,je,oo,Kt,no,$r,oe,Oe,ao,Gt,io,_r,ne,$e,Jt,Fe,lo,Qt,po,zr,U,Re,co,Xt,ho,fo,Yt,uo,wr,ae,_e,Zt,Ke,mo,er,vo,br,ie,Ge,ko,Je,go,Qe,$o,_o,yr,le,ze,tr,Xe,zo,rr,wo,Pr,pe,Ye,bo,at,yo,sr,Po,Er,ce,we,or,Ze,Eo,nr,To,Tr,he,et,xo,it,So,ar,Do,xr;return m=new B({}),W=new S({props:{name:"class tokenizers.pre_tokenizers.BertPreTokenizer",anchor:"tokenizers.pre_tokenizers.BertPreTokenizer",parameters:[]}}),Pe=new B({}),Ee=new S({props:{name:"class tokenizers.pre_tokenizers.ByteLevel",anchor:"tokenizers.pre_tokenizers.ByteLevel",parameters:[{name:"add_prefix_space",val:" = True"},{name:"use_regex",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.ByteLevel.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"},{anchor:"tokenizers.pre_tokenizers.ByteLevel.use_regex",description:`<strong>use_regex</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Set this to <code>False</code> to prevent this <em>pre_tokenizer</em> from using
the GPT2 specific regexp for spliting on whitespace.`,name:"use_regex"}]}}),Te=new S({props:{name:"alphabet",anchor:"tokenizers.pre_tokenizers.ByteLevel.alphabet",parameters:[],returnDescription:`
<p>A list of characters that compose the alphabet</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),xe=new B({}),Se=new S({props:{name:"class tokenizers.pre_tokenizers.CharDelimiterSplit",anchor:"tokenizers.pre_tokenizers.CharDelimiterSplit",parameters:""}}),De=new B({}),Ae=new S({props:{name:"class tokenizers.pre_tokenizers.Digits",anchor:"tokenizers.pre_tokenizers.Digits",parameters:[{name:"individual_digits",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Digits.individual_digits",description:"<strong>individual_digits</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;",name:"individual_digits"}]}}),Ce=new Kn({props:{code:'"Call 123 please" -> "Call ", "1", "2", "3", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;1&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>, <span class="hljs-string">&quot;3&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),qe=new Kn({props:{code:'"Call 123 please" -> "Call ", "123", " please"',highlighted:'<span class="hljs-string">&quot;Call 123 please&quot;</span> -&gt; <span class="hljs-string">&quot;Call &quot;</span>, <span class="hljs-string">&quot;123&quot;</span>, <span class="hljs-string">&quot; please&quot;</span>'}}),Ie=new B({}),Be=new S({props:{name:"class tokenizers.pre_tokenizers.Metaspace",anchor:"tokenizers.pre_tokenizers.Metaspace",parameters:[{name:"replacement",val:" = '_'"},{name:"add_prefix_space",val:" = True"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Metaspace.replacement",description:`<strong>replacement</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&#x2581;</code>) &#x2014;
The replacement character. Must be exactly one character. By default we
use the <em>&#x2581;</em> (U+2581) meta symbol (Same as in SentencePiece).`,name:"replacement"},{anchor:"tokenizers.pre_tokenizers.Metaspace.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a space to the first word if there isn&#x2019;t already one. This
lets us treat <em>hello</em> exactly like <em>say hello</em>.`,name:"add_prefix_space"}]}}),Ne=new B({}),We=new S({props:{name:"class tokenizers.pre_tokenizers.PreTokenizer",anchor:"tokenizers.pre_tokenizers.PreTokenizer",parameters:[]}}),Le=new S({props:{name:"pre_tokenize",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize",parameters:[{name:"pretok",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize.pretok",description:"<strong>pretok</strong> (<code>PreTokenizedString) -- The pre-tokenized string on which to apply this :class:</code>~tokenizers.pre_tokenizers.PreTokenizer`",name:"pretok"}]}}),Me=new S({props:{name:"pre_tokenize_str",anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.PreTokenizer.pre_tokenize_str.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A string to pre-tokeize`,name:"sequence"}],returnDescription:`
<p>A list of tuple with the pre-tokenized parts and their offsets</p>
`,returnType:`
<p><code>List[Tuple[str, Offsets]]</code></p>
`}}),Ve=new B({}),Ue=new S({props:{name:"class tokenizers.pre_tokenizers.Punctuation",anchor:"tokenizers.pre_tokenizers.Punctuation",parameters:[{name:"behavior",val:" = 'isolated'"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Punctuation.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D; (default), &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"}]}}),je=new B({}),Oe=new S({props:{name:"class tokenizers.pre_tokenizers.Sequence",anchor:"tokenizers.pre_tokenizers.Sequence",parameters:[{name:"pretokenizers",val:""}]}}),Fe=new B({}),Re=new S({props:{name:"class tokenizers.pre_tokenizers.Split",anchor:"tokenizers.pre_tokenizers.Split",parameters:[{name:"pattern",val:""},{name:"behavior",val:""},{name:"invert",val:" = False"}],parametersDescription:[{anchor:"tokenizers.pre_tokenizers.Split.pattern",description:`<strong>pattern</strong> (<code>str</code> or <code>Regex</code>) &#x2014;
A pattern used to split the string. Usually a string or a Regex`,name:"pattern"},{anchor:"tokenizers.pre_tokenizers.Split.behavior",description:`<strong>behavior</strong> (<code>SplitDelimiterBehavior</code>) &#x2014;
The behavior to use when splitting.
Choices: &#x201C;removed&#x201D;, &#x201C;isolated&#x201D;, &#x201C;merged_with_previous&#x201D;, &#x201C;merged_with_next&#x201D;,
&#x201C;contiguous&#x201D;`,name:"behavior"},{anchor:"tokenizers.pre_tokenizers.Split.invert",description:`<strong>invert</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to invert the pattern.`,name:"invert"}]}}),Ke=new B({}),Ge=new S({props:{name:"class tokenizers.pre_tokenizers.UnicodeScripts",anchor:"tokenizers.pre_tokenizers.UnicodeScripts",parameters:[]}}),Xe=new B({}),Ye=new S({props:{name:"class tokenizers.pre_tokenizers.Whitespace",anchor:"tokenizers.pre_tokenizers.Whitespace",parameters:[]}}),Ze=new B({}),et=new S({props:{name:"class tokenizers.pre_tokenizers.WhitespaceSplit",anchor:"tokenizers.pre_tokenizers.WhitespaceSplit",parameters:[]}}),{c(){c=s("h2"),v=s("a"),p=s("span"),k(m.$$.fragment),y=i(),P=s("span"),D=h("BertPreTokenizer"),C=i(),T=s("div"),k(W.$$.fragment),L=i(),j=s("p"),b=h("BertPreTokenizer"),A=i(),q=s("p"),st=h(`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),ye=i(),H=s("h2"),de=s("a"),ut=s("span"),k(Pe.$$.fragment),Gr=i(),mt=s("span"),Jr=h("ByteLevel"),ir=i(),I=s("div"),k(Ee.$$.fragment),Qr=i(),vt=s("p"),Xr=h("ByteLevel PreTokenizer"),Yr=i(),kt=s("p"),Zr=h(`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),es=i(),O=s("div"),k(Te.$$.fragment),ts=i(),gt=s("p"),rs=h("Returns the alphabet used by this PreTokenizer."),ss=i(),$t=s("p"),os=h(`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),lr=i(),Q=s("h2"),fe=s("a"),_t=s("span"),k(xe.$$.fragment),ns=i(),zt=s("span"),as=h("CharDelimiterSplit"),pr=i(),X=s("div"),k(Se.$$.fragment),is=i(),ot=s("p"),ls=h("This pre-tokenizer simply splits on the provided char. Works like "),wt=s("code"),ps=h(".split(delimiter)"),cr=i(),Y=s("h2"),ue=s("a"),bt=s("span"),k(De.$$.fragment),cs=i(),yt=s("span"),hs=h("Digits"),hr=i(),E=s("div"),k(Ae.$$.fragment),ds=i(),Pt=s("p"),fs=h("This pre-tokenizer simply splits using the digits in separate tokens"),us=i(),Et=s("p"),ms=h("If set to True, digits will each be separated as follows:"),vs=i(),k(Ce.$$.fragment),ks=i(),Tt=s("p"),gs=h("If set to False, digits will grouped as follows:"),$s=i(),k(qe.$$.fragment),dr=i(),Z=s("h2"),me=s("a"),xt=s("span"),k(Ie.$$.fragment),_s=i(),St=s("span"),zs=h("Metaspace"),fr=i(),V=s("div"),k(Be.$$.fragment),ws=i(),Dt=s("p"),bs=h("Metaspace pre-tokenizer"),ys=i(),At=s("p"),Ps=h(`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),ur=i(),ee=s("h2"),ve=s("a"),Ct=s("span"),k(Ne.$$.fragment),Es=i(),qt=s("span"),Ts=h("PreTokenizer"),mr=i(),x=s("div"),k(We.$$.fragment),xs=i(),It=s("p"),Ss=h("Base class for all pre-tokenizers"),Ds=i(),Bt=s("p"),As=h(`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),Cs=i(),F=s("div"),k(Le.$$.fragment),qs=i(),He=s("p"),Is=h("Pre-tokenize a "),Nt=s("code"),Bs=h("PyPreTokenizedString"),Ns=h(" in-place"),Ws=i(),R=s("p"),Ls=h("This method allows to modify a "),Wt=s("code"),Hs=h("PreTokenizedString"),Ms=h(` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Lt=s("code"),Vs=h("PreTokenizedString"),Us=h(`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Ht=s("code"),js=h("pre_tokenize_str()"),Os=i(),K=s("div"),k(Me.$$.fragment),Fs=i(),Mt=s("p"),Rs=h("Pre tokenize the given string"),Ks=i(),G=s("p"),Gs=h(`This method provides a way to visualize the effect of a
`),nt=s("a"),Js=h("PreTokenizer"),Qs=h(` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Vt=s("code"),Xs=h("PreTokenizedString"),Ys=h(`. If you need some of these, you can use
`),Ut=s("code"),Zs=h("pre_tokenize()"),vr=i(),te=s("h2"),ke=s("a"),jt=s("span"),k(Ve.$$.fragment),eo=i(),Ot=s("span"),to=h("Punctuation"),kr=i(),re=s("div"),k(Ue.$$.fragment),ro=i(),Ft=s("p"),so=h("This pre-tokenizer simply splits on punctuation as individual characters."),gr=i(),se=s("h2"),ge=s("a"),Rt=s("span"),k(je.$$.fragment),oo=i(),Kt=s("span"),no=h("Sequence"),$r=i(),oe=s("div"),k(Oe.$$.fragment),ao=i(),Gt=s("p"),io=h("This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),_r=i(),ne=s("h2"),$e=s("a"),Jt=s("span"),k(Fe.$$.fragment),lo=i(),Qt=s("span"),po=h("Split"),zr=i(),U=s("div"),k(Re.$$.fragment),co=i(),Xt=s("p"),ho=h("Split PreTokenizer"),fo=i(),Yt=s("p"),uo=h(`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),wr=i(),ae=s("h2"),_e=s("a"),Zt=s("span"),k(Ke.$$.fragment),mo=i(),er=s("span"),vo=h("UnicodeScripts"),br=i(),ie=s("div"),k(Ge.$$.fragment),ko=i(),Je=s("p"),go=h(`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=s("a"),$o=h("https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),_o=h(`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),yr=i(),le=s("h2"),ze=s("a"),tr=s("span"),k(Xe.$$.fragment),zo=i(),rr=s("span"),wo=h("Whitespace"),Pr=i(),pe=s("div"),k(Ye.$$.fragment),bo=i(),at=s("p"),yo=h("This pre-tokenizer simply splits using the following regex: "),sr=s("code"),Po=h("\\w+|[^\\w\\s]+"),Er=i(),ce=s("h2"),we=s("a"),or=s("span"),k(Ze.$$.fragment),Eo=i(),nr=s("span"),To=h("WhitespaceSplit"),Tr=i(),he=s("div"),k(et.$$.fragment),xo=i(),it=s("p"),So=h("This pre-tokenizer simply splits on the whitespace. Works like "),ar=s("code"),Do=h(".split()"),this.h()},l(e){c=o(e,"H2",{class:!0});var f=n(c);v=o(f,"A",{id:!0,class:!0,href:!0});var Bo=n(v);p=o(Bo,"SPAN",{});var No=n(p);g(m.$$.fragment,No),No.forEach(r),Bo.forEach(r),y=l(f),P=o(f,"SPAN",{});var Wo=n(P);D=d(Wo,"BertPreTokenizer"),Wo.forEach(r),f.forEach(r),C=l(e),T=o(e,"DIV",{class:!0});var lt=n(T);g(W.$$.fragment,lt),L=l(lt),j=o(lt,"P",{});var Lo=n(j);b=d(Lo,"BertPreTokenizer"),Lo.forEach(r),A=l(lt),q=o(lt,"P",{});var Ho=n(q);st=d(Ho,`This pre-tokenizer splits tokens on spaces, and also on punctuation.
Each occurence of a punctuation character will be treated separately.`),Ho.forEach(r),lt.forEach(r),ye=l(e),H=o(e,"H2",{class:!0});var Sr=n(H);de=o(Sr,"A",{id:!0,class:!0,href:!0});var Mo=n(de);ut=o(Mo,"SPAN",{});var Vo=n(ut);g(Pe.$$.fragment,Vo),Vo.forEach(r),Mo.forEach(r),Gr=l(Sr),mt=o(Sr,"SPAN",{});var Uo=n(mt);Jr=d(Uo,"ByteLevel"),Uo.forEach(r),Sr.forEach(r),ir=l(e),I=o(e,"DIV",{class:!0});var be=n(I);g(Ee.$$.fragment,be),Qr=l(be),vt=o(be,"P",{});var jo=n(vt);Xr=d(jo,"ByteLevel PreTokenizer"),jo.forEach(r),Yr=l(be),kt=o(be,"P",{});var Oo=n(kt);Zr=d(Oo,`This pre-tokenizer takes care of replacing all bytes of the given string
with a corresponding representation, as well as splitting into words.`),Oo.forEach(r),es=l(be),O=o(be,"DIV",{class:!0});var pt=n(O);g(Te.$$.fragment,pt),ts=l(pt),gt=o(pt,"P",{});var Fo=n(gt);rs=d(Fo,"Returns the alphabet used by this PreTokenizer."),Fo.forEach(r),ss=l(pt),$t=o(pt,"P",{});var Ro=n($t);os=d(Ro,`Since the ByteLevel works as its name suggests, at the byte level, it
encodes each byte value to a unique visible character. This means that there is a
total of 256 different characters composing this alphabet.`),Ro.forEach(r),pt.forEach(r),be.forEach(r),lr=l(e),Q=o(e,"H2",{class:!0});var Dr=n(Q);fe=o(Dr,"A",{id:!0,class:!0,href:!0});var Ko=n(fe);_t=o(Ko,"SPAN",{});var Go=n(_t);g(xe.$$.fragment,Go),Go.forEach(r),Ko.forEach(r),ns=l(Dr),zt=o(Dr,"SPAN",{});var Jo=n(zt);as=d(Jo,"CharDelimiterSplit"),Jo.forEach(r),Dr.forEach(r),pr=l(e),X=o(e,"DIV",{class:!0});var Ar=n(X);g(Se.$$.fragment,Ar),is=l(Ar),ot=o(Ar,"P",{});var Ao=n(ot);ls=d(Ao,"This pre-tokenizer simply splits on the provided char. Works like "),wt=o(Ao,"CODE",{});var Qo=n(wt);ps=d(Qo,".split(delimiter)"),Qo.forEach(r),Ao.forEach(r),Ar.forEach(r),cr=l(e),Y=o(e,"H2",{class:!0});var Cr=n(Y);ue=o(Cr,"A",{id:!0,class:!0,href:!0});var Xo=n(ue);bt=o(Xo,"SPAN",{});var Yo=n(bt);g(De.$$.fragment,Yo),Yo.forEach(r),Xo.forEach(r),cs=l(Cr),yt=o(Cr,"SPAN",{});var Zo=n(yt);hs=d(Zo,"Digits"),Zo.forEach(r),Cr.forEach(r),hr=l(e),E=o(e,"DIV",{class:!0});var M=n(E);g(Ae.$$.fragment,M),ds=l(M),Pt=o(M,"P",{});var en=n(Pt);fs=d(en,"This pre-tokenizer simply splits using the digits in separate tokens"),en.forEach(r),us=l(M),Et=o(M,"P",{});var tn=n(Et);ms=d(tn,"If set to True, digits will each be separated as follows:"),tn.forEach(r),vs=l(M),g(Ce.$$.fragment,M),ks=l(M),Tt=o(M,"P",{});var rn=n(Tt);gs=d(rn,"If set to False, digits will grouped as follows:"),rn.forEach(r),$s=l(M),g(qe.$$.fragment,M),M.forEach(r),dr=l(e),Z=o(e,"H2",{class:!0});var qr=n(Z);me=o(qr,"A",{id:!0,class:!0,href:!0});var sn=n(me);xt=o(sn,"SPAN",{});var on=n(xt);g(Ie.$$.fragment,on),on.forEach(r),sn.forEach(r),_s=l(qr),St=o(qr,"SPAN",{});var nn=n(St);zs=d(nn,"Metaspace"),nn.forEach(r),qr.forEach(r),fr=l(e),V=o(e,"DIV",{class:!0});var ct=n(V);g(Be.$$.fragment,ct),ws=l(ct),Dt=o(ct,"P",{});var an=n(Dt);bs=d(an,"Metaspace pre-tokenizer"),an.forEach(r),ys=l(ct),At=o(ct,"P",{});var ln=n(At);Ps=d(ln,`This pre-tokenizer replaces any whitespace by the provided replacement character.
It then tries to split on these spaces.`),ln.forEach(r),ct.forEach(r),ur=l(e),ee=o(e,"H2",{class:!0});var Ir=n(ee);ve=o(Ir,"A",{id:!0,class:!0,href:!0});var pn=n(ve);Ct=o(pn,"SPAN",{});var cn=n(Ct);g(Ne.$$.fragment,cn),cn.forEach(r),pn.forEach(r),Es=l(Ir),qt=o(Ir,"SPAN",{});var hn=n(qt);Ts=d(hn,"PreTokenizer"),hn.forEach(r),Ir.forEach(r),mr=l(e),x=o(e,"DIV",{class:!0});var J=n(x);g(We.$$.fragment,J),xs=l(J),It=o(J,"P",{});var dn=n(It);Ss=d(dn,"Base class for all pre-tokenizers"),dn.forEach(r),Ds=l(J),Bt=o(J,"P",{});var fn=n(Bt);As=d(fn,`This class is not supposed to be instantiated directly. Instead, any implementation of a
PreTokenizer will return an instance of this class when instantiated.`),fn.forEach(r),Cs=l(J),F=o(J,"DIV",{class:!0});var ht=n(F);g(Le.$$.fragment,ht),qs=l(ht),He=o(ht,"P",{});var Br=n(He);Is=d(Br,"Pre-tokenize a "),Nt=o(Br,"CODE",{});var un=n(Nt);Bs=d(un,"PyPreTokenizedString"),un.forEach(r),Ns=d(Br," in-place"),Br.forEach(r),Ws=l(ht),R=o(ht,"P",{});var tt=n(R);Ls=d(tt,"This method allows to modify a "),Wt=o(tt,"CODE",{});var mn=n(Wt);Hs=d(mn,"PreTokenizedString"),mn.forEach(r),Ms=d(tt,` to
keep track of the pre-tokenization, and leverage the capabilities of the
`),Lt=o(tt,"CODE",{});var vn=n(Lt);Vs=d(vn,"PreTokenizedString"),vn.forEach(r),Us=d(tt,`. If you just want to see the result of
the pre-tokenization of a raw string, you can use
`),Ht=o(tt,"CODE",{});var kn=n(Ht);js=d(kn,"pre_tokenize_str()"),kn.forEach(r),tt.forEach(r),ht.forEach(r),Os=l(J),K=o(J,"DIV",{class:!0});var dt=n(K);g(Me.$$.fragment,dt),Fs=l(dt),Mt=o(dt,"P",{});var gn=n(Mt);Rs=d(gn,"Pre tokenize the given string"),gn.forEach(r),Ks=l(dt),G=o(dt,"P",{});var rt=n(G);Gs=d(rt,`This method provides a way to visualize the effect of a
`),nt=o(rt,"A",{href:!0});var $n=n(nt);Js=d($n,"PreTokenizer"),$n.forEach(r),Qs=d(rt,` but it does not keep track of the
alignment, nor does it provide all the capabilities of the
`),Vt=o(rt,"CODE",{});var _n=n(Vt);Xs=d(_n,"PreTokenizedString"),_n.forEach(r),Ys=d(rt,`. If you need some of these, you can use
`),Ut=o(rt,"CODE",{});var zn=n(Ut);Zs=d(zn,"pre_tokenize()"),zn.forEach(r),rt.forEach(r),dt.forEach(r),J.forEach(r),vr=l(e),te=o(e,"H2",{class:!0});var Nr=n(te);ke=o(Nr,"A",{id:!0,class:!0,href:!0});var wn=n(ke);jt=o(wn,"SPAN",{});var bn=n(jt);g(Ve.$$.fragment,bn),bn.forEach(r),wn.forEach(r),eo=l(Nr),Ot=o(Nr,"SPAN",{});var yn=n(Ot);to=d(yn,"Punctuation"),yn.forEach(r),Nr.forEach(r),kr=l(e),re=o(e,"DIV",{class:!0});var Wr=n(re);g(Ue.$$.fragment,Wr),ro=l(Wr),Ft=o(Wr,"P",{});var Pn=n(Ft);so=d(Pn,"This pre-tokenizer simply splits on punctuation as individual characters."),Pn.forEach(r),Wr.forEach(r),gr=l(e),se=o(e,"H2",{class:!0});var Lr=n(se);ge=o(Lr,"A",{id:!0,class:!0,href:!0});var En=n(ge);Rt=o(En,"SPAN",{});var Tn=n(Rt);g(je.$$.fragment,Tn),Tn.forEach(r),En.forEach(r),oo=l(Lr),Kt=o(Lr,"SPAN",{});var xn=n(Kt);no=d(xn,"Sequence"),xn.forEach(r),Lr.forEach(r),$r=l(e),oe=o(e,"DIV",{class:!0});var Hr=n(oe);g(Oe.$$.fragment,Hr),ao=l(Hr),Gt=o(Hr,"P",{});var Sn=n(Gt);io=d(Sn,"This pre-tokenizer composes other pre_tokenizers and applies them in sequence"),Sn.forEach(r),Hr.forEach(r),_r=l(e),ne=o(e,"H2",{class:!0});var Mr=n(ne);$e=o(Mr,"A",{id:!0,class:!0,href:!0});var Dn=n($e);Jt=o(Dn,"SPAN",{});var An=n(Jt);g(Fe.$$.fragment,An),An.forEach(r),Dn.forEach(r),lo=l(Mr),Qt=o(Mr,"SPAN",{});var Cn=n(Qt);po=d(Cn,"Split"),Cn.forEach(r),Mr.forEach(r),zr=l(e),U=o(e,"DIV",{class:!0});var ft=n(U);g(Re.$$.fragment,ft),co=l(ft),Xt=o(ft,"P",{});var qn=n(Xt);ho=d(qn,"Split PreTokenizer"),qn.forEach(r),fo=l(ft),Yt=o(ft,"P",{});var In=n(Yt);uo=d(In,`This versatile pre-tokenizer splits using the provided pattern and
according to the provided behavior. The pattern can be inverted by
making use of the invert flag.`),In.forEach(r),ft.forEach(r),wr=l(e),ae=o(e,"H2",{class:!0});var Vr=n(ae);_e=o(Vr,"A",{id:!0,class:!0,href:!0});var Bn=n(_e);Zt=o(Bn,"SPAN",{});var Nn=n(Zt);g(Ke.$$.fragment,Nn),Nn.forEach(r),Bn.forEach(r),mo=l(Vr),er=o(Vr,"SPAN",{});var Wn=n(er);vo=d(Wn,"UnicodeScripts"),Wn.forEach(r),Vr.forEach(r),br=l(e),ie=o(e,"DIV",{class:!0});var Ur=n(ie);g(Ge.$$.fragment,Ur),ko=l(Ur),Je=o(Ur,"P",{});var jr=n(Je);go=d(jr,`This pre-tokenizer splits on characters that belong to different language family
It roughly follows `),Qe=o(jr,"A",{href:!0,rel:!0});var Ln=n(Qe);$o=d(Ln,"https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),Ln.forEach(r),_o=d(jr,`
Actually Hiragana and Katakana are fused with Han, and 0x30FC is Han too.
This mimicks SentencePiece Unigram implementation.`),jr.forEach(r),Ur.forEach(r),yr=l(e),le=o(e,"H2",{class:!0});var Or=n(le);ze=o(Or,"A",{id:!0,class:!0,href:!0});var Hn=n(ze);tr=o(Hn,"SPAN",{});var Mn=n(tr);g(Xe.$$.fragment,Mn),Mn.forEach(r),Hn.forEach(r),zo=l(Or),rr=o(Or,"SPAN",{});var Vn=n(rr);wo=d(Vn,"Whitespace"),Vn.forEach(r),Or.forEach(r),Pr=l(e),pe=o(e,"DIV",{class:!0});var Fr=n(pe);g(Ye.$$.fragment,Fr),bo=l(Fr),at=o(Fr,"P",{});var Co=n(at);yo=d(Co,"This pre-tokenizer simply splits using the following regex: "),sr=o(Co,"CODE",{});var Un=n(sr);Po=d(Un,"\\w+|[^\\w\\s]+"),Un.forEach(r),Co.forEach(r),Fr.forEach(r),Er=l(e),ce=o(e,"H2",{class:!0});var Rr=n(ce);we=o(Rr,"A",{id:!0,class:!0,href:!0});var jn=n(we);or=o(jn,"SPAN",{});var On=n(or);g(Ze.$$.fragment,On),On.forEach(r),jn.forEach(r),Eo=l(Rr),nr=o(Rr,"SPAN",{});var Fn=n(nr);To=d(Fn,"WhitespaceSplit"),Fn.forEach(r),Rr.forEach(r),Tr=l(e),he=o(e,"DIV",{class:!0});var Kr=n(he);g(et.$$.fragment,Kr),xo=l(Kr),it=o(Kr,"P",{});var qo=n(it);So=d(qo,"This pre-tokenizer simply splits on the whitespace. Works like "),ar=o(qo,"CODE",{});var Rn=n(ar);Do=d(Rn,".split()"),Rn.forEach(r),qo.forEach(r),Kr.forEach(r),this.h()},h(){a(v,"id","tokenizers.pre_tokenizers.BertPreTokenizer"),a(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(v,"href","#tokenizers.pre_tokenizers.BertPreTokenizer"),a(c,"class","relative group"),a(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(de,"id","tokenizers.pre_tokenizers.ByteLevel"),a(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(de,"href","#tokenizers.pre_tokenizers.ByteLevel"),a(H,"class","relative group"),a(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(fe,"id","tokenizers.pre_tokenizers.CharDelimiterSplit"),a(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(fe,"href","#tokenizers.pre_tokenizers.CharDelimiterSplit"),a(Q,"class","relative group"),a(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ue,"id","tokenizers.pre_tokenizers.Digits"),a(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ue,"href","#tokenizers.pre_tokenizers.Digits"),a(Y,"class","relative group"),a(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(me,"id","tokenizers.pre_tokenizers.Metaspace"),a(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(me,"href","#tokenizers.pre_tokenizers.Metaspace"),a(Z,"class","relative group"),a(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ve,"id","tokenizers.pre_tokenizers.PreTokenizer"),a(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ve,"href","#tokenizers.pre_tokenizers.PreTokenizer"),a(ee,"class","relative group"),a(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(nt,"href","/docs/tokenizers/main/en/api/pre-tokenizers#tokenizers.pre_tokenizers.PreTokenizer"),a(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ke,"id","tokenizers.pre_tokenizers.Punctuation"),a(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ke,"href","#tokenizers.pre_tokenizers.Punctuation"),a(te,"class","relative group"),a(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ge,"id","tokenizers.pre_tokenizers.Sequence"),a(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ge,"href","#tokenizers.pre_tokenizers.Sequence"),a(se,"class","relative group"),a(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a($e,"id","tokenizers.pre_tokenizers.Split"),a($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a($e,"href","#tokenizers.pre_tokenizers.Split"),a(ne,"class","relative group"),a(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(_e,"id","tokenizers.pre_tokenizers.UnicodeScripts"),a(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(_e,"href","#tokenizers.pre_tokenizers.UnicodeScripts"),a(ae,"class","relative group"),a(Qe,"href","https://github.com/google/sentencepiece/blob/master/data/Scripts.txt"),a(Qe,"rel","nofollow"),a(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(ze,"id","tokenizers.pre_tokenizers.Whitespace"),a(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(ze,"href","#tokenizers.pre_tokenizers.Whitespace"),a(le,"class","relative group"),a(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),a(we,"id","tokenizers.pre_tokenizers.WhitespaceSplit"),a(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(we,"href","#tokenizers.pre_tokenizers.WhitespaceSplit"),a(ce,"class","relative group"),a(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,f){u(e,c,f),t(c,v),t(v,p),$(m,p,null),t(c,y),t(c,P),t(P,D),u(e,C,f),u(e,T,f),$(W,T,null),t(T,L),t(T,j),t(j,b),t(T,A),t(T,q),t(q,st),u(e,ye,f),u(e,H,f),t(H,de),t(de,ut),$(Pe,ut,null),t(H,Gr),t(H,mt),t(mt,Jr),u(e,ir,f),u(e,I,f),$(Ee,I,null),t(I,Qr),t(I,vt),t(vt,Xr),t(I,Yr),t(I,kt),t(kt,Zr),t(I,es),t(I,O),$(Te,O,null),t(O,ts),t(O,gt),t(gt,rs),t(O,ss),t(O,$t),t($t,os),u(e,lr,f),u(e,Q,f),t(Q,fe),t(fe,_t),$(xe,_t,null),t(Q,ns),t(Q,zt),t(zt,as),u(e,pr,f),u(e,X,f),$(Se,X,null),t(X,is),t(X,ot),t(ot,ls),t(ot,wt),t(wt,ps),u(e,cr,f),u(e,Y,f),t(Y,ue),t(ue,bt),$(De,bt,null),t(Y,cs),t(Y,yt),t(yt,hs),u(e,hr,f),u(e,E,f),$(Ae,E,null),t(E,ds),t(E,Pt),t(Pt,fs),t(E,us),t(E,Et),t(Et,ms),t(E,vs),$(Ce,E,null),t(E,ks),t(E,Tt),t(Tt,gs),t(E,$s),$(qe,E,null),u(e,dr,f),u(e,Z,f),t(Z,me),t(me,xt),$(Ie,xt,null),t(Z,_s),t(Z,St),t(St,zs),u(e,fr,f),u(e,V,f),$(Be,V,null),t(V,ws),t(V,Dt),t(Dt,bs),t(V,ys),t(V,At),t(At,Ps),u(e,ur,f),u(e,ee,f),t(ee,ve),t(ve,Ct),$(Ne,Ct,null),t(ee,Es),t(ee,qt),t(qt,Ts),u(e,mr,f),u(e,x,f),$(We,x,null),t(x,xs),t(x,It),t(It,Ss),t(x,Ds),t(x,Bt),t(Bt,As),t(x,Cs),t(x,F),$(Le,F,null),t(F,qs),t(F,He),t(He,Is),t(He,Nt),t(Nt,Bs),t(He,Ns),t(F,Ws),t(F,R),t(R,Ls),t(R,Wt),t(Wt,Hs),t(R,Ms),t(R,Lt),t(Lt,Vs),t(R,Us),t(R,Ht),t(Ht,js),t(x,Os),t(x,K),$(Me,K,null),t(K,Fs),t(K,Mt),t(Mt,Rs),t(K,Ks),t(K,G),t(G,Gs),t(G,nt),t(nt,Js),t(G,Qs),t(G,Vt),t(Vt,Xs),t(G,Ys),t(G,Ut),t(Ut,Zs),u(e,vr,f),u(e,te,f),t(te,ke),t(ke,jt),$(Ve,jt,null),t(te,eo),t(te,Ot),t(Ot,to),u(e,kr,f),u(e,re,f),$(Ue,re,null),t(re,ro),t(re,Ft),t(Ft,so),u(e,gr,f),u(e,se,f),t(se,ge),t(ge,Rt),$(je,Rt,null),t(se,oo),t(se,Kt),t(Kt,no),u(e,$r,f),u(e,oe,f),$(Oe,oe,null),t(oe,ao),t(oe,Gt),t(Gt,io),u(e,_r,f),u(e,ne,f),t(ne,$e),t($e,Jt),$(Fe,Jt,null),t(ne,lo),t(ne,Qt),t(Qt,po),u(e,zr,f),u(e,U,f),$(Re,U,null),t(U,co),t(U,Xt),t(Xt,ho),t(U,fo),t(U,Yt),t(Yt,uo),u(e,wr,f),u(e,ae,f),t(ae,_e),t(_e,Zt),$(Ke,Zt,null),t(ae,mo),t(ae,er),t(er,vo),u(e,br,f),u(e,ie,f),$(Ge,ie,null),t(ie,ko),t(ie,Je),t(Je,go),t(Je,Qe),t(Qe,$o),t(Je,_o),u(e,yr,f),u(e,le,f),t(le,ze),t(ze,tr),$(Xe,tr,null),t(le,zo),t(le,rr),t(rr,wo),u(e,Pr,f),u(e,pe,f),$(Ye,pe,null),t(pe,bo),t(pe,at),t(at,yo),t(at,sr),t(sr,Po),u(e,Er,f),u(e,ce,f),t(ce,we),t(we,or),$(Ze,or,null),t(ce,Eo),t(ce,nr),t(nr,To),u(e,Tr,f),u(e,he,f),$(et,he,null),t(he,xo),t(he,it),t(it,So),t(it,ar),t(ar,Do),xr=!0},p:Zn,i(e){xr||(_(m.$$.fragment,e),_(W.$$.fragment,e),_(Pe.$$.fragment,e),_(Ee.$$.fragment,e),_(Te.$$.fragment,e),_(xe.$$.fragment,e),_(Se.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(Ce.$$.fragment,e),_(qe.$$.fragment,e),_(Ie.$$.fragment,e),_(Be.$$.fragment,e),_(Ne.$$.fragment,e),_(We.$$.fragment,e),_(Le.$$.fragment,e),_(Me.$$.fragment,e),_(Ve.$$.fragment,e),_(Ue.$$.fragment,e),_(je.$$.fragment,e),_(Oe.$$.fragment,e),_(Fe.$$.fragment,e),_(Re.$$.fragment,e),_(Ke.$$.fragment,e),_(Ge.$$.fragment,e),_(Xe.$$.fragment,e),_(Ye.$$.fragment,e),_(Ze.$$.fragment,e),_(et.$$.fragment,e),xr=!0)},o(e){z(m.$$.fragment,e),z(W.$$.fragment,e),z(Pe.$$.fragment,e),z(Ee.$$.fragment,e),z(Te.$$.fragment,e),z(xe.$$.fragment,e),z(Se.$$.fragment,e),z(De.$$.fragment,e),z(Ae.$$.fragment,e),z(Ce.$$.fragment,e),z(qe.$$.fragment,e),z(Ie.$$.fragment,e),z(Be.$$.fragment,e),z(Ne.$$.fragment,e),z(We.$$.fragment,e),z(Le.$$.fragment,e),z(Me.$$.fragment,e),z(Ve.$$.fragment,e),z(Ue.$$.fragment,e),z(je.$$.fragment,e),z(Oe.$$.fragment,e),z(Fe.$$.fragment,e),z(Re.$$.fragment,e),z(Ke.$$.fragment,e),z(Ge.$$.fragment,e),z(Xe.$$.fragment,e),z(Ye.$$.fragment,e),z(Ze.$$.fragment,e),z(et.$$.fragment,e),xr=!1},d(e){e&&r(c),w(m),e&&r(C),e&&r(T),w(W),e&&r(ye),e&&r(H),w(Pe),e&&r(ir),e&&r(I),w(Ee),w(Te),e&&r(lr),e&&r(Q),w(xe),e&&r(pr),e&&r(X),w(Se),e&&r(cr),e&&r(Y),w(De),e&&r(hr),e&&r(E),w(Ae),w(Ce),w(qe),e&&r(dr),e&&r(Z),w(Ie),e&&r(fr),e&&r(V),w(Be),e&&r(ur),e&&r(ee),w(Ne),e&&r(mr),e&&r(x),w(We),w(Le),w(Me),e&&r(vr),e&&r(te),w(Ve),e&&r(kr),e&&r(re),w(Ue),e&&r(gr),e&&r(se),w(je),e&&r($r),e&&r(oe),w(Oe),e&&r(_r),e&&r(ne),w(Fe),e&&r(zr),e&&r(U),w(Re),e&&r(wr),e&&r(ae),w(Ke),e&&r(br),e&&r(ie),w(Ge),e&&r(yr),e&&r(le),w(Xe),e&&r(Pr),e&&r(pe),w(Ye),e&&r(Er),e&&r(ce),w(Ze),e&&r(Tr),e&&r(he),w(et)}}}function ra(N){let c,v;return c=new Io({props:{$$slots:{default:[ta]},$$scope:{ctx:N}}}),{c(){k(c.$$.fragment)},l(p){g(c.$$.fragment,p)},m(p,m){$(c,p,m),v=!0},p(p,m){const y={};m&2&&(y.$$scope={dirty:m,ctx:p}),c.$set(y)},i(p){v||(_(c.$$.fragment,p),v=!0)},o(p){z(c.$$.fragment,p),v=!1},d(p){w(c,p)}}}function sa(N){let c,v,p,m,y;return{c(){c=s("p"),v=h("The Rust API Reference is available directly on the "),p=s("a"),m=h("Docs.rs"),y=h(" website."),this.h()},l(P){c=o(P,"P",{});var D=n(c);v=d(D,"The Rust API Reference is available directly on the "),p=o(D,"A",{href:!0,rel:!0});var C=n(p);m=d(C,"Docs.rs"),C.forEach(r),y=d(D," website."),D.forEach(r),this.h()},h(){a(p,"href","https://docs.rs/tokenizers/latest/tokenizers/"),a(p,"rel","nofollow")},m(P,D){u(P,c,D),t(c,v),t(c,p),t(p,m),t(c,y)},d(P){P&&r(c)}}}function oa(N){let c,v;return c=new Io({props:{$$slots:{default:[sa]},$$scope:{ctx:N}}}),{c(){k(c.$$.fragment)},l(p){g(c.$$.fragment,p)},m(p,m){$(c,p,m),v=!0},p(p,m){const y={};m&2&&(y.$$scope={dirty:m,ctx:p}),c.$set(y)},i(p){v||(_(c.$$.fragment,p),v=!0)},o(p){z(c.$$.fragment,p),v=!1},d(p){w(c,p)}}}function na(N){let c,v;return{c(){c=s("p"),v=h("The node API has not been documented yet.")},l(p){c=o(p,"P",{});var m=n(c);v=d(m,"The node API has not been documented yet."),m.forEach(r)},m(p,m){u(p,c,m),t(c,v)},d(p){p&&r(c)}}}function aa(N){let c,v;return c=new Io({props:{$$slots:{default:[na]},$$scope:{ctx:N}}}),{c(){k(c.$$.fragment)},l(p){g(c.$$.fragment,p)},m(p,m){$(c,p,m),v=!0},p(p,m){const y={};m&2&&(y.$$scope={dirty:m,ctx:p}),c.$set(y)},i(p){v||(_(c.$$.fragment,p),v=!0)},o(p){z(c.$$.fragment,p),v=!1},d(p){w(c,p)}}}function ia(N){let c,v,p,m,y,P,D,C,T,W,L,j;return P=new B({}),L=new ea({props:{python:!0,rust:!0,node:!0,$$slots:{node:[aa],rust:[oa],python:[ra]},$$scope:{ctx:N}}}),{c(){c=s("meta"),v=i(),p=s("h1"),m=s("a"),y=s("span"),k(P.$$.fragment),D=i(),C=s("span"),T=h("Pre-tokenizers"),W=i(),k(L.$$.fragment),this.h()},l(b){const A=Xn('[data-svelte="svelte-1phssyn"]',document.head);c=o(A,"META",{name:!0,content:!0}),A.forEach(r),v=l(b),p=o(b,"H1",{class:!0});var q=n(p);m=o(q,"A",{id:!0,class:!0,href:!0});var st=n(m);y=o(st,"SPAN",{});var ye=n(y);g(P.$$.fragment,ye),ye.forEach(r),st.forEach(r),D=l(q),C=o(q,"SPAN",{});var H=n(C);T=d(H,"Pre-tokenizers"),H.forEach(r),q.forEach(r),W=l(b),g(L.$$.fragment,b),this.h()},h(){a(c,"name","hf:doc:metadata"),a(c,"content",JSON.stringify(la)),a(m,"id","pretokenizers"),a(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),a(m,"href","#pretokenizers"),a(p,"class","relative group")},m(b,A){t(document.head,c),u(b,v,A),u(b,p,A),t(p,m),t(m,y),$(P,y,null),t(p,D),t(p,C),t(C,T),u(b,W,A),$(L,b,A),j=!0},p(b,[A]){const q={};A&2&&(q.$$scope={dirty:A,ctx:b}),L.$set(q)},i(b){j||(_(P.$$.fragment,b),_(L.$$.fragment,b),j=!0)},o(b){z(P.$$.fragment,b),z(L.$$.fragment,b),j=!1},d(b){r(c),b&&r(v),b&&r(p),w(P),b&&r(W),w(L,b)}}}const la={local:"pretokenizers",sections:[{local:"tokenizers.pre_tokenizers.BertPreTokenizer",title:"BertPreTokenizer"},{local:"tokenizers.pre_tokenizers.ByteLevel",title:"ByteLevel"},{local:"tokenizers.pre_tokenizers.CharDelimiterSplit",title:"CharDelimiterSplit"},{local:"tokenizers.pre_tokenizers.Digits",title:"Digits"},{local:"tokenizers.pre_tokenizers.Metaspace",title:"Metaspace"},{local:"tokenizers.pre_tokenizers.PreTokenizer",title:"PreTokenizer"},{local:"tokenizers.pre_tokenizers.Punctuation",title:"Punctuation"},{local:"tokenizers.pre_tokenizers.Sequence",title:"Sequence"},{local:"tokenizers.pre_tokenizers.Split",title:"Split"},{local:"tokenizers.pre_tokenizers.UnicodeScripts",title:"UnicodeScripts"},{local:"tokenizers.pre_tokenizers.Whitespace",title:"Whitespace"},{local:"tokenizers.pre_tokenizers.WhitespaceSplit",title:"WhitespaceSplit"}],title:"Pre-tokenizers"};function pa(N){return Yn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ma extends Gn{constructor(c){super();Jn(this,c,pa,ia,Qn,{})}}export{ma as default,la as metadata};
