import{S as fa,i as va,s as ua,e as r,k as c,w as v,t as s,M as ga,c as n,d as t,m,a,x as u,h as i,b as p,F as e,g as y,y as g,q as $,o as _,B as b,v as $a,L as _a}from"../../chunks/vendor-0d3f0756.js";import{D as W}from"../../chunks/Docstring-f99fb2a0.js";import{C as hn}from"../../chunks/CodeBlock-7b0cb15c.js";import{I as nt}from"../../chunks/IconCopyLink-9193371d.js";import{T as ba,M as fn}from"../../chunks/TokenizersLanguageContent-ca787841.js";function ka(S){let l,f,d,h,P,x,I,q,D,C,V,O,k,B,z,R,je,de,mo,po,at,ho,fo,$e,vo,U,uo,st,go,$o,it,_o,bo,Ge,ko,Eo,j,_e,yo,te,wo,dt,Po,zo,lt,xo,To,Do,ct,Wo,Gt,oe,le,mt,be,Ao,pt,Io,Ht,w,ke,Bo,ht,Lo,Mo,ft,qo,No,vt,So,Co,G,Ee,Vo,He,Oo,ut,Ro,Uo,re,jo,gt,Go,Ho,Fe,Fo,Jo,Ko,ce,ye,Qo,$t,Xo,Yo,H,we,Zo,_t,er,tr,bt,or,rr,me,Pe,nr,kt,ar,sr,pe,ze,ir,Et,dr,Ft,ne,he,yt,xe,lr,wt,cr,Jt,ae,Te,mr,Pt,pr,Kt,se,fe,zt,De,hr,xt,fr,Qt,A,We,vr,Tt,ur,gr,Dt,$r,_r,L,Ae,br,Wt,kr,Er,At,yr,wr,Ie,Pr,F,zr,It,xr,Tr,Bt,Dr,Wr,Je,Ar,Ir,J,Be,Br,Ke,Lr,Lt,Mr,qr,Mt,Nr,Xt,ie,ve,qt,Le,Sr,Nt,Cr,Yt,N,Me,Vr,St,Or,Rr,M,qe,Ur,Ct,jr,Gr,Vt,Hr,Fr,Ne,Jr,K,Kr,Ot,Qr,Xr,Rt,Yr,Zr,Qe,en,tn,Q,Se,on,Ce,rn,Ut,nn,an,sn,Ve,dn,jt,ln,cn,Zt;return h=new nt({}),C=new W({props:{name:"class tokenizers.models.BPE",anchor:"tokenizers.models.BPE",parameters:[{name:"vocab",val:" = None"},{name:"merges",val:" = None"},{name:"cache_capacity",val:" = None"},{name:"dropout",val:" = None"},{name:"unk_token",val:" = None"},{name:"continuing_subword_prefix",val:" = None"},{name:"end_of_word_suffix",val:" = None"},{name:"fuse_unk",val:" = None"}],parametersDescription:[{anchor:"tokenizers.models.BPE.vocab",description:`<strong>vocab</strong> (<code>Dict[str, int]</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.BPE.merges",description:`<strong>merges</strong> (<code>List[Tuple[str, str]]</code>, <em>optional</em>) &#x2014;
A list of pairs of tokens (<code>Tuple[str, str]</code>) <code>[(&quot;a&quot;, &quot;b&quot;),...]</code>`,name:"merges"},{anchor:"tokenizers.models.BPE.cache_capacity",description:`<strong>cache_capacity</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of words that the BPE cache can contain. The cache allows
to speed-up the process by keeping the result of the merge operations
for a number of words.`,name:"cache_capacity"},{anchor:"tokenizers.models.BPE.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>) &#x2014;
A float between 0 and 1 that represents the BPE dropout to use.`,name:"dropout"},{anchor:"tokenizers.models.BPE.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"},{anchor:"tokenizers.models.BPE.continuing_subword_prefix",description:`<strong>continuing_subword_prefix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The prefix to attach to subword units that don&#x2019;t represent a beginning of word.`,name:"continuing_subword_prefix"},{anchor:"tokenizers.models.BPE.end_of_word_suffix",description:`<strong>end_of_word_suffix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The suffix to attach to subword units that represent an end of word.`,name:"end_of_word_suffix"},{anchor:"tokenizers.models.BPE.fuse_unk",description:`<strong>fuse_unk</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to fuse any subsequent unknown tokens into a single one`,name:"fuse_unk"}]}}),R=new W({props:{name:"from_file",anchor:"tokenizers.models.BPE.from_file",parameters:[{name:"vocab",val:""},{name:"merge",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"tokenizers.models.BPE.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"},{anchor:"tokenizers.models.BPE.from_file.merges",description:`<strong>merges</strong> (<code>str</code>) &#x2014;
The path to a <code>merges.txt</code> file`,name:"merges"}],returnDescription:`
<p>An instance of BPE loaded from these files</p>
`,returnType:`
<p><a href="/docs/tokenizers/main/en/api/models#tokenizers.models.BPE">BPE</a></p>
`}}),$e=new hn({props:{code:`vocab, merges = BPE.read_file(vocab_filename, merges_filename)
bpe = BPE(vocab, merges)`,highlighted:`vocab, merges = BPE.read_file(vocab_filename, merges_filename)
bpe = BPE(vocab, merges)`}}),_e=new W({props:{name:"read_file",anchor:"tokenizers.models.BPE.read_file",parameters:[{name:"vocab",val:""},{name:"merges",val:""}],parametersDescription:[{anchor:"tokenizers.models.BPE.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"},{anchor:"tokenizers.models.BPE.read_file.merges",description:`<strong>merges</strong> (<code>str</code>) &#x2014;
The path to a <code>merges.txt</code> file`,name:"merges"}],returnDescription:`
<p>The vocabulary and merges loaded into memory</p>
`,returnType:`
<p>A <code>Tuple</code> with the vocab and the merges</p>
`}}),be=new nt({}),ke=new W({props:{name:"class tokenizers.models.Model",anchor:"tokenizers.models.Model",parameters:""}}),Ee=new W({props:{name:"get_trainer",anchor:"tokenizers.models.Model.get_trainer",parameters:"",returnDescription:`
<p>The Trainer used to train this model</p>
`,returnType:`
<p><code>Trainer</code></p>
`}}),ye=new W({props:{name:"id_to_token",anchor:"tokenizers.models.Model.id_to_token",parameters:[{name:"id",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.id_to_token.id",description:`<strong>id</strong> (<code>int</code>) &#x2014;
An ID to convert to a token`,name:"id"}],returnDescription:`
<p>The token associated to the ID</p>
`,returnType:`
<p><code>str</code></p>
`}}),we=new W({props:{name:"save",anchor:"tokenizers.models.Model.save",parameters:[{name:"folder",val:""},{name:"prefix",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.save.folder",description:`<strong>folder</strong> (<code>str</code>) &#x2014;
The path to the target folder in which to save the various files`,name:"folder"},{anchor:"tokenizers.models.Model.save.prefix",description:`<strong>prefix</strong> (<code>str</code>, <em>optional</em>) &#x2014;
An optional prefix, used to prefix each file name`,name:"prefix"}],returnDescription:`
<p>The list of saved files</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),Pe=new W({props:{name:"token_to_id",anchor:"tokenizers.models.Model.token_to_id",parameters:[{name:"tokens",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.token_to_id.token",description:`<strong>token</strong> (<code>str</code>) &#x2014;
A token to convert to an ID`,name:"token"}],returnDescription:`
<p>The ID associated to the token</p>
`,returnType:`
<p><code>int</code></p>
`}}),ze=new W({props:{name:"tokenize",anchor:"tokenizers.models.Model.tokenize",parameters:[{name:"sequence",val:""}],parametersDescription:[{anchor:"tokenizers.models.Model.tokenize.sequence",description:`<strong>sequence</strong> (<code>str</code>) &#x2014;
A sequence to tokenize`,name:"sequence"}],returnDescription:`
<p>The generated tokens</p>
`,returnType:`
<p>A <code>List</code> of <code>Token</code></p>
`}}),xe=new nt({}),Te=new W({props:{name:"class tokenizers.models.Unigram",anchor:"tokenizers.models.Unigram",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.Unigram.vocab",description:`<strong>vocab</strong> (<code>List[Tuple[str, float]]</code>, <em>optional</em>) &#x2014;
A list of vocabulary items and their relative score [(&#x201C;am&#x201D;, -0.2442),&#x2026;]`,name:"vocab"}]}}),De=new nt({}),We=new W({props:{name:"class tokenizers.models.WordLevel",anchor:"tokenizers.models.WordLevel",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.vocab",description:`<strong>vocab</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.WordLevel.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"}]}}),Ae=new W({props:{name:"from_file",anchor:"tokenizers.models.WordLevel.from_file",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"}],returnDescription:`
<p>An instance of WordLevel loaded from file</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/main/en/api/models#tokenizers.models.WordLevel"
>WordLevel</a></p>
`}}),Ie=new hn({props:{code:`vocab = WordLevel.read_file(vocab_filename)
wordlevel = WordLevel(vocab)`,highlighted:`vocab = WordLevel.read_file(vocab_filename)
wordlevel = WordLevel(vocab)`}}),Be=new W({props:{name:"read_file",anchor:"tokenizers.models.WordLevel.read_file",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordLevel.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.json</code> file`,name:"vocab"}],returnDescription:`
<p>The vocabulary as a <code>dict</code></p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),Le=new nt({}),Me=new W({props:{name:"class tokenizers.models.WordPiece",anchor:"tokenizers.models.WordPiece",parameters:[{name:"vocab",val:""},{name:"unk_token",val:""},{name:"max_input_chars_per_word",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.vocab",description:`<strong>vocab</strong> (<code>Dict[str, int]</code>, <em>optional</em>) &#x2014;
A dictionnary of string keys and their ids <code>{&quot;am&quot;: 0,...}</code>`,name:"vocab"},{anchor:"tokenizers.models.WordPiece.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The unknown token to be used by the model.`,name:"unk_token"},{anchor:"tokenizers.models.WordPiece.max_input_chars_per_word",description:`<strong>max_input_chars_per_word</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum number of characters to authorize in a single word.`,name:"max_input_chars_per_word"}]}}),qe=new W({props:{name:"from_file",anchor:"tokenizers.models.WordPiece.from_file",parameters:[{name:"vocab",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.from_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.txt</code> file`,name:"vocab"}],returnDescription:`
<p>An instance of WordPiece loaded from file</p>
`,returnType:`
<p><a
  href="/docs/tokenizers/main/en/api/models#tokenizers.models.WordPiece"
>WordPiece</a></p>
`}}),Ne=new hn({props:{code:`vocab = WordPiece.read_file(vocab_filename)
wordpiece = WordPiece(vocab)`,highlighted:`vocab = WordPiece.read_file(vocab_filename)
wordpiece = WordPiece(vocab)`}}),Se=new W({props:{name:"read_file",anchor:"tokenizers.models.WordPiece.read_file",parameters:[{name:"vocab",val:""}],parametersDescription:[{anchor:"tokenizers.models.WordPiece.read_file.vocab",description:`<strong>vocab</strong> (<code>str</code>) &#x2014;
The path to a <code>vocab.txt</code> file`,name:"vocab"}],returnDescription:`
<p>The vocabulary as a <code>dict</code></p>
`,returnType:`
<p><code>Dict[str, int]</code></p>
`}}),{c(){l=r("h2"),f=r("a"),d=r("span"),v(h.$$.fragment),P=c(),x=r("span"),I=s("BPE"),q=c(),D=r("div"),v(C.$$.fragment),V=c(),O=r("p"),k=s("An implementation of the BPE (Byte-Pair Encoding) algorithm"),B=c(),z=r("div"),v(R.$$.fragment),je=c(),de=r("p"),mo=s("Instantiate a BPE model from the given files."),po=c(),at=r("p"),ho=s("This method is roughly equivalent to doing:"),fo=c(),v($e.$$.fragment),vo=c(),U=r("p"),uo=s("If you don\u2019t need to keep the "),st=r("code"),go=s("vocab, merges"),$o=s(` values lying around,
this method is more optimized than manually calling
`),it=r("code"),_o=s("read_file()"),bo=s(" to initialize a "),Ge=r("a"),ko=s("BPE"),Eo=c(),j=r("div"),v(_e.$$.fragment),yo=c(),te=r("p"),wo=s("Read a "),dt=r("code"),Po=s("vocab.json"),zo=s(" and a "),lt=r("code"),xo=s("merges.txt"),To=s(" files"),Do=c(),ct=r("p"),Wo=s(`This method provides a way to read and parse the content of these files,
returning the relevant data structures. If you want to instantiate some BPE models
from memory, this method gives you the expected input from the standard files.`),Gt=c(),oe=r("h2"),le=r("a"),mt=r("span"),v(be.$$.fragment),Ao=c(),pt=r("span"),Io=s("Model"),Ht=c(),w=r("div"),v(ke.$$.fragment),Bo=c(),ht=r("p"),Lo=s("Base class for all models"),Mo=c(),ft=r("p"),qo=s(`The model represents the actual tokenization algorithm. This is the part that
will contain and manage the learned vocabulary.`),No=c(),vt=r("p"),So=s("This class cannot be constructed directly. Please use one of the concrete models."),Co=c(),G=r("div"),v(Ee.$$.fragment),Vo=c(),He=r("p"),Oo=s("Get the associated "),ut=r("code"),Ro=s("Trainer"),Uo=c(),re=r("p"),jo=s("Retrieve the "),gt=r("code"),Go=s("Trainer"),Ho=s(` associated to this
`),Fe=r("a"),Fo=s("Model"),Jo=s("."),Ko=c(),ce=r("div"),v(ye.$$.fragment),Qo=c(),$t=r("p"),Xo=s("Get the token associated to an ID"),Yo=c(),H=r("div"),v(we.$$.fragment),Zo=c(),_t=r("p"),er=s("Save the current model"),tr=c(),bt=r("p"),or=s(`Save the current model in the given folder, using the given prefix for the various
files that will get created.
Any file with the same name that already exists in this folder will be overwritten.`),rr=c(),me=r("div"),v(Pe.$$.fragment),nr=c(),kt=r("p"),ar=s("Get the ID associated to a token"),sr=c(),pe=r("div"),v(ze.$$.fragment),ir=c(),Et=r("p"),dr=s("Tokenize a sequence"),Ft=c(),ne=r("h2"),he=r("a"),yt=r("span"),v(xe.$$.fragment),lr=c(),wt=r("span"),cr=s("Unigram"),Jt=c(),ae=r("div"),v(Te.$$.fragment),mr=c(),Pt=r("p"),pr=s("An implementation of the Unigram algorithm"),Kt=c(),se=r("h2"),fe=r("a"),zt=r("span"),v(De.$$.fragment),hr=c(),xt=r("span"),fr=s("WordLevel"),Qt=c(),A=r("div"),v(We.$$.fragment),vr=c(),Tt=r("p"),ur=s("An implementation of the WordLevel algorithm"),gr=c(),Dt=r("p"),$r=s("Most simple tokenizer model based on mapping tokens to their corresponding id."),_r=c(),L=r("div"),v(Ae.$$.fragment),br=c(),Wt=r("p"),kr=s("Instantiate a WordLevel model from the given file"),Er=c(),At=r("p"),yr=s("This method is roughly equivalent to doing:"),wr=c(),v(Ie.$$.fragment),Pr=c(),F=r("p"),zr=s("If you don\u2019t need to keep the "),It=r("code"),xr=s("vocab"),Tr=s(` values lying around, this method is
more optimized than manually calling `),Bt=r("code"),Dr=s("read_file()"),Wr=s(` to
initialize a `),Je=r("a"),Ar=s("WordLevel"),Ir=c(),J=r("div"),v(Be.$$.fragment),Br=c(),Ke=r("p"),Lr=s("Read a "),Lt=r("code"),Mr=s("vocab.json"),qr=c(),Mt=r("p"),Nr=s(`This method provides a way to read and parse the content of a vocabulary file,
returning the relevant data structures. If you want to instantiate some WordLevel models
from memory, this method gives you the expected input from the standard files.`),Xt=c(),ie=r("h2"),ve=r("a"),qt=r("span"),v(Le.$$.fragment),Sr=c(),Nt=r("span"),Cr=s("WordPiece"),Yt=c(),N=r("div"),v(Me.$$.fragment),Vr=c(),St=r("p"),Or=s("An implementation of the WordPiece algorithm"),Rr=c(),M=r("div"),v(qe.$$.fragment),Ur=c(),Ct=r("p"),jr=s("Instantiate a WordPiece model from the given file"),Gr=c(),Vt=r("p"),Hr=s("This method is roughly equivalent to doing:"),Fr=c(),v(Ne.$$.fragment),Jr=c(),K=r("p"),Kr=s("If you don\u2019t need to keep the "),Ot=r("code"),Qr=s("vocab"),Xr=s(` values lying around, this method is
more optimized than manually calling `),Rt=r("code"),Yr=s("read_file()"),Zr=s(` to
initialize a `),Qe=r("a"),en=s("WordPiece"),tn=c(),Q=r("div"),v(Se.$$.fragment),on=c(),Ce=r("p"),rn=s("Read a "),Ut=r("code"),nn=s("vocab.txt"),an=s(" file"),sn=c(),Ve=r("p"),dn=s("This method provides a way to read and parse the content of a standard "),jt=r("em"),ln=s("vocab.txt"),cn=s(`
file as used by the WordPiece Model, returning the relevant data structures. If you
want to instantiate some WordPiece models from memory, this method gives you the
expected input from the standard files.`),this.h()},l(o){l=n(o,"H2",{class:!0});var E=a(l);f=n(E,"A",{id:!0,class:!0,href:!0});var vn=a(f);d=n(vn,"SPAN",{});var un=a(d);u(h.$$.fragment,un),un.forEach(t),vn.forEach(t),P=m(E),x=n(E,"SPAN",{});var gn=a(x);I=i(gn,"BPE"),gn.forEach(t),E.forEach(t),q=m(o),D=n(o,"DIV",{class:!0});var ue=a(D);u(C.$$.fragment,ue),V=m(ue),O=n(ue,"P",{});var $n=a(O);k=i($n,"An implementation of the BPE (Byte-Pair Encoding) algorithm"),$n.forEach(t),B=m(ue),z=n(ue,"DIV",{class:!0});var X=a(z);u(R.$$.fragment,X),je=m(X),de=n(X,"P",{});var _n=a(de);mo=i(_n,"Instantiate a BPE model from the given files."),_n.forEach(t),po=m(X),at=n(X,"P",{});var bn=a(at);ho=i(bn,"This method is roughly equivalent to doing:"),bn.forEach(t),fo=m(X),u($e.$$.fragment,X),vo=m(X),U=n(X,"P",{});var Oe=a(U);uo=i(Oe,"If you don\u2019t need to keep the "),st=n(Oe,"CODE",{});var kn=a(st);go=i(kn,"vocab, merges"),kn.forEach(t),$o=i(Oe,` values lying around,
this method is more optimized than manually calling
`),it=n(Oe,"CODE",{});var En=a(it);_o=i(En,"read_file()"),En.forEach(t),bo=i(Oe," to initialize a "),Ge=n(Oe,"A",{href:!0});var yn=a(Ge);ko=i(yn,"BPE"),yn.forEach(t),Oe.forEach(t),X.forEach(t),Eo=m(ue),j=n(ue,"DIV",{class:!0});var Xe=a(j);u(_e.$$.fragment,Xe),yo=m(Xe),te=n(Xe,"P",{});var Ye=a(te);wo=i(Ye,"Read a "),dt=n(Ye,"CODE",{});var wn=a(dt);Po=i(wn,"vocab.json"),wn.forEach(t),zo=i(Ye," and a "),lt=n(Ye,"CODE",{});var Pn=a(lt);xo=i(Pn,"merges.txt"),Pn.forEach(t),To=i(Ye," files"),Ye.forEach(t),Do=m(Xe),ct=n(Xe,"P",{});var zn=a(ct);Wo=i(zn,`This method provides a way to read and parse the content of these files,
returning the relevant data structures. If you want to instantiate some BPE models
from memory, this method gives you the expected input from the standard files.`),zn.forEach(t),Xe.forEach(t),ue.forEach(t),Gt=m(o),oe=n(o,"H2",{class:!0});var eo=a(oe);le=n(eo,"A",{id:!0,class:!0,href:!0});var xn=a(le);mt=n(xn,"SPAN",{});var Tn=a(mt);u(be.$$.fragment,Tn),Tn.forEach(t),xn.forEach(t),Ao=m(eo),pt=n(eo,"SPAN",{});var Dn=a(pt);Io=i(Dn,"Model"),Dn.forEach(t),eo.forEach(t),Ht=m(o),w=n(o,"DIV",{class:!0});var T=a(w);u(ke.$$.fragment,T),Bo=m(T),ht=n(T,"P",{});var Wn=a(ht);Lo=i(Wn,"Base class for all models"),Wn.forEach(t),Mo=m(T),ft=n(T,"P",{});var An=a(ft);qo=i(An,`The model represents the actual tokenization algorithm. This is the part that
will contain and manage the learned vocabulary.`),An.forEach(t),No=m(T),vt=n(T,"P",{});var In=a(vt);So=i(In,"This class cannot be constructed directly. Please use one of the concrete models."),In.forEach(t),Co=m(T),G=n(T,"DIV",{class:!0});var Ze=a(G);u(Ee.$$.fragment,Ze),Vo=m(Ze),He=n(Ze,"P",{});var mn=a(He);Oo=i(mn,"Get the associated "),ut=n(mn,"CODE",{});var Bn=a(ut);Ro=i(Bn,"Trainer"),Bn.forEach(t),mn.forEach(t),Uo=m(Ze),re=n(Ze,"P",{});var et=a(re);jo=i(et,"Retrieve the "),gt=n(et,"CODE",{});var Ln=a(gt);Go=i(Ln,"Trainer"),Ln.forEach(t),Ho=i(et,` associated to this
`),Fe=n(et,"A",{href:!0});var Mn=a(Fe);Fo=i(Mn,"Model"),Mn.forEach(t),Jo=i(et,"."),et.forEach(t),Ze.forEach(t),Ko=m(T),ce=n(T,"DIV",{class:!0});var to=a(ce);u(ye.$$.fragment,to),Qo=m(to),$t=n(to,"P",{});var qn=a($t);Xo=i(qn,"Get the token associated to an ID"),qn.forEach(t),to.forEach(t),Yo=m(T),H=n(T,"DIV",{class:!0});var tt=a(H);u(we.$$.fragment,tt),Zo=m(tt),_t=n(tt,"P",{});var Nn=a(_t);er=i(Nn,"Save the current model"),Nn.forEach(t),tr=m(tt),bt=n(tt,"P",{});var Sn=a(bt);or=i(Sn,`Save the current model in the given folder, using the given prefix for the various
files that will get created.
Any file with the same name that already exists in this folder will be overwritten.`),Sn.forEach(t),tt.forEach(t),rr=m(T),me=n(T,"DIV",{class:!0});var oo=a(me);u(Pe.$$.fragment,oo),nr=m(oo),kt=n(oo,"P",{});var Cn=a(kt);ar=i(Cn,"Get the ID associated to a token"),Cn.forEach(t),oo.forEach(t),sr=m(T),pe=n(T,"DIV",{class:!0});var ro=a(pe);u(ze.$$.fragment,ro),ir=m(ro),Et=n(ro,"P",{});var Vn=a(Et);dr=i(Vn,"Tokenize a sequence"),Vn.forEach(t),ro.forEach(t),T.forEach(t),Ft=m(o),ne=n(o,"H2",{class:!0});var no=a(ne);he=n(no,"A",{id:!0,class:!0,href:!0});var On=a(he);yt=n(On,"SPAN",{});var Rn=a(yt);u(xe.$$.fragment,Rn),Rn.forEach(t),On.forEach(t),lr=m(no),wt=n(no,"SPAN",{});var Un=a(wt);cr=i(Un,"Unigram"),Un.forEach(t),no.forEach(t),Jt=m(o),ae=n(o,"DIV",{class:!0});var ao=a(ae);u(Te.$$.fragment,ao),mr=m(ao),Pt=n(ao,"P",{});var jn=a(Pt);pr=i(jn,"An implementation of the Unigram algorithm"),jn.forEach(t),ao.forEach(t),Kt=m(o),se=n(o,"H2",{class:!0});var so=a(se);fe=n(so,"A",{id:!0,class:!0,href:!0});var Gn=a(fe);zt=n(Gn,"SPAN",{});var Hn=a(zt);u(De.$$.fragment,Hn),Hn.forEach(t),Gn.forEach(t),hr=m(so),xt=n(so,"SPAN",{});var Fn=a(xt);fr=i(Fn,"WordLevel"),Fn.forEach(t),so.forEach(t),Qt=m(o),A=n(o,"DIV",{class:!0});var Y=a(A);u(We.$$.fragment,Y),vr=m(Y),Tt=n(Y,"P",{});var Jn=a(Tt);ur=i(Jn,"An implementation of the WordLevel algorithm"),Jn.forEach(t),gr=m(Y),Dt=n(Y,"P",{});var Kn=a(Dt);$r=i(Kn,"Most simple tokenizer model based on mapping tokens to their corresponding id."),Kn.forEach(t),_r=m(Y),L=n(Y,"DIV",{class:!0});var Z=a(L);u(Ae.$$.fragment,Z),br=m(Z),Wt=n(Z,"P",{});var Qn=a(Wt);kr=i(Qn,"Instantiate a WordLevel model from the given file"),Qn.forEach(t),Er=m(Z),At=n(Z,"P",{});var Xn=a(At);yr=i(Xn,"This method is roughly equivalent to doing:"),Xn.forEach(t),wr=m(Z),u(Ie.$$.fragment,Z),Pr=m(Z),F=n(Z,"P",{});var Re=a(F);zr=i(Re,"If you don\u2019t need to keep the "),It=n(Re,"CODE",{});var Yn=a(It);xr=i(Yn,"vocab"),Yn.forEach(t),Tr=i(Re,` values lying around, this method is
more optimized than manually calling `),Bt=n(Re,"CODE",{});var Zn=a(Bt);Dr=i(Zn,"read_file()"),Zn.forEach(t),Wr=i(Re,` to
initialize a `),Je=n(Re,"A",{href:!0});var ea=a(Je);Ar=i(ea,"WordLevel"),ea.forEach(t),Re.forEach(t),Z.forEach(t),Ir=m(Y),J=n(Y,"DIV",{class:!0});var ot=a(J);u(Be.$$.fragment,ot),Br=m(ot),Ke=n(ot,"P",{});var pn=a(Ke);Lr=i(pn,"Read a "),Lt=n(pn,"CODE",{});var ta=a(Lt);Mr=i(ta,"vocab.json"),ta.forEach(t),pn.forEach(t),qr=m(ot),Mt=n(ot,"P",{});var oa=a(Mt);Nr=i(oa,`This method provides a way to read and parse the content of a vocabulary file,
returning the relevant data structures. If you want to instantiate some WordLevel models
from memory, this method gives you the expected input from the standard files.`),oa.forEach(t),ot.forEach(t),Y.forEach(t),Xt=m(o),ie=n(o,"H2",{class:!0});var io=a(ie);ve=n(io,"A",{id:!0,class:!0,href:!0});var ra=a(ve);qt=n(ra,"SPAN",{});var na=a(qt);u(Le.$$.fragment,na),na.forEach(t),ra.forEach(t),Sr=m(io),Nt=n(io,"SPAN",{});var aa=a(Nt);Cr=i(aa,"WordPiece"),aa.forEach(t),io.forEach(t),Yt=m(o),N=n(o,"DIV",{class:!0});var ge=a(N);u(Me.$$.fragment,ge),Vr=m(ge),St=n(ge,"P",{});var sa=a(St);Or=i(sa,"An implementation of the WordPiece algorithm"),sa.forEach(t),Rr=m(ge),M=n(ge,"DIV",{class:!0});var ee=a(M);u(qe.$$.fragment,ee),Ur=m(ee),Ct=n(ee,"P",{});var ia=a(Ct);jr=i(ia,"Instantiate a WordPiece model from the given file"),ia.forEach(t),Gr=m(ee),Vt=n(ee,"P",{});var da=a(Vt);Hr=i(da,"This method is roughly equivalent to doing:"),da.forEach(t),Fr=m(ee),u(Ne.$$.fragment,ee),Jr=m(ee),K=n(ee,"P",{});var Ue=a(K);Kr=i(Ue,"If you don\u2019t need to keep the "),Ot=n(Ue,"CODE",{});var la=a(Ot);Qr=i(la,"vocab"),la.forEach(t),Xr=i(Ue,` values lying around, this method is
more optimized than manually calling `),Rt=n(Ue,"CODE",{});var ca=a(Rt);Yr=i(ca,"read_file()"),ca.forEach(t),Zr=i(Ue,` to
initialize a `),Qe=n(Ue,"A",{href:!0});var ma=a(Qe);en=i(ma,"WordPiece"),ma.forEach(t),Ue.forEach(t),ee.forEach(t),tn=m(ge),Q=n(ge,"DIV",{class:!0});var rt=a(Q);u(Se.$$.fragment,rt),on=m(rt),Ce=n(rt,"P",{});var lo=a(Ce);rn=i(lo,"Read a "),Ut=n(lo,"CODE",{});var pa=a(Ut);nn=i(pa,"vocab.txt"),pa.forEach(t),an=i(lo," file"),lo.forEach(t),sn=m(rt),Ve=n(rt,"P",{});var co=a(Ve);dn=i(co,"This method provides a way to read and parse the content of a standard "),jt=n(co,"EM",{});var ha=a(jt);ln=i(ha,"vocab.txt"),ha.forEach(t),cn=i(co,`
file as used by the WordPiece Model, returning the relevant data structures. If you
want to instantiate some WordPiece models from memory, this method gives you the
expected input from the standard files.`),co.forEach(t),rt.forEach(t),ge.forEach(t),this.h()},h(){p(f,"id","tokenizers.models.BPE"),p(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(f,"href","#tokenizers.models.BPE"),p(l,"class","relative group"),p(Ge,"href","/docs/tokenizers/main/en/api/models#tokenizers.models.BPE"),p(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(le,"id","tokenizers.models.Model"),p(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(le,"href","#tokenizers.models.Model"),p(oe,"class","relative group"),p(Fe,"href","/docs/tokenizers/main/en/api/models#tokenizers.models.Model"),p(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(he,"id","tokenizers.models.Unigram"),p(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(he,"href","#tokenizers.models.Unigram"),p(ne,"class","relative group"),p(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(fe,"id","tokenizers.models.WordLevel"),p(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(fe,"href","#tokenizers.models.WordLevel"),p(se,"class","relative group"),p(Je,"href","/docs/tokenizers/main/en/api/models#tokenizers.models.WordLevel"),p(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ve,"id","tokenizers.models.WordPiece"),p(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ve,"href","#tokenizers.models.WordPiece"),p(ie,"class","relative group"),p(Qe,"href","/docs/tokenizers/main/en/api/models#tokenizers.models.WordPiece"),p(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,E){y(o,l,E),e(l,f),e(f,d),g(h,d,null),e(l,P),e(l,x),e(x,I),y(o,q,E),y(o,D,E),g(C,D,null),e(D,V),e(D,O),e(O,k),e(D,B),e(D,z),g(R,z,null),e(z,je),e(z,de),e(de,mo),e(z,po),e(z,at),e(at,ho),e(z,fo),g($e,z,null),e(z,vo),e(z,U),e(U,uo),e(U,st),e(st,go),e(U,$o),e(U,it),e(it,_o),e(U,bo),e(U,Ge),e(Ge,ko),e(D,Eo),e(D,j),g(_e,j,null),e(j,yo),e(j,te),e(te,wo),e(te,dt),e(dt,Po),e(te,zo),e(te,lt),e(lt,xo),e(te,To),e(j,Do),e(j,ct),e(ct,Wo),y(o,Gt,E),y(o,oe,E),e(oe,le),e(le,mt),g(be,mt,null),e(oe,Ao),e(oe,pt),e(pt,Io),y(o,Ht,E),y(o,w,E),g(ke,w,null),e(w,Bo),e(w,ht),e(ht,Lo),e(w,Mo),e(w,ft),e(ft,qo),e(w,No),e(w,vt),e(vt,So),e(w,Co),e(w,G),g(Ee,G,null),e(G,Vo),e(G,He),e(He,Oo),e(He,ut),e(ut,Ro),e(G,Uo),e(G,re),e(re,jo),e(re,gt),e(gt,Go),e(re,Ho),e(re,Fe),e(Fe,Fo),e(re,Jo),e(w,Ko),e(w,ce),g(ye,ce,null),e(ce,Qo),e(ce,$t),e($t,Xo),e(w,Yo),e(w,H),g(we,H,null),e(H,Zo),e(H,_t),e(_t,er),e(H,tr),e(H,bt),e(bt,or),e(w,rr),e(w,me),g(Pe,me,null),e(me,nr),e(me,kt),e(kt,ar),e(w,sr),e(w,pe),g(ze,pe,null),e(pe,ir),e(pe,Et),e(Et,dr),y(o,Ft,E),y(o,ne,E),e(ne,he),e(he,yt),g(xe,yt,null),e(ne,lr),e(ne,wt),e(wt,cr),y(o,Jt,E),y(o,ae,E),g(Te,ae,null),e(ae,mr),e(ae,Pt),e(Pt,pr),y(o,Kt,E),y(o,se,E),e(se,fe),e(fe,zt),g(De,zt,null),e(se,hr),e(se,xt),e(xt,fr),y(o,Qt,E),y(o,A,E),g(We,A,null),e(A,vr),e(A,Tt),e(Tt,ur),e(A,gr),e(A,Dt),e(Dt,$r),e(A,_r),e(A,L),g(Ae,L,null),e(L,br),e(L,Wt),e(Wt,kr),e(L,Er),e(L,At),e(At,yr),e(L,wr),g(Ie,L,null),e(L,Pr),e(L,F),e(F,zr),e(F,It),e(It,xr),e(F,Tr),e(F,Bt),e(Bt,Dr),e(F,Wr),e(F,Je),e(Je,Ar),e(A,Ir),e(A,J),g(Be,J,null),e(J,Br),e(J,Ke),e(Ke,Lr),e(Ke,Lt),e(Lt,Mr),e(J,qr),e(J,Mt),e(Mt,Nr),y(o,Xt,E),y(o,ie,E),e(ie,ve),e(ve,qt),g(Le,qt,null),e(ie,Sr),e(ie,Nt),e(Nt,Cr),y(o,Yt,E),y(o,N,E),g(Me,N,null),e(N,Vr),e(N,St),e(St,Or),e(N,Rr),e(N,M),g(qe,M,null),e(M,Ur),e(M,Ct),e(Ct,jr),e(M,Gr),e(M,Vt),e(Vt,Hr),e(M,Fr),g(Ne,M,null),e(M,Jr),e(M,K),e(K,Kr),e(K,Ot),e(Ot,Qr),e(K,Xr),e(K,Rt),e(Rt,Yr),e(K,Zr),e(K,Qe),e(Qe,en),e(N,tn),e(N,Q),g(Se,Q,null),e(Q,on),e(Q,Ce),e(Ce,rn),e(Ce,Ut),e(Ut,nn),e(Ce,an),e(Q,sn),e(Q,Ve),e(Ve,dn),e(Ve,jt),e(jt,ln),e(Ve,cn),Zt=!0},p:_a,i(o){Zt||($(h.$$.fragment,o),$(C.$$.fragment,o),$(R.$$.fragment,o),$($e.$$.fragment,o),$(_e.$$.fragment,o),$(be.$$.fragment,o),$(ke.$$.fragment,o),$(Ee.$$.fragment,o),$(ye.$$.fragment,o),$(we.$$.fragment,o),$(Pe.$$.fragment,o),$(ze.$$.fragment,o),$(xe.$$.fragment,o),$(Te.$$.fragment,o),$(De.$$.fragment,o),$(We.$$.fragment,o),$(Ae.$$.fragment,o),$(Ie.$$.fragment,o),$(Be.$$.fragment,o),$(Le.$$.fragment,o),$(Me.$$.fragment,o),$(qe.$$.fragment,o),$(Ne.$$.fragment,o),$(Se.$$.fragment,o),Zt=!0)},o(o){_(h.$$.fragment,o),_(C.$$.fragment,o),_(R.$$.fragment,o),_($e.$$.fragment,o),_(_e.$$.fragment,o),_(be.$$.fragment,o),_(ke.$$.fragment,o),_(Ee.$$.fragment,o),_(ye.$$.fragment,o),_(we.$$.fragment,o),_(Pe.$$.fragment,o),_(ze.$$.fragment,o),_(xe.$$.fragment,o),_(Te.$$.fragment,o),_(De.$$.fragment,o),_(We.$$.fragment,o),_(Ae.$$.fragment,o),_(Ie.$$.fragment,o),_(Be.$$.fragment,o),_(Le.$$.fragment,o),_(Me.$$.fragment,o),_(qe.$$.fragment,o),_(Ne.$$.fragment,o),_(Se.$$.fragment,o),Zt=!1},d(o){o&&t(l),b(h),o&&t(q),o&&t(D),b(C),b(R),b($e),b(_e),o&&t(Gt),o&&t(oe),b(be),o&&t(Ht),o&&t(w),b(ke),b(Ee),b(ye),b(we),b(Pe),b(ze),o&&t(Ft),o&&t(ne),b(xe),o&&t(Jt),o&&t(ae),b(Te),o&&t(Kt),o&&t(se),b(De),o&&t(Qt),o&&t(A),b(We),b(Ae),b(Ie),b(Be),o&&t(Xt),o&&t(ie),b(Le),o&&t(Yt),o&&t(N),b(Me),b(qe),b(Ne),b(Se)}}}function Ea(S){let l,f;return l=new fn({props:{$$slots:{default:[ka]},$$scope:{ctx:S}}}),{c(){v(l.$$.fragment)},l(d){u(l.$$.fragment,d)},m(d,h){g(l,d,h),f=!0},p(d,h){const P={};h&2&&(P.$$scope={dirty:h,ctx:d}),l.$set(P)},i(d){f||($(l.$$.fragment,d),f=!0)},o(d){_(l.$$.fragment,d),f=!1},d(d){b(l,d)}}}function ya(S){let l,f,d,h,P;return{c(){l=r("p"),f=s("The Rust API Reference is available directly on the "),d=r("a"),h=s("Docs.rs"),P=s(" website."),this.h()},l(x){l=n(x,"P",{});var I=a(l);f=i(I,"The Rust API Reference is available directly on the "),d=n(I,"A",{href:!0,rel:!0});var q=a(d);h=i(q,"Docs.rs"),q.forEach(t),P=i(I," website."),I.forEach(t),this.h()},h(){p(d,"href","https://docs.rs/tokenizers/latest/tokenizers/"),p(d,"rel","nofollow")},m(x,I){y(x,l,I),e(l,f),e(l,d),e(d,h),e(l,P)},d(x){x&&t(l)}}}function wa(S){let l,f;return l=new fn({props:{$$slots:{default:[ya]},$$scope:{ctx:S}}}),{c(){v(l.$$.fragment)},l(d){u(l.$$.fragment,d)},m(d,h){g(l,d,h),f=!0},p(d,h){const P={};h&2&&(P.$$scope={dirty:h,ctx:d}),l.$set(P)},i(d){f||($(l.$$.fragment,d),f=!0)},o(d){_(l.$$.fragment,d),f=!1},d(d){b(l,d)}}}function Pa(S){let l,f;return{c(){l=r("p"),f=s("The node API has not been documented yet.")},l(d){l=n(d,"P",{});var h=a(l);f=i(h,"The node API has not been documented yet."),h.forEach(t)},m(d,h){y(d,l,h),e(l,f)},d(d){d&&t(l)}}}function za(S){let l,f;return l=new fn({props:{$$slots:{default:[Pa]},$$scope:{ctx:S}}}),{c(){v(l.$$.fragment)},l(d){u(l.$$.fragment,d)},m(d,h){g(l,d,h),f=!0},p(d,h){const P={};h&2&&(P.$$scope={dirty:h,ctx:d}),l.$set(P)},i(d){f||($(l.$$.fragment,d),f=!0)},o(d){_(l.$$.fragment,d),f=!1},d(d){b(l,d)}}}function xa(S){let l,f,d,h,P,x,I,q,D,C,V,O;return x=new nt({}),V=new ba({props:{python:!0,rust:!0,node:!0,$$slots:{node:[za],rust:[wa],python:[Ea]},$$scope:{ctx:S}}}),{c(){l=r("meta"),f=c(),d=r("h1"),h=r("a"),P=r("span"),v(x.$$.fragment),I=c(),q=r("span"),D=s("Models"),C=c(),v(V.$$.fragment),this.h()},l(k){const B=ga('[data-svelte="svelte-1phssyn"]',document.head);l=n(B,"META",{name:!0,content:!0}),B.forEach(t),f=m(k),d=n(k,"H1",{class:!0});var z=a(d);h=n(z,"A",{id:!0,class:!0,href:!0});var R=a(h);P=n(R,"SPAN",{});var je=a(P);u(x.$$.fragment,je),je.forEach(t),R.forEach(t),I=m(z),q=n(z,"SPAN",{});var de=a(q);D=i(de,"Models"),de.forEach(t),z.forEach(t),C=m(k),u(V.$$.fragment,k),this.h()},h(){p(l,"name","hf:doc:metadata"),p(l,"content",JSON.stringify(Ta)),p(h,"id","models"),p(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(h,"href","#models"),p(d,"class","relative group")},m(k,B){e(document.head,l),y(k,f,B),y(k,d,B),e(d,h),e(h,P),g(x,P,null),e(d,I),e(d,q),e(q,D),y(k,C,B),g(V,k,B),O=!0},p(k,[B]){const z={};B&2&&(z.$$scope={dirty:B,ctx:k}),V.$set(z)},i(k){O||($(x.$$.fragment,k),$(V.$$.fragment,k),O=!0)},o(k){_(x.$$.fragment,k),_(V.$$.fragment,k),O=!1},d(k){t(l),k&&t(f),k&&t(d),b(x),k&&t(C),b(V,k)}}}const Ta={local:"models",sections:[{local:"tokenizers.models.BPE",title:"BPE"},{local:"tokenizers.models.Model",title:"Model"},{local:"tokenizers.models.Unigram",title:"Unigram"},{local:"tokenizers.models.WordLevel",title:"WordLevel"},{local:"tokenizers.models.WordPiece",title:"WordPiece"}],title:"Models"};function Da(S){return $a(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ma extends fa{constructor(l){super();va(this,l,Da,xa,ua,{})}}export{Ma as default,Ta as metadata};
