import{S as zs,i as Ls,s as Ws,e as l,k as u,w as G,t as a,M as Js,c as o,d as t,m as d,a as i,x as z,h as n,b as p,T as Gs,G as s,g as c,y as L,q as W,o as J,B as R,v as Rs}from"../chunks/vendor-hf-doc-builder.js";import{T as Us}from"../chunks/Tip-hf-doc-builder.js";import{I as Vs}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as ue}from"../chunks/CodeBlock-hf-doc-builder.js";function Qs(de){let f,E,h,_,$;return{c(){f=l("p"),E=a("If you want to make the HTTP calls directly, please refer to "),h=l("a"),_=a("Accelerated Inference API Documentation"),$=a(" or to the sample snippets visible on every supported model page."),this.h()},l(g){f=o(g,"P",{});var w=i(f);E=n(w,"If you want to make the HTTP calls directly, please refer to "),h=o(w,"A",{href:!0,rel:!0});var x=i(h);_=n(x,"Accelerated Inference API Documentation"),x.forEach(t),$=n(w," or to the sample snippets visible on every supported model page."),w.forEach(t),this.h()},h(){p(h,"href","https://api-inference.huggingface.co/docs/python/html/index.html"),p(h,"rel","nofollow")},m(g,w){c(g,f,w),s(f,E),s(f,h),s(h,_),s(f,$)},d(g){g&&t(f)}}}function Xs(de){let f,E,h,_,$,g,w,x,Pe,me,v,Te,Z,Oe,Ne,ee,De,Ce,ge,q,_e,j,U,Is,Ke,V,ks,be,b,Se,Q,He,Fe,se,Be,Me,te,Ye,Ge,ye,D,Ie,y,ze,C,Le,We,K,Je,Re,ae,Ue,Ve,ke,S,we,I,Qe,ne,Xe,Ze,re,es,ss,le,ts,as,ve,H,Ae,k,ns,F,rs,ls,oe,os,is,ie,cs,ps,$e,B,je,m,fs,ce,hs,us,pe,ds,ms,fe,gs,_s,he,bs,ys,Ee,M,xe;return g=new Vs({}),q=new Us({props:{$$slots:{default:[Qs]},$$scope:{ctx:de}}}),D=new ue({props:{code:`from huggingface_hub.inference_api import InferenceApi
inference = InferenceApi(repo_id="bert-base-uncased", token=API_TOKEN)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub.inference_api <span class="hljs-keyword">import</span> InferenceApi
<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, token=API_TOKEN)`}}),S=new ue({props:{code:`from huggingface_hub.inference_api import InferenceApi
inference = InferenceApi(repo_id="bert-base-uncased", token=API_TOKEN)
inference(inputs="The goal of life is [MASK].")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub.inference_api <span class="hljs-keyword">import</span> InferenceApi
<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs=<span class="hljs-string">&quot;The goal of life is [MASK].&quot;</span>)
[{<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;the goal of life is life.&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.10933292657136917</span>, <span class="hljs-string">&#x27;token&#x27;</span>: <span class="hljs-number">2166</span>, <span class="hljs-string">&#x27;token_str&#x27;</span>: <span class="hljs-string">&#x27;life&#x27;</span>}]`}}),H=new ue({props:{code:`inference = InferenceApi(repo_id="deepset/roberta-base-squad2", token=API_TOKEN)
inputs = {"question":"Where is Hugging Face headquarters?", "context":"Hugging Face is based in Brooklyn, New York. There is also an office in Paris, France."}
inference(inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {<span class="hljs-string">&quot;question&quot;</span>:<span class="hljs-string">&quot;Where is Hugging Face headquarters?&quot;</span>, <span class="hljs-string">&quot;context&quot;</span>:<span class="hljs-string">&quot;Hugging Face is based in Brooklyn, New York. There is also an office in Paris, France.&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs)
{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.94622403383255</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">25</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">43</span>, <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn, New York&#x27;</span>}`}}),B=new ue({props:{code:`inference = InferenceApi(repo_id="typeform/distilbert-base-uncased-mnli", token=API_TOKEN)
inputs = "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!"
params = {"candidate_labels":["refund", "legal", "faq"]}
inference(inputs, params)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;typeform/distilbert-base-uncased-mnli&quot;</span>, token=API_TOKEN)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>params = {<span class="hljs-string">&quot;candidate_labels&quot;</span>:[<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>inference(inputs, params)
{<span class="hljs-string">&#x27;sequence&#x27;</span>: <span class="hljs-string">&#x27;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>: [<span class="hljs-string">&#x27;refund&#x27;</span>, <span class="hljs-string">&#x27;faq&#x27;</span>, <span class="hljs-string">&#x27;legal&#x27;</span>], <span class="hljs-string">&#x27;scores&#x27;</span>: [<span class="hljs-number">0.9378499388694763</span>, <span class="hljs-number">0.04914155602455139</span>, <span class="hljs-number">0.013008488342165947</span>]}`}}),M=new ue({props:{code:`inference = InferenceApi(repo_id="paraphrase-xlm-r-multilingual-v1", 
                         task="feature-extraction", 
                         token=API_TOKEN,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inference = InferenceApi(repo_id=<span class="hljs-string">&quot;paraphrase-xlm-r-multilingual-v1&quot;</span>, 
<span class="hljs-meta">... </span>                         task=<span class="hljs-string">&quot;feature-extraction&quot;</span>, 
<span class="hljs-meta">... </span>                         token=API_TOKEN,
<span class="hljs-meta">... </span>)`}}),{c(){f=l("meta"),E=u(),h=l("h1"),_=l("a"),$=l("span"),G(g.$$.fragment),w=u(),x=l("span"),Pe=a("Access the Inference API"),me=u(),v=l("p"),Te=a("The Inference API provides fast inference for your hosted models. The Inference API can be accessed via usual HTTP requests with your favorite programming language, but the "),Z=l("code"),Oe=a("huggingface_hub"),Ne=a(" library has a client wrapper to access the Inference API programmatically. This guide will show you how to make calls to the Inference API with the "),ee=l("code"),De=a("huggingface_hub"),Ce=a(" library."),ge=u(),G(q.$$.fragment),_e=u(),j=l("div"),U=l("img"),Ke=u(),V=l("img"),be=u(),b=l("p"),Se=a("Begin by creating an instance of the "),Q=l("a"),He=a("InferenceApi"),Fe=a(" with the model repository ID of the model you want to use. You can find your "),se=l("code"),Be=a("API_TOKEN"),Me=a(" under Settings from your Hugging Face account. The "),te=l("code"),Ye=a("API_TOKEN"),Ge=a(" will allow you to send requests to the Inference API."),ye=u(),G(D.$$.fragment),Ie=u(),y=l("p"),ze=a("The metadata in the model card and configuration files (see "),C=l("a"),Le=a("here"),We=a(" for more details) determines the pipeline type. For example, when using the "),K=l("a"),Je=a("bert-base-uncased"),Re=a(" model, the Inference API can automatically infer that this model should be used for a "),ae=l("code"),Ue=a("fill-mask"),Ve=a(" task."),ke=u(),G(S.$$.fragment),we=u(),I=l("p"),Qe=a("Each task requires a different type of input. A "),ne=l("code"),Xe=a("question-answering"),Ze=a(" task expects a dictionary with the "),re=l("code"),es=a("question"),ss=a(" and "),le=l("code"),ts=a("context"),as=a(" keys as the input:"),ve=u(),G(H.$$.fragment),Ae=u(),k=l("p"),ns=a("Some tasks may require additional parameters (see "),F=l("a"),rs=a("here"),ls=a(" for a detailed list of all parameters for each task). As an example, for "),oe=l("code"),os=a("zero-shot-classification"),is=a(" tasks, the model needs candidate labels that can be supplied to "),ie=l("code"),cs=a("params"),ps=a(":"),$e=u(),G(B.$$.fragment),je=u(),m=l("p"),fs=a("Some models may support multiple tasks. The "),ce=l("code"),hs=a("sentence-transformers"),us=a(" models can complete both "),pe=l("code"),ds=a("sentence-similarity"),ms=a(" and "),fe=l("code"),gs=a("feature-extraction"),_s=a(" tasks. Specify which task you want to perform with the "),he=l("code"),bs=a("task"),ys=a(" parameter:"),Ee=u(),G(M.$$.fragment),this.h()},l(e){const r=Js('[data-svelte="svelte-1phssyn"]',document.head);f=o(r,"META",{name:!0,content:!0}),r.forEach(t),E=d(e),h=o(e,"H1",{class:!0});var Y=i(h);_=o(Y,"A",{id:!0,class:!0,href:!0});var ws=i(_);$=o(ws,"SPAN",{});var vs=i($);z(g.$$.fragment,vs),vs.forEach(t),ws.forEach(t),w=d(Y),x=o(Y,"SPAN",{});var As=i(x);Pe=n(As,"Access the Inference API"),As.forEach(t),Y.forEach(t),me=d(e),v=o(e,"P",{});var X=i(v);Te=n(X,"The Inference API provides fast inference for your hosted models. The Inference API can be accessed via usual HTTP requests with your favorite programming language, but the "),Z=o(X,"CODE",{});var $s=i(Z);Oe=n($s,"huggingface_hub"),$s.forEach(t),Ne=n(X," library has a client wrapper to access the Inference API programmatically. This guide will show you how to make calls to the Inference API with the "),ee=o(X,"CODE",{});var js=i(ee);De=n(js,"huggingface_hub"),js.forEach(t),Ce=n(X," library."),X.forEach(t),ge=d(e),z(q.$$.fragment,e),_e=d(e),j=o(e,"DIV",{class:!0});var qe=i(j);U=o(qe,"IMG",{class:!0,src:!0}),Ke=d(qe),V=o(qe,"IMG",{class:!0,src:!0}),qe.forEach(t),be=d(e),b=o(e,"P",{});var P=i(b);Se=n(P,"Begin by creating an instance of the "),Q=o(P,"A",{href:!0});var Es=i(Q);He=n(Es,"InferenceApi"),Es.forEach(t),Fe=n(P," with the model repository ID of the model you want to use. You can find your "),se=o(P,"CODE",{});var xs=i(se);Be=n(xs,"API_TOKEN"),xs.forEach(t),Me=n(P," under Settings from your Hugging Face account. The "),te=o(P,"CODE",{});var qs=i(te);Ye=n(qs,"API_TOKEN"),qs.forEach(t),Ge=n(P," will allow you to send requests to the Inference API."),P.forEach(t),ye=d(e),z(D.$$.fragment,e),Ie=d(e),y=o(e,"P",{});var T=i(y);ze=n(T,"The metadata in the model card and configuration files (see "),C=o(T,"A",{href:!0,rel:!0});var Ps=i(C);Le=n(Ps,"here"),Ps.forEach(t),We=n(T," for more details) determines the pipeline type. For example, when using the "),K=o(T,"A",{href:!0,rel:!0});var Ts=i(K);Je=n(Ts,"bert-base-uncased"),Ts.forEach(t),Re=n(T," model, the Inference API can automatically infer that this model should be used for a "),ae=o(T,"CODE",{});var Os=i(ae);Ue=n(Os,"fill-mask"),Os.forEach(t),Ve=n(T," task."),T.forEach(t),ke=d(e),z(S.$$.fragment,e),we=d(e),I=o(e,"P",{});var O=i(I);Qe=n(O,"Each task requires a different type of input. A "),ne=o(O,"CODE",{});var Ns=i(ne);Xe=n(Ns,"question-answering"),Ns.forEach(t),Ze=n(O," task expects a dictionary with the "),re=o(O,"CODE",{});var Ds=i(re);es=n(Ds,"question"),Ds.forEach(t),ss=n(O," and "),le=o(O,"CODE",{});var Cs=i(le);ts=n(Cs,"context"),Cs.forEach(t),as=n(O," keys as the input:"),O.forEach(t),ve=d(e),z(H.$$.fragment,e),Ae=d(e),k=o(e,"P",{});var N=i(k);ns=n(N,"Some tasks may require additional parameters (see "),F=o(N,"A",{href:!0,rel:!0});var Ks=i(F);rs=n(Ks,"here"),Ks.forEach(t),ls=n(N," for a detailed list of all parameters for each task). As an example, for "),oe=o(N,"CODE",{});var Ss=i(oe);os=n(Ss,"zero-shot-classification"),Ss.forEach(t),is=n(N," tasks, the model needs candidate labels that can be supplied to "),ie=o(N,"CODE",{});var Hs=i(ie);cs=n(Hs,"params"),Hs.forEach(t),ps=n(N,":"),N.forEach(t),$e=d(e),z(B.$$.fragment,e),je=d(e),m=o(e,"P",{});var A=i(m);fs=n(A,"Some models may support multiple tasks. The "),ce=o(A,"CODE",{});var Fs=i(ce);hs=n(Fs,"sentence-transformers"),Fs.forEach(t),us=n(A," models can complete both "),pe=o(A,"CODE",{});var Bs=i(pe);ds=n(Bs,"sentence-similarity"),Bs.forEach(t),ms=n(A," and "),fe=o(A,"CODE",{});var Ms=i(fe);gs=n(Ms,"feature-extraction"),Ms.forEach(t),_s=n(A," tasks. Specify which task you want to perform with the "),he=o(A,"CODE",{});var Ys=i(he);bs=n(Ys,"task"),Ys.forEach(t),ys=n(A," parameter:"),A.forEach(t),Ee=d(e),z(M.$$.fragment,e),this.h()},h(){p(f,"name","hf:doc:metadata"),p(f,"content",JSON.stringify(Zs)),p(_,"id","access-the-inference-api"),p(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(_,"href","#access-the-inference-api"),p(h,"class","relative group"),p(U,"class","block dark:hidden"),Gs(U.src,Is="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/inference_api_snippet.png")||p(U,"src",Is),p(V,"class","hidden dark:block"),Gs(V.src,ks="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/inference_api_snippet-dark.png")||p(V,"src",ks),p(j,"class","flex justify-center"),p(Q,"href","/docs/huggingface_hub/v0.11.0.rc1/en/package_reference/inference_api#huggingface_hub.InferenceApi"),p(C,"href","https://huggingface.co/docs/hub/models-widgets#enabling-a-widget"),p(C,"rel","nofollow"),p(K,"href","https://huggingface.co/bert-base-uncased"),p(K,"rel","nofollow"),p(F,"href","https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html"),p(F,"rel","nofollow")},m(e,r){s(document.head,f),c(e,E,r),c(e,h,r),s(h,_),s(_,$),L(g,$,null),s(h,w),s(h,x),s(x,Pe),c(e,me,r),c(e,v,r),s(v,Te),s(v,Z),s(Z,Oe),s(v,Ne),s(v,ee),s(ee,De),s(v,Ce),c(e,ge,r),L(q,e,r),c(e,_e,r),c(e,j,r),s(j,U),s(j,Ke),s(j,V),c(e,be,r),c(e,b,r),s(b,Se),s(b,Q),s(Q,He),s(b,Fe),s(b,se),s(se,Be),s(b,Me),s(b,te),s(te,Ye),s(b,Ge),c(e,ye,r),L(D,e,r),c(e,Ie,r),c(e,y,r),s(y,ze),s(y,C),s(C,Le),s(y,We),s(y,K),s(K,Je),s(y,Re),s(y,ae),s(ae,Ue),s(y,Ve),c(e,ke,r),L(S,e,r),c(e,we,r),c(e,I,r),s(I,Qe),s(I,ne),s(ne,Xe),s(I,Ze),s(I,re),s(re,es),s(I,ss),s(I,le),s(le,ts),s(I,as),c(e,ve,r),L(H,e,r),c(e,Ae,r),c(e,k,r),s(k,ns),s(k,F),s(F,rs),s(k,ls),s(k,oe),s(oe,os),s(k,is),s(k,ie),s(ie,cs),s(k,ps),c(e,$e,r),L(B,e,r),c(e,je,r),c(e,m,r),s(m,fs),s(m,ce),s(ce,hs),s(m,us),s(m,pe),s(pe,ds),s(m,ms),s(m,fe),s(fe,gs),s(m,_s),s(m,he),s(he,bs),s(m,ys),c(e,Ee,r),L(M,e,r),xe=!0},p(e,[r]){const Y={};r&2&&(Y.$$scope={dirty:r,ctx:e}),q.$set(Y)},i(e){xe||(W(g.$$.fragment,e),W(q.$$.fragment,e),W(D.$$.fragment,e),W(S.$$.fragment,e),W(H.$$.fragment,e),W(B.$$.fragment,e),W(M.$$.fragment,e),xe=!0)},o(e){J(g.$$.fragment,e),J(q.$$.fragment,e),J(D.$$.fragment,e),J(S.$$.fragment,e),J(H.$$.fragment,e),J(B.$$.fragment,e),J(M.$$.fragment,e),xe=!1},d(e){t(f),e&&t(E),e&&t(h),R(g),e&&t(me),e&&t(v),e&&t(ge),R(q,e),e&&t(_e),e&&t(j),e&&t(be),e&&t(b),e&&t(ye),R(D,e),e&&t(Ie),e&&t(y),e&&t(ke),R(S,e),e&&t(we),e&&t(I),e&&t(ve),R(H,e),e&&t(Ae),e&&t(k),e&&t($e),R(B,e),e&&t(je),e&&t(m),e&&t(Ee),R(M,e)}}}const Zs={local:"access-the-inference-api",title:"Access the Inference API"};function et(de){return Rs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class rt extends zs{constructor(f){super();Ls(this,f,et,Xs,Ws,{})}}export{rt as default,Zs as metadata};
