import{S as li,i as ii,s as pi,e as r,k as c,w as g,t,M as di,c as l,d as s,m,a as i,x as _,h as n,b as u,G as a,g as p,y as b,q as $,o as j,B as v,v as ci}from"../../chunks/vendor-hf-doc-builder.js";import{T as Os}from"../../chunks/Tip-hf-doc-builder.js";import{Y as mi}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Qt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as P}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as ui}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function fi(V){let d,I,f,w,C,h,D,q,A,k,z,E,y,O;return{c(){d=r("p"),I=t("\u270E Por defecto, \u{1F917} Datasets va a descomprimir los archivos necesarios para cargar un dataset. Si quieres ahorrar espacio de almacenamiento, puedes usar "),f=r("code"),w=t("DownloadConfig(delete_extracted=True)"),C=t(" al argumento "),h=r("code"),D=t("download_config"),q=t(" de "),A=r("code"),k=t("load_dataset()"),z=t(". Revisa la "),E=r("a"),y=t("documentaci\xF3n"),O=t(" para m\xE1s detalles."),this.h()},l(M){d=l(M,"P",{});var T=i(d);I=n(T,"\u270E Por defecto, \u{1F917} Datasets va a descomprimir los archivos necesarios para cargar un dataset. Si quieres ahorrar espacio de almacenamiento, puedes usar "),f=l(T,"CODE",{});var G=i(f);w=n(G,"DownloadConfig(delete_extracted=True)"),G.forEach(s),C=n(T," al argumento "),h=l(T,"CODE",{});var x=i(h);D=n(x,"download_config"),x.forEach(s),q=n(T," de "),A=l(T,"CODE",{});var R=i(A);k=n(R,"load_dataset()"),R.forEach(s),z=n(T,". Revisa la "),E=l(T,"A",{href:!0,rel:!0});var Y=i(E);y=n(Y,"documentaci\xF3n"),Y.forEach(s),O=n(T," para m\xE1s detalles."),T.forEach(s),this.h()},h(){u(E,"href","https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig"),u(E,"rel","nofollow")},m(M,T){p(M,d,T),a(d,I),a(d,f),a(f,w),a(d,C),a(d,h),a(h,D),a(d,q),a(d,A),a(A,k),a(d,z),a(d,E),a(E,y),a(d,O)},d(M){M&&s(d)}}}function hi(V){let d,I,f,w,C,h,D,q,A,k,z,E,y,O,M,T;return{c(){d=r("p"),I=t("\u270F\uFE0F "),f=r("strong"),w=t("\xA1Int\xE9ntalo!"),C=t(" Escoge alguno de los "),h=r("a"),D=t("subconjuntos"),q=t(" del "),A=r("em"),k=t("Pile"),z=t(" que sea m\xE1s grande que la RAM de tu computador port\xE1til o de escritorio, c\xE1rgalo con \u{1F917} Datasets y mide la cantidad de RAM utilizada. Recuerda que para tener una medici\xF3n precisa, tienes que hacerlo en un nuevo proceso. Puedes encontrar los tama\xF1os de cada uno de los subconjuntos sin comprimir en la Tabla 1 del "),E=r("a"),y=t("paper de "),O=r("em"),M=t("Pile"),T=t("."),this.h()},l(G){d=l(G,"P",{});var x=i(d);I=n(x,"\u270F\uFE0F "),f=l(x,"STRONG",{});var R=i(f);w=n(R,"\xA1Int\xE9ntalo!"),R.forEach(s),C=n(x," Escoge alguno de los "),h=l(x,"A",{href:!0,rel:!0});var Y=i(h);D=n(Y,"subconjuntos"),Y.forEach(s),q=n(x," del "),A=l(x,"EM",{});var X=i(A);k=n(X,"Pile"),X.forEach(s),z=n(x," que sea m\xE1s grande que la RAM de tu computador port\xE1til o de escritorio, c\xE1rgalo con \u{1F917} Datasets y mide la cantidad de RAM utilizada. Recuerda que para tener una medici\xF3n precisa, tienes que hacerlo en un nuevo proceso. Puedes encontrar los tama\xF1os de cada uno de los subconjuntos sin comprimir en la Tabla 1 del "),E=l(x,"A",{href:!0,rel:!0});var ee=i(E);y=n(ee,"paper de "),O=l(ee,"EM",{});var ce=i(O);M=n(ce,"Pile"),ce.forEach(s),ee.forEach(s),T=n(x,"."),x.forEach(s),this.h()},h(){u(h,"href","https://mystic.the-eye.eu/public/AI/pile_preliminary_components/"),u(h,"rel","nofollow"),u(E,"href","https://arxiv.org/abs/2101.00027"),u(E,"rel","nofollow")},m(G,x){p(G,d,x),a(d,I),a(d,f),a(f,w),a(d,C),a(d,h),a(h,D),a(d,q),a(d,A),a(A,k),a(d,z),a(d,E),a(E,y),a(E,O),a(O,M),a(d,T)},d(G){G&&s(d)}}}function gi(V){let d,I,f,w,C,h;return{c(){d=r("p"),I=t("\u{1F4A1} En los cuadernos de Jupyter tambi\xE9n puedes medir el tiempo de ejecuci\xF3n de las celdas usando "),f=r("a"),w=r("code"),C=t("%%timeit"),h=t("."),this.h()},l(D){d=l(D,"P",{});var q=i(d);I=n(q,"\u{1F4A1} En los cuadernos de Jupyter tambi\xE9n puedes medir el tiempo de ejecuci\xF3n de las celdas usando "),f=l(q,"A",{href:!0,rel:!0});var A=i(f);w=l(A,"CODE",{});var k=i(w);C=n(k,"%%timeit"),k.forEach(s),A.forEach(s),h=n(q,"."),q.forEach(s),this.h()},h(){u(f,"href","https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit"),u(f,"rel","nofollow")},m(D,q){p(D,d,q),a(d,I),a(d,f),a(f,w),a(w,C),a(d,h)},d(D){D&&s(d)}}}function _i(V){let d,I,f,w,C,h,D,q,A,k,z;return{c(){d=r("p"),I=t("\u{1F4A1} Para acelerar la tokenizaci\xF3n con "),f=r("em"),w=t("streaming"),C=t(" puedes definir "),h=r("code"),D=t("batched=True"),q=t(", como lo vimos en la secci\xF3n anterior. Esto va a procesar los ejemplos lote por lote. Recuerda que el tama\xF1o por defecto de los lotes es 1.000 y puede ser expecificado con el argumento "),A=r("code"),k=t("batch_size"),z=t(".")},l(E){d=l(E,"P",{});var y=i(d);I=n(y,"\u{1F4A1} Para acelerar la tokenizaci\xF3n con "),f=l(y,"EM",{});var O=i(f);w=n(O,"streaming"),O.forEach(s),C=n(y," puedes definir "),h=l(y,"CODE",{});var M=i(h);D=n(M,"batched=True"),M.forEach(s),q=n(y,", como lo vimos en la secci\xF3n anterior. Esto va a procesar los ejemplos lote por lote. Recuerda que el tama\xF1o por defecto de los lotes es 1.000 y puede ser expecificado con el argumento "),A=l(y,"CODE",{});var T=i(A);k=n(T,"batch_size"),T.forEach(s),z=n(y,"."),y.forEach(s)},m(E,y){p(E,d,y),a(d,I),a(d,f),a(f,w),a(d,C),a(d,h),a(h,D),a(d,q),a(d,A),a(A,k),a(d,z)},d(E){E&&s(d)}}}function bi(V){let d,I,f,w,C,h,D,q,A,k,z,E,y,O,M,T;return{c(){d=r("p"),I=t("\u270F\uFE0F "),f=r("strong"),w=t("\xA1Int\xE9ntalo!"),C=t(" Usa alguno de los corpus grandes de Common Crawl como "),h=r("a"),D=r("code"),q=t("mc4"),A=t(" u "),k=r("a"),z=r("code"),E=t("oscar"),y=t(" para crear un dataset "),O=r("em"),M=t("streaming"),T=t(" multilenguaje que represente las proporciones de lenguajes hablados en un pa\xEDs de tu elecci\xF3n. Por ejemplo, los 4 lenguajes nacionales en Suiza son alem\xE1n, franc\xE9s, italiano y romanche, as\xED que podr\xEDas crear un corpus suizo al hacer un muestreo de Oscar de acuerdo con su proporci\xF3n de lenguaje."),this.h()},l(G){d=l(G,"P",{});var x=i(d);I=n(x,"\u270F\uFE0F "),f=l(x,"STRONG",{});var R=i(f);w=n(R,"\xA1Int\xE9ntalo!"),R.forEach(s),C=n(x," Usa alguno de los corpus grandes de Common Crawl como "),h=l(x,"A",{href:!0,rel:!0});var Y=i(h);D=l(Y,"CODE",{});var X=i(D);q=n(X,"mc4"),X.forEach(s),Y.forEach(s),A=n(x," u "),k=l(x,"A",{href:!0,rel:!0});var ee=i(k);z=l(ee,"CODE",{});var ce=i(z);E=n(ce,"oscar"),ce.forEach(s),ee.forEach(s),y=n(x," para crear un dataset "),O=l(x,"EM",{});var he=i(O);M=n(he,"streaming"),he.forEach(s),T=n(x," multilenguaje que represente las proporciones de lenguajes hablados en un pa\xEDs de tu elecci\xF3n. Por ejemplo, los 4 lenguajes nacionales en Suiza son alem\xE1n, franc\xE9s, italiano y romanche, as\xED que podr\xEDas crear un corpus suizo al hacer un muestreo de Oscar de acuerdo con su proporci\xF3n de lenguaje."),x.forEach(s),this.h()},h(){u(h,"href","https://huggingface.co/datasets/mc4"),u(h,"rel","nofollow"),u(k,"href","https://huggingface.co/datasets/oscar"),u(k,"rel","nofollow")},m(G,x){p(G,d,x),a(d,I),a(d,f),a(f,w),a(d,C),a(d,h),a(h,D),a(D,q),a(d,A),a(d,k),a(k,z),a(z,E),a(d,y),a(d,O),a(O,M),a(d,T)},d(G){G&&s(d)}}}function $i(V){let d,I,f,w,C,h,D,q,A,k,z,E,y,O,M,T,G,x,R,Y,X,ee,ce,he,Xt,en,Oa,an,sn,Ms,Ce,Rs,ge,tn,ze,nn,on,Ss,me,_e,Ma,Ie,rn,Ra,ln,Gs,S,pn,Sa,dn,cn,Oe,mn,un,Me,fn,hn,Re,gn,_n,Se,bn,$n,Ge,jn,vn,Ga,xn,En,Bs,Be,Ns,be,yn,ya,wn,qn,Ls,Ne,Hs,Le,Us,wa,An,Fs,$e,Js,qa,Pn,Ws,He,Ks,Ue,Vs,Aa,Dn,Ys,ue,je,Ba,Fe,kn,Na,Tn,Zs,ae,Cn,Je,La,zn,In,Ha,On,Mn,Qs,We,Xs,ve,Rn,Ua,Sn,Gn,et,Ke,at,Ve,st,Z,Bn,Fa,Nn,Ln,Ja,Hn,Un,Wa,Fn,Jn,tt,Ye,nt,Ze,ot,Pa,Wn,rt,xe,lt,se,Kn,Qe,Vn,Yn,Xe,Zn,Qn,it,L,Xn,Ka,eo,ao,ea,so,to,aa,Va,no,oo,sa,ro,lo,pt,ta,dt,na,ct,H,io,Ya,po,co,Za,mo,uo,Qa,fo,ho,Xa,go,_o,mt,Ee,ut,fe,ye,es,oa,bo,as,$o,ft,U,jo,ss,vo,xo,ts,Eo,yo,ns,wo,qo,os,Ao,Po,ht,ra,gt,F,Do,rs,ko,To,ls,Co,zo,is,Io,Oo,ps,Mo,Ro,_t,la,bt,ia,$t,Q,So,ds,Go,Bo,cs,No,Lo,Da,Ho,Uo,jt,pa,vt,da,xt,we,Et,J,Fo,ms,Jo,Wo,us,Ko,Vo,fs,Yo,Zo,hs,Qo,Xo,yt,ca,wt,ma,qt,W,er,gs,ar,sr,_s,tr,nr,bs,or,rr,$s,lr,ir,At,ua,Pt,fa,Dt,qe,pr,js,dr,cr,kt,ha,Tt,B,mr,vs,ur,fr,xs,hr,gr,Es,_r,br,ys,$r,jr,ws,vr,xr,qs,Er,yr,Ct,ga,zt,_a,It,Ae,wr,As,qr,Ar,Ot,ba,Mt,$a,Rt,te,Pr,Ps,Dr,kr,Ds,Tr,Cr,St,ne,zr,ks,Ir,Or,Ts,Mr,Rr,Gt,ja,Bt,va,Nt,Pe,Lt,ka,Sr,Ht;return h=new Qt({}),z=new ui({props:{chapter:5,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter5/section4.ipynb"}]}}),Ce=new mi({props:{id:"JwISwTCPPWo"}}),Ie=new Qt({}),Be=new P({props:{code:"!pip install zstandard",highlighted:"!pip install zstandard"}}),Ne=new P({props:{code:`from datasets import load_dataset

# Esto toma algunos minutos para ejecutarse, as\xED que ve por un te o un caf\xE9 mientras esperas :)
data_files = "https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst"
pubmed_dataset = load_dataset("json", data_files=data_files, split="train")
pubmed_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># Esto toma algunos minutos para ejecutarse, as\xED que ve por un te o un caf\xE9 mientras esperas :)</span>
data_files = <span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst&quot;</span>
pubmed_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)
pubmed_dataset`}}),Le=new P({props:{code:`Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})`,highlighted:`Dataset({
    features: [<span class="hljs-string">&#x27;meta&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>],
    num_rows: <span class="hljs-number">15518009</span>
})`}}),$e=new Os({props:{$$slots:{default:[fi]},$$scope:{ctx:V}}}),He=new P({props:{code:"pubmed_dataset[0]",highlighted:'pubmed_dataset[<span class="hljs-number">0</span>]'}}),Ue=new P({props:{code:`{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...&#x27;</span>}`}}),Fe=new Qt({}),We=new P({props:{code:"!pip install psutil",highlighted:"!pip install psutil"}}),Ke=new P({props:{code:`import psutil

# Process.memory_info est\xE1 expresado en bytes, as\xED que lo convertimos en megabytes
print(f"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB")`,highlighted:`<span class="hljs-keyword">import</span> psutil

<span class="hljs-comment"># Process.memory_info est\xE1 expresado en bytes, as\xED que lo convertimos en megabytes</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;RAM used: <span class="hljs-subst">{psutil.Process().memory_info().rss / (<span class="hljs-number">1024</span> * <span class="hljs-number">1024</span>):<span class="hljs-number">.2</span>f}</span> MB&quot;</span>)`}}),Ve=new P({props:{code:"RAM used: 5678.33 MB",highlighted:'RAM used: <span class="hljs-number">5678.33</span> MB'}}),Ye=new P({props:{code:`print(f"Number of files in dataset : {pubmed_dataset.dataset_size}")
size_gb = pubmed_dataset.dataset_size / (1024**3)
print(f"Dataset size (cache file) : {size_gb:.2f} GB")`,highlighted:`<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Number of files in dataset : <span class="hljs-subst">{pubmed_dataset.dataset_size}</span>&quot;</span>)
size_gb = pubmed_dataset.dataset_size / (<span class="hljs-number">1024</span>**<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Dataset size (cache file) : <span class="hljs-subst">{size_gb:<span class="hljs-number">.2</span>f}</span> GB&quot;</span>)`}}),Ze=new P({props:{code:`Number of files in dataset : 20979437051
Dataset size (cache file) : 19.54 GB`,highlighted:`Number of files <span class="hljs-keyword">in</span> dataset : <span class="hljs-number">20979437051</span>
Dataset size (cache file) : <span class="hljs-number">19.54</span> GB`}}),xe=new Os({props:{$$slots:{default:[hi]},$$scope:{ctx:V}}}),ta=new P({props:{code:`import timeit

code_snippet = """batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
"""

time = timeit.timeit(stmt=code_snippet, number=1, globals=globals())
print(
    f"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in "
    f"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s"
)`,highlighted:`<span class="hljs-keyword">import</span> timeit

code_snippet = <span class="hljs-string">&quot;&quot;&quot;batch_size = 1000

for idx in range(0, len(pubmed_dataset), batch_size):
    _ = pubmed_dataset[idx:idx + batch_size]
&quot;&quot;&quot;</span>

time = timeit.timeit(stmt=code_snippet, number=<span class="hljs-number">1</span>, <span class="hljs-built_in">globals</span>=<span class="hljs-built_in">globals</span>())
<span class="hljs-built_in">print</span>(
    <span class="hljs-string">f&quot;Iterated over <span class="hljs-subst">{<span class="hljs-built_in">len</span>(pubmed_dataset)}</span> examples (about <span class="hljs-subst">{size_gb:<span class="hljs-number">.1</span>f}</span> GB) in &quot;</span>
    <span class="hljs-string">f&quot;<span class="hljs-subst">{time:<span class="hljs-number">.1</span>f}</span>s, i.e. <span class="hljs-subst">{size_gb/time:<span class="hljs-number">.3</span>f}</span> GB/s&quot;</span>
)`}}),na=new P({props:{code:"'Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s'",highlighted:'<span class="hljs-string">&#x27;Iterated over 15518009 examples (about 19.5 GB) in 64.2s, i.e. 0.304 GB/s&#x27;</span>'}}),Ee=new Os({props:{$$slots:{default:[gi]},$$scope:{ctx:V}}}),oa=new Qt({}),ra=new P({props:{code:`pubmed_dataset_streamed = load_dataset(
    "json", data_files=data_files, split="train", streaming=True
)`,highlighted:`pubmed_dataset_streamed = load_dataset(
    <span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>, streaming=<span class="hljs-literal">True</span>
)`}}),la=new P({props:{code:"next(iter(pubmed_dataset_streamed))",highlighted:'<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(pubmed_dataset_streamed))'}}),ia=new P({props:{code:`{'meta': {'pmid': 11409574, 'language': 'eng'},
 'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection.\\nTo determine the prevalence of hypoxaemia in children aged under 5 years suffering acute lower respiratory infections (ALRI), the risk factors for hypoxaemia in children under 5 years of age with ALRI, and the association of hypoxaemia with an increased risk of dying in children of the same age ...&#x27;</span>}`}}),pa=new P({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x["text"]))
next(iter(tokenized_dataset))`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
tokenized_dataset = pubmed_dataset_streamed.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: tokenizer(x[<span class="hljs-string">&quot;text&quot;</span>]))
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(tokenized_dataset))`}}),da=new P({props:{code:"{'input_ids': [101, 4958, 5178, 4328, 6779, ...], 'attention_mask': [1, 1, 1, 1, 1, ...]}",highlighted:'{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">4958</span>, <span class="hljs-number">5178</span>, <span class="hljs-number">4328</span>, <span class="hljs-number">6779</span>, ...], <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ...]}'}}),we=new Os({props:{$$slots:{default:[_i]},$$scope:{ctx:V}}}),ca=new P({props:{code:`shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)
next(iter(shuffled_dataset))`,highlighted:`shuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=<span class="hljs-number">10_000</span>, seed=<span class="hljs-number">42</span>)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(shuffled_dataset))`}}),ma=new P({props:{code:`{'meta': {'pmid': 11410799, 'language': 'eng'},
 'text': 'Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11410799</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Randomized study of dose or schedule modification of granulocyte colony-stimulating factor in platinum-based chemotherapy for elderly patients with lung cancer ...&#x27;</span>}`}}),ua=new P({props:{code:`dataset_head = pubmed_dataset_streamed.take(5)
list(dataset_head)`,highlighted:`dataset_head = pubmed_dataset_streamed.take(<span class="hljs-number">5</span>)
<span class="hljs-built_in">list</span>(dataset_head)`}}),fa=new P({props:{code:`[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'pmid': 11409575, 'language': 'eng'},
  'text': 'Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...'},
 {'meta': {'pmid': 11409576, 'language': 'eng'},
  'text': "Hypoxaemia in children with severe pneumonia in Papua New Guinea ..."},
 {'meta': {'pmid': 11409577, 'language': 'eng'},
  'text': 'Oxygen concentrators and cylinders ...'},
 {'meta': {'pmid': 11409578, 'language': 'eng'},
  'text': 'Oxygen supply in rural africa: a personal experience ...'}]`,highlighted:`[{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409575</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Clinical signs of hypoxaemia in children with acute lower respiratory infection: indicators of oxygen therapy ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409576</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;Hypoxaemia in children with severe pneumonia in Papua New Guinea ...&quot;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409577</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Oxygen concentrators and cylinders ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409578</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Oxygen supply in rural africa: a personal experience ...&#x27;</span>}]`}}),ha=new P({props:{code:`# Skip the first 1,000 examples and include the rest in the training set
train_dataset = shuffled_dataset.skip(1000)
# Take the first 1,000 examples for the validation set
validation_dataset = shuffled_dataset.take(1000)`,highlighted:`<span class="hljs-comment"># Skip the first 1,000 examples and include the rest in the training set</span>
train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)
<span class="hljs-comment"># Take the first 1,000 examples for the validation set</span>
validation_dataset = shuffled_dataset.take(<span class="hljs-number">1000</span>)`}}),ga=new P({props:{code:`law_dataset_streamed = load_dataset(
    "json",
    data_files="https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst",
    split="train",
    streaming=True,
)
next(iter(law_dataset_streamed))`,highlighted:`law_dataset_streamed = load_dataset(
    <span class="hljs-string">&quot;json&quot;</span>,
    data_files=<span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst&quot;</span>,
    split=<span class="hljs-string">&quot;train&quot;</span>,
    streaming=<span class="hljs-literal">True</span>,
)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(law_dataset_streamed))`}}),_a=new P({props:{code:`{'meta': {'case_ID': '110921.json',
  'case_jurisdiction': 'scotus.tar.gz',
  'date_created': '2010-04-28T17:12:49Z'},
 'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;case_ID&#x27;</span>: <span class="hljs-string">&#x27;110921.json&#x27;</span>,
  <span class="hljs-string">&#x27;case_jurisdiction&#x27;</span>: <span class="hljs-string">&#x27;scotus.tar.gz&#x27;</span>,
  <span class="hljs-string">&#x27;date_created&#x27;</span>: <span class="hljs-string">&#x27;2010-04-28T17:12:49Z&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;</span>}`}}),ba=new P({props:{code:`from itertools import islice
from datasets import interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
list(islice(combined_dataset, 2))`,highlighted:`<span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets

combined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])
<span class="hljs-built_in">list</span>(islice(combined_dataset, <span class="hljs-number">2</span>))`}}),$a=new P({props:{code:`[{'meta': {'pmid': 11409574, 'language': 'eng'},
  'text': 'Epidemiology of hypoxaemia in children with acute lower respiratory infection ...'},
 {'meta': {'case_ID': '110921.json',
   'case_jurisdiction': 'scotus.tar.gz',
   'date_created': '2010-04-28T17:12:49Z'},
  'text': '\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...'}]`,highlighted:`[{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pmid&#x27;</span>: <span class="hljs-number">11409574</span>, <span class="hljs-string">&#x27;language&#x27;</span>: <span class="hljs-string">&#x27;eng&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Epidemiology of hypoxaemia in children with acute lower respiratory infection ...&#x27;</span>},
 {<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;case_ID&#x27;</span>: <span class="hljs-string">&#x27;110921.json&#x27;</span>,
   <span class="hljs-string">&#x27;case_jurisdiction&#x27;</span>: <span class="hljs-string">&#x27;scotus.tar.gz&#x27;</span>,
   <span class="hljs-string">&#x27;date_created&#x27;</span>: <span class="hljs-string">&#x27;2010-04-28T17:12:49Z&#x27;</span>},
  <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;\\n461 U.S. 238 (1983)\\nOLIM ET AL.\\nv.\\nWAKINEKONA\\nNo. 81-1581.\\nSupreme Court of United States.\\nArgued January 19, 1983.\\nDecided April 26, 1983.\\nCERTIORARI TO THE UNITED STATES COURT OF APPEALS FOR THE NINTH CIRCUIT\\n*239 Michael A. Lilly, First Deputy Attorney General of Hawaii, argued the cause for petitioners. With him on the brief was James H. Dannenberg, Deputy Attorney General...&#x27;</span>}]`}}),ja=new P({props:{code:`base_url = "https://mystic.the-eye.eu/public/AI/pile/"
data_files = {
    "train": [base_url + "train/" + f"{idx:02d}.jsonl.zst" for idx in range(30)],
    "validation": base_url + "val.jsonl.zst",
    "test": base_url + "test.jsonl.zst",
}
pile_dataset = load_dataset("json", data_files=data_files, streaming=True)
next(iter(pile_dataset["train"]))`,highlighted:`base_url = <span class="hljs-string">&quot;https://mystic.the-eye.eu/public/AI/pile/&quot;</span>
data_files = {
    <span class="hljs-string">&quot;train&quot;</span>: [base_url + <span class="hljs-string">&quot;train/&quot;</span> + <span class="hljs-string">f&quot;<span class="hljs-subst">{idx:02d}</span>.jsonl.zst&quot;</span> <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">30</span>)],
    <span class="hljs-string">&quot;validation&quot;</span>: base_url + <span class="hljs-string">&quot;val.jsonl.zst&quot;</span>,
    <span class="hljs-string">&quot;test&quot;</span>: base_url + <span class="hljs-string">&quot;test.jsonl.zst&quot;</span>,
}
pile_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(pile_dataset[<span class="hljs-string">&quot;train&quot;</span>]))`}}),va=new P({props:{code:`{'meta': {'pile_set_name': 'Pile-CC'},
 'text': 'It is done, and submitted. You can play \u201CSurvival of the Tastiest\u201D on Android, and on the web...'}`,highlighted:`{<span class="hljs-string">&#x27;meta&#x27;</span>: {<span class="hljs-string">&#x27;pile_set_name&#x27;</span>: <span class="hljs-string">&#x27;Pile-CC&#x27;</span>},
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;It is done, and submitted. You can play \u201CSurvival of the Tastiest\u201D on Android, and on the web...&#x27;</span>}`}}),Pe=new Os({props:{$$slots:{default:[bi]},$$scope:{ctx:V}}}),{c(){d=r("meta"),I=c(),f=r("h1"),w=r("a"),C=r("span"),g(h.$$.fragment),D=c(),q=r("span"),A=t("\xBFBig data? \u{1F917} \xA1Datasets al rescate!"),k=c(),g(z.$$.fragment),E=c(),y=r("p"),O=t("Hoy en d\xEDa es com\xFAn que tengas que trabajar con dataset de varios GB, especialmente si planeas pre-entrenar un transformador como BERT o GPT-2 desde ceros. En estos casos, "),M=r("em"),T=t("solamente cargar"),G=t(" los datos puede ser un desaf\xEDo. Por ejemplo, el corpus de WebText utilizado para preentrenar GPT-2 consiste de m\xE1s de 8 millones de documentos y 40 GB de texto. \xA1Cargarlo en la RAM de tu computador portatil le va a causar un paro cardiaco!"),x=c(),R=r("p"),Y=t("Afortunadamente, \u{1F917} Datasets est\xE1 dise\xF1ado para superar estas limitaciones: te libera de problemas de manejo de memoria al tratar los datasets como archivos "),X=r("em"),ee=t("proyectados en memoria"),ce=t(" ("),he=r("em"),Xt=t("memory-mapped"),en=t(") y de l\xEDmites de almacenamiento al hacer "),Oa=r("em"),an=t("streaming"),sn=t(" de las entradas en un corpus."),Ms=c(),g(Ce.$$.fragment),Rs=c(),ge=r("p"),tn=t("En esta secci\xF3n vamos a explorar estas funcionalidades de \u{1F917} Datasets con un corpus enorme de 825 GB conocido como el "),ze=r("a"),nn=t("Pile"),on=t(". \xA1Comencemos!"),Ss=c(),me=r("h2"),_e=r("a"),Ma=r("span"),g(Ie.$$.fragment),rn=c(),Ra=r("span"),ln=t("\xBFQu\xE9 es el Pile?"),Gs=c(),S=r("p"),pn=t("El "),Sa=r("em"),dn=t("Pile"),cn=t(" es un corpus de textos en ingl\xE9s creado por "),Oe=r("a"),mn=t("EleutherAI"),un=t(" para entrenar modelos de lenguaje de gran escala. Incluye una selecci\xF3n diversa de datasets que abarca art\xEDculos cient\xEDficos, repositorios de c\xF3digo de Github y texto filtrado de la web. El corpus de entrenamiento est\xE1 disponible en "),Me=r("a"),fn=t("partes de 14 GB"),hn=t(" y tambi\xE9n puedes descargar varios de los "),Re=r("a"),gn=t("componentes individuales"),_n=t(". Arranquemos viendo el dataset de los abstracts de PubMed, un corpus de abstracts de 15 millones de publicaciones biom\xE9dicas en "),Se=r("a"),bn=t("PubMed"),$n=t(". Este dataset est\xE1 en formato "),Ge=r("a"),jn=t("JSON Lines"),vn=t(" y est\xE1 comprimido con la librer\xEDa "),Ga=r("code"),xn=t("zstandard"),En=t(", as\xED que primero tenemos que instalarla:"),Bs=c(),g(Be.$$.fragment),Ns=c(),be=r("p"),yn=t("A continuaci\xF3n, podemos cargar el dataset usando el m\xE9todo para archivos remotos que aprendimos en la "),ya=r("a"),wn=t("secci\xF3n 2"),qn=t(":"),Ls=c(),g(Ne.$$.fragment),Hs=c(),g(Le.$$.fragment),Us=c(),wa=r("p"),An=t("Como podemos ver, hay 15.518.009 filas y dos columnas en el dataset, \xA1un mont\xF3n!"),Fs=c(),g($e.$$.fragment),Js=c(),qa=r("p"),Pn=t("Veamos el contenido del primer ejemplo:"),Ws=c(),g(He.$$.fragment),Ks=c(),g(Ue.$$.fragment),Vs=c(),Aa=r("p"),Dn=t("Ok, esto parece el abstract de un art\xEDculo m\xE9dico. Ahora miremos cu\xE1nta RAM hemos usado para cargar el dataset."),Ys=c(),ue=r("h2"),je=r("a"),Ba=r("span"),g(Fe.$$.fragment),kn=c(),Na=r("span"),Tn=t("La magia de la proyecci\xF3n en memoria"),Zs=c(),ae=r("p"),Cn=t("Una forma simple de medir el uso de memoria en Python es con la librer\xEDa "),Je=r("a"),La=r("code"),zn=t("psutil"),In=t(", que se puede instalar con "),Ha=r("code"),On=t("pip"),Mn=t(" as\xED:"),Qs=c(),g(We.$$.fragment),Xs=c(),ve=r("p"),Rn=t("Esta librer\xEDa contiene una clase "),Ua=r("code"),Sn=t("Process"),Gn=t(" que nos permite revisar el uso de memoria del proceso actual:"),et=c(),g(Ke.$$.fragment),at=c(),g(Ve.$$.fragment),st=c(),Z=r("p"),Bn=t("El atributo "),Fa=r("code"),Nn=t("rss"),Ln=t(" se refiere al "),Ja=r("em"),Hn=t("resident set size"),Un=t(", que es la fracci\xF3n de memoria que un proceso ocupa en RAM. Esta medici\xF3n tambi\xE9n incluye la memoria usada por el int\xE9rprete de Python y las librer\xEDas que hemos cargado, as\xED que la cantidad real de memoria usada para cargar el dataset es un poco m\xE1s peque\xF1a. A modo de comparaci\xF3n, veamos qu\xE9 tan grande es el dataset en disco, usando el atributo "),Wa=r("code"),Fn=t("dataset_size"),Jn=t(". Dado que el resultado est\xE1 expresado en bytes, tenemos que convertirlo manualmente en gigabytes:"),tt=c(),g(Ye.$$.fragment),nt=c(),g(Ze.$$.fragment),ot=c(),Pa=r("p"),Wn=t("Bien, a pesar de que el archivo es de casi 20 GB, \xA1podemos cargarlo y acceder a su contenido con mucha menos RAM!"),rt=c(),g(xe.$$.fragment),lt=c(),se=r("p"),Kn=t("Si est\xE1s familiarizado con Pandas, este resultado puede ser sorprendente por la famosa "),Qe=r("a"),Vn=t("regla de Wes Kinney"),Yn=t(" que indica que t\xEDpicamente necesitas de 5 a 10 veces la RAM que el tama\xF1o del archivo de tu dataset. \xBFC\xF3mo resuelve entonces \u{1F917} Datasets este problema de manejo de memoria? \u{1F917} Datasets trata cada dataset como un "),Xe=r("a"),Zn=t("archivo proyectado en memoria"),Qn=t(", lo que permite un mapeo entre la RAM y el sistema de almacenamiento de archivos, que le permite a la librer\xEDa acceder y operar los elementos del dataset sin necesidad de tenerlos cargados completamente en memoria."),it=c(),L=r("p"),Xn=t("Los archivos proyectados en memoria tambi\xE9n pueden ser compartidos por m\xFAltiples procesos, lo que habilita la paralelizaci\xF3n de m\xE9todos como "),Ka=r("code"),eo=t("Dataset.map()"),ao=t(" sin que sea obligatorio mover o copiar el dataset. Internamente, estas capacidades se logran gracias al formato de memoria "),ea=r("a"),so=t("Apache Arrow"),to=t(" y la librer\xEDa "),aa=r("a"),Va=r("code"),no=t("pyarrow"),oo=t(", que permiten la carga y procesamiento de datos a gran velocidad. (Para ahondar m\xE1s en Apache Arrow y algunas comparaciones con Pandas, revisa el "),sa=r("a"),ro=t("blog de Dejan Simic"),lo=t("). Para verlo en acci\xF3n, ejecutemos un test de velocidad iterando sobre todos los elementos del dataset de abstracts de PubMed:"),pt=c(),g(ta.$$.fragment),dt=c(),g(na.$$.fragment),ct=c(),H=r("p"),io=t("Aqu\xED usamos el m\xF3dulo "),Ya=r("code"),po=t("timeit"),co=t(" de Python para medir el tiempo de ejecuci\xF3n que se toma "),Za=r("code"),mo=t("code_snippet"),uo=t(". Tipicamemente, puedes iterar a lo largo de un dataset a una velocidad de unas cuantas d\xE9cimas de un GB por segundo. Esto funciona muy bien para la gran mayor\xEDa de aplicaciones, pero algunas veces tendr\xE1s que trabajar con un dataset que es tan grande para incluso almacenarse en el disco de tu computador. Por ejemplo, si quisieramos descargar el "),Qa=r("em"),fo=t("Pile"),ho=t(" completo \xA1necesitar\xEDamos 825 GB de almacenamiento libre! Para trabajar con esos casos, \u{1F917} Datasets puede trabajar haciendo "),Xa=r("em"),go=t("streaming"),_o=t(", lo que permite la descarga y acceso a los elementos sobre la marcha, sin necesidad de descargar todo el dataset. Veamos c\xF3mo funciona:"),mt=c(),g(Ee.$$.fragment),ut=c(),fe=r("h2"),ye=r("a"),es=r("span"),g(oa.$$.fragment),bo=c(),as=r("span"),$o=t("Haciendo _streaming_ de datasets"),ft=c(),U=r("p"),jo=t("Para habilitar el "),ss=r("em"),vo=t("streaming"),xo=t(" basta con pasar el argumento "),ts=r("code"),Eo=t("streaming=True"),yo=t(" a la funci\xF3n "),ns=r("code"),wo=t("load_dataset()"),qo=t(". Por ejemplo, carguemos el dataset de abstracts de PubMed de nuevo, pero en modo "),os=r("em"),Ao=t("streaming"),Po=t("."),ht=c(),g(ra.$$.fragment),gt=c(),F=r("p"),Do=t("En vez del "),rs=r("code"),ko=t("Dataset"),To=t(" com\xFAn y corriente que nos hemos encontrado en el resto del cap\xEDtulo, el objeto devuelto con "),ls=r("code"),Co=t("streaming=True"),zo=t(" es un "),is=r("code"),Io=t("IterableDataset"),Oo=t(". Como su nombre lo indica, para acceder a los elementos de un "),ps=r("code"),Mo=t("IterableDataset"),Ro=t(" tenemos que iterar sobre \xE9l. Podemos acceder al primer elemento de nuestro dataset de la siguiente manera:"),_t=c(),g(la.$$.fragment),bt=c(),g(ia.$$.fragment),$t=c(),Q=r("p"),So=t("Los elementos de un dataset "),ds=r("em"),Go=t("streamed"),Bo=t(" pueden ser procesados sobre la marcha usando "),cs=r("code"),No=t("IterableDataset.map()"),Lo=t(", lo que puede servirte si tienes que tokenizar los inputs. El proceso es exactamente el mismo que el que usamos para tokenizar nuestro dataset en el "),Da=r("a"),Ho=t("Cap\xEDtulo 3"),Uo=t(", con la \xFAnica diferencia de que los outputs se devuelven uno por uno."),jt=c(),g(pa.$$.fragment),vt=c(),g(da.$$.fragment),xt=c(),g(we.$$.fragment),Et=c(),J=r("p"),Fo=t("Tambi\xE9n puedes aleatorizar el orden de un dataset "),ms=r("em"),Jo=t("streamed"),Wo=t(" usando "),us=r("code"),Ko=t("IterableDataset.shuffle()"),Vo=t(", pero a diferencia de "),fs=r("code"),Yo=t("Dataset.shuffle()"),Zo=t(" esto s\xF3lo afecta a los elementos en un "),hs=r("code"),Qo=t("buffer_size"),Xo=t(" determinado:"),yt=c(),g(ca.$$.fragment),wt=c(),g(ma.$$.fragment),qt=c(),W=r("p"),er=t("En este ejemplo, seleccionamos un ejemplo aleat\xF3rio de los primeros 10.000 ejemplos en el buffer. Apenas se accede a un ejemplo, su lugar en el buffer se llena con el siguiente ejemplo en el corpus (i.e., el ejemplo n\xFAmero 10.001). Tambi\xE9n peudes seleccionar elementos de un dataset "),gs=r("em"),ar=t("streamed"),sr=t(" usando las funciones "),_s=r("code"),tr=t("IterableDataset.take()"),nr=t(" y "),bs=r("code"),or=t("IterableDataset.skip()"),rr=t(", que funcionan de manera similar a "),$s=r("code"),lr=t("Dataset.select()"),ir=t(". Por ejemplo, para seleccionar los 5 primeros ejemplos en el dataset de abstracts de PubMed podemos hacer lo siguiente:"),At=c(),g(ua.$$.fragment),Pt=c(),g(fa.$$.fragment),Dt=c(),qe=r("p"),pr=t("Tambi\xE9n podemos usar la funci\xF3n "),js=r("code"),dr=t("IterableDataset.skip()"),cr=t(" para crear conjuntos de entrenamiento y validaci\xF3n de un dataset ordenado aleat\xF3riamente as\xED:"),kt=c(),g(ha.$$.fragment),Tt=c(),B=r("p"),mr=t("Vamos a repasar la exploraci\xF3n del "),vs=r("em"),ur=t("streaming"),fr=t(" de datasets con una aplicaci\xF3n com\xFAn: combinar m\xFAltiples datasets para crear un solo corpus. \u{1F917} Datasets provee una funci\xF3n "),xs=r("code"),hr=t("interleave_datasets()"),gr=t(" que convierte una lista de objetos "),Es=r("code"),_r=t("IterableDataset"),br=t(" en un solo "),ys=r("code"),$r=t("IterableDataset"),jr=t(", donde la lista de elementos del nuevo dataset se obtiene al alternar entre los ejemplos originales. Esta funci\xF3n es particularmente \xFAtil cuando quieres combinar datasets grandes, as\xED que como ejemplo hagamos "),ws=r("em"),vr=t("streaming"),xr=t(" del conjunto FreeLaw del "),qs=r("em"),Er=t("Pile"),yr=t(", que es un dataset de 51 GB con opiniones legales de las cortes en Estados Unidos."),Ct=c(),g(ga.$$.fragment),zt=c(),g(_a.$$.fragment),It=c(),Ae=r("p"),wr=t("Este dataset es lo suficientemente grande como para llevar al l\xEDmite la RAM de la mayor\xEDa de computadores port\xE1tiles. Sin embargo, \xA1podemos cargarla y acceder a el sin esfuerzo! Ahora combinemos los ejemplos de FreeLaw y PubMed usando la funci\xF3n "),As=r("code"),qr=t("interleave_datasets()"),Ar=t(":"),Ot=c(),g(ba.$$.fragment),Mt=c(),g($a.$$.fragment),Rt=c(),te=r("p"),Pr=t("Usamos la funci\xF3n "),Ps=r("code"),Dr=t("islice()"),kr=t(" del m\xF3dulo "),Ds=r("code"),Tr=t("itertools"),Cr=t(" de Python para seleccionar los primeros dos ejemplos del dataset combinado y podemos ver que corresponden con los primeros dos ejemplos de cada uno de los dos datasets de origen."),St=c(),ne=r("p"),zr=t("Finalmente, si quieres hacer "),ks=r("em"),Ir=t("streaming"),Or=t(" del "),Ts=r("em"),Mr=t("Pile"),Rr=t(" de 825 GB en su totalidad, puedes usar todos los archivos preparados de la siguiente manera:"),Gt=c(),g(ja.$$.fragment),Bt=c(),g(va.$$.fragment),Nt=c(),g(Pe.$$.fragment),Lt=c(),ka=r("p"),Sr=t("Ya tienes todas las herramientas para cargar y procesar datasets de todas las formas y tama\xF1os, pero a menos que seas muy afortunado, llegar\xE1 un punto en tu camino de PLN en el que tendr\xE1s que crear el dataset tu mismo para resolver tu problema particular. De esto hablaremos en la siguiente secci\xF3n."),this.h()},l(e){const o=di('[data-svelte="svelte-1phssyn"]',document.head);d=l(o,"META",{name:!0,content:!0}),o.forEach(s),I=m(e),f=l(e,"H1",{class:!0});var xa=i(f);w=l(xa,"A",{id:!0,class:!0,href:!0});var Cs=i(w);C=l(Cs,"SPAN",{});var zs=i(C);_(h.$$.fragment,zs),zs.forEach(s),Cs.forEach(s),D=m(xa),q=l(xa,"SPAN",{});var Is=i(q);A=n(Is,"\xBFBig data? \u{1F917} \xA1Datasets al rescate!"),Is.forEach(s),xa.forEach(s),k=m(e),_(z.$$.fragment,e),E=m(e),y=l(e,"P",{});var Ea=i(y);O=n(Ea,"Hoy en d\xEDa es com\xFAn que tengas que trabajar con dataset de varios GB, especialmente si planeas pre-entrenar un transformador como BERT o GPT-2 desde ceros. En estos casos, "),M=l(Ea,"EM",{});var Gr=i(M);T=n(Gr,"solamente cargar"),Gr.forEach(s),G=n(Ea," los datos puede ser un desaf\xEDo. Por ejemplo, el corpus de WebText utilizado para preentrenar GPT-2 consiste de m\xE1s de 8 millones de documentos y 40 GB de texto. \xA1Cargarlo en la RAM de tu computador portatil le va a causar un paro cardiaco!"),Ea.forEach(s),x=m(e),R=l(e,"P",{});var De=i(R);Y=n(De,"Afortunadamente, \u{1F917} Datasets est\xE1 dise\xF1ado para superar estas limitaciones: te libera de problemas de manejo de memoria al tratar los datasets como archivos "),X=l(De,"EM",{});var Br=i(X);ee=n(Br,"proyectados en memoria"),Br.forEach(s),ce=n(De," ("),he=l(De,"EM",{});var Nr=i(he);Xt=n(Nr,"memory-mapped"),Nr.forEach(s),en=n(De,") y de l\xEDmites de almacenamiento al hacer "),Oa=l(De,"EM",{});var Lr=i(Oa);an=n(Lr,"streaming"),Lr.forEach(s),sn=n(De," de las entradas en un corpus."),De.forEach(s),Ms=m(e),_(Ce.$$.fragment,e),Rs=m(e),ge=l(e,"P",{});var Ut=i(ge);tn=n(Ut,"En esta secci\xF3n vamos a explorar estas funcionalidades de \u{1F917} Datasets con un corpus enorme de 825 GB conocido como el "),ze=l(Ut,"A",{href:!0,rel:!0});var Hr=i(ze);nn=n(Hr,"Pile"),Hr.forEach(s),on=n(Ut,". \xA1Comencemos!"),Ut.forEach(s),Ss=m(e),me=l(e,"H2",{class:!0});var Ft=i(me);_e=l(Ft,"A",{id:!0,class:!0,href:!0});var Ur=i(_e);Ma=l(Ur,"SPAN",{});var Fr=i(Ma);_(Ie.$$.fragment,Fr),Fr.forEach(s),Ur.forEach(s),rn=m(Ft),Ra=l(Ft,"SPAN",{});var Jr=i(Ra);ln=n(Jr,"\xBFQu\xE9 es el Pile?"),Jr.forEach(s),Ft.forEach(s),Gs=m(e),S=l(e,"P",{});var N=i(S);pn=n(N,"El "),Sa=l(N,"EM",{});var Wr=i(Sa);dn=n(Wr,"Pile"),Wr.forEach(s),cn=n(N," es un corpus de textos en ingl\xE9s creado por "),Oe=l(N,"A",{href:!0,rel:!0});var Kr=i(Oe);mn=n(Kr,"EleutherAI"),Kr.forEach(s),un=n(N," para entrenar modelos de lenguaje de gran escala. Incluye una selecci\xF3n diversa de datasets que abarca art\xEDculos cient\xEDficos, repositorios de c\xF3digo de Github y texto filtrado de la web. El corpus de entrenamiento est\xE1 disponible en "),Me=l(N,"A",{href:!0,rel:!0});var Vr=i(Me);fn=n(Vr,"partes de 14 GB"),Vr.forEach(s),hn=n(N," y tambi\xE9n puedes descargar varios de los "),Re=l(N,"A",{href:!0,rel:!0});var Yr=i(Re);gn=n(Yr,"componentes individuales"),Yr.forEach(s),_n=n(N,". Arranquemos viendo el dataset de los abstracts de PubMed, un corpus de abstracts de 15 millones de publicaciones biom\xE9dicas en "),Se=l(N,"A",{href:!0,rel:!0});var Zr=i(Se);bn=n(Zr,"PubMed"),Zr.forEach(s),$n=n(N,". Este dataset est\xE1 en formato "),Ge=l(N,"A",{href:!0,rel:!0});var Qr=i(Ge);jn=n(Qr,"JSON Lines"),Qr.forEach(s),vn=n(N," y est\xE1 comprimido con la librer\xEDa "),Ga=l(N,"CODE",{});var Xr=i(Ga);xn=n(Xr,"zstandard"),Xr.forEach(s),En=n(N,", as\xED que primero tenemos que instalarla:"),N.forEach(s),Bs=m(e),_(Be.$$.fragment,e),Ns=m(e),be=l(e,"P",{});var Jt=i(be);yn=n(Jt,"A continuaci\xF3n, podemos cargar el dataset usando el m\xE9todo para archivos remotos que aprendimos en la "),ya=l(Jt,"A",{href:!0});var el=i(ya);wn=n(el,"secci\xF3n 2"),el.forEach(s),qn=n(Jt,":"),Jt.forEach(s),Ls=m(e),_(Ne.$$.fragment,e),Hs=m(e),_(Le.$$.fragment,e),Us=m(e),wa=l(e,"P",{});var al=i(wa);An=n(al,"Como podemos ver, hay 15.518.009 filas y dos columnas en el dataset, \xA1un mont\xF3n!"),al.forEach(s),Fs=m(e),_($e.$$.fragment,e),Js=m(e),qa=l(e,"P",{});var sl=i(qa);Pn=n(sl,"Veamos el contenido del primer ejemplo:"),sl.forEach(s),Ws=m(e),_(He.$$.fragment,e),Ks=m(e),_(Ue.$$.fragment,e),Vs=m(e),Aa=l(e,"P",{});var tl=i(Aa);Dn=n(tl,"Ok, esto parece el abstract de un art\xEDculo m\xE9dico. Ahora miremos cu\xE1nta RAM hemos usado para cargar el dataset."),tl.forEach(s),Ys=m(e),ue=l(e,"H2",{class:!0});var Wt=i(ue);je=l(Wt,"A",{id:!0,class:!0,href:!0});var nl=i(je);Ba=l(nl,"SPAN",{});var ol=i(Ba);_(Fe.$$.fragment,ol),ol.forEach(s),nl.forEach(s),kn=m(Wt),Na=l(Wt,"SPAN",{});var rl=i(Na);Tn=n(rl,"La magia de la proyecci\xF3n en memoria"),rl.forEach(s),Wt.forEach(s),Zs=m(e),ae=l(e,"P",{});var Ta=i(ae);Cn=n(Ta,"Una forma simple de medir el uso de memoria en Python es con la librer\xEDa "),Je=l(Ta,"A",{href:!0,rel:!0});var ll=i(Je);La=l(ll,"CODE",{});var il=i(La);zn=n(il,"psutil"),il.forEach(s),ll.forEach(s),In=n(Ta,", que se puede instalar con "),Ha=l(Ta,"CODE",{});var pl=i(Ha);On=n(pl,"pip"),pl.forEach(s),Mn=n(Ta," as\xED:"),Ta.forEach(s),Qs=m(e),_(We.$$.fragment,e),Xs=m(e),ve=l(e,"P",{});var Kt=i(ve);Rn=n(Kt,"Esta librer\xEDa contiene una clase "),Ua=l(Kt,"CODE",{});var dl=i(Ua);Sn=n(dl,"Process"),dl.forEach(s),Gn=n(Kt," que nos permite revisar el uso de memoria del proceso actual:"),Kt.forEach(s),et=m(e),_(Ke.$$.fragment,e),at=m(e),_(Ve.$$.fragment,e),st=m(e),Z=l(e,"P",{});var ke=i(Z);Bn=n(ke,"El atributo "),Fa=l(ke,"CODE",{});var cl=i(Fa);Nn=n(cl,"rss"),cl.forEach(s),Ln=n(ke," se refiere al "),Ja=l(ke,"EM",{});var ml=i(Ja);Hn=n(ml,"resident set size"),ml.forEach(s),Un=n(ke,", que es la fracci\xF3n de memoria que un proceso ocupa en RAM. Esta medici\xF3n tambi\xE9n incluye la memoria usada por el int\xE9rprete de Python y las librer\xEDas que hemos cargado, as\xED que la cantidad real de memoria usada para cargar el dataset es un poco m\xE1s peque\xF1a. A modo de comparaci\xF3n, veamos qu\xE9 tan grande es el dataset en disco, usando el atributo "),Wa=l(ke,"CODE",{});var ul=i(Wa);Fn=n(ul,"dataset_size"),ul.forEach(s),Jn=n(ke,". Dado que el resultado est\xE1 expresado en bytes, tenemos que convertirlo manualmente en gigabytes:"),ke.forEach(s),tt=m(e),_(Ye.$$.fragment,e),nt=m(e),_(Ze.$$.fragment,e),ot=m(e),Pa=l(e,"P",{});var fl=i(Pa);Wn=n(fl,"Bien, a pesar de que el archivo es de casi 20 GB, \xA1podemos cargarlo y acceder a su contenido con mucha menos RAM!"),fl.forEach(s),rt=m(e),_(xe.$$.fragment,e),lt=m(e),se=l(e,"P",{});var Ca=i(se);Kn=n(Ca,"Si est\xE1s familiarizado con Pandas, este resultado puede ser sorprendente por la famosa "),Qe=l(Ca,"A",{href:!0,rel:!0});var hl=i(Qe);Vn=n(hl,"regla de Wes Kinney"),hl.forEach(s),Yn=n(Ca," que indica que t\xEDpicamente necesitas de 5 a 10 veces la RAM que el tama\xF1o del archivo de tu dataset. \xBFC\xF3mo resuelve entonces \u{1F917} Datasets este problema de manejo de memoria? \u{1F917} Datasets trata cada dataset como un "),Xe=l(Ca,"A",{href:!0,rel:!0});var gl=i(Xe);Zn=n(gl,"archivo proyectado en memoria"),gl.forEach(s),Qn=n(Ca,", lo que permite un mapeo entre la RAM y el sistema de almacenamiento de archivos, que le permite a la librer\xEDa acceder y operar los elementos del dataset sin necesidad de tenerlos cargados completamente en memoria."),Ca.forEach(s),it=m(e),L=l(e,"P",{});var oe=i(L);Xn=n(oe,"Los archivos proyectados en memoria tambi\xE9n pueden ser compartidos por m\xFAltiples procesos, lo que habilita la paralelizaci\xF3n de m\xE9todos como "),Ka=l(oe,"CODE",{});var _l=i(Ka);eo=n(_l,"Dataset.map()"),_l.forEach(s),ao=n(oe," sin que sea obligatorio mover o copiar el dataset. Internamente, estas capacidades se logran gracias al formato de memoria "),ea=l(oe,"A",{href:!0,rel:!0});var bl=i(ea);so=n(bl,"Apache Arrow"),bl.forEach(s),to=n(oe," y la librer\xEDa "),aa=l(oe,"A",{href:!0,rel:!0});var $l=i(aa);Va=l($l,"CODE",{});var jl=i(Va);no=n(jl,"pyarrow"),jl.forEach(s),$l.forEach(s),oo=n(oe,", que permiten la carga y procesamiento de datos a gran velocidad. (Para ahondar m\xE1s en Apache Arrow y algunas comparaciones con Pandas, revisa el "),sa=l(oe,"A",{href:!0,rel:!0});var vl=i(sa);ro=n(vl,"blog de Dejan Simic"),vl.forEach(s),lo=n(oe,"). Para verlo en acci\xF3n, ejecutemos un test de velocidad iterando sobre todos los elementos del dataset de abstracts de PubMed:"),oe.forEach(s),pt=m(e),_(ta.$$.fragment,e),dt=m(e),_(na.$$.fragment,e),ct=m(e),H=l(e,"P",{});var re=i(H);io=n(re,"Aqu\xED usamos el m\xF3dulo "),Ya=l(re,"CODE",{});var xl=i(Ya);po=n(xl,"timeit"),xl.forEach(s),co=n(re," de Python para medir el tiempo de ejecuci\xF3n que se toma "),Za=l(re,"CODE",{});var El=i(Za);mo=n(El,"code_snippet"),El.forEach(s),uo=n(re,". Tipicamemente, puedes iterar a lo largo de un dataset a una velocidad de unas cuantas d\xE9cimas de un GB por segundo. Esto funciona muy bien para la gran mayor\xEDa de aplicaciones, pero algunas veces tendr\xE1s que trabajar con un dataset que es tan grande para incluso almacenarse en el disco de tu computador. Por ejemplo, si quisieramos descargar el "),Qa=l(re,"EM",{});var yl=i(Qa);fo=n(yl,"Pile"),yl.forEach(s),ho=n(re," completo \xA1necesitar\xEDamos 825 GB de almacenamiento libre! Para trabajar con esos casos, \u{1F917} Datasets puede trabajar haciendo "),Xa=l(re,"EM",{});var wl=i(Xa);go=n(wl,"streaming"),wl.forEach(s),_o=n(re,", lo que permite la descarga y acceso a los elementos sobre la marcha, sin necesidad de descargar todo el dataset. Veamos c\xF3mo funciona:"),re.forEach(s),mt=m(e),_(Ee.$$.fragment,e),ut=m(e),fe=l(e,"H2",{class:!0});var Vt=i(fe);ye=l(Vt,"A",{id:!0,class:!0,href:!0});var ql=i(ye);es=l(ql,"SPAN",{});var Al=i(es);_(oa.$$.fragment,Al),Al.forEach(s),ql.forEach(s),bo=m(Vt),as=l(Vt,"SPAN",{});var Pl=i(as);$o=n(Pl,"Haciendo _streaming_ de datasets"),Pl.forEach(s),Vt.forEach(s),ft=m(e),U=l(e,"P",{});var le=i(U);jo=n(le,"Para habilitar el "),ss=l(le,"EM",{});var Dl=i(ss);vo=n(Dl,"streaming"),Dl.forEach(s),xo=n(le," basta con pasar el argumento "),ts=l(le,"CODE",{});var kl=i(ts);Eo=n(kl,"streaming=True"),kl.forEach(s),yo=n(le," a la funci\xF3n "),ns=l(le,"CODE",{});var Tl=i(ns);wo=n(Tl,"load_dataset()"),Tl.forEach(s),qo=n(le,". Por ejemplo, carguemos el dataset de abstracts de PubMed de nuevo, pero en modo "),os=l(le,"EM",{});var Cl=i(os);Ao=n(Cl,"streaming"),Cl.forEach(s),Po=n(le,"."),le.forEach(s),ht=m(e),_(ra.$$.fragment,e),gt=m(e),F=l(e,"P",{});var ie=i(F);Do=n(ie,"En vez del "),rs=l(ie,"CODE",{});var zl=i(rs);ko=n(zl,"Dataset"),zl.forEach(s),To=n(ie," com\xFAn y corriente que nos hemos encontrado en el resto del cap\xEDtulo, el objeto devuelto con "),ls=l(ie,"CODE",{});var Il=i(ls);Co=n(Il,"streaming=True"),Il.forEach(s),zo=n(ie," es un "),is=l(ie,"CODE",{});var Ol=i(is);Io=n(Ol,"IterableDataset"),Ol.forEach(s),Oo=n(ie,". Como su nombre lo indica, para acceder a los elementos de un "),ps=l(ie,"CODE",{});var Ml=i(ps);Mo=n(Ml,"IterableDataset"),Ml.forEach(s),Ro=n(ie," tenemos que iterar sobre \xE9l. Podemos acceder al primer elemento de nuestro dataset de la siguiente manera:"),ie.forEach(s),_t=m(e),_(la.$$.fragment,e),bt=m(e),_(ia.$$.fragment,e),$t=m(e),Q=l(e,"P",{});var Te=i(Q);So=n(Te,"Los elementos de un dataset "),ds=l(Te,"EM",{});var Rl=i(ds);Go=n(Rl,"streamed"),Rl.forEach(s),Bo=n(Te," pueden ser procesados sobre la marcha usando "),cs=l(Te,"CODE",{});var Sl=i(cs);No=n(Sl,"IterableDataset.map()"),Sl.forEach(s),Lo=n(Te,", lo que puede servirte si tienes que tokenizar los inputs. El proceso es exactamente el mismo que el que usamos para tokenizar nuestro dataset en el "),Da=l(Te,"A",{href:!0});var Gl=i(Da);Ho=n(Gl,"Cap\xEDtulo 3"),Gl.forEach(s),Uo=n(Te,", con la \xFAnica diferencia de que los outputs se devuelven uno por uno."),Te.forEach(s),jt=m(e),_(pa.$$.fragment,e),vt=m(e),_(da.$$.fragment,e),xt=m(e),_(we.$$.fragment,e),Et=m(e),J=l(e,"P",{});var pe=i(J);Fo=n(pe,"Tambi\xE9n puedes aleatorizar el orden de un dataset "),ms=l(pe,"EM",{});var Bl=i(ms);Jo=n(Bl,"streamed"),Bl.forEach(s),Wo=n(pe," usando "),us=l(pe,"CODE",{});var Nl=i(us);Ko=n(Nl,"IterableDataset.shuffle()"),Nl.forEach(s),Vo=n(pe,", pero a diferencia de "),fs=l(pe,"CODE",{});var Ll=i(fs);Yo=n(Ll,"Dataset.shuffle()"),Ll.forEach(s),Zo=n(pe," esto s\xF3lo afecta a los elementos en un "),hs=l(pe,"CODE",{});var Hl=i(hs);Qo=n(Hl,"buffer_size"),Hl.forEach(s),Xo=n(pe," determinado:"),pe.forEach(s),yt=m(e),_(ca.$$.fragment,e),wt=m(e),_(ma.$$.fragment,e),qt=m(e),W=l(e,"P",{});var de=i(W);er=n(de,"En este ejemplo, seleccionamos un ejemplo aleat\xF3rio de los primeros 10.000 ejemplos en el buffer. Apenas se accede a un ejemplo, su lugar en el buffer se llena con el siguiente ejemplo en el corpus (i.e., el ejemplo n\xFAmero 10.001). Tambi\xE9n peudes seleccionar elementos de un dataset "),gs=l(de,"EM",{});var Ul=i(gs);ar=n(Ul,"streamed"),Ul.forEach(s),sr=n(de," usando las funciones "),_s=l(de,"CODE",{});var Fl=i(_s);tr=n(Fl,"IterableDataset.take()"),Fl.forEach(s),nr=n(de," y "),bs=l(de,"CODE",{});var Jl=i(bs);or=n(Jl,"IterableDataset.skip()"),Jl.forEach(s),rr=n(de,", que funcionan de manera similar a "),$s=l(de,"CODE",{});var Wl=i($s);lr=n(Wl,"Dataset.select()"),Wl.forEach(s),ir=n(de,". Por ejemplo, para seleccionar los 5 primeros ejemplos en el dataset de abstracts de PubMed podemos hacer lo siguiente:"),de.forEach(s),At=m(e),_(ua.$$.fragment,e),Pt=m(e),_(fa.$$.fragment,e),Dt=m(e),qe=l(e,"P",{});var Yt=i(qe);pr=n(Yt,"Tambi\xE9n podemos usar la funci\xF3n "),js=l(Yt,"CODE",{});var Kl=i(js);dr=n(Kl,"IterableDataset.skip()"),Kl.forEach(s),cr=n(Yt," para crear conjuntos de entrenamiento y validaci\xF3n de un dataset ordenado aleat\xF3riamente as\xED:"),Yt.forEach(s),kt=m(e),_(ha.$$.fragment,e),Tt=m(e),B=l(e,"P",{});var K=i(B);mr=n(K,"Vamos a repasar la exploraci\xF3n del "),vs=l(K,"EM",{});var Vl=i(vs);ur=n(Vl,"streaming"),Vl.forEach(s),fr=n(K," de datasets con una aplicaci\xF3n com\xFAn: combinar m\xFAltiples datasets para crear un solo corpus. \u{1F917} Datasets provee una funci\xF3n "),xs=l(K,"CODE",{});var Yl=i(xs);hr=n(Yl,"interleave_datasets()"),Yl.forEach(s),gr=n(K," que convierte una lista de objetos "),Es=l(K,"CODE",{});var Zl=i(Es);_r=n(Zl,"IterableDataset"),Zl.forEach(s),br=n(K," en un solo "),ys=l(K,"CODE",{});var Ql=i(ys);$r=n(Ql,"IterableDataset"),Ql.forEach(s),jr=n(K,", donde la lista de elementos del nuevo dataset se obtiene al alternar entre los ejemplos originales. Esta funci\xF3n es particularmente \xFAtil cuando quieres combinar datasets grandes, as\xED que como ejemplo hagamos "),ws=l(K,"EM",{});var Xl=i(ws);vr=n(Xl,"streaming"),Xl.forEach(s),xr=n(K," del conjunto FreeLaw del "),qs=l(K,"EM",{});var ei=i(qs);Er=n(ei,"Pile"),ei.forEach(s),yr=n(K,", que es un dataset de 51 GB con opiniones legales de las cortes en Estados Unidos."),K.forEach(s),Ct=m(e),_(ga.$$.fragment,e),zt=m(e),_(_a.$$.fragment,e),It=m(e),Ae=l(e,"P",{});var Zt=i(Ae);wr=n(Zt,"Este dataset es lo suficientemente grande como para llevar al l\xEDmite la RAM de la mayor\xEDa de computadores port\xE1tiles. Sin embargo, \xA1podemos cargarla y acceder a el sin esfuerzo! Ahora combinemos los ejemplos de FreeLaw y PubMed usando la funci\xF3n "),As=l(Zt,"CODE",{});var ai=i(As);qr=n(ai,"interleave_datasets()"),ai.forEach(s),Ar=n(Zt,":"),Zt.forEach(s),Ot=m(e),_(ba.$$.fragment,e),Mt=m(e),_($a.$$.fragment,e),Rt=m(e),te=l(e,"P",{});var za=i(te);Pr=n(za,"Usamos la funci\xF3n "),Ps=l(za,"CODE",{});var si=i(Ps);Dr=n(si,"islice()"),si.forEach(s),kr=n(za," del m\xF3dulo "),Ds=l(za,"CODE",{});var ti=i(Ds);Tr=n(ti,"itertools"),ti.forEach(s),Cr=n(za," de Python para seleccionar los primeros dos ejemplos del dataset combinado y podemos ver que corresponden con los primeros dos ejemplos de cada uno de los dos datasets de origen."),za.forEach(s),St=m(e),ne=l(e,"P",{});var Ia=i(ne);zr=n(Ia,"Finalmente, si quieres hacer "),ks=l(Ia,"EM",{});var ni=i(ks);Ir=n(ni,"streaming"),ni.forEach(s),Or=n(Ia," del "),Ts=l(Ia,"EM",{});var oi=i(Ts);Mr=n(oi,"Pile"),oi.forEach(s),Rr=n(Ia," de 825 GB en su totalidad, puedes usar todos los archivos preparados de la siguiente manera:"),Ia.forEach(s),Gt=m(e),_(ja.$$.fragment,e),Bt=m(e),_(va.$$.fragment,e),Nt=m(e),_(Pe.$$.fragment,e),Lt=m(e),ka=l(e,"P",{});var ri=i(ka);Sr=n(ri,"Ya tienes todas las herramientas para cargar y procesar datasets de todas las formas y tama\xF1os, pero a menos que seas muy afortunado, llegar\xE1 un punto en tu camino de PLN en el que tendr\xE1s que crear el dataset tu mismo para resolver tu problema particular. De esto hablaremos en la siguiente secci\xF3n."),ri.forEach(s),this.h()},h(){u(d,"name","hf:doc:metadata"),u(d,"content",JSON.stringify(ji)),u(w,"id","big-data-datasets-al-rescate"),u(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(w,"href","#big-data-datasets-al-rescate"),u(f,"class","relative group"),u(ze,"href","https://pile.eleuther.ai"),u(ze,"rel","nofollow"),u(_e,"id","qu-es-el-pile"),u(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(_e,"href","#qu-es-el-pile"),u(me,"class","relative group"),u(Oe,"href","https://www.eleuther.ai"),u(Oe,"rel","nofollow"),u(Me,"href","https://mystic.the-eye.eu/public/AI/pile/"),u(Me,"rel","nofollow"),u(Re,"href","https://mystic.the-eye.eu/public/AI/pile_preliminary_components/"),u(Re,"rel","nofollow"),u(Se,"href","https://pubmed.ncbi.nlm.nih.gov/"),u(Se,"rel","nofollow"),u(Ge,"href","https://jsonlines.org"),u(Ge,"rel","nofollow"),u(ya,"href","/course/chapter5/2"),u(je,"id","la-magia-de-la-proyeccin-en-memoria"),u(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(je,"href","#la-magia-de-la-proyeccin-en-memoria"),u(ue,"class","relative group"),u(Je,"href","https://psutil.readthedocs.io/en/latest/"),u(Je,"rel","nofollow"),u(Qe,"href","https://wesmckinney.com/blog/apache-arrow-pandas-internals/"),u(Qe,"rel","nofollow"),u(Xe,"href","https://en.wikipedia.org/wiki/Memory-mapped_file"),u(Xe,"rel","nofollow"),u(ea,"href","https://arrow.apache.org"),u(ea,"rel","nofollow"),u(aa,"href","https://arrow.apache.org/docs/python/index.html"),u(aa,"rel","nofollow"),u(sa,"href","https://towardsdatascience.com/apache-arrow-read-dataframe-with-zero-memory-69634092b1a"),u(sa,"rel","nofollow"),u(ye,"id","haciendo-streaming-de-datasets"),u(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ye,"href","#haciendo-streaming-de-datasets"),u(fe,"class","relative group"),u(Da,"href","/course/chapter3")},m(e,o){a(document.head,d),p(e,I,o),p(e,f,o),a(f,w),a(w,C),b(h,C,null),a(f,D),a(f,q),a(q,A),p(e,k,o),b(z,e,o),p(e,E,o),p(e,y,o),a(y,O),a(y,M),a(M,T),a(y,G),p(e,x,o),p(e,R,o),a(R,Y),a(R,X),a(X,ee),a(R,ce),a(R,he),a(he,Xt),a(R,en),a(R,Oa),a(Oa,an),a(R,sn),p(e,Ms,o),b(Ce,e,o),p(e,Rs,o),p(e,ge,o),a(ge,tn),a(ge,ze),a(ze,nn),a(ge,on),p(e,Ss,o),p(e,me,o),a(me,_e),a(_e,Ma),b(Ie,Ma,null),a(me,rn),a(me,Ra),a(Ra,ln),p(e,Gs,o),p(e,S,o),a(S,pn),a(S,Sa),a(Sa,dn),a(S,cn),a(S,Oe),a(Oe,mn),a(S,un),a(S,Me),a(Me,fn),a(S,hn),a(S,Re),a(Re,gn),a(S,_n),a(S,Se),a(Se,bn),a(S,$n),a(S,Ge),a(Ge,jn),a(S,vn),a(S,Ga),a(Ga,xn),a(S,En),p(e,Bs,o),b(Be,e,o),p(e,Ns,o),p(e,be,o),a(be,yn),a(be,ya),a(ya,wn),a(be,qn),p(e,Ls,o),b(Ne,e,o),p(e,Hs,o),b(Le,e,o),p(e,Us,o),p(e,wa,o),a(wa,An),p(e,Fs,o),b($e,e,o),p(e,Js,o),p(e,qa,o),a(qa,Pn),p(e,Ws,o),b(He,e,o),p(e,Ks,o),b(Ue,e,o),p(e,Vs,o),p(e,Aa,o),a(Aa,Dn),p(e,Ys,o),p(e,ue,o),a(ue,je),a(je,Ba),b(Fe,Ba,null),a(ue,kn),a(ue,Na),a(Na,Tn),p(e,Zs,o),p(e,ae,o),a(ae,Cn),a(ae,Je),a(Je,La),a(La,zn),a(ae,In),a(ae,Ha),a(Ha,On),a(ae,Mn),p(e,Qs,o),b(We,e,o),p(e,Xs,o),p(e,ve,o),a(ve,Rn),a(ve,Ua),a(Ua,Sn),a(ve,Gn),p(e,et,o),b(Ke,e,o),p(e,at,o),b(Ve,e,o),p(e,st,o),p(e,Z,o),a(Z,Bn),a(Z,Fa),a(Fa,Nn),a(Z,Ln),a(Z,Ja),a(Ja,Hn),a(Z,Un),a(Z,Wa),a(Wa,Fn),a(Z,Jn),p(e,tt,o),b(Ye,e,o),p(e,nt,o),b(Ze,e,o),p(e,ot,o),p(e,Pa,o),a(Pa,Wn),p(e,rt,o),b(xe,e,o),p(e,lt,o),p(e,se,o),a(se,Kn),a(se,Qe),a(Qe,Vn),a(se,Yn),a(se,Xe),a(Xe,Zn),a(se,Qn),p(e,it,o),p(e,L,o),a(L,Xn),a(L,Ka),a(Ka,eo),a(L,ao),a(L,ea),a(ea,so),a(L,to),a(L,aa),a(aa,Va),a(Va,no),a(L,oo),a(L,sa),a(sa,ro),a(L,lo),p(e,pt,o),b(ta,e,o),p(e,dt,o),b(na,e,o),p(e,ct,o),p(e,H,o),a(H,io),a(H,Ya),a(Ya,po),a(H,co),a(H,Za),a(Za,mo),a(H,uo),a(H,Qa),a(Qa,fo),a(H,ho),a(H,Xa),a(Xa,go),a(H,_o),p(e,mt,o),b(Ee,e,o),p(e,ut,o),p(e,fe,o),a(fe,ye),a(ye,es),b(oa,es,null),a(fe,bo),a(fe,as),a(as,$o),p(e,ft,o),p(e,U,o),a(U,jo),a(U,ss),a(ss,vo),a(U,xo),a(U,ts),a(ts,Eo),a(U,yo),a(U,ns),a(ns,wo),a(U,qo),a(U,os),a(os,Ao),a(U,Po),p(e,ht,o),b(ra,e,o),p(e,gt,o),p(e,F,o),a(F,Do),a(F,rs),a(rs,ko),a(F,To),a(F,ls),a(ls,Co),a(F,zo),a(F,is),a(is,Io),a(F,Oo),a(F,ps),a(ps,Mo),a(F,Ro),p(e,_t,o),b(la,e,o),p(e,bt,o),b(ia,e,o),p(e,$t,o),p(e,Q,o),a(Q,So),a(Q,ds),a(ds,Go),a(Q,Bo),a(Q,cs),a(cs,No),a(Q,Lo),a(Q,Da),a(Da,Ho),a(Q,Uo),p(e,jt,o),b(pa,e,o),p(e,vt,o),b(da,e,o),p(e,xt,o),b(we,e,o),p(e,Et,o),p(e,J,o),a(J,Fo),a(J,ms),a(ms,Jo),a(J,Wo),a(J,us),a(us,Ko),a(J,Vo),a(J,fs),a(fs,Yo),a(J,Zo),a(J,hs),a(hs,Qo),a(J,Xo),p(e,yt,o),b(ca,e,o),p(e,wt,o),b(ma,e,o),p(e,qt,o),p(e,W,o),a(W,er),a(W,gs),a(gs,ar),a(W,sr),a(W,_s),a(_s,tr),a(W,nr),a(W,bs),a(bs,or),a(W,rr),a(W,$s),a($s,lr),a(W,ir),p(e,At,o),b(ua,e,o),p(e,Pt,o),b(fa,e,o),p(e,Dt,o),p(e,qe,o),a(qe,pr),a(qe,js),a(js,dr),a(qe,cr),p(e,kt,o),b(ha,e,o),p(e,Tt,o),p(e,B,o),a(B,mr),a(B,vs),a(vs,ur),a(B,fr),a(B,xs),a(xs,hr),a(B,gr),a(B,Es),a(Es,_r),a(B,br),a(B,ys),a(ys,$r),a(B,jr),a(B,ws),a(ws,vr),a(B,xr),a(B,qs),a(qs,Er),a(B,yr),p(e,Ct,o),b(ga,e,o),p(e,zt,o),b(_a,e,o),p(e,It,o),p(e,Ae,o),a(Ae,wr),a(Ae,As),a(As,qr),a(Ae,Ar),p(e,Ot,o),b(ba,e,o),p(e,Mt,o),b($a,e,o),p(e,Rt,o),p(e,te,o),a(te,Pr),a(te,Ps),a(Ps,Dr),a(te,kr),a(te,Ds),a(Ds,Tr),a(te,Cr),p(e,St,o),p(e,ne,o),a(ne,zr),a(ne,ks),a(ks,Ir),a(ne,Or),a(ne,Ts),a(Ts,Mr),a(ne,Rr),p(e,Gt,o),b(ja,e,o),p(e,Bt,o),b(va,e,o),p(e,Nt,o),b(Pe,e,o),p(e,Lt,o),p(e,ka,o),a(ka,Sr),Ht=!0},p(e,[o]){const xa={};o&2&&(xa.$$scope={dirty:o,ctx:e}),$e.$set(xa);const Cs={};o&2&&(Cs.$$scope={dirty:o,ctx:e}),xe.$set(Cs);const zs={};o&2&&(zs.$$scope={dirty:o,ctx:e}),Ee.$set(zs);const Is={};o&2&&(Is.$$scope={dirty:o,ctx:e}),we.$set(Is);const Ea={};o&2&&(Ea.$$scope={dirty:o,ctx:e}),Pe.$set(Ea)},i(e){Ht||($(h.$$.fragment,e),$(z.$$.fragment,e),$(Ce.$$.fragment,e),$(Ie.$$.fragment,e),$(Be.$$.fragment,e),$(Ne.$$.fragment,e),$(Le.$$.fragment,e),$($e.$$.fragment,e),$(He.$$.fragment,e),$(Ue.$$.fragment,e),$(Fe.$$.fragment,e),$(We.$$.fragment,e),$(Ke.$$.fragment,e),$(Ve.$$.fragment,e),$(Ye.$$.fragment,e),$(Ze.$$.fragment,e),$(xe.$$.fragment,e),$(ta.$$.fragment,e),$(na.$$.fragment,e),$(Ee.$$.fragment,e),$(oa.$$.fragment,e),$(ra.$$.fragment,e),$(la.$$.fragment,e),$(ia.$$.fragment,e),$(pa.$$.fragment,e),$(da.$$.fragment,e),$(we.$$.fragment,e),$(ca.$$.fragment,e),$(ma.$$.fragment,e),$(ua.$$.fragment,e),$(fa.$$.fragment,e),$(ha.$$.fragment,e),$(ga.$$.fragment,e),$(_a.$$.fragment,e),$(ba.$$.fragment,e),$($a.$$.fragment,e),$(ja.$$.fragment,e),$(va.$$.fragment,e),$(Pe.$$.fragment,e),Ht=!0)},o(e){j(h.$$.fragment,e),j(z.$$.fragment,e),j(Ce.$$.fragment,e),j(Ie.$$.fragment,e),j(Be.$$.fragment,e),j(Ne.$$.fragment,e),j(Le.$$.fragment,e),j($e.$$.fragment,e),j(He.$$.fragment,e),j(Ue.$$.fragment,e),j(Fe.$$.fragment,e),j(We.$$.fragment,e),j(Ke.$$.fragment,e),j(Ve.$$.fragment,e),j(Ye.$$.fragment,e),j(Ze.$$.fragment,e),j(xe.$$.fragment,e),j(ta.$$.fragment,e),j(na.$$.fragment,e),j(Ee.$$.fragment,e),j(oa.$$.fragment,e),j(ra.$$.fragment,e),j(la.$$.fragment,e),j(ia.$$.fragment,e),j(pa.$$.fragment,e),j(da.$$.fragment,e),j(we.$$.fragment,e),j(ca.$$.fragment,e),j(ma.$$.fragment,e),j(ua.$$.fragment,e),j(fa.$$.fragment,e),j(ha.$$.fragment,e),j(ga.$$.fragment,e),j(_a.$$.fragment,e),j(ba.$$.fragment,e),j($a.$$.fragment,e),j(ja.$$.fragment,e),j(va.$$.fragment,e),j(Pe.$$.fragment,e),Ht=!1},d(e){s(d),e&&s(I),e&&s(f),v(h),e&&s(k),v(z,e),e&&s(E),e&&s(y),e&&s(x),e&&s(R),e&&s(Ms),v(Ce,e),e&&s(Rs),e&&s(ge),e&&s(Ss),e&&s(me),v(Ie),e&&s(Gs),e&&s(S),e&&s(Bs),v(Be,e),e&&s(Ns),e&&s(be),e&&s(Ls),v(Ne,e),e&&s(Hs),v(Le,e),e&&s(Us),e&&s(wa),e&&s(Fs),v($e,e),e&&s(Js),e&&s(qa),e&&s(Ws),v(He,e),e&&s(Ks),v(Ue,e),e&&s(Vs),e&&s(Aa),e&&s(Ys),e&&s(ue),v(Fe),e&&s(Zs),e&&s(ae),e&&s(Qs),v(We,e),e&&s(Xs),e&&s(ve),e&&s(et),v(Ke,e),e&&s(at),v(Ve,e),e&&s(st),e&&s(Z),e&&s(tt),v(Ye,e),e&&s(nt),v(Ze,e),e&&s(ot),e&&s(Pa),e&&s(rt),v(xe,e),e&&s(lt),e&&s(se),e&&s(it),e&&s(L),e&&s(pt),v(ta,e),e&&s(dt),v(na,e),e&&s(ct),e&&s(H),e&&s(mt),v(Ee,e),e&&s(ut),e&&s(fe),v(oa),e&&s(ft),e&&s(U),e&&s(ht),v(ra,e),e&&s(gt),e&&s(F),e&&s(_t),v(la,e),e&&s(bt),v(ia,e),e&&s($t),e&&s(Q),e&&s(jt),v(pa,e),e&&s(vt),v(da,e),e&&s(xt),v(we,e),e&&s(Et),e&&s(J),e&&s(yt),v(ca,e),e&&s(wt),v(ma,e),e&&s(qt),e&&s(W),e&&s(At),v(ua,e),e&&s(Pt),v(fa,e),e&&s(Dt),e&&s(qe),e&&s(kt),v(ha,e),e&&s(Tt),e&&s(B),e&&s(Ct),v(ga,e),e&&s(zt),v(_a,e),e&&s(It),e&&s(Ae),e&&s(Ot),v(ba,e),e&&s(Mt),v($a,e),e&&s(Rt),e&&s(te),e&&s(St),e&&s(ne),e&&s(Gt),v(ja,e),e&&s(Bt),v(va,e),e&&s(Nt),v(Pe,e),e&&s(Lt),e&&s(ka)}}}const ji={local:"big-data-datasets-al-rescate",sections:[{local:"qu-es-el-pile",title:"\xBFQu\xE9 es el Pile?"},{local:"la-magia-de-la-proyeccin-en-memoria",title:"La magia de la proyecci\xF3n en memoria"},{local:"haciendo-streaming-de-datasets",title:"Haciendo _streaming_ de datasets"}],title:"\xBFBig data? \u{1F917} \xA1Datasets al rescate!"};function vi(V){return ci(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Pi extends li{constructor(d){super();ii(this,d,vi,$i,pi,{})}}export{Pi as default,ji as metadata};
