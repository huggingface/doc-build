import{S as mt,i as ft,s as ht,e as n,k as m,w,t as N,l as ct,M as $t,c as s,d as a,m as f,x as y,a as i,h as T,b as l,G as t,g as p,y as q,o as g,p as dt,q as k,B as z,v as vt,n as pt}from"../../chunks/vendor-hf-doc-builder.js";import{I as H}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ca}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as gt}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{Q as I}from"../../chunks/Question-hf-doc-builder.js";import{F as kt}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function bt(M){let c,d,$,h,A,b,S,E,v,j;return h=new H({}),v=new I({props:{choices:[{text:"Un modelo que entrena autom\xE1ticamente en tus datos",explain:"Incorrecto. \xBFEst\xE1s confundiendo esto con nuestro producto <a href='https://huggingface.co/autotrain'>AutoTrain</a>?"},{text:"Un objeto que devuelve la arquitectura correcta basado en el punto de control",explain:"Exacto: el <code>TFAutoModel</code> s\xF3lo necesita conocer el punto de control desde el cual inicializar para devolver la arquitectura correcta.",correct:!0},{text:"Un modelo que detecta autom\xE1ticamente el lenguaje usado por sus entradas para cargar los pesos correctos",explain:"Incorrecto; auqneu algunos puntos de control y modelos son capaced de manejar varios lenguajes, no hay herramientas integradas para la selecci\xF3n autom\xE1tica de punto de control de acuerdo al lenguaje. \xA1Deber\xEDas dirigirte a <a href='https://huggingface.co/models'>Model Hub</a> para encontrar el mejor punto de control para tu tarea!"}]}}),{c(){c=n("h3"),d=n("a"),$=n("span"),w(h.$$.fragment),A=m(),b=n("span"),S=N("5. \xBFQu\xE9 es un TFAutoModel?"),E=m(),w(v.$$.fragment),this.h()},l(r){c=s(r,"H3",{class:!0});var _=i(c);d=s(_,"A",{id:!0,class:!0,href:!0});var o=i(d);$=s(o,"SPAN",{});var x=i($);y(h.$$.fragment,x),x.forEach(a),o.forEach(a),A=f(_),b=s(_,"SPAN",{});var P=i(b);S=T(P,"5. \xBFQu\xE9 es un TFAutoModel?"),P.forEach(a),_.forEach(a),E=f(r),y(v.$$.fragment,r),this.h()},h(){l(d,"id","5.-\xBFqu\xE9-es-un-tfautomodel?"),l(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(d,"href","#5.-\xBFqu\xE9-es-un-tfautomodel?"),l(c,"class","relative group")},m(r,_){p(r,c,_),t(c,d),t(d,$),q(h,$,null),t(c,A),t(c,b),t(b,S),p(r,E,_),q(v,r,_),j=!0},i(r){j||(k(h.$$.fragment,r),k(v.$$.fragment,r),j=!0)},o(r){g(h.$$.fragment,r),g(v.$$.fragment,r),j=!1},d(r){r&&a(c),z(h),r&&a(E),z(v,r)}}}function _t(M){let c,d,$,h,A,b,S,E,v,j;return h=new H({}),v=new I({props:{choices:[{text:"Un modelo que entrena autom\xE1ticamente en tus datos",explain:"Incorrecto. \xBFEst\xE1s confundiendo esto con nuestro producto <a href='https://huggingface.co/autotrain'>AutoTrain</a>?"},{text:"Un objeto que devuelve la arquitectura correcta basado en el punto de control",explain:"Exacto: el <code>AutoModel</code> s\xF3lo necesita conocer el punto de control desde el cual inicializar para devolver la arquitectura correcta.",correct:!0},{text:"Un modelo que detecta autom\xE1ticamente el lenguaje usado por sus entradas para cargar los pesos correctos",explain:"Incorrecto; auqneu algunos puntos de control y modelos son capaced de manejar varios lenguajes, no hay herramientas integradas para la selecci\xF3n autom\xE1tica de punto de control de acuerdo al lenguaje. \xA1Deber\xEDas dirigirte a <a href='https://huggingface.co/models'>Model Hub</a> para encontrar el mejor punto de control para tu tarea!"}]}}),{c(){c=n("h3"),d=n("a"),$=n("span"),w(h.$$.fragment),A=m(),b=n("span"),S=N("5. \xBFQu\xE9 es un AutoModel?"),E=m(),w(v.$$.fragment),this.h()},l(r){c=s(r,"H3",{class:!0});var _=i(c);d=s(_,"A",{id:!0,class:!0,href:!0});var o=i(d);$=s(o,"SPAN",{});var x=i($);y(h.$$.fragment,x),x.forEach(a),o.forEach(a),A=f(_),b=s(_,"SPAN",{});var P=i(b);S=T(P,"5. \xBFQu\xE9 es un AutoModel?"),P.forEach(a),_.forEach(a),E=f(r),y(v.$$.fragment,r),this.h()},h(){l(d,"id","5.-\xBFqu\xE9-es-un-automodel?"),l(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(d,"href","#5.-\xBFqu\xE9-es-un-automodel?"),l(c,"class","relative group")},m(r,_){p(r,c,_),t(c,d),t(d,$),q(h,$,null),t(c,A),t(c,b),t(b,S),p(r,E,_),q(v,r,_),j=!0},i(r){j||(k(h.$$.fragment,r),k(v.$$.fragment,r),j=!0)},o(r){g(h.$$.fragment,r),g(v.$$.fragment,r),j=!1},d(r){r&&a(c),z(h),r&&a(E),z(v,r)}}}function xt(M){let c,d,$,h,A,b,S,E,v,j,r,_;return h=new H({}),v=new Ca({props:{code:`from transformers import AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = TFAutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`}}),r=new I({props:{choices:[{text:"No, parece correcto.",explain:"Desafortunadamente, acoplar un modelo con un tokenizador que fue entrenado con un punto de control distinto raramente es una buena idea. El modelo no fue entrenado para dar sentido a la salida de este tokenizador, as\xED la salida del modelo (\xA1si es que puede correr!) no tendr\xE1 ning\xFAn sentido."},{text:"El tokenizador y el modelo siempre deben ser del mismo punto de control.",explain:"\xA1Correcto!",correct:!0},{text:"Es una buena pr\xE1ctica rellenar y truncar con el tokenizador ya que cada entrada es un lote.",explain:"Es cierto que cada entrada de modelo necesita ser un lote. Sin embargo, truncar o rellenar esta secuencia no necesariamente hace sentido ya que s\xF3lo hay una, y esas son t\xE9cnicas para juntar una lista de oraciones."}]}}),{c(){c=n("h3"),d=n("a"),$=n("span"),w(h.$$.fragment),A=m(),b=n("span"),S=N("10. \xBFHay algo mal con el siguiente c\xF3digo?"),E=m(),w(v.$$.fragment),j=m(),w(r.$$.fragment),this.h()},l(o){c=s(o,"H3",{class:!0});var x=i(c);d=s(x,"A",{id:!0,class:!0,href:!0});var P=i(d);$=s(P,"SPAN",{});var C=i($);y(h.$$.fragment,C),C.forEach(a),P.forEach(a),A=f(x),b=s(x,"SPAN",{});var D=i(b);S=T(D,"10. \xBFHay algo mal con el siguiente c\xF3digo?"),D.forEach(a),x.forEach(a),E=f(o),y(v.$$.fragment,o),j=f(o),y(r.$$.fragment,o),this.h()},h(){l(d,"id","10.-\xBFhay-algo-mal-con-el-siguiente-c\xF3digo?"),l(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(d,"href","#10.-\xBFhay-algo-mal-con-el-siguiente-c\xF3digo?"),l(c,"class","relative group")},m(o,x){p(o,c,x),t(c,d),t(d,$),q(h,$,null),t(c,A),t(c,b),t(b,S),p(o,E,x),q(v,o,x),p(o,j,x),q(r,o,x),_=!0},i(o){_||(k(h.$$.fragment,o),k(v.$$.fragment,o),k(r.$$.fragment,o),_=!0)},o(o){g(h.$$.fragment,o),g(v.$$.fragment,o),g(r.$$.fragment,o),_=!1},d(o){o&&a(c),z(h),o&&a(E),z(v,o),o&&a(j),z(r,o)}}}function wt(M){let c,d,$,h,A,b,S,E,v,j,r,_;return h=new H({}),v=new Ca({props:{code:`from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")

encoded = tokenizer("Hey!", return_tensors="pt")
result = model(**encoded)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

encoded = tokenizer(<span class="hljs-string">&quot;Hey!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
result = model(**encoded)`}}),r=new I({props:{choices:[{text:"No, parece correcto.",explain:"Desafortunadamente, acoplar un modelo con un tokenizador que fue entrenado con un punto de control distinto raramente es una buena idea. El modelo no fue entrenado para dar sentido a la salida de este tokenizador, as\xED la salida del modelo (\xA1si es que puede correr!) no tendr\xE1 ning\xFAn sentido."},{text:"El tokenizador y el modelo siempre deben ser del mismo punto de control.",explain:"\xA1Correcto!",correct:!0},{text:"Es una buena pr\xE1ctica rellenar y truncar con el tokenizador ya que cada entrada es un lote.",explain:"Es cierto que cada entrada de modelo necesita ser un lote. Sin embargo, truncar o rellenar esta secuencia no necesariamente hace sentido ya que s\xF3lo hay una, y esas son t\xE9cnicas para juntar una lista de oraciones."}]}}),{c(){c=n("h3"),d=n("a"),$=n("span"),w(h.$$.fragment),A=m(),b=n("span"),S=N("10. \xBFHay algo mal con el siguiente c\xF3digo?"),E=m(),w(v.$$.fragment),j=m(),w(r.$$.fragment),this.h()},l(o){c=s(o,"H3",{class:!0});var x=i(c);d=s(x,"A",{id:!0,class:!0,href:!0});var P=i(d);$=s(P,"SPAN",{});var C=i($);y(h.$$.fragment,C),C.forEach(a),P.forEach(a),A=f(x),b=s(x,"SPAN",{});var D=i(b);S=T(D,"10. \xBFHay algo mal con el siguiente c\xF3digo?"),D.forEach(a),x.forEach(a),E=f(o),y(v.$$.fragment,o),j=f(o),y(r.$$.fragment,o),this.h()},h(){l(d,"id","10.-\xBFhay-algo-mal-con-el-siguiente-c\xF3digo?"),l(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(d,"href","#10.-\xBFhay-algo-mal-con-el-siguiente-c\xF3digo?"),l(c,"class","relative group")},m(o,x){p(o,c,x),t(c,d),t(d,$),q(h,$,null),t(c,A),t(c,b),t(b,S),p(o,E,x),q(v,o,x),p(o,j,x),q(r,o,x),_=!0},i(o){_||(k(h.$$.fragment,o),k(v.$$.fragment,o),k(r.$$.fragment,o),_=!0)},o(o){g(h.$$.fragment,o),g(v.$$.fragment,o),g(r.$$.fragment,o),_=!1},d(o){o&&a(c),z(h),o&&a(E),z(v,o),o&&a(j),z(r,o)}}}function yt(M){let c,d,$,h,A,b,S,E,v,j,r,_,o,x,P,C,D,oe,$a,Ae,va,Be,re,Re,B,V,je,ne,ga,Se,ka,Oe,se,Ge,R,X,Pe,le,ba,Ne,_a,Je,ie,We,O,Y,Te,ue,xa,Ce,wa,Ke,ce,Ve,L,Q,ye,G,Z,He,de,ya,Ie,qa,Xe,pe,Ye,J,ee,Me,me,za,De,Ea,Ze,fe,ea,W,ae,Le,he,Aa,Qe,ja,aa,$e,ta,K,te,Ue,ve,Sa,ge,Pa,Fe,Na,Ta,oa,ke,ra,be,na,U,F,qe,sa;$=new kt({props:{fw:M[0]}}),E=new H({}),o=new gt({props:{chapter:2,classNames:"absolute z-10 right-0 top-0"}}),oe=new H({}),re=new I({props:{choices:[{text:"Primero, el modelo que maneja el texto y devuelve las peticiones sin procesar. El tokenizador luego da sentido a estas predicciones y las convierte nuevamente en texto cuando es necesario.",explain:"\xA1El modelo no puede entender texto! El tokenizador primero debe tokenizar el texto y convertirlo a IDs para que as\xED sea comprensible por el modelo."},{text:"Primero, el tokenizador, que maneja el texto y regresa IDs. El modelo maneja estos IDs y produce una predicci\xF3n, la cual puede ser alg\xFAn texto.",explain:"La predicci\xF3n del modelo no puede ser texto de forma directa. \xA1El tokenizador tiene que ser usado de tal forma que convierta la predicci\xF3n de vuelta a texto!"},{text:"El tokenizador maneja texto y regresa IDs. El modelo maneja estos IDs y produce una predicci\xF3n. El tokenizador puede luego ser usado de nuevo para convertir estas predicciones de vuelta a texto.",explain:"\xA1Correcto! El tokenizador puede ser usado tanto para tokenizar como des-tokenizar.",correct:!0}]}}),ne=new H({}),se=new I({props:{choices:[{text:"1: La longitud de secuencia y el tama\xF1o del lote",explain:"\xA1Falso! El tensor producido por el modelo tiene una tercer dimensi\xF3n: tama\xF1o oculto."},{text:"2: La longitud de secuencia y el tama\xF1o oculto",explain:"\xA1Falso! All Todos los modelos Transformer manejan lotes, a\xFAn con una sola secuencia; lo cual ser\xEDa un lote de tama\xF1o 1!"},{text:"3: La longitud de secuencia, el tama\xF1o de lote y el tama\xF1o oculto",explain:"\xA1Correcto!",correct:!0}]}}),le=new H({}),ie=new I({props:{choices:[{text:"WordPiece",explain:"\xA1S\xED ese es un ejemplo de tokenizaci\xF3n de subpalabras!",correct:!0},{text:"Tokenizaci\xF3n basada en caracteres",explain:"La tokenizaci\xF3n basada en caracteres no es un tipo de tokenizaci\xF3n de subpalabras."},{text:"Divisi\xF3n por espacios en blanco y puntuaci\xF3n",explain:"\xA1Ese es un esquema de tokenizaci\xF3n basado en palabras!"},{text:"BPE",explain:"\xA1S\xED ese es un ejemplo de tokenizaci\xF3n de subpalabras!",correct:!0},{text:"Unigrama",explain:"\xA1S\xED ese es un ejemplo de tokenizaci\xF3n de subpalabras!",correct:!0},{text:"Ninguno de los anteriores",explain:"\xA1Incorrecto!"}]}}),ue=new H({}),ce=new I({props:{choices:[{text:"Un componente de la red de Transformer base que redirecciona los tensores a sus capas correctas",explain:"\xA1Incorrecto! No hay tal componente."},{text:"Tambi\xE9n conocido como el mecanismo de autoatenci\xF3n, adapta la representaci\xF3n de un token de acuerdo a los otros tokens de la secuencia",explain:'\xA1Incorrecto! La capa de autoatenci\xF3n contiene "cabezas", pero \xE9stas no son cabezas de adaptaci\xF3n.'},{text:"Un componente adcional, compuesto usualmente de una o unas pocas capas, para convertir las predicciones del transformador a una salida espec\xEDfica de la tarea",explain:"As\xED es, Las cabezas de adaptaci\xF3n, tambi\xE9n conocidas simplemente como habezas, vienen en diferentes formas: cabezas de modelado de lenguaje, cabezas de respuesta a preguntas, cabezas de clasificaci\xF3n de secuencia... ",correct:!0}]}});const Ha=[_t,bt],_e=[];function Ia(e,u){return e[0]==="pt"?0:1}L=Ia(M),Q=_e[L]=Ha[L](M),de=new H({}),pe=new I({props:{choices:[{text:"Truncado",explain:"S\xED, el truncamiento es una forma correcta de emparejar secuencias de modo que se ajusten a una forma rectangular. \xBFAunque, es la \xFAnica?",correct:!0},{text:"Returning tensors",explain:"Mientras las otras t\xE9cnicas te permiten devolver tensores rectangulares, returning tensors no es \xFAtil cuando se hace batching en secuencias juntas."},{text:"Relleno",explain:"S\xED, el relleno es una forma correcta de emparejar secuencias de modo que se ajusten a una forma rectangular. \xBFAunque, es la \xFAnica?",correct:!0},{text:"Enmascarado de atenci\xF3n",explain:"\xA1Absolutamente! Las m\xE1scaras de atenci\xF3n son de primera importancia cuando se manejan secuencias de diferentes longitudes. Sin embargo, no es la \xFAnica t\xE9cnica a tener en cuenta.",correct:!0}]}}),me=new H({}),fe=new I({props:{choices:[{text:"Suaviza los logits para que sean m\xE1s fiables.",explain:"No, la funci\xF3n SoftMax no afecta en la fiabilidad de los resultados."},{text:"Aplica un l\xEDmiete inferior y superior de modo que sean comprensibles.",explain:"\xA1Correcto! Los valores resultantes est\xE1n limitados entre 0 y 1. Aunque, no es la \xFAnica raz\xF3n por la cual usamos una funci\xF3n SoftMax.",correct:!0},{text:"La suma total de la salida es entonces 1, dando como resultado una posible interpretaci\xF3n probabil\xEDstica.",explain:"\xA1Correcto! Aunque, esa no es la \xFAnica raz\xF3n por la que usamos una funci\xF3n SoftMax.",correct:!0}]}}),he=new H({}),$e=new I({props:{choices:[{text:"<code>encode</code>, ya que puede codificar texto en IDs e IDs en predicciones",explain:"\xA1Incorrecto! Aunque el m\xE9todo <code>encode</code> existe en los tokenizadores, no existe en los modelos."},{text:"Llamar al objeto tokenizador directamente.",explain:"\xA1Exactamente! El m\xE9todo <code>__call__</code> del tokenizador es un m\xE9todo muy poderoso el cual puede manejar casi cualquier cosa.Tambi\xE9n es el m\xE9todo usado para recuperar las predicciones de un modelo.",correct:!0},{text:"<code>pad</code>",explain:"\xA1Incorrecto! El relleno es muy \xFAtil, pero es solo una parte de la API tokenizador."},{text:"<code>tokenize</code>",explain:"El m\xE9todo <code>tokenize</code> es posiblemente uno de los m\xE9todos m\xE1s \xFAtiles, pero no es el n\xFAcleo de la API tokenizador."}]}}),ve=new H({}),ke=new Ca({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
result = tokenizer.tokenize("Hello!")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
result = tokenizer.tokenize(<span class="hljs-string">&quot;Hello!&quot;</span>)`}}),be=new I({props:{choices:[{text:"Una lista de strings, cada string es un token",explain:"\xA1Por supuesto! \xA1Convierte esto a IDs, y los env\xEDa a los modelos!",correct:!0},{text:"Una lista de IDs",explain:"Incorrecto; \xA1para eso est\xE1n los m\xE9todos <code>__call__</code> o <code>convert_tokens_to_ids</code>!"},{text:"Una cadena que contiene todos los tokens",explain:"Esto ser\xEDa sub\xF3ptimo, ya que el objetivo es dividir la cadena en varios tokens."}]}});const Ma=[wt,xt],xe=[];function Da(e,u){return e[0]==="pt"?0:1}return U=Da(M),F=xe[U]=Ma[U](M),{c(){c=n("meta"),d=m(),w($.$$.fragment),h=m(),A=n("h1"),b=n("a"),S=n("span"),w(E.$$.fragment),v=m(),j=n("span"),r=N("Quiz de final de cap\xEDtulo"),_=m(),w(o.$$.fragment),x=m(),P=n("h3"),C=n("a"),D=n("span"),w(oe.$$.fragment),$a=m(),Ae=n("span"),va=N("1. \xBFCu\xE1l es el orden del pipeline de modelado del lenguaje?"),Be=m(),w(re.$$.fragment),Re=m(),B=n("h3"),V=n("a"),je=n("span"),w(ne.$$.fragment),ga=m(),Se=n("span"),ka=N("2. \xBFCu\xE1ntas dimensiones tiene el tensor producido por el modelo base de Transformer y cu\xE1les son?"),Oe=m(),w(se.$$.fragment),Ge=m(),R=n("h3"),X=n("a"),Pe=n("span"),w(le.$$.fragment),ba=m(),Ne=n("span"),_a=N("3. \xBFCu\xE1l de los siguientes es un ejemplo de tokenizaci\xF3n de subpalabras?"),Je=m(),w(ie.$$.fragment),We=m(),O=n("h3"),Y=n("a"),Te=n("span"),w(ue.$$.fragment),xa=m(),Ce=n("span"),wa=N("4. \xBFQu\xE9 es una cabeza del modelo?"),Ke=m(),w(ce.$$.fragment),Ve=m(),Q.c(),ye=m(),G=n("h3"),Z=n("a"),He=n("span"),w(de.$$.fragment),ya=m(),Ie=n("span"),qa=N("6. \xBFCu\xE1les son las t\xE9cnicas a tener en cuenta al realizar batching de secuencias de diferentes longitudes juntas?"),Xe=m(),w(pe.$$.fragment),Ye=m(),J=n("h3"),ee=n("a"),Me=n("span"),w(me.$$.fragment),za=m(),De=n("span"),Ea=N("7. \xBFCu\xE1l es el punto de aplicar una funcion SoftMax a las salidas logits por un modelo de clasificaci\xF3n de secuencias?"),Ze=m(),w(fe.$$.fragment),ea=m(),W=n("h3"),ae=n("a"),Le=n("span"),w(he.$$.fragment),Aa=m(),Qe=n("span"),ja=N("8. \xBFEn qu\xE9 m\xE9todo se centra la mayor parte de la API del tokenizador?"),aa=m(),w($e.$$.fragment),ta=m(),K=n("h3"),te=n("a"),Ue=n("span"),w(ve.$$.fragment),Sa=m(),ge=n("span"),Pa=N("9. \xBFQu\xE9 contiene la variable "),Fe=n("code"),Na=N("result"),Ta=N(" en este c\xF3digo de ejemplo?"),oa=m(),w(ke.$$.fragment),ra=m(),w(be.$$.fragment),na=m(),F.c(),qe=ct(),this.h()},l(e){const u=$t('[data-svelte="svelte-1phssyn"]',document.head);c=s(u,"META",{name:!0,content:!0}),u.forEach(a),d=f(e),y($.$$.fragment,e),h=f(e),A=s(e,"H1",{class:!0});var we=i(A);b=s(we,"A",{id:!0,class:!0,href:!0});var ze=i(b);S=s(ze,"SPAN",{});var Ee=i(S);y(E.$$.fragment,Ee),Ee.forEach(a),ze.forEach(a),v=f(we),j=s(we,"SPAN",{});var La=i(j);r=T(La,"Quiz de final de cap\xEDtulo"),La.forEach(a),we.forEach(a),_=f(e),y(o.$$.fragment,e),x=f(e),P=s(e,"H3",{class:!0});var la=i(P);C=s(la,"A",{id:!0,class:!0,href:!0});var Qa=i(C);D=s(Qa,"SPAN",{});var Ua=i(D);y(oe.$$.fragment,Ua),Ua.forEach(a),Qa.forEach(a),$a=f(la),Ae=s(la,"SPAN",{});var Fa=i(Ae);va=T(Fa,"1. \xBFCu\xE1l es el orden del pipeline de modelado del lenguaje?"),Fa.forEach(a),la.forEach(a),Be=f(e),y(re.$$.fragment,e),Re=f(e),B=s(e,"H3",{class:!0});var ia=i(B);V=s(ia,"A",{id:!0,class:!0,href:!0});var Ba=i(V);je=s(Ba,"SPAN",{});var Ra=i(je);y(ne.$$.fragment,Ra),Ra.forEach(a),Ba.forEach(a),ga=f(ia),Se=s(ia,"SPAN",{});var Oa=i(Se);ka=T(Oa,"2. \xBFCu\xE1ntas dimensiones tiene el tensor producido por el modelo base de Transformer y cu\xE1les son?"),Oa.forEach(a),ia.forEach(a),Oe=f(e),y(se.$$.fragment,e),Ge=f(e),R=s(e,"H3",{class:!0});var ua=i(R);X=s(ua,"A",{id:!0,class:!0,href:!0});var Ga=i(X);Pe=s(Ga,"SPAN",{});var Ja=i(Pe);y(le.$$.fragment,Ja),Ja.forEach(a),Ga.forEach(a),ba=f(ua),Ne=s(ua,"SPAN",{});var Wa=i(Ne);_a=T(Wa,"3. \xBFCu\xE1l de los siguientes es un ejemplo de tokenizaci\xF3n de subpalabras?"),Wa.forEach(a),ua.forEach(a),Je=f(e),y(ie.$$.fragment,e),We=f(e),O=s(e,"H3",{class:!0});var ca=i(O);Y=s(ca,"A",{id:!0,class:!0,href:!0});var Ka=i(Y);Te=s(Ka,"SPAN",{});var Va=i(Te);y(ue.$$.fragment,Va),Va.forEach(a),Ka.forEach(a),xa=f(ca),Ce=s(ca,"SPAN",{});var Xa=i(Ce);wa=T(Xa,"4. \xBFQu\xE9 es una cabeza del modelo?"),Xa.forEach(a),ca.forEach(a),Ke=f(e),y(ce.$$.fragment,e),Ve=f(e),Q.l(e),ye=f(e),G=s(e,"H3",{class:!0});var da=i(G);Z=s(da,"A",{id:!0,class:!0,href:!0});var Ya=i(Z);He=s(Ya,"SPAN",{});var Za=i(He);y(de.$$.fragment,Za),Za.forEach(a),Ya.forEach(a),ya=f(da),Ie=s(da,"SPAN",{});var et=i(Ie);qa=T(et,"6. \xBFCu\xE1les son las t\xE9cnicas a tener en cuenta al realizar batching de secuencias de diferentes longitudes juntas?"),et.forEach(a),da.forEach(a),Xe=f(e),y(pe.$$.fragment,e),Ye=f(e),J=s(e,"H3",{class:!0});var pa=i(J);ee=s(pa,"A",{id:!0,class:!0,href:!0});var at=i(ee);Me=s(at,"SPAN",{});var tt=i(Me);y(me.$$.fragment,tt),tt.forEach(a),at.forEach(a),za=f(pa),De=s(pa,"SPAN",{});var ot=i(De);Ea=T(ot,"7. \xBFCu\xE1l es el punto de aplicar una funcion SoftMax a las salidas logits por un modelo de clasificaci\xF3n de secuencias?"),ot.forEach(a),pa.forEach(a),Ze=f(e),y(fe.$$.fragment,e),ea=f(e),W=s(e,"H3",{class:!0});var ma=i(W);ae=s(ma,"A",{id:!0,class:!0,href:!0});var rt=i(ae);Le=s(rt,"SPAN",{});var nt=i(Le);y(he.$$.fragment,nt),nt.forEach(a),rt.forEach(a),Aa=f(ma),Qe=s(ma,"SPAN",{});var st=i(Qe);ja=T(st,"8. \xBFEn qu\xE9 m\xE9todo se centra la mayor parte de la API del tokenizador?"),st.forEach(a),ma.forEach(a),aa=f(e),y($e.$$.fragment,e),ta=f(e),K=s(e,"H3",{class:!0});var fa=i(K);te=s(fa,"A",{id:!0,class:!0,href:!0});var lt=i(te);Ue=s(lt,"SPAN",{});var it=i(Ue);y(ve.$$.fragment,it),it.forEach(a),lt.forEach(a),Sa=f(fa),ge=s(fa,"SPAN",{});var ha=i(ge);Pa=T(ha,"9. \xBFQu\xE9 contiene la variable "),Fe=s(ha,"CODE",{});var ut=i(Fe);Na=T(ut,"result"),ut.forEach(a),Ta=T(ha," en este c\xF3digo de ejemplo?"),ha.forEach(a),fa.forEach(a),oa=f(e),y(ke.$$.fragment,e),ra=f(e),y(be.$$.fragment,e),na=f(e),F.l(e),qe=ct(),this.h()},h(){l(c,"name","hf:doc:metadata"),l(c,"content",JSON.stringify(qt)),l(b,"id","quiz-de-final-de-captulo"),l(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(b,"href","#quiz-de-final-de-captulo"),l(A,"class","relative group"),l(C,"id","1.-\xBFcu\xE1l-es-el-orden-del-pipeline-de-modelado-del-lenguaje?"),l(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(C,"href","#1.-\xBFcu\xE1l-es-el-orden-del-pipeline-de-modelado-del-lenguaje?"),l(P,"class","relative group"),l(V,"id","2.-\xBFcu\xE1ntas-dimensiones-tiene-el-tensor-producido-por-el-modelo-base-de-transformer-y-cu\xE1les-son?"),l(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(V,"href","#2.-\xBFcu\xE1ntas-dimensiones-tiene-el-tensor-producido-por-el-modelo-base-de-transformer-y-cu\xE1les-son?"),l(B,"class","relative group"),l(X,"id","3.-\xBFcu\xE1l-de-los-siguientes-es-un-ejemplo-de-tokenizaci\xF3n-de-subpalabras?"),l(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(X,"href","#3.-\xBFcu\xE1l-de-los-siguientes-es-un-ejemplo-de-tokenizaci\xF3n-de-subpalabras?"),l(R,"class","relative group"),l(Y,"id","4.-\xBFqu\xE9-es-una-cabeza-del-modelo?"),l(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Y,"href","#4.-\xBFqu\xE9-es-una-cabeza-del-modelo?"),l(O,"class","relative group"),l(Z,"id","6.-\xBFcu\xE1les-son-las-t\xE9cnicas-a-tener-en-cuenta-al-realizar-batching-de-secuencias-de-diferentes-longitudes-juntas?"),l(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Z,"href","#6.-\xBFcu\xE1les-son-las-t\xE9cnicas-a-tener-en-cuenta-al-realizar-batching-de-secuencias-de-diferentes-longitudes-juntas?"),l(G,"class","relative group"),l(ee,"id","7.-\xBFcu\xE1l-es-el-punto-de-aplicar-una-funcion-softmax-a-las-salidas-logits-por-un-modelo-de-clasificaci\xF3n-de-secuencias?"),l(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ee,"href","#7.-\xBFcu\xE1l-es-el-punto-de-aplicar-una-funcion-softmax-a-las-salidas-logits-por-un-modelo-de-clasificaci\xF3n-de-secuencias?"),l(J,"class","relative group"),l(ae,"id","8.-\xBFen-qu\xE9-m\xE9todo-se-centra-la-mayor-parte-de-la-api-del-tokenizador?"),l(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ae,"href","#8.-\xBFen-qu\xE9-m\xE9todo-se-centra-la-mayor-parte-de-la-api-del-tokenizador?"),l(W,"class","relative group"),l(te,"id","9.-\xBFqu\xE9-contiene-la-variable-<code>result</code>-en-este-c\xF3digo-de-ejemplo?"),l(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(te,"href","#9.-\xBFqu\xE9-contiene-la-variable-<code>result</code>-en-este-c\xF3digo-de-ejemplo?"),l(K,"class","relative group")},m(e,u){t(document.head,c),p(e,d,u),q($,e,u),p(e,h,u),p(e,A,u),t(A,b),t(b,S),q(E,S,null),t(A,v),t(A,j),t(j,r),p(e,_,u),q(o,e,u),p(e,x,u),p(e,P,u),t(P,C),t(C,D),q(oe,D,null),t(P,$a),t(P,Ae),t(Ae,va),p(e,Be,u),q(re,e,u),p(e,Re,u),p(e,B,u),t(B,V),t(V,je),q(ne,je,null),t(B,ga),t(B,Se),t(Se,ka),p(e,Oe,u),q(se,e,u),p(e,Ge,u),p(e,R,u),t(R,X),t(X,Pe),q(le,Pe,null),t(R,ba),t(R,Ne),t(Ne,_a),p(e,Je,u),q(ie,e,u),p(e,We,u),p(e,O,u),t(O,Y),t(Y,Te),q(ue,Te,null),t(O,xa),t(O,Ce),t(Ce,wa),p(e,Ke,u),q(ce,e,u),p(e,Ve,u),_e[L].m(e,u),p(e,ye,u),p(e,G,u),t(G,Z),t(Z,He),q(de,He,null),t(G,ya),t(G,Ie),t(Ie,qa),p(e,Xe,u),q(pe,e,u),p(e,Ye,u),p(e,J,u),t(J,ee),t(ee,Me),q(me,Me,null),t(J,za),t(J,De),t(De,Ea),p(e,Ze,u),q(fe,e,u),p(e,ea,u),p(e,W,u),t(W,ae),t(ae,Le),q(he,Le,null),t(W,Aa),t(W,Qe),t(Qe,ja),p(e,aa,u),q($e,e,u),p(e,ta,u),p(e,K,u),t(K,te),t(te,Ue),q(ve,Ue,null),t(K,Sa),t(K,ge),t(ge,Pa),t(ge,Fe),t(Fe,Na),t(ge,Ta),p(e,oa,u),q(ke,e,u),p(e,ra,u),q(be,e,u),p(e,na,u),xe[U].m(e,u),p(e,qe,u),sa=!0},p(e,[u]){const we={};u&1&&(we.fw=e[0]),$.$set(we);let ze=L;L=Ia(e),L!==ze&&(pt(),g(_e[ze],1,1,()=>{_e[ze]=null}),dt(),Q=_e[L],Q||(Q=_e[L]=Ha[L](e),Q.c()),k(Q,1),Q.m(ye.parentNode,ye));let Ee=U;U=Da(e),U!==Ee&&(pt(),g(xe[Ee],1,1,()=>{xe[Ee]=null}),dt(),F=xe[U],F||(F=xe[U]=Ma[U](e),F.c()),k(F,1),F.m(qe.parentNode,qe))},i(e){sa||(k($.$$.fragment,e),k(E.$$.fragment,e),k(o.$$.fragment,e),k(oe.$$.fragment,e),k(re.$$.fragment,e),k(ne.$$.fragment,e),k(se.$$.fragment,e),k(le.$$.fragment,e),k(ie.$$.fragment,e),k(ue.$$.fragment,e),k(ce.$$.fragment,e),k(Q),k(de.$$.fragment,e),k(pe.$$.fragment,e),k(me.$$.fragment,e),k(fe.$$.fragment,e),k(he.$$.fragment,e),k($e.$$.fragment,e),k(ve.$$.fragment,e),k(ke.$$.fragment,e),k(be.$$.fragment,e),k(F),sa=!0)},o(e){g($.$$.fragment,e),g(E.$$.fragment,e),g(o.$$.fragment,e),g(oe.$$.fragment,e),g(re.$$.fragment,e),g(ne.$$.fragment,e),g(se.$$.fragment,e),g(le.$$.fragment,e),g(ie.$$.fragment,e),g(ue.$$.fragment,e),g(ce.$$.fragment,e),g(Q),g(de.$$.fragment,e),g(pe.$$.fragment,e),g(me.$$.fragment,e),g(fe.$$.fragment,e),g(he.$$.fragment,e),g($e.$$.fragment,e),g(ve.$$.fragment,e),g(ke.$$.fragment,e),g(be.$$.fragment,e),g(F),sa=!1},d(e){a(c),e&&a(d),z($,e),e&&a(h),e&&a(A),z(E),e&&a(_),z(o,e),e&&a(x),e&&a(P),z(oe),e&&a(Be),z(re,e),e&&a(Re),e&&a(B),z(ne),e&&a(Oe),z(se,e),e&&a(Ge),e&&a(R),z(le),e&&a(Je),z(ie,e),e&&a(We),e&&a(O),z(ue),e&&a(Ke),z(ce,e),e&&a(Ve),_e[L].d(e),e&&a(ye),e&&a(G),z(de),e&&a(Xe),z(pe,e),e&&a(Ye),e&&a(J),z(me),e&&a(Ze),z(fe,e),e&&a(ea),e&&a(W),z(he),e&&a(aa),z($e,e),e&&a(ta),e&&a(K),z(ve),e&&a(oa),z(ke,e),e&&a(ra),z(be,e),e&&a(na),xe[U].d(e),e&&a(qe)}}}const qt={local:"quiz-de-final-de-captulo",title:"Quiz de final de cap\xEDtulo"};function zt(M,c,d){let $="pt";return vt(()=>{const h=new URLSearchParams(window.location.search);d(0,$=h.get("fw")||"pt")}),[$]}class Tt extends mt{constructor(c){super();ft(this,c,zt,yt,ht,{})}}export{Tt as default,qt as metadata};
