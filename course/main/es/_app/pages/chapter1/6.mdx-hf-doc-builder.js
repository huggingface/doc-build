import{S as qe,i as Ae,s as Me,e as t,k as c,w as Le,t as f,M as Ie,c as r,d as a,m,a as l,x as ge,h as p,b as n,G as o,g as i,y as ye,L as Se,q as Pe,o as xe,B as Te,v as ke}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ce}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ge}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Ne(se){let u,G,h,_,q,$,Q,A,j,N,w,R,v,z,M,D,F,U,P,K,X,x,V,Y,T,W,B,d,I,b,Z,ee,S,L,ae,oe,k,g,te,re,C,y,le,H;return $=new Ge({}),w=new Ce({props:{id:"d_ixlCubqQw"}}),{c(){u=t("meta"),G=c(),h=t("h1"),_=t("a"),q=t("span"),Le($.$$.fragment),Q=c(),A=t("span"),j=f("Modelos de decodificadores"),N=c(),Le(w.$$.fragment),R=c(),v=t("p"),z=f("Los modelos de decodificadores usan \xFAnicamente el decodificador del Transformador. En cada etapa, para una palabra dada las capas de atenci\xF3n pueden acceder s\xF3lamente a las palabras que se ubican antes en la oraci\xF3n. Estos modelos se suelen llamar modelos "),M=t("em"),D=f("auto-regressive"),F=f("."),U=c(),P=t("p"),K=f("El preentrenamiento de los modelos de decodificadores generalmente gira en torno a la predicci\xF3n de la siguiente palabra en la oraci\xF3n."),X=c(),x=t("p"),V=f("Estos modelos son m\xE1s adecuados para tareas que implican la generaci\xF3n de texto."),Y=c(),T=t("p"),W=f("Los miembros de esta familia de modelos incluyen:"),B=c(),d=t("ul"),I=t("li"),b=t("a"),Z=f("CTRL"),ee=c(),S=t("li"),L=t("a"),ae=f("GPT"),oe=c(),k=t("li"),g=t("a"),te=f("GPT-2"),re=c(),C=t("li"),y=t("a"),le=f("Transformer XL"),this.h()},l(e){const s=Ie('[data-svelte="svelte-1phssyn"]',document.head);u=r(s,"META",{name:!0,content:!0}),s.forEach(a),G=m(e),h=r(e,"H1",{class:!0});var J=l(h);_=r(J,"A",{id:!0,class:!0,href:!0});var ne=l(_);q=r(ne,"SPAN",{});var ie=l(q);ge($.$$.fragment,ie),ie.forEach(a),ne.forEach(a),Q=m(J),A=r(J,"SPAN",{});var de=l(A);j=p(de,"Modelos de decodificadores"),de.forEach(a),J.forEach(a),N=m(e),ge(w.$$.fragment,e),R=m(e),v=r(e,"P",{});var O=l(v);z=p(O,"Los modelos de decodificadores usan \xFAnicamente el decodificador del Transformador. En cada etapa, para una palabra dada las capas de atenci\xF3n pueden acceder s\xF3lamente a las palabras que se ubican antes en la oraci\xF3n. Estos modelos se suelen llamar modelos "),M=r(O,"EM",{});var ce=l(M);D=p(ce,"auto-regressive"),ce.forEach(a),F=p(O,"."),O.forEach(a),U=m(e),P=r(e,"P",{});var fe=l(P);K=p(fe,"El preentrenamiento de los modelos de decodificadores generalmente gira en torno a la predicci\xF3n de la siguiente palabra en la oraci\xF3n."),fe.forEach(a),X=m(e),x=r(e,"P",{});var me=l(x);V=p(me,"Estos modelos son m\xE1s adecuados para tareas que implican la generaci\xF3n de texto."),me.forEach(a),Y=m(e),T=r(e,"P",{});var pe=l(T);W=p(pe,"Los miembros de esta familia de modelos incluyen:"),pe.forEach(a),B=m(e),d=r(e,"UL",{});var E=l(d);I=r(E,"LI",{});var ue=l(I);b=r(ue,"A",{href:!0,rel:!0});var he=l(b);Z=p(he,"CTRL"),he.forEach(a),ue.forEach(a),ee=m(E),S=r(E,"LI",{});var _e=l(S);L=r(_e,"A",{href:!0,rel:!0});var ve=l(L);ae=p(ve,"GPT"),ve.forEach(a),_e.forEach(a),oe=m(E),k=r(E,"LI",{});var Ee=l(k);g=r(Ee,"A",{href:!0,rel:!0});var $e=l(g);te=p($e,"GPT-2"),$e.forEach(a),Ee.forEach(a),re=m(E),C=r(E,"LI",{});var we=l(C);y=r(we,"A",{href:!0,rel:!0});var be=l(y);le=p(be,"Transformer XL"),be.forEach(a),we.forEach(a),E.forEach(a),this.h()},h(){n(u,"name","hf:doc:metadata"),n(u,"content",JSON.stringify(Re)),n(_,"id","modelos-de-decodificadores"),n(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(_,"href","#modelos-de-decodificadores"),n(h,"class","relative group"),n(b,"href","https://huggingface.co/transformers/model_doc/ctrl.html"),n(b,"rel","nofollow"),n(L,"href","https://huggingface.co/transformers/model_doc/gpt.html"),n(L,"rel","nofollow"),n(g,"href","https://huggingface.co/transformers/model_doc/gpt2.html"),n(g,"rel","nofollow"),n(y,"href","https://huggingface.co/transformers/model_doc/transformerxl.html"),n(y,"rel","nofollow")},m(e,s){o(document.head,u),i(e,G,s),i(e,h,s),o(h,_),o(_,q),ye($,q,null),o(h,Q),o(h,A),o(A,j),i(e,N,s),ye(w,e,s),i(e,R,s),i(e,v,s),o(v,z),o(v,M),o(M,D),o(v,F),i(e,U,s),i(e,P,s),o(P,K),i(e,X,s),i(e,x,s),o(x,V),i(e,Y,s),i(e,T,s),o(T,W),i(e,B,s),i(e,d,s),o(d,I),o(I,b),o(b,Z),o(d,ee),o(d,S),o(S,L),o(L,ae),o(d,oe),o(d,k),o(k,g),o(g,te),o(d,re),o(d,C),o(C,y),o(y,le),H=!0},p:Se,i(e){H||(Pe($.$$.fragment,e),Pe(w.$$.fragment,e),H=!0)},o(e){xe($.$$.fragment,e),xe(w.$$.fragment,e),H=!1},d(e){a(u),e&&a(G),e&&a(h),Te($),e&&a(N),Te(w,e),e&&a(R),e&&a(v),e&&a(U),e&&a(P),e&&a(X),e&&a(x),e&&a(Y),e&&a(T),e&&a(B),e&&a(d)}}}const Re={local:"modelos-de-decodificadores",title:"Modelos de decodificadores"};function Ue(se){return ke(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class He extends qe{constructor(u){super();Ae(this,u,Ue,Ne,Me,{})}}export{He as default,Re as metadata};
