import{S as Se,i as ke,s as Ce,e as t,k as d,w as qe,t as m,M as Ne,c as r,d as a,m as f,a as l,x as Pe,h as u,b as n,G as o,g as c,y as xe,L as je,q as Be,o as Me,B as Ie,v as Ue}from"../../chunks/vendor-hf-doc-builder.js";import{Y as ze}from"../../chunks/Youtube-hf-doc-builder.js";import{I as De}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Je(de){let h,N,_,E,P,y,Q,x,F,j,b,U,v,K,B,V,W,z,R,X,D,T,Z,J,q,ee,Y,i,M,g,ae,oe,I,L,te,re,S,w,le,se,k,$,ne,ie,C,A,ce,G;return y=new De({}),b=new ze({props:{id:"MUqNwgPjJvQ"}}),{c(){h=t("meta"),N=d(),_=t("h1"),E=t("a"),P=t("span"),qe(y.$$.fragment),Q=d(),x=t("span"),F=m("Modelos de codificadores"),j=d(),qe(b.$$.fragment),U=d(),v=t("p"),K=m("Los modelos de codificadores usan \xFAnicamente el codificador del Transformador. En cada etapa, las capas de atenci\xF3n pueden acceder a todas las palabras de la oraci\xF3n inicial. Estos modelos se caracterizan generalmente por tener atenci\xF3n \u201Cbidireccional\u201D y se suelen llamar modelos "),B=t("em"),V=m("auto-encoding"),W=m("."),z=d(),R=t("p"),X=m("El preentrenamiento de estos modelos generalmente gira en torno a corromper de alguna manera una oraci\xF3n dada (por ejemplo, ocultando aleat\xF3riamente palabras en ella) y pidi\xE9ndole al modelo que encuentre o reconstruya la oraci\xF3n inicial."),D=d(),T=t("p"),Z=m("Los modelos de codificadores son m\xE1s adecuados para tareas que requieren un entendimiento de la oraci\xF3n completa, como la clasificaci\xF3n de oraciones, reconocimiento de entidades nombradas (y m\xE1s generalmente clasificaci\xF3n de palabras) y respuesta extractiva a preguntas."),J=d(),q=t("p"),ee=m("Los miembros de esta familia de modelos incluyen:"),Y=d(),i=t("ul"),M=t("li"),g=t("a"),ae=m("ALBERT"),oe=d(),I=t("li"),L=t("a"),te=m("BERT"),re=d(),S=t("li"),w=t("a"),le=m("DistilBERT"),se=d(),k=t("li"),$=t("a"),ne=m("ELECTRA"),ie=d(),C=t("li"),A=t("a"),ce=m("RoBERTa"),this.h()},l(e){const s=Ne('[data-svelte="svelte-1phssyn"]',document.head);h=r(s,"META",{name:!0,content:!0}),s.forEach(a),N=f(e),_=r(e,"H1",{class:!0});var H=l(_);E=r(H,"A",{id:!0,class:!0,href:!0});var me=l(E);P=r(me,"SPAN",{});var fe=l(P);Pe(y.$$.fragment,fe),fe.forEach(a),me.forEach(a),Q=f(H),x=r(H,"SPAN",{});var ue=l(x);F=u(ue,"Modelos de codificadores"),ue.forEach(a),H.forEach(a),j=f(e),Pe(b.$$.fragment,e),U=f(e),v=r(e,"P",{});var O=l(v);K=u(O,"Los modelos de codificadores usan \xFAnicamente el codificador del Transformador. En cada etapa, las capas de atenci\xF3n pueden acceder a todas las palabras de la oraci\xF3n inicial. Estos modelos se caracterizan generalmente por tener atenci\xF3n \u201Cbidireccional\u201D y se suelen llamar modelos "),B=r(O,"EM",{});var pe=l(B);V=u(pe,"auto-encoding"),pe.forEach(a),W=u(O,"."),O.forEach(a),z=f(e),R=r(e,"P",{});var he=l(R);X=u(he,"El preentrenamiento de estos modelos generalmente gira en torno a corromper de alguna manera una oraci\xF3n dada (por ejemplo, ocultando aleat\xF3riamente palabras en ella) y pidi\xE9ndole al modelo que encuentre o reconstruya la oraci\xF3n inicial."),he.forEach(a),D=f(e),T=r(e,"P",{});var _e=l(T);Z=u(_e,"Los modelos de codificadores son m\xE1s adecuados para tareas que requieren un entendimiento de la oraci\xF3n completa, como la clasificaci\xF3n de oraciones, reconocimiento de entidades nombradas (y m\xE1s generalmente clasificaci\xF3n de palabras) y respuesta extractiva a preguntas."),_e.forEach(a),J=f(e),q=r(e,"P",{});var Ee=l(q);ee=u(Ee,"Los miembros de esta familia de modelos incluyen:"),Ee.forEach(a),Y=f(e),i=r(e,"UL",{});var p=l(i);M=r(p,"LI",{});var ve=l(M);g=r(ve,"A",{href:!0,rel:!0});var ye=l(g);ae=u(ye,"ALBERT"),ye.forEach(a),ve.forEach(a),oe=f(p),I=r(p,"LI",{});var be=l(I);L=r(be,"A",{href:!0,rel:!0});var ge=l(L);te=u(ge,"BERT"),ge.forEach(a),be.forEach(a),re=f(p),S=r(p,"LI",{});var Le=l(S);w=r(Le,"A",{href:!0,rel:!0});var we=l(w);le=u(we,"DistilBERT"),we.forEach(a),Le.forEach(a),se=f(p),k=r(p,"LI",{});var $e=l(k);$=r($e,"A",{href:!0,rel:!0});var Ae=l($);ne=u(Ae,"ELECTRA"),Ae.forEach(a),$e.forEach(a),ie=f(p),C=r(p,"LI",{});var Re=l(C);A=r(Re,"A",{href:!0,rel:!0});var Te=l(A);ce=u(Te,"RoBERTa"),Te.forEach(a),Re.forEach(a),p.forEach(a),this.h()},h(){n(h,"name","hf:doc:metadata"),n(h,"content",JSON.stringify(Ye)),n(E,"id","modelos-de-codificadores"),n(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(E,"href","#modelos-de-codificadores"),n(_,"class","relative group"),n(g,"href","https://huggingface.co/transformers/model_doc/albert.html"),n(g,"rel","nofollow"),n(L,"href","https://huggingface.co/transformers/model_doc/bert.html"),n(L,"rel","nofollow"),n(w,"href","https://huggingface.co/transformers/model_doc/distilbert.html"),n(w,"rel","nofollow"),n($,"href","https://huggingface.co/transformers/model_doc/electra.html"),n($,"rel","nofollow"),n(A,"href","https://huggingface.co/transformers/model_doc/roberta.html"),n(A,"rel","nofollow")},m(e,s){o(document.head,h),c(e,N,s),c(e,_,s),o(_,E),o(E,P),xe(y,P,null),o(_,Q),o(_,x),o(x,F),c(e,j,s),xe(b,e,s),c(e,U,s),c(e,v,s),o(v,K),o(v,B),o(B,V),o(v,W),c(e,z,s),c(e,R,s),o(R,X),c(e,D,s),c(e,T,s),o(T,Z),c(e,J,s),c(e,q,s),o(q,ee),c(e,Y,s),c(e,i,s),o(i,M),o(M,g),o(g,ae),o(i,oe),o(i,I),o(I,L),o(L,te),o(i,re),o(i,S),o(S,w),o(w,le),o(i,se),o(i,k),o(k,$),o($,ne),o(i,ie),o(i,C),o(C,A),o(A,ce),G=!0},p:je,i(e){G||(Be(y.$$.fragment,e),Be(b.$$.fragment,e),G=!0)},o(e){Me(y.$$.fragment,e),Me(b.$$.fragment,e),G=!1},d(e){a(h),e&&a(N),e&&a(_),Ie(y),e&&a(j),Ie(b,e),e&&a(U),e&&a(v),e&&a(z),e&&a(R),e&&a(D),e&&a(T),e&&a(J),e&&a(q),e&&a(Y),e&&a(i)}}}const Ye={local:"modelos-de-codificadores",title:"Modelos de codificadores"};function Ge(de){return Ue(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fe extends Se{constructor(h){super();ke(this,h,Ge,Je,Ce,{})}}export{Fe as default,Ye as metadata};
