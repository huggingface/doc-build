import{S as Ie,i as Se,s as ke,e as s,k as m,w as ye,t as c,M as Be,c as t,d as a,m as p,a as r,x as Le,h as i,b as n,G as o,g as d,y as Pe,L as Re,q as Te,o as Me,B as je,v as ze}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ne}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ce}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Ue(de){let f,R,h,_,T,$,D,M,F,z,g,N,v,Q,j,V,W,C,b,X,w,Z,ee,U,L,ae,Y,P,oe,G,u,I,q,se,te,S,A,re,le,k,x,ne,ce,B,y,ie,H;return $=new Ce({}),g=new Ne({props:{id:"0_4KEb08xrE"}}),{c(){f=s("meta"),R=m(),h=s("h1"),_=s("a"),T=s("span"),ye($.$$.fragment),D=m(),M=s("span"),F=c("Modelos secuencia a secuencia"),z=m(),ye(g.$$.fragment),N=m(),v=s("p"),Q=c("Los modelos codificador/decodificador (tamb\xE9n llamados "),j=s("em"),V=c("modelos secuencia a secuencia"),W=c(") usan ambas partes de la arquitectura del Transformador. En cada etapa, las capas de atenci\xF3n del codificador pueden acceder a todas las palabras de la sencuencia inicial, mientras que las capas de atenci\xF3n del decodificador s\xF3lo pueden acceder a las palabras que se ubican antes de una palabra dada en el texto de entrada."),C=m(),b=s("p"),X=c("El preentrenamiento de estos modelos se puede hacer usando los objetivos de los modelos de codificadores o decodificadores, pero usualmente implican algo m\xE1s complejo. Por ejemplo, "),w=s("a"),Z=c("T5"),ee=c(" est\xE1 preentrenado al reemplazar segmentos aleat\xF3rios de texto (que pueden contener varias palabras) con una palabra especial que las oculta, y el objetivo es predecir el texto que esta palabra reemplaza."),U=m(),L=s("p"),ae=c("Los modelos secuencia a secuencia son m\xE1s adecuados para tareas relacionadas con la generaci\xF3n de nuevas oraciones dependiendo de una entrada dada, como resumir, traducir o responder generativamente preguntas."),Y=m(),P=s("p"),oe=c("Algunos miembros de esta familia de modelos son:"),G=m(),u=s("ul"),I=s("li"),q=s("a"),se=c("BART"),te=m(),S=s("li"),A=s("a"),re=c("mBART"),le=m(),k=s("li"),x=s("a"),ne=c("Marian"),ce=m(),B=s("li"),y=s("a"),ie=c("T5"),this.h()},l(e){const l=Be('[data-svelte="svelte-1phssyn"]',document.head);f=t(l,"META",{name:!0,content:!0}),l.forEach(a),R=p(e),h=t(e,"H1",{class:!0});var J=r(h);_=t(J,"A",{id:!0,class:!0,href:!0});var ue=r(_);T=t(ue,"SPAN",{});var me=r(T);Le($.$$.fragment,me),me.forEach(a),ue.forEach(a),D=p(J),M=t(J,"SPAN",{});var pe=r(M);F=i(pe,"Modelos secuencia a secuencia"),pe.forEach(a),J.forEach(a),z=p(e),Le(g.$$.fragment,e),N=p(e),v=t(e,"P",{});var K=r(v);Q=i(K,"Los modelos codificador/decodificador (tamb\xE9n llamados "),j=t(K,"EM",{});var fe=r(j);V=i(fe,"modelos secuencia a secuencia"),fe.forEach(a),W=i(K,") usan ambas partes de la arquitectura del Transformador. En cada etapa, las capas de atenci\xF3n del codificador pueden acceder a todas las palabras de la sencuencia inicial, mientras que las capas de atenci\xF3n del decodificador s\xF3lo pueden acceder a las palabras que se ubican antes de una palabra dada en el texto de entrada."),K.forEach(a),C=p(e),b=t(e,"P",{});var O=r(b);X=i(O,"El preentrenamiento de estos modelos se puede hacer usando los objetivos de los modelos de codificadores o decodificadores, pero usualmente implican algo m\xE1s complejo. Por ejemplo, "),w=t(O,"A",{href:!0,rel:!0});var he=r(w);Z=i(he,"T5"),he.forEach(a),ee=i(O," est\xE1 preentrenado al reemplazar segmentos aleat\xF3rios de texto (que pueden contener varias palabras) con una palabra especial que las oculta, y el objetivo es predecir el texto que esta palabra reemplaza."),O.forEach(a),U=p(e),L=t(e,"P",{});var _e=r(L);ae=i(_e,"Los modelos secuencia a secuencia son m\xE1s adecuados para tareas relacionadas con la generaci\xF3n de nuevas oraciones dependiendo de una entrada dada, como resumir, traducir o responder generativamente preguntas."),_e.forEach(a),Y=p(e),P=t(e,"P",{});var ve=r(P);oe=i(ve,"Algunos miembros de esta familia de modelos son:"),ve.forEach(a),G=p(e),u=t(e,"UL",{});var E=r(u);I=t(E,"LI",{});var be=r(I);q=t(be,"A",{href:!0,rel:!0});var Ee=r(q);se=i(Ee,"BART"),Ee.forEach(a),be.forEach(a),te=p(E),S=t(E,"LI",{});var $e=r(S);A=t($e,"A",{href:!0,rel:!0});var ge=r(A);re=i(ge,"mBART"),ge.forEach(a),$e.forEach(a),le=p(E),k=t(E,"LI",{});var we=r(k);x=t(we,"A",{href:!0,rel:!0});var qe=r(x);ne=i(qe,"Marian"),qe.forEach(a),we.forEach(a),ce=p(E),B=t(E,"LI",{});var Ae=r(B);y=t(Ae,"A",{href:!0,rel:!0});var xe=r(y);ie=i(xe,"T5"),xe.forEach(a),Ae.forEach(a),E.forEach(a),this.h()},h(){n(f,"name","hf:doc:metadata"),n(f,"content",JSON.stringify(Ye)),n(_,"id","modelos-secuencia-a-secuencia"),n(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(_,"href","#modelos-secuencia-a-secuencia"),n(h,"class","relative group"),n(w,"href","https://huggingface.co/t5-base"),n(w,"rel","nofollow"),n(q,"href","https://huggingface.co/transformers/model_doc/bart.html"),n(q,"rel","nofollow"),n(A,"href","https://huggingface.co/transformers/model_doc/mbart.html"),n(A,"rel","nofollow"),n(x,"href","https://huggingface.co/transformers/model_doc/marian.html"),n(x,"rel","nofollow"),n(y,"href","https://huggingface.co/transformers/model_doc/t5.html"),n(y,"rel","nofollow")},m(e,l){o(document.head,f),d(e,R,l),d(e,h,l),o(h,_),o(_,T),Pe($,T,null),o(h,D),o(h,M),o(M,F),d(e,z,l),Pe(g,e,l),d(e,N,l),d(e,v,l),o(v,Q),o(v,j),o(j,V),o(v,W),d(e,C,l),d(e,b,l),o(b,X),o(b,w),o(w,Z),o(b,ee),d(e,U,l),d(e,L,l),o(L,ae),d(e,Y,l),d(e,P,l),o(P,oe),d(e,G,l),d(e,u,l),o(u,I),o(I,q),o(q,se),o(u,te),o(u,S),o(S,A),o(A,re),o(u,le),o(u,k),o(k,x),o(x,ne),o(u,ce),o(u,B),o(B,y),o(y,ie),H=!0},p:Re,i(e){H||(Te($.$$.fragment,e),Te(g.$$.fragment,e),H=!0)},o(e){Me($.$$.fragment,e),Me(g.$$.fragment,e),H=!1},d(e){a(f),e&&a(R),e&&a(h),je($),e&&a(z),je(g,e),e&&a(N),e&&a(v),e&&a(C),e&&a(b),e&&a(U),e&&a(L),e&&a(Y),e&&a(P),e&&a(G),e&&a(u)}}}const Ye={local:"modelos-secuencia-a-secuencia",title:"Modelos secuencia a secuencia"};function Ge(de){return ze(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Oe extends Ie{constructor(f){super();Se(this,f,Ge,Ue,ke,{})}}export{Oe as default,Ye as metadata};
