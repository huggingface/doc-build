<meta charset="utf-8" /><meta http-equiv="content-security-policy" content=""><meta name="hf:doc:metadata" content="{&quot;local&quot;:&quot;introduction&quot;,&quot;sections&quot;:[{&quot;local&quot;:&quot;bienvenue-au-cours&quot;,&quot;title&quot;:&quot;Bienvenue au cours ğŸ¤— !&quot;},{&quot;local&quot;:&quot;quoi-sattendre&quot;,&quot;title&quot;:&quot;Ã€ quoi s&#39;attendre ?&quot;},{&quot;local&quot;:&quot;qui-sommesnous&quot;,&quot;title&quot;:&quot;Qui sommes-nous ?&quot;}],&quot;title&quot;:&quot;Introduction&quot;}" data-svelte="svelte-1phssyn">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/start-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/chunks/vendor-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/chunks/paths-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/pages/__layout.svelte-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/pages/chapter1/1.mdx-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/chunks/Youtube-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/chunks/IconCopyLink-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/course/main/fr/_app/chunks/CourseFloatingBanner-hf-doc-builder.js"> 





<h1 class="relative group"><a id="introduction" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#introduction"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Introduction
	</span></h1>



<div class="flex space-x-1 absolute z-10 right-0 top-0"><a href="https://discuss.huggingface.co/t/chapter-1-questions" target="_blank"><img alt="Ask a Question" class="!m-0" src="https://img.shields.io/badge/Ask%20a%20question-ffcb4c.svg?logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgLTEgMTA0IDEwNiI+PGRlZnM+PHN0eWxlPi5jbHMtMXtmaWxsOiMyMzFmMjA7fS5jbHMtMntmaWxsOiNmZmY5YWU7fS5jbHMtM3tmaWxsOiMwMGFlZWY7fS5jbHMtNHtmaWxsOiMwMGE5NGY7fS5jbHMtNXtmaWxsOiNmMTVkMjI7fS5jbHMtNntmaWxsOiNlMzFiMjM7fTwvc3R5bGU+PC9kZWZzPjx0aXRsZT5EaXNjb3Vyc2VfbG9nbzwvdGl0bGU+PGcgaWQ9IkxheWVyXzIiPjxnIGlkPSJMYXllcl8zIj48cGF0aCBjbGFzcz0iY2xzLTEiIGQ9Ik01MS44NywwQzIzLjcxLDAsMCwyMi44MywwLDUxYzAsLjkxLDAsNTIuODEsMCw1Mi44MWw1MS44Ni0uMDVjMjguMTYsMCw1MS0yMy43MSw1MS01MS44N1M4MCwwLDUxLjg3LDBaIi8+PHBhdGggY2xhc3M9ImNscy0yIiBkPSJNNTIuMzcsMTkuNzRBMzEuNjIsMzEuNjIsMCwwLDAsMjQuNTgsNjYuNDFsLTUuNzIsMTguNEwzOS40LDgwLjE3YTMxLjYxLDMxLjYxLDAsMSwwLDEzLTYwLjQzWiIvPjxwYXRoIGNsYXNzPSJjbHMtMyIgZD0iTTc3LjQ1LDMyLjEyYTMxLjYsMzEuNiwwLDAsMS0zOC4wNSw0OEwxOC44Niw4NC44MmwyMC45MS0yLjQ3QTMxLjYsMzEuNiwwLDAsMCw3Ny40NSwzMi4xMloiLz48cGF0aCBjbGFzcz0iY2xzLTQiIGQ9Ik03MS42MywyNi4yOUEzMS42LDMxLjYsMCwwLDEsMzguOCw3OEwxOC44Niw4NC44MiwzOS40LDgwLjE3QTMxLjYsMzEuNiwwLDAsMCw3MS42MywyNi4yOVoiLz48cGF0aCBjbGFzcz0iY2xzLTUiIGQ9Ik0yNi40Nyw2Ny4xMWEzMS42MSwzMS42MSwwLDAsMSw1MS0zNUEzMS42MSwzMS42MSwwLDAsMCwyNC41OCw2Ni40MWwtNS43MiwxOC40WiIvPjxwYXRoIGNsYXNzPSJjbHMtNiIgZD0iTTI0LjU4LDY2LjQxQTMxLjYxLDMxLjYxLDAsMCwxLDcxLjYzLDI2LjI5YTMxLjYxLDMxLjYxLDAsMCwwLTQ5LDM5LjYzbC0zLjc2LDE4LjlaIi8+PC9nPjwvZz48L3N2Zz4="></a>
	
	</div>
<h2 class="relative group"><a id="bienvenue-au-cours" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#bienvenue-au-cours"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Bienvenue au cours ğŸ¤— !
	</span></h2>

<iframe class="w-full xl:w-4/6 h-80" src="https://www.youtube-nocookie.com/embed/00GKzGyWFEs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>Ce cours vous apprendra Ã  utiliser les bibliothÃ¨ques de NLP de lâ€™Ã©cosystÃ¨me <a href="https://huggingface.co/" rel="nofollow">Hugging Face</a> : <a href="https://github.com/huggingface/transformers" rel="nofollow">ğŸ¤— <em>Transformers</em></a>, <a href="https://github.com/huggingface/datasets" rel="nofollow">ğŸ¤— <em>Datasets</em></a>, <a href="https://github.com/huggingface/tokenizers" rel="nofollow">ğŸ¤— <em>Tokenizers</em></a> et <a href="https://github.com/huggingface/accelerate" rel="nofollow">ğŸ¤— <em>Accelerate</em></a>, ainsi que le <a href="https://huggingface.co/models" rel="nofollow"><em>Hub</em></a>. Câ€™est totalement gratuit et sans publicitÃ©.</p>
<h2 class="relative group"><a id="quoi-sattendre" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#quoi-sattendre"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Ã€ quoi s&#39;attendre ?
	</span></h2>

<p>Voici un bref aperÃ§u du cours :</p>
<div class="flex justify-center"><img class="block dark:hidden" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg" alt="Bref aperÃ§u du contenu du cours.">
<img class="hidden dark:block" src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg" alt="Bref aperÃ§u des diffÃ©rents chapitres du cours."></div>
<ul><li>Les chapitres 1 Ã  4 prÃ©sentent les principaux concepts de la bibliothÃ¨que ğŸ¤— <em>Transformers</em>. Ã€ la fin de ce chapitre, vous serez familier avec le fonctionnement des <em>transformers</em> et vous saurez comment utiliser un modÃ¨le prÃ©sent sur le <a href="https://huggingface.co/models" rel="nofollow"><em>Hub</em></a>, le <em>finetuner</em> sur un jeu de donnÃ©es, et partager vos rÃ©sultats sur le <em>Hub</em> !</li>
<li>Les chapitres 5 Ã  8 prÃ©sentent les bases des librairies ğŸ¤— <em>Datasets</em> et ğŸ¤— <em>Tokenizers</em> ainsi quâ€™une dÃ©couverte des problÃ¨mes classiques de NLP. Ã€ la fin de ce chapitre, vous serez capable de rÃ©soudre les problÃ¨mes de NLP les plus communs par vous-mÃªme.</li>
<li>Les chapitres 9 Ã  12 proposent dâ€™aller plus loin et dâ€™explorer comment les <em>transformers</em> peuvent Ãªtre utilisÃ©s pour rÃ©soudre des problÃ¨mes de traitement de la parole et de vision par ordinateur. En suivant ces chapitres, vous apprendrez Ã  construire et Ã  partager vos modÃ¨les via des dÃ©monstrateurs, et vous serez capable dâ€™optimiser ces modÃ¨les pour des environnements de production. Enfin, vous serez prÃªt Ã  appliquer ğŸ¤— <em>Transformers</em> Ã  (presque) nâ€™importe quel problÃ¨me dâ€™apprentissage automatique !</li></ul>
<p>Ce cours :</p>
<ul><li>requiert un bon niveau en Python,</li>
<li>se comprend mieux si vous avez dÃ©jÃ  suivi un cours dâ€™introduction Ã  lâ€™apprentissage profond comme <a href="https://www.fast.ai/" rel="nofollow">fast.aiâ€™s</a>, <a href="https://course.fast.ai/" rel="nofollow"><em>Practical Deep Learning for Coders</em></a> ou un des cours dÃ©veloppÃ©s par <a href="https://www.deeplearning.ai/" rel="nofollow"><em>DeepLearning.AI</em></a>,</li>
<li>nâ€™attend pas une connaissance appronfondie de <a href="https://pytorch.org/" rel="nofollow">PyTorch</a> ou de <a href="https://www.tensorflow.org/" rel="nofollow">TensorFlow</a>, bien quâ€™Ãªtre familiarisÃ© avec lâ€™un dâ€™entre eux peut aider.</li></ul>
<p>AprÃ¨s avoir terminÃ© ce cours, nous vous recommandons de suivre la <a href="https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh" rel="nofollow">SpÃ©cialisation en NLP</a> dispensÃ©e par DeepLearning.AI, qui couvre une grande partie des modÃ¨les traditionnels de NLP comme le BayÃ©sien naÃ¯f et les LSTMs qui sont importants Ã  connaÃ®tre!</p>
<h2 class="relative group"><a id="qui-sommesnous" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#qui-sommesnous"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Qui sommes-nous ?
	</span></h2>

<p>Ã€ propos des auteurs de ce cours :</p>
<p>*Abubakar Abid** a obtenu son doctorat Ã  Stanford en apprentissage automatique appliquÃ©. Pendant son doctorat, il a fondÃ© <a href="https://github.com/gradio-app/gradio" rel="nofollow">Gradio</a>, une bibliothÃ¨que Python open-source qui a Ã©tÃ© utilisÃ©e pour construire plus de 600 000 dÃ©mos dâ€™apprentissage automatique. Gradio a Ã©tÃ© rachetÃ©e par Hugging Face, oÃ¹ Abubakar occupe dÃ©sormais le poste de responsable de lâ€™Ã©quipe dâ€™apprentissage automatique.</p>
<p><strong>Matthew Carrigan</strong> est ingÃ©nieur en apprentissage machine chez Hugging Face. Il vit Ã  Dublin en Irlande. Il a travaillÃ© auparavant comme ingÃ©nieur en apprentissage machine chez Parse.ly et avant cela comme chercheur postdoctoral au Trinity College Dublin. Il ne croit pas que nous arrivions Ã  lâ€™<em>AGI</em> en mettant Ã  lâ€™Ã©chelle les architectures existantes mais a tout de mÃªme beaucoup dâ€™espoir dans lâ€™immortalitÃ© des robots.</p>
<p><strong>Lysandre Debut</strong> est ingÃ©nieur en apprentissage machine chez Hugging Face et a travaillÃ© sur la bibliothÃ¨que ğŸ¤— <em>Transformers</em> depuis les premiÃ¨res phases de dÃ©veloppement. Son but est de rendre le NLP accessible Ã  tous en dÃ©veloppant des outils disposant dâ€™une API trÃ¨s simple.</p>
<p><strong>Sylvain Gugger</strong> est ingÃ©nieur recherche chez Hugging Face et un des principaux responsables de la bibliothÃ¨que ğŸ¤— <em>Transformers</em>. Avant cela, il Ã©tait chercheur en en apprentissage machine chez fast.ai et a Ã©crit le livre <a href="https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/" rel="nofollow"><em>Deep Learning for Coders with fastai and PyTorch</em></a> avec Jeremy Howard. Son but est de rendre lâ€™apprentissage profond plus accessible, en dÃ©veloppant et en amÃ©liorant des techniques permettant aux modÃ¨les dâ€™apprendre rapidement sur des ressources limitÃ©es.</p>
<p><strong>Dawood Khan</strong> est un ingÃ©nieur en apprentissage automatique chez Hugging Face. Il vient de New York et est diplÃ´mÃ© de lâ€™UniversitÃ© de New York en informatique. AprÃ¨s avoir travaillÃ© comme ingÃ©nieur iOS pendant quelques annÃ©es, Dawood a quittÃ© son poste pour crÃ©er Gradio avec ses cofondateurs. Gradio a finalement Ã©tÃ© acquis par Hugging Face.</p>
<p><strong>Merve Noyan</strong> est dÃ©veloppeuse <em>advocate</em> chez Hugging Face et travaille Ã  la crÃ©ation dâ€™outils et de contenus visant Ã  dÃ©mocratiser lâ€™apprentissage machine pour tous.</p>
<p><strong>Lucile Saulnier</strong> est ingÃ©nieure en apprentissage machine chez Hugging Face et travaille au dÃ©veloppement et Ã  lâ€™implÃ©mentation de nombreux outils <em>open source</em>. Elle est Ã©galement activement impliquÃ©e dans de nombreux projets de recherche dans le domaine du NLP comme lâ€™entraÃ®nement collaboratif de modÃ¨les et le projet BigScience.</p>
<p><strong>Lewis Tunstall</strong> est ingÃ©nieur en apprentissage machine chez Hugging Face et dÃ©vouÃ© au dÃ©veloppement dâ€™outils open source avec la volontÃ© de les rendre accessibles Ã  une communautÃ© plus large. Il est Ã©galement co-auteur du livre <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/" rel="nofollow"><em>Natural Language Processing with Transformers</em></a>.</p>
<p><strong>Leandro von Werra</strong> est ingÃ©nieur en apprentissage machine dans lâ€™Ã©quipe <em>open source</em> dâ€™Hugging Face et Ã©galement co-auteur du livre <a href="https://www.oreilly.com/library/view/natural-language-processing/9781098103231/" rel="nofollow"><em>Natural Language Processing with Transformers</em></a>. Il a plusieurs annÃ©es dâ€™expÃ©rience dans lâ€™industrie oÃ¹ il a pu dÃ©ployer des projets de NLP en production et travailler sur toutes les Ã©tapes clefs du dÃ©ploiement.</p>
<p>ÃŠtes-vous prÃªt Ã  commencer ? Dans ce chapitre, vous apprendrez :</p>
<ul><li>Ã  utiliser la fonction <code>pipeline()</code> pour rÃ©soudre des problÃ¨mes de NLP comme la gÃ©nÃ©ration de texte et la classification,</li>
<li>lâ€™architecture dâ€™un <em>transformer</em>,</li>
<li>comment faire la distinction entre les diffÃ©rentes architectures dâ€™encodeur, de dÃ©codeur et dâ€™encodeur-dÃ©codeur ainsi que leurs diffÃ©rents cas dâ€™usage.</li></ul>


		<script type="module" data-hydrate="lwmy4v">
		import { start } from "/docs/course/main/fr/_app/start-hf-doc-builder.js";
		start({
			target: document.querySelector('[data-hydrate="lwmy4v"]').parentNode,
			paths: {"base":"/docs/course/main/fr","assets":"/docs/course/main/fr"},
			session: {},
			route: false,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/docs/course/main/fr/_app/pages/__layout.svelte-hf-doc-builder.js"),
						import("/docs/course/main/fr/_app/pages/chapter1/1.mdx-hf-doc-builder.js")
				],
				params: {}
			}
		});
	</script>
