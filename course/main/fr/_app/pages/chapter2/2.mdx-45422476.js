import{S as Ap,i as Np,s as Ip,e as o,k as d,w as y,t as l,M as Fp,c as i,d as t,m as f,x,a as u,h as a,b as E,N as tl,F as s,g as m,y as P,o as b,p as ge,q as $,B as T,v as Sp,n as ke}from"../../chunks/vendor-1e8b365d.js";import{T as Mp}from"../../chunks/Tip-62b14c6e.js";import{Y as Cp}from"../../chunks/Youtube-c2a8cc39.js";import{I as kt}from"../../chunks/IconCopyLink-483c28ba.js";import{C as N}from"../../chunks/CodeBlock-e5764662.js";import{D as zp}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as Dp}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function Lp(g){let n,c;return n=new zp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"}]}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function Op(g){let n,c;return n=new zp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"}]}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function Vp(g){let n;return{c(){n=l("Il s'agit de la premi\xE8re section dont le contenu est l\xE9g\xE8rement diff\xE9rent selon que vous utilisez PyTorch ou TensorFlow. Cliquez sur le bouton situ\xE9 au-dessus du titre pour s\xE9lectionner la plateforme que vous pr\xE9f\xE9rez !")},l(c){n=a(c,"Il s'agit de la premi\xE8re section dont le contenu est l\xE9g\xE8rement diff\xE9rent selon que vous utilisez PyTorch ou TensorFlow. Cliquez sur le bouton situ\xE9 au-dessus du titre pour s\xE9lectionner la plateforme que vous pr\xE9f\xE9rez !")},m(c,r){m(c,n,r)},d(c){c&&t(n)}}}function Hp(g){let n,c;return n=new Cp({props:{id:"wVN12smEvqg"}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function Gp(g){let n,c;return n=new Cp({props:{id:"1pedAIvTWXk"}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function Jp(g){let n,c;return n=new N({props:{code:`raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",  # J'ai attendu un cours de HuggingFace toute ma vie.
    "I hate this so much!",  # Je d\xE9teste tellement \xE7a !
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)`,highlighted:`raw_inputs = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,  <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
    <span class="hljs-string">&quot;I hate this so much!&quot;</span>,  <span class="hljs-comment"># Je d\xE9teste tellement \xE7a !</span>
]
inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(inputs)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function Up(g){let n,c;return n=new N({props:{code:`raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",  # J'ai attendu un cours de HuggingFace toute ma vie.
    "I hate this so much!",  # Je d\xE9teste tellement \xE7a !
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)`,highlighted:`raw_inputs = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,  <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
    <span class="hljs-string">&quot;I hate this so much!&quot;</span>,  <span class="hljs-comment"># Je d\xE9teste tellement \xE7a !</span>
]
inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function Bp(g){let n,c,r,h,v;return h=new N({props:{code:`{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}`,highlighted:`{
    <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
        array([
            [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>],
            [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">5223</span>,  <span class="hljs-number">2023</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2172</span>,   <span class="hljs-number">999</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]
        ], dtype=int32)&gt;, 
    <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
        array([
            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
        ], dtype=int32)&gt;
}`}}),{c(){n=o("p"),c=l("Voici \xE0 quoi ressemblent les r\xE9sultats sous forme de tenseurs TensorFlow :"),r=d(),y(h.$$.fragment)},l(_){n=i(_,"P",{});var M=u(n);c=a(M,"Voici \xE0 quoi ressemblent les r\xE9sultats sous forme de tenseurs TensorFlow :"),M.forEach(t),r=f(_),x(h.$$.fragment,_)},m(_,M){m(_,n,M),s(n,c),m(_,r,M),P(h,_,M),v=!0},i(_){v||($(h.$$.fragment,_),v=!0)},o(_){b(h.$$.fragment,_),v=!1},d(_){_&&t(n),_&&t(r),T(h,_)}}}function Rp(g){let n,c,r,h,v;return h=new N({props:{code:`{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}`,highlighted:`{
    <span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([
        [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>],
        [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">5223</span>,  <span class="hljs-number">2023</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2172</span>,   <span class="hljs-number">999</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]
    ]), 
    <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
    ])
}`}}),{c(){n=o("p"),c=l("Voici \xE0 quoi ressemblent les r\xE9sultats sous forme de tenseurs PyTorch :"),r=d(),y(h.$$.fragment)},l(_){n=i(_,"P",{});var M=u(n);c=a(M,"Voici \xE0 quoi ressemblent les r\xE9sultats sous forme de tenseurs PyTorch :"),M.forEach(t),r=f(_),x(h.$$.fragment,_)},m(_,M){m(_,n,M),s(n,c),m(_,r,M),P(h,_,M),v=!0},i(_){v||($(h.$$.fragment,_),v=!0)},o(_){b(h.$$.fragment,_),v=!1},d(_){_&&t(n),_&&t(r),T(h,_)}}}function Qp(g){let n,c,r,h,v,_,M,A,z,C,F,k,q,I,D,S,J;return S=new N({props:{code:`from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = TFAutoModel.from_pretrained(checkpoint)`}}),{c(){n=o("p"),c=l("Nous pouvons t\xE9l\xE9charger notre mod\xE8le pr\xE9tra\xEEn\xE9 de la m\xEAme mani\xE8re que nous l\u2019avons fait avec notre "),r=o("em"),h=l("tokenizer"),v=l(". \u{1F917} "),_=o("em"),M=l("Transformers"),A=l(" fournit une classe "),z=o("code"),C=l("TFAutoModel"),F=l(" qui poss\xE8de \xE9galement une m\xE9thode "),k=o("code"),q=l("from_pretrained()"),I=l(" :"),D=d(),y(S.$$.fragment)},l(j){n=i(j,"P",{});var w=u(n);c=a(w,"Nous pouvons t\xE9l\xE9charger notre mod\xE8le pr\xE9tra\xEEn\xE9 de la m\xEAme mani\xE8re que nous l\u2019avons fait avec notre "),r=i(w,"EM",{});var W=u(r);h=a(W,"tokenizer"),W.forEach(t),v=a(w,". \u{1F917} "),_=i(w,"EM",{});var U=u(_);M=a(U,"Transformers"),U.forEach(t),A=a(w," fournit une classe "),z=i(w,"CODE",{});var Me=u(z);C=a(Me,"TFAutoModel"),Me.forEach(t),F=a(w," qui poss\xE8de \xE9galement une m\xE9thode "),k=i(w,"CODE",{});var X=u(k);q=a(X,"from_pretrained()"),X.forEach(t),I=a(w," :"),w.forEach(t),D=f(j),x(S.$$.fragment,j)},m(j,w){m(j,n,w),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A),s(n,z),s(z,C),s(n,F),s(n,k),s(k,q),s(n,I),m(j,D,w),P(S,j,w),J=!0},i(j){J||($(S.$$.fragment,j),J=!0)},o(j){b(S.$$.fragment,j),J=!1},d(j){j&&t(n),j&&t(D),T(S,j)}}}function Yp(g){let n,c,r,h,v,_,M,A,z,C,F,k,q,I,D,S,J;return S=new N({props:{code:`from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModel.from_pretrained(checkpoint)`}}),{c(){n=o("p"),c=l("Nous pouvons t\xE9l\xE9charger notre mod\xE8le pr\xE9-entra\xEEn\xE9 de la m\xEAme mani\xE8re que nous l\u2019avons fait avec notre "),r=o("em"),h=l("tokenizer"),v=l(". \u{1F917} "),_=o("em"),M=l("Transformers"),A=l(" fournit une classe "),z=o("code"),C=l("AutoModel"),F=l(" qui poss\xE8de \xE9galement une m\xE9thode "),k=o("code"),q=l("from_pretrained()"),I=l(" :"),D=d(),y(S.$$.fragment)},l(j){n=i(j,"P",{});var w=u(n);c=a(w,"Nous pouvons t\xE9l\xE9charger notre mod\xE8le pr\xE9-entra\xEEn\xE9 de la m\xEAme mani\xE8re que nous l\u2019avons fait avec notre "),r=i(w,"EM",{});var W=u(r);h=a(W,"tokenizer"),W.forEach(t),v=a(w,". \u{1F917} "),_=i(w,"EM",{});var U=u(_);M=a(U,"Transformers"),U.forEach(t),A=a(w," fournit une classe "),z=i(w,"CODE",{});var Me=u(z);C=a(Me,"AutoModel"),Me.forEach(t),F=a(w," qui poss\xE8de \xE9galement une m\xE9thode "),k=i(w,"CODE",{});var X=u(k);q=a(X,"from_pretrained()"),X.forEach(t),I=a(w," :"),w.forEach(t),D=f(j),x(S.$$.fragment,j)},m(j,w){m(j,n,w),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A),s(n,z),s(z,C),s(n,F),s(n,k),s(k,q),s(n,I),m(j,D,w),P(S,j,w),J=!0},i(j){J||($(S.$$.fragment,j),J=!0)},o(j){b(S.$$.fragment,j),J=!1},d(j){j&&t(n),j&&t(D),T(S,j)}}}function Wp(g){let n,c,r,h;return n=new N({props:{code:`outputs = model(inputs)
print(outputs.last_hidden_state.shape)`,highlighted:`outputs = model(inputs)
<span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)`}}),r=new N({props:{code:"(2, 16, 768)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">768</span>)'}}),{c(){y(n.$$.fragment),c=d(),y(r.$$.fragment)},l(v){x(n.$$.fragment,v),c=f(v),x(r.$$.fragment,v)},m(v,_){P(n,v,_),m(v,c,_),P(r,v,_),h=!0},i(v){h||($(n.$$.fragment,v),$(r.$$.fragment,v),h=!0)},o(v){b(n.$$.fragment,v),b(r.$$.fragment,v),h=!1},d(v){T(n,v),v&&t(c),T(r,v)}}}function Xp(g){let n,c,r,h;return n=new N({props:{code:`outputs = model(**inputs)
print(outputs.last_hidden_state.shape)`,highlighted:`outputs = model(**inputs)
<span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)`}}),r=new N({props:{code:"torch.Size([2, 16, 768])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">768</span>])'}}),{c(){y(n.$$.fragment),c=d(),y(r.$$.fragment)},l(v){x(n.$$.fragment,v),c=f(v),x(r.$$.fragment,v)},m(v,_){P(n,v,_),m(v,c,_),P(r,v,_),h=!0},i(v){h||($(n.$$.fragment,v),$(r.$$.fragment,v),h=!0)},o(v){b(n.$$.fragment,v),b(r.$$.fragment,v),h=!1},d(v){T(n,v),v&&t(c),T(r,v)}}}function Kp(g){let n,c,r,h,v,_,M,A,z,C,F;return C=new N({props:{code:`from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)`}}),{c(){n=o("p"),c=l("Pour notre exemple, nous avons besoin d\u2019un mod\xE8le avec une t\xEAte de classification de s\xE9quence (pour pouvoir classer les phrases comme positives ou n\xE9gatives). Donc, nous n\u2019utilisons pas r\xE9ellement la classe "),r=o("code"),h=l("TFAutoModel"),v=l(" mais plut\xF4t "),_=o("code"),M=l("TFAutoModelForSequenceClassification"),A=l(" :"),z=d(),y(C.$$.fragment)},l(k){n=i(k,"P",{});var q=u(n);c=a(q,"Pour notre exemple, nous avons besoin d\u2019un mod\xE8le avec une t\xEAte de classification de s\xE9quence (pour pouvoir classer les phrases comme positives ou n\xE9gatives). Donc, nous n\u2019utilisons pas r\xE9ellement la classe "),r=i(q,"CODE",{});var I=u(r);h=a(I,"TFAutoModel"),I.forEach(t),v=a(q," mais plut\xF4t "),_=i(q,"CODE",{});var D=u(_);M=a(D,"TFAutoModelForSequenceClassification"),D.forEach(t),A=a(q," :"),q.forEach(t),z=f(k),x(C.$$.fragment,k)},m(k,q){m(k,n,q),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A),m(k,z,q),P(C,k,q),F=!0},i(k){F||($(C.$$.fragment,k),F=!0)},o(k){b(C.$$.fragment,k),F=!1},d(k){k&&t(n),k&&t(z),T(C,k)}}}function Zp(g){let n,c,r,h,v,_,M,A,z,C,F;return C=new N({props:{code:`from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)`}}),{c(){n=o("p"),c=l("Pour notre exemple, nous avons besoin d\u2019un mod\xE8le avec une t\xEAte de classification de s\xE9quence (pour pouvoir classer les phrases comme positives ou n\xE9gatives). Donc, nous n\u2019utilisons pas r\xE9ellement la classe "),r=o("code"),h=l("AutoModel"),v=l(" mais plut\xF4t "),_=o("code"),M=l("AutoModelForSequenceClassification"),A=l(" :"),z=d(),y(C.$$.fragment)},l(k){n=i(k,"P",{});var q=u(n);c=a(q,"Pour notre exemple, nous avons besoin d\u2019un mod\xE8le avec une t\xEAte de classification de s\xE9quence (pour pouvoir classer les phrases comme positives ou n\xE9gatives). Donc, nous n\u2019utilisons pas r\xE9ellement la classe "),r=i(q,"CODE",{});var I=u(r);h=a(I,"AutoModel"),I.forEach(t),v=a(q," mais plut\xF4t "),_=i(q,"CODE",{});var D=u(_);M=a(D,"AutoModelForSequenceClassification"),D.forEach(t),A=a(q," :"),q.forEach(t),z=f(k),x(C.$$.fragment,k)},m(k,q){m(k,n,q),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A),m(k,z,q),P(C,k,q),F=!0},i(k){F||($(C.$$.fragment,k),F=!0)},o(k){b(C.$$.fragment,k),F=!1},d(k){k&&t(n),k&&t(z),T(C,k)}}}function ec(g){let n,c;return n=new N({props:{code:"(2, 2)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)'}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function sc(g){let n,c;return n=new N({props:{code:"torch.Size([2, 2])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])'}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function tc(g){let n,c;return n=new N({props:{code:`<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
    array([[-<span class="hljs-number">1.5606991</span>,  <span class="hljs-number">1.6122842</span>],
           [ <span class="hljs-number">4.169231</span> , -<span class="hljs-number">3.3464472</span>]], dtype=float32)&gt;`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function nc(g){let n,c;return n=new N({props:{code:`tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[-<span class="hljs-number">1.5607</span>,  <span class="hljs-number">1.6123</span>],
        [ <span class="hljs-number">4.1692</span>, -<span class="hljs-number">3.3464</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function rc(g){let n,c;return n=new N({props:{code:`import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

predictions = tf.math.softmax(outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function lc(g){let n,c;return n=new N({props:{code:`import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)`,highlighted:`<span class="hljs-keyword">import</span> torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function ac(g){let n,c;return n=new N({props:{code:`tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor(
[[<span class="hljs-number">4.01951671e-02</span> <span class="hljs-number">9.59804833e-01</span>]
 [<span class="hljs-number">9.9945587e-01</span> <span class="hljs-number">5.4418424e-04</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function oc(g){let n,c;return n=new N({props:{code:`tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)`,highlighted:`tensor([[<span class="hljs-number">4.0195e-02</span>, <span class="hljs-number">9.5980e-01</span>],
        [<span class="hljs-number">9.9946e-01</span>, <span class="hljs-number">5.4418e-04</span>]], grad_fn=&lt;SoftmaxBackward&gt;)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function ic(g){let n,c,r,h,v,_,M,A;return{c(){n=o("p"),c=l("\u270F\uFE0F "),r=o("strong"),h=l("Essayez !"),v=l(" Choisissez deux (ou plus) textes de votre choix (en anglais) et faites-les passer par le pipeline "),_=o("code"),M=l("sentiment-analysis"),A=l(". Reproduisez ensuite vous-m\xEAme les \xE9tapes vues ici et v\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats !")},l(z){n=i(z,"P",{});var C=u(n);c=a(C,"\u270F\uFE0F "),r=i(C,"STRONG",{});var F=u(r);h=a(F,"Essayez !"),F.forEach(t),v=a(C," Choisissez deux (ou plus) textes de votre choix (en anglais) et faites-les passer par le pipeline "),_=i(C,"CODE",{});var k=u(_);M=a(k,"sentiment-analysis"),k.forEach(t),A=a(C,". Reproduisez ensuite vous-m\xEAme les \xE9tapes vues ici et v\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats !"),C.forEach(t)},m(z,C){m(z,n,C),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A)},d(z){z&&t(n)}}}function uc(g){let n,c,r,h,v,_,M,A,z,C,F,k,q,I,D,S,J,j,w,W,U,Me,X,nl,rl,Rn,ls,Qn,Ds,ll,Yn,as,Wn,De,al,Ls,ol,il,Xn,Ce,os,Ei,ul,is,ji,Kn,Os,pl,Zn,ze,Le,Et,us,cl,jt,ml,er,Ee,dl,qt,fl,hl,wt,_l,vl,sr,je,ps,bl,yt,$l,gl,kl,cs,El,xt,jl,ql,wl,Pt,yl,tr,V,xl,ms,Tt,Pl,Tl,Mt,Ml,Cl,Ct,zl,Al,zt,Nl,Il,At,Fl,Sl,nr,B,Dl,Nt,Ll,Ol,It,Vl,Hl,Ft,Gl,Jl,ds,Ul,Bl,rr,fs,lr,Oe,Rl,St,Ql,Yl,ar,O,Wl,Dt,Xl,Kl,Lt,Zl,ea,Ot,sa,ta,Vt,na,ra,Ht,la,aa,Gt,oa,ia,or,Ve,ua,Jt,pa,ca,ir,te,ne,Vs,He,ma,Ut,da,fa,ur,re,le,Hs,H,ha,Bt,_a,va,Rt,ba,$a,Qt,ga,ka,Yt,Ea,ja,Wt,qa,wa,pr,Ae,Ge,Xt,hs,ya,Kt,xa,cr,ae,oe,Gs,Je,Pa,Zt,Ta,Ma,mr,R,Ca,en,za,Aa,sn,Na,Ia,tn,Fa,Sa,Js,Da,nn,La,Oa,dr,Us,Va,fr,qe,Ha,rn,Ga,Ja,Bs,Ua,Ba,hr,Ne,Ue,ln,_s,Ra,an,Qa,_r,Be,Ya,on,Wa,Xa,vr,we,Rs,un,Ka,Za,eo,Qs,pn,so,to,no,Ys,cn,ro,lo,br,Ws,ao,$r,Xs,oo,gr,ie,ue,Ks,K,io,mn,uo,po,dn,co,mo,fn,fo,ho,kr,Ie,Re,hn,vs,_o,_n,vo,Er,Fe,bs,qi,bo,$s,wi,jr,Z,$o,vn,go,ko,bn,Eo,jo,$n,qo,wo,qr,L,Zs,gn,yo,xo,Po,kn,En,To,Mo,jn,qn,Co,zo,wn,yn,Ao,No,xn,Pn,Io,Fo,Tn,Mn,So,Do,Cn,zn,Lo,Oo,An,Vo,wr,pe,ce,et,st,Ho,yr,gs,xr,me,de,tt,nt,Go,Pr,Se,Qe,Nn,ks,Jo,In,Uo,Tr,rt,Bo,Mr,Es,Cr,fe,he,lt,Q,Ro,Fn,Qo,Yo,Sn,Wo,Xo,Dn,Ko,Zo,js,ei,si,zr,_e,ve,at,be,$e,ot,ye,ti,Ln,ni,ri,On,li,ai,Ar,Ye,oi,Vn,ii,ui,Nr,qs,Ir,ws,Fr,it,pi,Sr,We,Hn,ci,mi,Gn,di,Dr,Xe,fi,Jn,hi,_i,Lr,Ke,Or;r=new Dp({props:{fw:g[0]}}),A=new kt({});const yi=[Op,Lp],ys=[];function xi(e,p){return e[0]==="pt"?0:1}q=xi(g),I=ys[q]=yi[q](g),S=new Mp({props:{$$slots:{default:[Vp]},$$scope:{ctx:g}}});const Pi=[Gp,Hp],xs=[];function Ti(e,p){return e[0]==="pt"?0:1}j=Ti(g),w=xs[j]=Pi[j](g),ls=new N({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",  # J'ai attendu un cours de HuggingFace toute ma vie.
        "I hate this so much!",  # Je d\xE9teste tellement \xE7a !
    ]
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)
classifier(
    [
        <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,  <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
        <span class="hljs-string">&quot;I hate this so much!&quot;</span>,  <span class="hljs-comment"># Je d\xE9teste tellement \xE7a !</span>
    ]
)`}}),as=new N({props:{code:`[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]`,highlighted:`[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9598047137260437</span>},
 {<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9994558095932007</span>}]`}}),us=new kt({}),fs=new N({props:{code:`from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)`}});const Mi=[Up,Jp],Ps=[];function Ci(e,p){return e[0]==="pt"?0:1}te=Ci(g),ne=Ps[te]=Mi[te](g);const zi=[Rp,Bp],Ts=[];function Ai(e,p){return e[0]==="pt"?0:1}re=Ai(g),le=Ts[re]=zi[re](g),hs=new kt({});const Ni=[Yp,Qp],Ms=[];function Ii(e,p){return e[0]==="pt"?0:1}ae=Ii(g),oe=Ms[ae]=Ni[ae](g),_s=new kt({});const Fi=[Xp,Wp],Cs=[];function Si(e,p){return e[0]==="pt"?0:1}ie=Si(g),ue=Cs[ie]=Fi[ie](g),vs=new kt({});const Di=[Zp,Kp],zs=[];function Li(e,p){return e[0]==="pt"?0:1}pe=Li(g),ce=zs[pe]=Di[pe](g),gs=new N({props:{code:"print(outputs.logits.shape)",highlighted:'<span class="hljs-built_in">print</span>(outputs.logits.shape)'}});const Oi=[sc,ec],As=[];function Vi(e,p){return e[0]==="pt"?0:1}me=Vi(g),de=As[me]=Oi[me](g),ks=new kt({}),Es=new N({props:{code:"print(outputs.logits)",highlighted:'<span class="hljs-built_in">print</span>(outputs.logits)'}});const Hi=[nc,tc],Ns=[];function Gi(e,p){return e[0]==="pt"?0:1}fe=Gi(g),he=Ns[fe]=Hi[fe](g);const Ji=[lc,rc],Is=[];function Ui(e,p){return e[0]==="pt"?0:1}_e=Ui(g),ve=Is[_e]=Ji[_e](g);const Bi=[oc,ac],Fs=[];function Ri(e,p){return e[0]==="pt"?0:1}return be=Ri(g),$e=Fs[be]=Bi[be](g),qs=new N({props:{code:"model.config.id2label",highlighted:"model.config.id2label"}}),ws=new N({props:{code:"{0: 'NEGATIVE', 1: 'POSITIVE'}",highlighted:'{<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>}'}}),Ke=new Mp({props:{$$slots:{default:[ic]},$$scope:{ctx:g}}}),{c(){n=o("meta"),c=d(),y(r.$$.fragment),h=d(),v=o("h1"),_=o("a"),M=o("span"),y(A.$$.fragment),z=d(),C=o("span"),F=l("Derri\xE8re le pipeline"),k=d(),I.c(),D=d(),y(S.$$.fragment),J=d(),w.c(),W=d(),U=o("p"),Me=l("Commen\xE7ons par un exemple complet en regardant ce qui s\u2019est pass\xE9 en coulisses lorsque nous avons ex\xE9cut\xE9 le code suivant dans le "),X=o("a"),nl=l("Chapitre 1"),rl=l(" :"),Rn=d(),y(ls.$$.fragment),Qn=d(),Ds=o("p"),ll=l("la sortie :"),Yn=d(),y(as.$$.fragment),Wn=d(),De=o("p"),al=l("Comme nous l\u2019avons vu dans le "),Ls=o("a"),ol=l("Chapitre 1"),il=l(", ce pipeline regroupe trois \xE9tapes : le pr\xE9traitement, le passage des entr\xE9es dans le mod\xE8le et le post-traitement."),Xn=d(),Ce=o("div"),os=o("img"),ul=d(),is=o("img"),Kn=d(),Os=o("p"),pl=l("Passons rapidement en revue chacun de ces \xE9l\xE9ments."),Zn=d(),ze=o("h2"),Le=o("a"),Et=o("span"),y(us.$$.fragment),cl=d(),jt=o("span"),ml=l("Pr\xE9traitement avec un *tokenizer*"),er=d(),Ee=o("p"),dl=l("Comme d\u2019autres r\xE9seaux de neurones, les "),qt=o("em"),fl=l("transformers"),hl=l(" ne peuvent pas traiter directement le texte brut, donc la premi\xE8re \xE9tape de notre pipeline est de convertir les entr\xE9es textuelles en nombres afin que le mod\xE8le puisse les comprendre. Pour ce faire, nous utilisons un "),wt=o("em"),_l=l("tokenizer"),vl=l(", qui sera responsable de :"),sr=d(),je=o("ul"),ps=o("li"),bl=l("diviser l\u2019entr\xE9e en mots, sous-mots, ou symboles (comme la ponctuation) qui sont appel\xE9s "),yt=o("em"),$l=l("tokens"),gl=l(","),kl=d(),cs=o("li"),El=l("associer chaque "),xt=o("em"),jl=l("token"),ql=l(" \xE0 un nombre entier,"),wl=d(),Pt=o("li"),yl=l("ajouter des entr\xE9es suppl\xE9mentaires qui peuvent \xEAtre utiles au mod\xE8le."),tr=d(),V=o("p"),xl=l("Tout ce pr\xE9traitement doit \xEAtre effectu\xE9 exactement de la m\xEAme mani\xE8re que celui appliqu\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le. Nous devons donc d\u2019abord t\xE9l\xE9charger ces informations depuis le "),ms=o("a"),Tt=o("em"),Pl=l("Model Hub"),Tl=l(". Pour ce faire, nous utilisons la classe "),Mt=o("code"),Ml=l("AutoTokenizer"),Cl=l(" et sa m\xE9thode "),Ct=o("code"),zl=l("from_pretrained()"),Al=l(". En utilisant le nom du "),zt=o("em"),Nl=l("checkpoint"),Il=l(" de notre mod\xE8le, elle va automatiquement r\xE9cup\xE9rer les donn\xE9es associ\xE9es au "),At=o("em"),Fl=l("tokenizer"),Sl=l(" du mod\xE8le et les mettre en cache (afin qu\u2019elles ne soient t\xE9l\xE9charg\xE9es que la premi\xE8re fois que vous ex\xE9cutez le code ci-dessous)."),nr=d(),B=o("p"),Dl=l("Puisque le "),Nt=o("em"),Ll=l("checkpoint"),Ol=l(" par d\xE9faut du pipeline "),It=o("code"),Vl=l("sentiment-analysis"),Hl=l(" (analyse de sentiment) est "),Ft=o("code"),Gl=l("distilbert-base-uncased-finetuned-sst-2-english"),Jl=l(" (vous pouvez voir la carte de ce mod\xE8le "),ds=o("a"),Ul=l("ici"),Bl=l("), nous ex\xE9cutons ce qui suit :"),rr=d(),y(fs.$$.fragment),lr=d(),Oe=o("p"),Rl=l("Une fois que nous avons le "),St=o("em"),Ql=l("tokenizer"),Yl=l(" nous pouvons lui passer directement nos phrases et obtenir un dictionnaire pr\xEAt \xE0 \xEAtre donn\xE9 \xE0 notre mod\xE8le ! La seule chose qui reste \xE0 faire est de convertir en tenseurs la liste des identifiants d\u2019entr\xE9e."),ar=d(),O=o("p"),Wl=l("Vous pouvez utiliser \u{1F917} "),Dt=o("em"),Xl=l("Transformers"),Kl=l(" sans avoir \xE0 vous soucier du "),Lt=o("em"),Zl=l("framework"),ea=l(" utilis\xE9 comme "),Ot=o("em"),sa=l("backend"),ta=l(". Il peut s\u2019agir de PyTorch, de TensorFlow ou de Flax pour certains mod\xE8les. Cependant, les "),Vt=o("em"),na=l("transformers"),ra=l(" n\u2019acceptent que les "),Ht=o("em"),la=l("tenseurs"),aa=l(" en entr\xE9e. Si c\u2019est la premi\xE8re fois que vous entendez parler de tenseurs, vous pouvez les consid\xE9rer comme des tableaux NumPy. Un tableau NumPy peut \xEAtre un scalaire (0D), un vecteur (1D), une matrice (2D), ou avoir davantage de dimensions. Les tenseurs des autres "),Gt=o("em"),oa=l("frameworks"),ia=l(" d\u2019apprentissage machine se comportent de mani\xE8re similaire et sont g\xE9n\xE9ralement aussi simples \xE0 instancier que les tableaux NumPy."),or=d(),Ve=o("p"),ua=l("Pour sp\xE9cifier le type de tenseurs que nous voulons r\xE9cup\xE9rer (PyTorch, TensorFlow, ou simplement NumPy), nous utilisons l\u2019argument "),Jt=o("code"),pa=l("return_tensors"),ca=l(" :"),ir=d(),ne.c(),Vs=d(),He=o("p"),ma=l("Ne vous pr\xE9occupez pas encore du remplissage ("),Ut=o("em"),da=l("padding"),fa=l(") et de la troncature, nous les expliquerons plus tard. Les principales choses \xE0 retenir ici sont que vous pouvez passer une phrase ou une liste de phrases, ainsi que sp\xE9cifier le type de tenseurs que vous voulez r\xE9cup\xE9rer (si aucun type n\u2019est pass\xE9, par d\xE9faut vous obtiendrez une liste de listes comme r\xE9sultat)."),ur=d(),le.c(),Hs=d(),H=o("p"),ha=l("La sortie elle-m\xEAme est un dictionnaire contenant deux cl\xE9s : "),Bt=o("code"),_a=l("input_ids"),va=l(" et "),Rt=o("code"),ba=l("attention_mask"),$a=l(". "),Qt=o("code"),ga=l("input_ids"),ka=l(" contient deux lignes d\u2019entiers (une pour chaque phrase) qui sont les identifiants uniques des "),Yt=o("em"),Ea=l("tokens"),ja=l(" dans chaque phrase. Nous expliquerons ce qu\u2019est l\u2019"),Wt=o("code"),qa=l("attention_mask"),wa=l(" plus tard dans ce chapitre."),pr=d(),Ae=o("h2"),Ge=o("a"),Xt=o("span"),y(hs.$$.fragment),ya=d(),Kt=o("span"),xa=l("Passage au mod\xE8le"),cr=d(),oe.c(),Gs=d(),Je=o("p"),Pa=l("Dans cet extrait de code, nous avons t\xE9l\xE9charg\xE9 le m\xEAme "),Zt=o("em"),Ta=l("checkpoint"),Ma=l(" que nous avons utilis\xE9 dans notre pipeline auparavant (il devrait en fait avoir d\xE9j\xE0 \xE9t\xE9 mis en cache) et instanci\xE9 un mod\xE8le avec lui."),mr=d(),R=o("p"),Ca=l("Cette architecture ne contient que le module de "),en=o("em"),za=l("transformer"),Aa=l(" de base : \xE9tant donn\xE9 certaines entr\xE9es, il produit ce que nous appellerons des "),sn=o("em"),Na=l("\xE9tats cach\xE9s"),Ia=l(", \xE9galement connus sous le nom de "),tn=o("em"),Fa=l("caract\xE9ristiques"),Sa=l(`.
Pour chaque entr\xE9e du mod\xE8le, nous r\xE9cup\xE9rons un vecteur en grande dimension repr\xE9sentant la `),Js=o("strong"),Da=l("compr\xE9hension contextuelle de cette entr\xE9e par le "),nn=o("em"),La=l("transformer"),Oa=l("."),dr=d(),Us=o("p"),Va=l("Si cela ne fait pas sens, ne vous inqui\xE9tez pas. Nous expliquons tout plus tard."),fr=d(),qe=o("p"),Ha=l("Bien que ces \xE9tats cach\xE9s puissent \xEAtre utiles en eux-m\xEAmes, ils sont g\xE9n\xE9ralement les entr\xE9es d\u2019une autre partie du mod\xE8le, connue sous le nom de "),rn=o("em"),Ga=l("t\xEAte"),Ja=l(". Dans le "),Bs=o("a"),Ua=l("Chapitre 1"),Ba=l(", les diff\xE9rentes t\xE2ches auraient pu \xEAtre r\xE9alis\xE9es avec la m\xEAme architecture mais en ayant chacune d\u2019elles une t\xEAte diff\xE9rente."),hr=d(),Ne=o("h3"),Ue=o("a"),ln=o("span"),y(_s.$$.fragment),Ra=d(),an=o("span"),Qa=l("Un vecteur de grande dimension ?"),_r=d(),Be=o("p"),Ya=l("Le vecteur produit en sortie par le "),on=o("em"),Wa=l("transformer"),Xa=l(" est g\xE9n\xE9ralement de grande dimension. Il a g\xE9n\xE9ralement trois dimensions :"),vr=d(),we=o("ul"),Rs=o("li"),un=o("strong"),Ka=l("la taille du lot"),Za=l(" : le nombre de s\xE9quences trait\xE9es \xE0 la fois (2 dans notre exemple),"),eo=d(),Qs=o("li"),pn=o("strong"),so=l("la longueur de la s\xE9quence"),to=l(" : la longueur de la repr\xE9sentation num\xE9rique de la s\xE9quence (16 dans notre exemple),"),no=d(),Ys=o("li"),cn=o("strong"),ro=l("la taille cach\xE9e"),lo=l(" : la dimension du vecteur de chaque entr\xE9e du mod\xE8le."),br=d(),Ws=o("p"),ao=l("On dit qu\u2019il est de \xAB grande dimension \xBB en raison de la derni\xE8re valeur. La taille cach\xE9e peut \xEAtre tr\xE8s grande (g\xE9n\xE9ralement 768 pour les petits mod\xE8les et pour les grands mod\xE8les cela peut atteindre 3072 voire plus)."),$r=d(),Xs=o("p"),oo=l("Nous pouvons le constater si nous alimentons notre mod\xE8le avec les entr\xE9es que nous avons pr\xE9trait\xE9es :"),gr=d(),ue.c(),Ks=d(),K=o("p"),io=l("Notez que les sorties des mod\xE8les de la biblioth\xE8que \u{1F917} Transformers se comportent comme des "),mn=o("code"),uo=l("namedtuples"),po=l(" ou des dictionnaires. Vous pouvez acc\xE9der aux \xE9l\xE9ments par attributs (comme nous l\u2019avons fait), par cl\xE9 ("),dn=o("code"),co=l('outputs["last_hidden_state"]'),mo=l("), ou m\xEAme par l\u2019index si vous savez exactement o\xF9 se trouve la chose que vous cherchez ("),fn=o("code"),fo=l("outputs[0]"),ho=l(")."),kr=d(),Ie=o("h3"),Re=o("a"),hn=o("span"),y(vs.$$.fragment),_o=d(),_n=o("span"),vo=l("Les t\xEAtes des mod\xE8les : donner du sens aux chiffres"),Er=l(`

Les t\xEAtes des mod\xE8les prennent en entr\xE9e le vecteur de grande dimension des \xE9tats cach\xE9s et le projettent sur une autre dimension. Elles sont g\xE9n\xE9ralement compos\xE9es d'une ou de quelques couches lin\xE9aires :
`),Fe=o("div"),bs=o("img"),bo=d(),$s=o("img"),jr=d(),Z=o("p"),$o=l("La sortie du "),vn=o("em"),go=l("transformer"),ko=l(` est envoy\xE9e directement \xE0 la t\xEAte du mod\xE8le pour \xEAtre trait\xE9e.
Dans ce diagramme, le mod\xE8le est repr\xE9sent\xE9 par sa couche d\u2019ench\xE2ssement et les couches suivantes. La couche d\u2019ench\xE2ssement convertit chaque identifiant d\u2019entr\xE9e dans l\u2019entr\xE9e tokenis\xE9e en un vecteur qui repr\xE9sente le `),bn=o("em"),Eo=l("token"),jo=l(` associ\xE9. Les couches suivantes manipulent ces vecteurs en utilisant le m\xE9canisme d\u2019attention pour produire la repr\xE9sentation finale des phrases.
Il existe de nombreuses architectures diff\xE9rentes disponibles dans la biblioth\xE8que \u{1F917} `),$n=o("em"),qo=l("Transformers"),wo=l(", chacune \xE9tant con\xE7ue autour de la prise en charge d\u2019une t\xE2che sp\xE9cifique. En voici une liste non exhaustive :"),qr=d(),L=o("ul"),Zs=o("li"),gn=o("code"),yo=l("*Model"),xo=l(" (r\xE9cup\xE9rer les \xE9tats cach\xE9s)"),Po=d(),kn=o("li"),En=o("code"),To=l("*ForCausalLM"),Mo=d(),jn=o("li"),qn=o("code"),Co=l("*ForMaskedLM"),zo=d(),wn=o("li"),yn=o("code"),Ao=l("*ForMultipleChoice"),No=d(),xn=o("li"),Pn=o("code"),Io=l("*ForQuestionAnswering"),Fo=d(),Tn=o("li"),Mn=o("code"),So=l("*ForSequenceClassification"),Do=d(),Cn=o("li"),zn=o("code"),Lo=l("*ForTokenClassification"),Oo=d(),An=o("li"),Vo=l("et autres \u{1F917}"),wr=d(),ce.c(),et=d(),st=o("p"),Ho=l("Maintenant, si nous examinons la forme de nos entr\xE9es, la dimensionnalit\xE9 est beaucoup plus faible. La t\xEAte du mod\xE8le prend en entr\xE9e les vecteurs de grande dimension que nous avons vus pr\xE9c\xE9demment et elle produit des vecteurs contenant deux valeurs (une par \xE9tiquette) :"),yr=d(),y(gs.$$.fragment),xr=d(),de.c(),tt=d(),nt=o("p"),Go=l("Comme nous n\u2019avons que deux phrases et deux \xE9tiquettes, le r\xE9sultat que nous obtenons est de forme 2 x 2"),Pr=d(),Se=o("h2"),Qe=o("a"),Nn=o("span"),y(ks.$$.fragment),Jo=d(),In=o("span"),Uo=l("Post-traitement de la sortie"),Tr=d(),rt=o("p"),Bo=l("Les valeurs que nous obtenons en sortie de notre mod\xE8le n\u2019ont pas n\xE9cessairement de sens en elles-m\xEAmes. Jetons-y un coup d\u2019oeil :"),Mr=d(),y(Es.$$.fragment),Cr=d(),he.c(),lt=d(),Q=o("p"),Ro=l("Notre mod\xE8le a pr\xE9dit "),Fn=o("code"),Qo=l("[-1.5607, 1.6123]"),Yo=l(" pour la premi\xE8re phrase et "),Sn=o("code"),Wo=l("[ 4.1692, -3.3464]"),Xo=l(" pour la seconde. Ce ne sont pas des probabilit\xE9s mais des "),Dn=o("em"),Ko=l("logits"),Zo=l(", les scores bruts, non normalis\xE9s, produits par la derni\xE8re couche du mod\xE8le. Pour \xEAtre convertis en probabilit\xE9s, ils doivent passer par une couche "),js=o("a"),ei=l("SoftMax"),si=l(" (tous les mod\xE8les de la biblioth\xE8que \u{1F917} Transformers sortent les logits car la fonction de perte de l\u2019entra\xEEnement fusionne g\xE9n\xE9ralement la derni\xE8re fonction d\u2019activation, comme la SoftMax, avec la fonction de perte r\xE9elle, comme l\u2019entropie crois\xE9e) :"),zr=d(),ve.c(),at=d(),$e.c(),ot=d(),ye=o("p"),ti=l("Maintenant nous pouvons voir que le mod\xE8le a pr\xE9dit "),Ln=o("code"),ni=l("[0.0402, 0.9598]"),ri=l(" pour la premi\xE8re phrase et "),On=o("code"),li=l("[0.9995, 0.0005]"),ai=l(" pour la seconde. Ce sont des scores de probabilit\xE9 reconnaissables."),Ar=d(),Ye=o("p"),oi=l("Pour obtenir les \xE9tiquettes correspondant \xE0 chaque position, nous pouvons inspecter l\u2019attribut "),Vn=o("code"),ii=l("id2label"),ui=l(" de la configuration du mod\xE8le (plus de d\xE9tails dans la section suivante) :"),Nr=d(),y(qs.$$.fragment),Ir=d(),y(ws.$$.fragment),Fr=d(),it=o("p"),pi=l("Nous pouvons maintenant conclure que le mod\xE8le a pr\xE9dit ce qui suit :"),Sr=d(),We=o("ul"),Hn=o("li"),ci=l("premi\xE8re phrase : NEGATIVE: 0.0402, POSITIVE: 0.9598"),mi=d(),Gn=o("li"),di=l("deuxi\xE8me phrase : NEGATIVE: 0.9995, POSITIVE: 0.0005"),Dr=d(),Xe=o("p"),fi=l("Nous avons reproduit avec succ\xE8s les trois \xE9tapes du pipeline : pr\xE9traitement avec les "),Jn=o("em"),hi=l("tokenizers"),_i=l(", passage des entr\xE9es dans le mod\xE8le et post-traitement ! Prenons maintenant le temps de nous plonger plus profond\xE9ment dans chacune de ces \xE9tapes."),Lr=d(),y(Ke.$$.fragment),this.h()},l(e){const p=Fp('[data-svelte="svelte-1phssyn"]',document.head);n=i(p,"META",{name:!0,content:!0}),p.forEach(t),c=f(e),x(r.$$.fragment,e),h=f(e),v=i(e,"H1",{class:!0});var Ss=u(v);_=i(Ss,"A",{id:!0,class:!0,href:!0});var ut=u(_);M=i(ut,"SPAN",{});var Un=u(M);x(A.$$.fragment,Un),Un.forEach(t),ut.forEach(t),z=f(Ss),C=i(Ss,"SPAN",{});var pt=u(C);F=a(pt,"Derri\xE8re le pipeline"),pt.forEach(t),Ss.forEach(t),k=f(e),I.l(e),D=f(e),x(S.$$.fragment,e),J=f(e),w.l(e),W=f(e),U=i(e,"P",{});var Ze=u(U);Me=a(Ze,"Commen\xE7ons par un exemple complet en regardant ce qui s\u2019est pass\xE9 en coulisses lorsque nous avons ex\xE9cut\xE9 le code suivant dans le "),X=i(Ze,"A",{href:!0});var ct=u(X);nl=a(ct,"Chapitre 1"),ct.forEach(t),rl=a(Ze," :"),Ze.forEach(t),Rn=f(e),x(ls.$$.fragment,e),Qn=f(e),Ds=i(e,"P",{});var mt=u(Ds);ll=a(mt,"la sortie :"),mt.forEach(t),Yn=f(e),x(as.$$.fragment,e),Wn=f(e),De=i(e,"P",{});var es=u(De);al=a(es,"Comme nous l\u2019avons vu dans le "),Ls=i(es,"A",{href:!0});var dt=u(Ls);ol=a(dt,"Chapitre 1"),dt.forEach(t),il=a(es,", ce pipeline regroupe trois \xE9tapes : le pr\xE9traitement, le passage des entr\xE9es dans le mod\xE8le et le post-traitement."),es.forEach(t),Xn=f(e),Ce=i(e,"DIV",{class:!0});var ss=u(Ce);os=i(ss,"IMG",{class:!0,src:!0,alt:!0}),ul=f(ss),is=i(ss,"IMG",{class:!0,src:!0,alt:!0}),ss.forEach(t),Kn=f(e),Os=i(e,"P",{});var ft=u(Os);pl=a(ft,"Passons rapidement en revue chacun de ces \xE9l\xE9ments."),ft.forEach(t),Zn=f(e),ze=i(e,"H2",{class:!0});var ts=u(ze);Le=i(ts,"A",{id:!0,class:!0,href:!0});var ht=u(Le);Et=i(ht,"SPAN",{});var Bn=u(Et);x(us.$$.fragment,Bn),Bn.forEach(t),ht.forEach(t),cl=f(ts),jt=i(ts,"SPAN",{});var Qi=u(jt);ml=a(Qi,"Pr\xE9traitement avec un *tokenizer*"),Qi.forEach(t),ts.forEach(t),er=f(e),Ee=i(e,"P",{});var _t=u(Ee);dl=a(_t,"Comme d\u2019autres r\xE9seaux de neurones, les "),qt=i(_t,"EM",{});var Yi=u(qt);fl=a(Yi,"transformers"),Yi.forEach(t),hl=a(_t," ne peuvent pas traiter directement le texte brut, donc la premi\xE8re \xE9tape de notre pipeline est de convertir les entr\xE9es textuelles en nombres afin que le mod\xE8le puisse les comprendre. Pour ce faire, nous utilisons un "),wt=i(_t,"EM",{});var Wi=u(wt);_l=a(Wi,"tokenizer"),Wi.forEach(t),vl=a(_t,", qui sera responsable de :"),_t.forEach(t),sr=f(e),je=i(e,"UL",{});var vt=u(je);ps=i(vt,"LI",{});var Vr=u(ps);bl=a(Vr,"diviser l\u2019entr\xE9e en mots, sous-mots, ou symboles (comme la ponctuation) qui sont appel\xE9s "),yt=i(Vr,"EM",{});var Xi=u(yt);$l=a(Xi,"tokens"),Xi.forEach(t),gl=a(Vr,","),Vr.forEach(t),kl=f(vt),cs=i(vt,"LI",{});var Hr=u(cs);El=a(Hr,"associer chaque "),xt=i(Hr,"EM",{});var Ki=u(xt);jl=a(Ki,"token"),Ki.forEach(t),ql=a(Hr," \xE0 un nombre entier,"),Hr.forEach(t),wl=f(vt),Pt=i(vt,"LI",{});var Zi=u(Pt);yl=a(Zi,"ajouter des entr\xE9es suppl\xE9mentaires qui peuvent \xEAtre utiles au mod\xE8le."),Zi.forEach(t),vt.forEach(t),tr=f(e),V=i(e,"P",{});var ee=u(V);xl=a(ee,"Tout ce pr\xE9traitement doit \xEAtre effectu\xE9 exactement de la m\xEAme mani\xE8re que celui appliqu\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le. Nous devons donc d\u2019abord t\xE9l\xE9charger ces informations depuis le "),ms=i(ee,"A",{href:!0,rel:!0});var eu=u(ms);Tt=i(eu,"EM",{});var su=u(Tt);Pl=a(su,"Model Hub"),su.forEach(t),eu.forEach(t),Tl=a(ee,". Pour ce faire, nous utilisons la classe "),Mt=i(ee,"CODE",{});var tu=u(Mt);Ml=a(tu,"AutoTokenizer"),tu.forEach(t),Cl=a(ee," et sa m\xE9thode "),Ct=i(ee,"CODE",{});var nu=u(Ct);zl=a(nu,"from_pretrained()"),nu.forEach(t),Al=a(ee,". En utilisant le nom du "),zt=i(ee,"EM",{});var ru=u(zt);Nl=a(ru,"checkpoint"),ru.forEach(t),Il=a(ee," de notre mod\xE8le, elle va automatiquement r\xE9cup\xE9rer les donn\xE9es associ\xE9es au "),At=i(ee,"EM",{});var lu=u(At);Fl=a(lu,"tokenizer"),lu.forEach(t),Sl=a(ee," du mod\xE8le et les mettre en cache (afin qu\u2019elles ne soient t\xE9l\xE9charg\xE9es que la premi\xE8re fois que vous ex\xE9cutez le code ci-dessous)."),ee.forEach(t),nr=f(e),B=i(e,"P",{});var xe=u(B);Dl=a(xe,"Puisque le "),Nt=i(xe,"EM",{});var au=u(Nt);Ll=a(au,"checkpoint"),au.forEach(t),Ol=a(xe," par d\xE9faut du pipeline "),It=i(xe,"CODE",{});var ou=u(It);Vl=a(ou,"sentiment-analysis"),ou.forEach(t),Hl=a(xe," (analyse de sentiment) est "),Ft=i(xe,"CODE",{});var iu=u(Ft);Gl=a(iu,"distilbert-base-uncased-finetuned-sst-2-english"),iu.forEach(t),Jl=a(xe," (vous pouvez voir la carte de ce mod\xE8le "),ds=i(xe,"A",{href:!0,rel:!0});var uu=u(ds);Ul=a(uu,"ici"),uu.forEach(t),Bl=a(xe,"), nous ex\xE9cutons ce qui suit :"),xe.forEach(t),rr=f(e),x(fs.$$.fragment,e),lr=f(e),Oe=i(e,"P",{});var Gr=u(Oe);Rl=a(Gr,"Une fois que nous avons le "),St=i(Gr,"EM",{});var pu=u(St);Ql=a(pu,"tokenizer"),pu.forEach(t),Yl=a(Gr," nous pouvons lui passer directement nos phrases et obtenir un dictionnaire pr\xEAt \xE0 \xEAtre donn\xE9 \xE0 notre mod\xE8le ! La seule chose qui reste \xE0 faire est de convertir en tenseurs la liste des identifiants d\u2019entr\xE9e."),Gr.forEach(t),ar=f(e),O=i(e,"P",{});var Y=u(O);Wl=a(Y,"Vous pouvez utiliser \u{1F917} "),Dt=i(Y,"EM",{});var cu=u(Dt);Xl=a(cu,"Transformers"),cu.forEach(t),Kl=a(Y," sans avoir \xE0 vous soucier du "),Lt=i(Y,"EM",{});var mu=u(Lt);Zl=a(mu,"framework"),mu.forEach(t),ea=a(Y," utilis\xE9 comme "),Ot=i(Y,"EM",{});var du=u(Ot);sa=a(du,"backend"),du.forEach(t),ta=a(Y,". Il peut s\u2019agir de PyTorch, de TensorFlow ou de Flax pour certains mod\xE8les. Cependant, les "),Vt=i(Y,"EM",{});var fu=u(Vt);na=a(fu,"transformers"),fu.forEach(t),ra=a(Y," n\u2019acceptent que les "),Ht=i(Y,"EM",{});var hu=u(Ht);la=a(hu,"tenseurs"),hu.forEach(t),aa=a(Y," en entr\xE9e. Si c\u2019est la premi\xE8re fois que vous entendez parler de tenseurs, vous pouvez les consid\xE9rer comme des tableaux NumPy. Un tableau NumPy peut \xEAtre un scalaire (0D), un vecteur (1D), une matrice (2D), ou avoir davantage de dimensions. Les tenseurs des autres "),Gt=i(Y,"EM",{});var _u=u(Gt);oa=a(_u,"frameworks"),_u.forEach(t),ia=a(Y," d\u2019apprentissage machine se comportent de mani\xE8re similaire et sont g\xE9n\xE9ralement aussi simples \xE0 instancier que les tableaux NumPy."),Y.forEach(t),or=f(e),Ve=i(e,"P",{});var Jr=u(Ve);ua=a(Jr,"Pour sp\xE9cifier le type de tenseurs que nous voulons r\xE9cup\xE9rer (PyTorch, TensorFlow, ou simplement NumPy), nous utilisons l\u2019argument "),Jt=i(Jr,"CODE",{});var vu=u(Jt);pa=a(vu,"return_tensors"),vu.forEach(t),ca=a(Jr," :"),Jr.forEach(t),ir=f(e),ne.l(e),Vs=f(e),He=i(e,"P",{});var Ur=u(He);ma=a(Ur,"Ne vous pr\xE9occupez pas encore du remplissage ("),Ut=i(Ur,"EM",{});var bu=u(Ut);da=a(bu,"padding"),bu.forEach(t),fa=a(Ur,") et de la troncature, nous les expliquerons plus tard. Les principales choses \xE0 retenir ici sont que vous pouvez passer une phrase ou une liste de phrases, ainsi que sp\xE9cifier le type de tenseurs que vous voulez r\xE9cup\xE9rer (si aucun type n\u2019est pass\xE9, par d\xE9faut vous obtiendrez une liste de listes comme r\xE9sultat)."),Ur.forEach(t),ur=f(e),le.l(e),Hs=f(e),H=i(e,"P",{});var se=u(H);ha=a(se,"La sortie elle-m\xEAme est un dictionnaire contenant deux cl\xE9s : "),Bt=i(se,"CODE",{});var $u=u(Bt);_a=a($u,"input_ids"),$u.forEach(t),va=a(se," et "),Rt=i(se,"CODE",{});var gu=u(Rt);ba=a(gu,"attention_mask"),gu.forEach(t),$a=a(se,". "),Qt=i(se,"CODE",{});var ku=u(Qt);ga=a(ku,"input_ids"),ku.forEach(t),ka=a(se," contient deux lignes d\u2019entiers (une pour chaque phrase) qui sont les identifiants uniques des "),Yt=i(se,"EM",{});var Eu=u(Yt);Ea=a(Eu,"tokens"),Eu.forEach(t),ja=a(se," dans chaque phrase. Nous expliquerons ce qu\u2019est l\u2019"),Wt=i(se,"CODE",{});var ju=u(Wt);qa=a(ju,"attention_mask"),ju.forEach(t),wa=a(se," plus tard dans ce chapitre."),se.forEach(t),pr=f(e),Ae=i(e,"H2",{class:!0});var Br=u(Ae);Ge=i(Br,"A",{id:!0,class:!0,href:!0});var qu=u(Ge);Xt=i(qu,"SPAN",{});var wu=u(Xt);x(hs.$$.fragment,wu),wu.forEach(t),qu.forEach(t),ya=f(Br),Kt=i(Br,"SPAN",{});var yu=u(Kt);xa=a(yu,"Passage au mod\xE8le"),yu.forEach(t),Br.forEach(t),cr=f(e),oe.l(e),Gs=f(e),Je=i(e,"P",{});var Rr=u(Je);Pa=a(Rr,"Dans cet extrait de code, nous avons t\xE9l\xE9charg\xE9 le m\xEAme "),Zt=i(Rr,"EM",{});var xu=u(Zt);Ta=a(xu,"checkpoint"),xu.forEach(t),Ma=a(Rr," que nous avons utilis\xE9 dans notre pipeline auparavant (il devrait en fait avoir d\xE9j\xE0 \xE9t\xE9 mis en cache) et instanci\xE9 un mod\xE8le avec lui."),Rr.forEach(t),mr=f(e),R=i(e,"P",{});var Pe=u(R);Ca=a(Pe,"Cette architecture ne contient que le module de "),en=i(Pe,"EM",{});var Pu=u(en);za=a(Pu,"transformer"),Pu.forEach(t),Aa=a(Pe," de base : \xE9tant donn\xE9 certaines entr\xE9es, il produit ce que nous appellerons des "),sn=i(Pe,"EM",{});var Tu=u(sn);Na=a(Tu,"\xE9tats cach\xE9s"),Tu.forEach(t),Ia=a(Pe,", \xE9galement connus sous le nom de "),tn=i(Pe,"EM",{});var Mu=u(tn);Fa=a(Mu,"caract\xE9ristiques"),Mu.forEach(t),Sa=a(Pe,`.
Pour chaque entr\xE9e du mod\xE8le, nous r\xE9cup\xE9rons un vecteur en grande dimension repr\xE9sentant la `),Js=i(Pe,"STRONG",{});var vi=u(Js);Da=a(vi,"compr\xE9hension contextuelle de cette entr\xE9e par le "),nn=i(vi,"EM",{});var Cu=u(nn);La=a(Cu,"transformer"),Cu.forEach(t),vi.forEach(t),Oa=a(Pe,"."),Pe.forEach(t),dr=f(e),Us=i(e,"P",{});var zu=u(Us);Va=a(zu,"Si cela ne fait pas sens, ne vous inqui\xE9tez pas. Nous expliquons tout plus tard."),zu.forEach(t),fr=f(e),qe=i(e,"P",{});var bt=u(qe);Ha=a(bt,"Bien que ces \xE9tats cach\xE9s puissent \xEAtre utiles en eux-m\xEAmes, ils sont g\xE9n\xE9ralement les entr\xE9es d\u2019une autre partie du mod\xE8le, connue sous le nom de "),rn=i(bt,"EM",{});var Au=u(rn);Ga=a(Au,"t\xEAte"),Au.forEach(t),Ja=a(bt,". Dans le "),Bs=i(bt,"A",{href:!0});var Nu=u(Bs);Ua=a(Nu,"Chapitre 1"),Nu.forEach(t),Ba=a(bt,", les diff\xE9rentes t\xE2ches auraient pu \xEAtre r\xE9alis\xE9es avec la m\xEAme architecture mais en ayant chacune d\u2019elles une t\xEAte diff\xE9rente."),bt.forEach(t),hr=f(e),Ne=i(e,"H3",{class:!0});var Qr=u(Ne);Ue=i(Qr,"A",{id:!0,class:!0,href:!0});var Iu=u(Ue);ln=i(Iu,"SPAN",{});var Fu=u(ln);x(_s.$$.fragment,Fu),Fu.forEach(t),Iu.forEach(t),Ra=f(Qr),an=i(Qr,"SPAN",{});var Su=u(an);Qa=a(Su,"Un vecteur de grande dimension ?"),Su.forEach(t),Qr.forEach(t),_r=f(e),Be=i(e,"P",{});var Yr=u(Be);Ya=a(Yr,"Le vecteur produit en sortie par le "),on=i(Yr,"EM",{});var Du=u(on);Wa=a(Du,"transformer"),Du.forEach(t),Xa=a(Yr," est g\xE9n\xE9ralement de grande dimension. Il a g\xE9n\xE9ralement trois dimensions :"),Yr.forEach(t),vr=f(e),we=i(e,"UL",{});var $t=u(we);Rs=i($t,"LI",{});var bi=u(Rs);un=i(bi,"STRONG",{});var Lu=u(un);Ka=a(Lu,"la taille du lot"),Lu.forEach(t),Za=a(bi," : le nombre de s\xE9quences trait\xE9es \xE0 la fois (2 dans notre exemple),"),bi.forEach(t),eo=f($t),Qs=i($t,"LI",{});var $i=u(Qs);pn=i($i,"STRONG",{});var Ou=u(pn);so=a(Ou,"la longueur de la s\xE9quence"),Ou.forEach(t),to=a($i," : la longueur de la repr\xE9sentation num\xE9rique de la s\xE9quence (16 dans notre exemple),"),$i.forEach(t),no=f($t),Ys=i($t,"LI",{});var gi=u(Ys);cn=i(gi,"STRONG",{});var Vu=u(cn);ro=a(Vu,"la taille cach\xE9e"),Vu.forEach(t),lo=a(gi," : la dimension du vecteur de chaque entr\xE9e du mod\xE8le."),gi.forEach(t),$t.forEach(t),br=f(e),Ws=i(e,"P",{});var Hu=u(Ws);ao=a(Hu,"On dit qu\u2019il est de \xAB grande dimension \xBB en raison de la derni\xE8re valeur. La taille cach\xE9e peut \xEAtre tr\xE8s grande (g\xE9n\xE9ralement 768 pour les petits mod\xE8les et pour les grands mod\xE8les cela peut atteindre 3072 voire plus)."),Hu.forEach(t),$r=f(e),Xs=i(e,"P",{});var Gu=u(Xs);oo=a(Gu,"Nous pouvons le constater si nous alimentons notre mod\xE8le avec les entr\xE9es que nous avons pr\xE9trait\xE9es :"),Gu.forEach(t),gr=f(e),ue.l(e),Ks=f(e),K=i(e,"P",{});var ns=u(K);io=a(ns,"Notez que les sorties des mod\xE8les de la biblioth\xE8que \u{1F917} Transformers se comportent comme des "),mn=i(ns,"CODE",{});var Ju=u(mn);uo=a(Ju,"namedtuples"),Ju.forEach(t),po=a(ns," ou des dictionnaires. Vous pouvez acc\xE9der aux \xE9l\xE9ments par attributs (comme nous l\u2019avons fait), par cl\xE9 ("),dn=i(ns,"CODE",{});var Uu=u(dn);co=a(Uu,'outputs["last_hidden_state"]'),Uu.forEach(t),mo=a(ns,"), ou m\xEAme par l\u2019index si vous savez exactement o\xF9 se trouve la chose que vous cherchez ("),fn=i(ns,"CODE",{});var Bu=u(fn);fo=a(Bu,"outputs[0]"),Bu.forEach(t),ho=a(ns,")."),ns.forEach(t),kr=f(e),Ie=i(e,"H3",{class:!0});var Wr=u(Ie);Re=i(Wr,"A",{id:!0,class:!0,href:!0});var Ru=u(Re);hn=i(Ru,"SPAN",{});var Qu=u(hn);x(vs.$$.fragment,Qu),Qu.forEach(t),Ru.forEach(t),_o=f(Wr),_n=i(Wr,"SPAN",{});var Yu=u(_n);vo=a(Yu,"Les t\xEAtes des mod\xE8les : donner du sens aux chiffres"),Yu.forEach(t),Wr.forEach(t),Er=a(e,`

Les t\xEAtes des mod\xE8les prennent en entr\xE9e le vecteur de grande dimension des \xE9tats cach\xE9s et le projettent sur une autre dimension. Elles sont g\xE9n\xE9ralement compos\xE9es d'une ou de quelques couches lin\xE9aires :
`),Fe=i(e,"DIV",{class:!0});var Xr=u(Fe);bs=i(Xr,"IMG",{class:!0,src:!0,alt:!0}),bo=f(Xr),$s=i(Xr,"IMG",{class:!0,src:!0,alt:!0}),Xr.forEach(t),jr=f(e),Z=i(e,"P",{});var rs=u(Z);$o=a(rs,"La sortie du "),vn=i(rs,"EM",{});var Wu=u(vn);go=a(Wu,"transformer"),Wu.forEach(t),ko=a(rs,` est envoy\xE9e directement \xE0 la t\xEAte du mod\xE8le pour \xEAtre trait\xE9e.
Dans ce diagramme, le mod\xE8le est repr\xE9sent\xE9 par sa couche d\u2019ench\xE2ssement et les couches suivantes. La couche d\u2019ench\xE2ssement convertit chaque identifiant d\u2019entr\xE9e dans l\u2019entr\xE9e tokenis\xE9e en un vecteur qui repr\xE9sente le `),bn=i(rs,"EM",{});var Xu=u(bn);Eo=a(Xu,"token"),Xu.forEach(t),jo=a(rs,` associ\xE9. Les couches suivantes manipulent ces vecteurs en utilisant le m\xE9canisme d\u2019attention pour produire la repr\xE9sentation finale des phrases.
Il existe de nombreuses architectures diff\xE9rentes disponibles dans la biblioth\xE8que \u{1F917} `),$n=i(rs,"EM",{});var Ku=u($n);qo=a(Ku,"Transformers"),Ku.forEach(t),wo=a(rs,", chacune \xE9tant con\xE7ue autour de la prise en charge d\u2019une t\xE2che sp\xE9cifique. En voici une liste non exhaustive :"),rs.forEach(t),qr=f(e),L=i(e,"UL",{});var G=u(L);Zs=i(G,"LI",{});var ki=u(Zs);gn=i(ki,"CODE",{});var Zu=u(gn);yo=a(Zu,"*Model"),Zu.forEach(t),xo=a(ki," (r\xE9cup\xE9rer les \xE9tats cach\xE9s)"),ki.forEach(t),Po=f(G),kn=i(G,"LI",{});var ep=u(kn);En=i(ep,"CODE",{});var sp=u(En);To=a(sp,"*ForCausalLM"),sp.forEach(t),ep.forEach(t),Mo=f(G),jn=i(G,"LI",{});var tp=u(jn);qn=i(tp,"CODE",{});var np=u(qn);Co=a(np,"*ForMaskedLM"),np.forEach(t),tp.forEach(t),zo=f(G),wn=i(G,"LI",{});var rp=u(wn);yn=i(rp,"CODE",{});var lp=u(yn);Ao=a(lp,"*ForMultipleChoice"),lp.forEach(t),rp.forEach(t),No=f(G),xn=i(G,"LI",{});var ap=u(xn);Pn=i(ap,"CODE",{});var op=u(Pn);Io=a(op,"*ForQuestionAnswering"),op.forEach(t),ap.forEach(t),Fo=f(G),Tn=i(G,"LI",{});var ip=u(Tn);Mn=i(ip,"CODE",{});var up=u(Mn);So=a(up,"*ForSequenceClassification"),up.forEach(t),ip.forEach(t),Do=f(G),Cn=i(G,"LI",{});var pp=u(Cn);zn=i(pp,"CODE",{});var cp=u(zn);Lo=a(cp,"*ForTokenClassification"),cp.forEach(t),pp.forEach(t),Oo=f(G),An=i(G,"LI",{});var mp=u(An);Vo=a(mp,"et autres \u{1F917}"),mp.forEach(t),G.forEach(t),wr=f(e),ce.l(e),et=f(e),st=i(e,"P",{});var dp=u(st);Ho=a(dp,"Maintenant, si nous examinons la forme de nos entr\xE9es, la dimensionnalit\xE9 est beaucoup plus faible. La t\xEAte du mod\xE8le prend en entr\xE9e les vecteurs de grande dimension que nous avons vus pr\xE9c\xE9demment et elle produit des vecteurs contenant deux valeurs (une par \xE9tiquette) :"),dp.forEach(t),yr=f(e),x(gs.$$.fragment,e),xr=f(e),de.l(e),tt=f(e),nt=i(e,"P",{});var fp=u(nt);Go=a(fp,"Comme nous n\u2019avons que deux phrases et deux \xE9tiquettes, le r\xE9sultat que nous obtenons est de forme 2 x 2"),fp.forEach(t),Pr=f(e),Se=i(e,"H2",{class:!0});var Kr=u(Se);Qe=i(Kr,"A",{id:!0,class:!0,href:!0});var hp=u(Qe);Nn=i(hp,"SPAN",{});var _p=u(Nn);x(ks.$$.fragment,_p),_p.forEach(t),hp.forEach(t),Jo=f(Kr),In=i(Kr,"SPAN",{});var vp=u(In);Uo=a(vp,"Post-traitement de la sortie"),vp.forEach(t),Kr.forEach(t),Tr=f(e),rt=i(e,"P",{});var bp=u(rt);Bo=a(bp,"Les valeurs que nous obtenons en sortie de notre mod\xE8le n\u2019ont pas n\xE9cessairement de sens en elles-m\xEAmes. Jetons-y un coup d\u2019oeil :"),bp.forEach(t),Mr=f(e),x(Es.$$.fragment,e),Cr=f(e),he.l(e),lt=f(e),Q=i(e,"P",{});var Te=u(Q);Ro=a(Te,"Notre mod\xE8le a pr\xE9dit "),Fn=i(Te,"CODE",{});var $p=u(Fn);Qo=a($p,"[-1.5607, 1.6123]"),$p.forEach(t),Yo=a(Te," pour la premi\xE8re phrase et "),Sn=i(Te,"CODE",{});var gp=u(Sn);Wo=a(gp,"[ 4.1692, -3.3464]"),gp.forEach(t),Xo=a(Te," pour la seconde. Ce ne sont pas des probabilit\xE9s mais des "),Dn=i(Te,"EM",{});var kp=u(Dn);Ko=a(kp,"logits"),kp.forEach(t),Zo=a(Te,", les scores bruts, non normalis\xE9s, produits par la derni\xE8re couche du mod\xE8le. Pour \xEAtre convertis en probabilit\xE9s, ils doivent passer par une couche "),js=i(Te,"A",{href:!0,rel:!0});var Ep=u(js);ei=a(Ep,"SoftMax"),Ep.forEach(t),si=a(Te," (tous les mod\xE8les de la biblioth\xE8que \u{1F917} Transformers sortent les logits car la fonction de perte de l\u2019entra\xEEnement fusionne g\xE9n\xE9ralement la derni\xE8re fonction d\u2019activation, comme la SoftMax, avec la fonction de perte r\xE9elle, comme l\u2019entropie crois\xE9e) :"),Te.forEach(t),zr=f(e),ve.l(e),at=f(e),$e.l(e),ot=f(e),ye=i(e,"P",{});var gt=u(ye);ti=a(gt,"Maintenant nous pouvons voir que le mod\xE8le a pr\xE9dit "),Ln=i(gt,"CODE",{});var jp=u(Ln);ni=a(jp,"[0.0402, 0.9598]"),jp.forEach(t),ri=a(gt," pour la premi\xE8re phrase et "),On=i(gt,"CODE",{});var qp=u(On);li=a(qp,"[0.9995, 0.0005]"),qp.forEach(t),ai=a(gt," pour la seconde. Ce sont des scores de probabilit\xE9 reconnaissables."),gt.forEach(t),Ar=f(e),Ye=i(e,"P",{});var Zr=u(Ye);oi=a(Zr,"Pour obtenir les \xE9tiquettes correspondant \xE0 chaque position, nous pouvons inspecter l\u2019attribut "),Vn=i(Zr,"CODE",{});var wp=u(Vn);ii=a(wp,"id2label"),wp.forEach(t),ui=a(Zr," de la configuration du mod\xE8le (plus de d\xE9tails dans la section suivante) :"),Zr.forEach(t),Nr=f(e),x(qs.$$.fragment,e),Ir=f(e),x(ws.$$.fragment,e),Fr=f(e),it=i(e,"P",{});var yp=u(it);pi=a(yp,"Nous pouvons maintenant conclure que le mod\xE8le a pr\xE9dit ce qui suit :"),yp.forEach(t),Sr=f(e),We=i(e,"UL",{});var el=u(We);Hn=i(el,"LI",{});var xp=u(Hn);ci=a(xp,"premi\xE8re phrase : NEGATIVE: 0.0402, POSITIVE: 0.9598"),xp.forEach(t),mi=f(el),Gn=i(el,"LI",{});var Pp=u(Gn);di=a(Pp,"deuxi\xE8me phrase : NEGATIVE: 0.9995, POSITIVE: 0.0005"),Pp.forEach(t),el.forEach(t),Dr=f(e),Xe=i(e,"P",{});var sl=u(Xe);fi=a(sl,"Nous avons reproduit avec succ\xE8s les trois \xE9tapes du pipeline : pr\xE9traitement avec les "),Jn=i(sl,"EM",{});var Tp=u(Jn);hi=a(Tp,"tokenizers"),Tp.forEach(t),_i=a(sl,", passage des entr\xE9es dans le mod\xE8le et post-traitement ! Prenons maintenant le temps de nous plonger plus profond\xE9ment dans chacune de ces \xE9tapes."),sl.forEach(t),Lr=f(e),x(Ke.$$.fragment,e),this.h()},h(){E(n,"name","hf:doc:metadata"),E(n,"content",JSON.stringify(pc)),E(_,"id","derrire-le-pipeline"),E(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(_,"href","#derrire-le-pipeline"),E(v,"class","relative group"),E(X,"href","/course/chapter1"),E(Ls,"href","/course/fr/chapter1"),E(os,"class","block dark:hidden"),tl(os.src,Ei="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg")||E(os,"src",Ei),E(os,"alt","The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."),E(is,"class","hidden dark:block"),tl(is.src,ji="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg")||E(is,"src",ji),E(is,"alt","The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."),E(Ce,"class","flex justify-center"),E(Le,"id","prtraitement-avec-un-tokenizer"),E(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Le,"href","#prtraitement-avec-un-tokenizer"),E(ze,"class","relative group"),E(ms,"href","https://huggingface.co/models"),E(ms,"rel","nofollow"),E(ds,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),E(ds,"rel","nofollow"),E(Ge,"id","passage-au-modle"),E(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Ge,"href","#passage-au-modle"),E(Ae,"class","relative group"),E(Bs,"href","/course/fr/chapter1"),E(Ue,"id","un-vecteur-de-grande-dimension"),E(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Ue,"href","#un-vecteur-de-grande-dimension"),E(Ne,"class","relative group"),E(Re,"id","les-ttes-des-modles-donner-du-sens-aux-chiffres"),E(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Re,"href","#les-ttes-des-modles-donner-du-sens-aux-chiffres"),E(Ie,"class","relative group"),E(bs,"class","block dark:hidden"),tl(bs.src,qi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg")||E(bs,"src",qi),E(bs,"alt","A Transformer network alongside its head."),E($s,"class","hidden dark:block"),tl($s.src,wi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg")||E($s,"src",wi),E($s,"alt","A Transformer network alongside its head."),E(Fe,"class","flex justify-center"),E(Qe,"id","posttraitement-de-la-sortie"),E(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Qe,"href","#posttraitement-de-la-sortie"),E(Se,"class","relative group"),E(js,"href","https://fr.wikipedia.org/wiki/Fonction_softmax"),E(js,"rel","nofollow")},m(e,p){s(document.head,n),m(e,c,p),P(r,e,p),m(e,h,p),m(e,v,p),s(v,_),s(_,M),P(A,M,null),s(v,z),s(v,C),s(C,F),m(e,k,p),ys[q].m(e,p),m(e,D,p),P(S,e,p),m(e,J,p),xs[j].m(e,p),m(e,W,p),m(e,U,p),s(U,Me),s(U,X),s(X,nl),s(U,rl),m(e,Rn,p),P(ls,e,p),m(e,Qn,p),m(e,Ds,p),s(Ds,ll),m(e,Yn,p),P(as,e,p),m(e,Wn,p),m(e,De,p),s(De,al),s(De,Ls),s(Ls,ol),s(De,il),m(e,Xn,p),m(e,Ce,p),s(Ce,os),s(Ce,ul),s(Ce,is),m(e,Kn,p),m(e,Os,p),s(Os,pl),m(e,Zn,p),m(e,ze,p),s(ze,Le),s(Le,Et),P(us,Et,null),s(ze,cl),s(ze,jt),s(jt,ml),m(e,er,p),m(e,Ee,p),s(Ee,dl),s(Ee,qt),s(qt,fl),s(Ee,hl),s(Ee,wt),s(wt,_l),s(Ee,vl),m(e,sr,p),m(e,je,p),s(je,ps),s(ps,bl),s(ps,yt),s(yt,$l),s(ps,gl),s(je,kl),s(je,cs),s(cs,El),s(cs,xt),s(xt,jl),s(cs,ql),s(je,wl),s(je,Pt),s(Pt,yl),m(e,tr,p),m(e,V,p),s(V,xl),s(V,ms),s(ms,Tt),s(Tt,Pl),s(V,Tl),s(V,Mt),s(Mt,Ml),s(V,Cl),s(V,Ct),s(Ct,zl),s(V,Al),s(V,zt),s(zt,Nl),s(V,Il),s(V,At),s(At,Fl),s(V,Sl),m(e,nr,p),m(e,B,p),s(B,Dl),s(B,Nt),s(Nt,Ll),s(B,Ol),s(B,It),s(It,Vl),s(B,Hl),s(B,Ft),s(Ft,Gl),s(B,Jl),s(B,ds),s(ds,Ul),s(B,Bl),m(e,rr,p),P(fs,e,p),m(e,lr,p),m(e,Oe,p),s(Oe,Rl),s(Oe,St),s(St,Ql),s(Oe,Yl),m(e,ar,p),m(e,O,p),s(O,Wl),s(O,Dt),s(Dt,Xl),s(O,Kl),s(O,Lt),s(Lt,Zl),s(O,ea),s(O,Ot),s(Ot,sa),s(O,ta),s(O,Vt),s(Vt,na),s(O,ra),s(O,Ht),s(Ht,la),s(O,aa),s(O,Gt),s(Gt,oa),s(O,ia),m(e,or,p),m(e,Ve,p),s(Ve,ua),s(Ve,Jt),s(Jt,pa),s(Ve,ca),m(e,ir,p),Ps[te].m(e,p),m(e,Vs,p),m(e,He,p),s(He,ma),s(He,Ut),s(Ut,da),s(He,fa),m(e,ur,p),Ts[re].m(e,p),m(e,Hs,p),m(e,H,p),s(H,ha),s(H,Bt),s(Bt,_a),s(H,va),s(H,Rt),s(Rt,ba),s(H,$a),s(H,Qt),s(Qt,ga),s(H,ka),s(H,Yt),s(Yt,Ea),s(H,ja),s(H,Wt),s(Wt,qa),s(H,wa),m(e,pr,p),m(e,Ae,p),s(Ae,Ge),s(Ge,Xt),P(hs,Xt,null),s(Ae,ya),s(Ae,Kt),s(Kt,xa),m(e,cr,p),Ms[ae].m(e,p),m(e,Gs,p),m(e,Je,p),s(Je,Pa),s(Je,Zt),s(Zt,Ta),s(Je,Ma),m(e,mr,p),m(e,R,p),s(R,Ca),s(R,en),s(en,za),s(R,Aa),s(R,sn),s(sn,Na),s(R,Ia),s(R,tn),s(tn,Fa),s(R,Sa),s(R,Js),s(Js,Da),s(Js,nn),s(nn,La),s(R,Oa),m(e,dr,p),m(e,Us,p),s(Us,Va),m(e,fr,p),m(e,qe,p),s(qe,Ha),s(qe,rn),s(rn,Ga),s(qe,Ja),s(qe,Bs),s(Bs,Ua),s(qe,Ba),m(e,hr,p),m(e,Ne,p),s(Ne,Ue),s(Ue,ln),P(_s,ln,null),s(Ne,Ra),s(Ne,an),s(an,Qa),m(e,_r,p),m(e,Be,p),s(Be,Ya),s(Be,on),s(on,Wa),s(Be,Xa),m(e,vr,p),m(e,we,p),s(we,Rs),s(Rs,un),s(un,Ka),s(Rs,Za),s(we,eo),s(we,Qs),s(Qs,pn),s(pn,so),s(Qs,to),s(we,no),s(we,Ys),s(Ys,cn),s(cn,ro),s(Ys,lo),m(e,br,p),m(e,Ws,p),s(Ws,ao),m(e,$r,p),m(e,Xs,p),s(Xs,oo),m(e,gr,p),Cs[ie].m(e,p),m(e,Ks,p),m(e,K,p),s(K,io),s(K,mn),s(mn,uo),s(K,po),s(K,dn),s(dn,co),s(K,mo),s(K,fn),s(fn,fo),s(K,ho),m(e,kr,p),m(e,Ie,p),s(Ie,Re),s(Re,hn),P(vs,hn,null),s(Ie,_o),s(Ie,_n),s(_n,vo),m(e,Er,p),m(e,Fe,p),s(Fe,bs),s(Fe,bo),s(Fe,$s),m(e,jr,p),m(e,Z,p),s(Z,$o),s(Z,vn),s(vn,go),s(Z,ko),s(Z,bn),s(bn,Eo),s(Z,jo),s(Z,$n),s($n,qo),s(Z,wo),m(e,qr,p),m(e,L,p),s(L,Zs),s(Zs,gn),s(gn,yo),s(Zs,xo),s(L,Po),s(L,kn),s(kn,En),s(En,To),s(L,Mo),s(L,jn),s(jn,qn),s(qn,Co),s(L,zo),s(L,wn),s(wn,yn),s(yn,Ao),s(L,No),s(L,xn),s(xn,Pn),s(Pn,Io),s(L,Fo),s(L,Tn),s(Tn,Mn),s(Mn,So),s(L,Do),s(L,Cn),s(Cn,zn),s(zn,Lo),s(L,Oo),s(L,An),s(An,Vo),m(e,wr,p),zs[pe].m(e,p),m(e,et,p),m(e,st,p),s(st,Ho),m(e,yr,p),P(gs,e,p),m(e,xr,p),As[me].m(e,p),m(e,tt,p),m(e,nt,p),s(nt,Go),m(e,Pr,p),m(e,Se,p),s(Se,Qe),s(Qe,Nn),P(ks,Nn,null),s(Se,Jo),s(Se,In),s(In,Uo),m(e,Tr,p),m(e,rt,p),s(rt,Bo),m(e,Mr,p),P(Es,e,p),m(e,Cr,p),Ns[fe].m(e,p),m(e,lt,p),m(e,Q,p),s(Q,Ro),s(Q,Fn),s(Fn,Qo),s(Q,Yo),s(Q,Sn),s(Sn,Wo),s(Q,Xo),s(Q,Dn),s(Dn,Ko),s(Q,Zo),s(Q,js),s(js,ei),s(Q,si),m(e,zr,p),Is[_e].m(e,p),m(e,at,p),Fs[be].m(e,p),m(e,ot,p),m(e,ye,p),s(ye,ti),s(ye,Ln),s(Ln,ni),s(ye,ri),s(ye,On),s(On,li),s(ye,ai),m(e,Ar,p),m(e,Ye,p),s(Ye,oi),s(Ye,Vn),s(Vn,ii),s(Ye,ui),m(e,Nr,p),P(qs,e,p),m(e,Ir,p),P(ws,e,p),m(e,Fr,p),m(e,it,p),s(it,pi),m(e,Sr,p),m(e,We,p),s(We,Hn),s(Hn,ci),s(We,mi),s(We,Gn),s(Gn,di),m(e,Dr,p),m(e,Xe,p),s(Xe,fi),s(Xe,Jn),s(Jn,hi),s(Xe,_i),m(e,Lr,p),P(Ke,e,p),Or=!0},p(e,[p]){const Ss={};p&1&&(Ss.fw=e[0]),r.$set(Ss);let ut=q;q=xi(e),q!==ut&&(ke(),b(ys[ut],1,1,()=>{ys[ut]=null}),ge(),I=ys[q],I||(I=ys[q]=yi[q](e),I.c()),$(I,1),I.m(D.parentNode,D));const Un={};p&2&&(Un.$$scope={dirty:p,ctx:e}),S.$set(Un);let pt=j;j=Ti(e),j!==pt&&(ke(),b(xs[pt],1,1,()=>{xs[pt]=null}),ge(),w=xs[j],w||(w=xs[j]=Pi[j](e),w.c()),$(w,1),w.m(W.parentNode,W));let Ze=te;te=Ci(e),te!==Ze&&(ke(),b(Ps[Ze],1,1,()=>{Ps[Ze]=null}),ge(),ne=Ps[te],ne||(ne=Ps[te]=Mi[te](e),ne.c()),$(ne,1),ne.m(Vs.parentNode,Vs));let ct=re;re=Ai(e),re!==ct&&(ke(),b(Ts[ct],1,1,()=>{Ts[ct]=null}),ge(),le=Ts[re],le||(le=Ts[re]=zi[re](e),le.c()),$(le,1),le.m(Hs.parentNode,Hs));let mt=ae;ae=Ii(e),ae!==mt&&(ke(),b(Ms[mt],1,1,()=>{Ms[mt]=null}),ge(),oe=Ms[ae],oe||(oe=Ms[ae]=Ni[ae](e),oe.c()),$(oe,1),oe.m(Gs.parentNode,Gs));let es=ie;ie=Si(e),ie!==es&&(ke(),b(Cs[es],1,1,()=>{Cs[es]=null}),ge(),ue=Cs[ie],ue||(ue=Cs[ie]=Fi[ie](e),ue.c()),$(ue,1),ue.m(Ks.parentNode,Ks));let dt=pe;pe=Li(e),pe!==dt&&(ke(),b(zs[dt],1,1,()=>{zs[dt]=null}),ge(),ce=zs[pe],ce||(ce=zs[pe]=Di[pe](e),ce.c()),$(ce,1),ce.m(et.parentNode,et));let ss=me;me=Vi(e),me!==ss&&(ke(),b(As[ss],1,1,()=>{As[ss]=null}),ge(),de=As[me],de||(de=As[me]=Oi[me](e),de.c()),$(de,1),de.m(tt.parentNode,tt));let ft=fe;fe=Gi(e),fe!==ft&&(ke(),b(Ns[ft],1,1,()=>{Ns[ft]=null}),ge(),he=Ns[fe],he||(he=Ns[fe]=Hi[fe](e),he.c()),$(he,1),he.m(lt.parentNode,lt));let ts=_e;_e=Ui(e),_e!==ts&&(ke(),b(Is[ts],1,1,()=>{Is[ts]=null}),ge(),ve=Is[_e],ve||(ve=Is[_e]=Ji[_e](e),ve.c()),$(ve,1),ve.m(at.parentNode,at));let ht=be;be=Ri(e),be!==ht&&(ke(),b(Fs[ht],1,1,()=>{Fs[ht]=null}),ge(),$e=Fs[be],$e||($e=Fs[be]=Bi[be](e),$e.c()),$($e,1),$e.m(ot.parentNode,ot));const Bn={};p&2&&(Bn.$$scope={dirty:p,ctx:e}),Ke.$set(Bn)},i(e){Or||($(r.$$.fragment,e),$(A.$$.fragment,e),$(I),$(S.$$.fragment,e),$(w),$(ls.$$.fragment,e),$(as.$$.fragment,e),$(us.$$.fragment,e),$(fs.$$.fragment,e),$(ne),$(le),$(hs.$$.fragment,e),$(oe),$(_s.$$.fragment,e),$(ue),$(vs.$$.fragment,e),$(ce),$(gs.$$.fragment,e),$(de),$(ks.$$.fragment,e),$(Es.$$.fragment,e),$(he),$(ve),$($e),$(qs.$$.fragment,e),$(ws.$$.fragment,e),$(Ke.$$.fragment,e),Or=!0)},o(e){b(r.$$.fragment,e),b(A.$$.fragment,e),b(I),b(S.$$.fragment,e),b(w),b(ls.$$.fragment,e),b(as.$$.fragment,e),b(us.$$.fragment,e),b(fs.$$.fragment,e),b(ne),b(le),b(hs.$$.fragment,e),b(oe),b(_s.$$.fragment,e),b(ue),b(vs.$$.fragment,e),b(ce),b(gs.$$.fragment,e),b(de),b(ks.$$.fragment,e),b(Es.$$.fragment,e),b(he),b(ve),b($e),b(qs.$$.fragment,e),b(ws.$$.fragment,e),b(Ke.$$.fragment,e),Or=!1},d(e){t(n),e&&t(c),T(r,e),e&&t(h),e&&t(v),T(A),e&&t(k),ys[q].d(e),e&&t(D),T(S,e),e&&t(J),xs[j].d(e),e&&t(W),e&&t(U),e&&t(Rn),T(ls,e),e&&t(Qn),e&&t(Ds),e&&t(Yn),T(as,e),e&&t(Wn),e&&t(De),e&&t(Xn),e&&t(Ce),e&&t(Kn),e&&t(Os),e&&t(Zn),e&&t(ze),T(us),e&&t(er),e&&t(Ee),e&&t(sr),e&&t(je),e&&t(tr),e&&t(V),e&&t(nr),e&&t(B),e&&t(rr),T(fs,e),e&&t(lr),e&&t(Oe),e&&t(ar),e&&t(O),e&&t(or),e&&t(Ve),e&&t(ir),Ps[te].d(e),e&&t(Vs),e&&t(He),e&&t(ur),Ts[re].d(e),e&&t(Hs),e&&t(H),e&&t(pr),e&&t(Ae),T(hs),e&&t(cr),Ms[ae].d(e),e&&t(Gs),e&&t(Je),e&&t(mr),e&&t(R),e&&t(dr),e&&t(Us),e&&t(fr),e&&t(qe),e&&t(hr),e&&t(Ne),T(_s),e&&t(_r),e&&t(Be),e&&t(vr),e&&t(we),e&&t(br),e&&t(Ws),e&&t($r),e&&t(Xs),e&&t(gr),Cs[ie].d(e),e&&t(Ks),e&&t(K),e&&t(kr),e&&t(Ie),T(vs),e&&t(Er),e&&t(Fe),e&&t(jr),e&&t(Z),e&&t(qr),e&&t(L),e&&t(wr),zs[pe].d(e),e&&t(et),e&&t(st),e&&t(yr),T(gs,e),e&&t(xr),As[me].d(e),e&&t(tt),e&&t(nt),e&&t(Pr),e&&t(Se),T(ks),e&&t(Tr),e&&t(rt),e&&t(Mr),T(Es,e),e&&t(Cr),Ns[fe].d(e),e&&t(lt),e&&t(Q),e&&t(zr),Is[_e].d(e),e&&t(at),Fs[be].d(e),e&&t(ot),e&&t(ye),e&&t(Ar),e&&t(Ye),e&&t(Nr),T(qs,e),e&&t(Ir),T(ws,e),e&&t(Fr),e&&t(it),e&&t(Sr),e&&t(We),e&&t(Dr),e&&t(Xe),e&&t(Lr),T(Ke,e)}}}const pc={local:"derrire-le-pipeline",sections:[{local:"prtraitement-avec-un-tokenizer",title:"Pr\xE9traitement avec un *tokenizer*"},{local:"passage-au-modle",sections:[{local:"un-vecteur-de-grande-dimension",title:"Un vecteur de grande dimension ?"},{local:"les-ttes-des-modles-donner-du-sens-aux-chiffres",title:"Les t\xEAtes des mod\xE8les : donner du sens aux chiffres"}],title:"Passage au mod\xE8le"},{local:"posttraitement-de-la-sortie",title:"Post-traitement de la sortie"}],title:"Derri\xE8re le pipeline"};function cc(g,n,c){let r="pt";return Sp(()=>{const h=new URLSearchParams(window.location.search);c(0,r=h.get("fw")||"pt")}),[r]}class $c extends Ap{constructor(n){super();Np(this,n,cc,uc,Ip,{})}}export{$c as default,pc as metadata};
