import{S as kr,i as gr,s as qr,e as p,k as h,w as g,t as o,M as jr,c,d as s,m as _,x as q,a as m,h as i,b as E,F as n,g as d,y as j,o as v,p as we,q as $,B as w,v as wr,n as Ee}from"../../chunks/vendor-1e8b365d.js";import{T as br}from"../../chunks/Tip-62b14c6e.js";import{Y as vr}from"../../chunks/Youtube-c2a8cc39.js";import{I as mn}from"../../chunks/IconCopyLink-483c28ba.js";import{C as y}from"../../chunks/CodeBlock-e5764662.js";import{D as $r}from"../../chunks/DocNotebookDropdown-37d928d3.js";import{F as Er}from"../../chunks/FrameworkSwitchCourse-7f8f0f31.js";function yr(k){let a,u;return a=new $r({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_tf.ipynb"}]}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function zr(k){let a,u;return a=new $r({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section5_pt.ipynb"}]}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Ar(k){let a,u;return a=new vr({props:{id:"ROxrFOEbsQE"}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function xr(k){let a,u;return a=new vr({props:{id:"M6adb1j2jPI"}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Ir(k){let a,u,t,f;return a=new y({props:{code:`import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."  # J'ai attendu un cours d\u2019HuggingFace toute ma vie.

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
# This line will fail.
model(input_ids)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>  <span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = tf.constant(ids)
<span class="hljs-comment"># This line will fail.</span>
model(input_ids)`}}),t=new y({props:{code:"InvalidArgumentError: Input to reshape is a tensor with 14 values, but the requested shape has 196 [Op:Reshape]",highlighted:'InvalidArgumentError: Input to reshape <span class="hljs-keyword">is</span> a tensor <span class="hljs-keyword">with</span> <span class="hljs-number">14</span> values, but the requested shape has <span class="hljs-number">196</span> [Op:Reshape]'}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Mr(k){let a,u,t,f;return a=new y({props:{code:`import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."  # J'ai attendu un cours d\u2019HuggingFace toute ma vie.

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
# This line will fail.
model(input_ids)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>  <span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)
<span class="hljs-comment"># This line will fail.</span>
model(input_ids)`}}),t=new y({props:{code:"IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)",highlighted:'IndexError: Dimension out of <span class="hljs-built_in">range</span> (expected to be <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span> of [-<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], but got <span class="hljs-number">1</span>)'}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Pr(k){let a,u,t,f;return a=new y({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="tf")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),t=new y({props:{code:`<tf.Tensor: shape=(1, 16), dtype=int32, numpy=
array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,
        12172,  2607,  2026,  2878,  2166,  1012,   102]], dtype=int32)>`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">1</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>,
        <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]], dtype=int32)&gt;`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Tr(k){let a,u,t,f;return a=new y({props:{code:`tokenized_inputs = tokenizer(sequence, return_tensors="pt")
print(tokenized_inputs["input_ids"])`,highlighted:`tokenized_inputs = tokenizer(sequence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(tokenized_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),t=new y({props:{code:`tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,
          2607,  2026,  2878,  2166,  1012,   102]])`,highlighted:`tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,
          <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>]])`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Fr(k){let a,u;return a=new y({props:{code:`import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."
# J'ai attendu un cours d\u2019HuggingFace toute ma vie.


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>
<span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = tf.constant([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Sr(k){let a,u;return a=new y({props:{code:`import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."  # J'ai attendu un cours d\u2019HuggingFace toute ma vie.


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
print("Input IDs:", input_ids)

output = model(input_ids)
print("Logits:", output.logits)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>  <span class="hljs-comment"># J&#x27;ai attendu un cours d\u2019HuggingFace toute ma vie.</span>


tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)

input_ids = torch.tensor([ids])
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs:&quot;</span>, input_ids)

output = model(input_ids)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Logits:&quot;</span>, output.logits)`}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Cr(k){let a,u;return a=new y({props:{code:`Input IDs: tf.Tensor(
[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878
   2166  1012]], shape=(1, 14), dtype=int32)
Logits: tf.Tensor([[-2.7276208  2.8789377]], shape=(1, 2), dtype=float32)`,highlighted:`Input IDs: tf.Tensor(
[[ <span class="hljs-number">1045</span>  <span class="hljs-number">1005</span>  <span class="hljs-number">2310</span>  <span class="hljs-number">2042</span>  <span class="hljs-number">3403</span>  <span class="hljs-number">2005</span>  <span class="hljs-number">1037</span> <span class="hljs-number">17662</span> <span class="hljs-number">12172</span>  <span class="hljs-number">2607</span>  <span class="hljs-number">2026</span>  <span class="hljs-number">2878</span>
   <span class="hljs-number">2166</span>  <span class="hljs-number">1012</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">14</span>), dtype=int32)
Logits: tf.Tensor([[-<span class="hljs-number">2.7276208</span>  <span class="hljs-number">2.8789377</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Lr(k){let a,u;return a=new y({props:{code:`Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]
Logits: [[-2.7276,  2.8789]]`,highlighted:`Input IDs: [[ <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>]]
Logits: [[-<span class="hljs-number">2.7276</span>,  <span class="hljs-number">2.8789</span>]]`}}),{c(){g(a.$$.fragment)},l(t){q(a.$$.fragment,t)},m(t,f){j(a,t,f),u=!0},i(t){u||($(a.$$.fragment,t),u=!0)},o(t){v(a.$$.fragment,t),u=!1},d(t){w(a,t)}}}function Nr(k){let a,u,t,f,l,b,T,I;return{c(){a=p("p"),u=o("\u270F\uFE0F "),t=p("strong"),f=o("Essayez !"),l=o(" Convertissez cette liste "),b=p("code"),T=o("batched_ids"),I=o(" en un tenseur et passez-la dans votre mod\xE8le. V\xE9rifiez que vous obtenez les m\xEAmes logits que pr\xE9c\xE9demment (mais deux fois) !")},l(M){a=c(M,"P",{});var z=m(a);u=i(z,"\u270F\uFE0F "),t=c(z,"STRONG",{});var le=m(t);f=i(le,"Essayez !"),le.forEach(s),l=i(z," Convertissez cette liste "),b=c(z,"CODE",{});var ee=m(b);T=i(ee,"batched_ids"),ee.forEach(s),I=i(z," en un tenseur et passez-la dans votre mod\xE8le. V\xE9rifiez que vous obtenez les m\xEAmes logits que pr\xE9c\xE9demment (mais deux fois) !"),z.forEach(s)},m(M,z){d(M,a,z),n(a,u),n(a,t),n(t,f),n(a,l),n(a,b),n(b,T),n(a,I)},d(M){M&&s(a)}}}function Dr(k){let a,u,t,f;return a=new y({props:{code:`model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(tf.constant(sequence1_ids)).logits)
print(model(tf.constant(sequence2_ids)).logits)
print(model(tf.constant(batched_ids)).logits)`,highlighted:`model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(tf.constant(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(tf.constant(batched_ids)).logits)`}}),t=new y({props:{code:`tf.Tensor([[ 1.5693678 -1.3894581]], shape=(1, 2), dtype=float32)
tf.Tensor([[ 0.5803005  -0.41252428]], shape=(1, 2), dtype=float32)
tf.Tensor(
[[ 1.5693681 -1.3894582]
 [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor([[ <span class="hljs-number">1.5693678</span> -<span class="hljs-number">1.3894581</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor([[ <span class="hljs-number">0.5803005</span>  -<span class="hljs-number">0.41252428</span>]], shape=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>), dtype=float32)
tf.Tensor(
[[ <span class="hljs-number">1.5693681</span> -<span class="hljs-number">1.3894582</span>]
 [ <span class="hljs-number">1.3373486</span> -<span class="hljs-number">1.2163193</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Hr(k){let a,u,t,f;return a=new y({props:{code:`model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)`,highlighted:`model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
sequence2_ids = [[<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]]
batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

<span class="hljs-built_in">print</span>(model(torch.tensor(sequence1_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(sequence2_ids)).logits)
<span class="hljs-built_in">print</span>(model(torch.tensor(batched_ids)).logits)`}}),t=new y({props:{code:`tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)
tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">1.3373</span>, -<span class="hljs-number">1.2163</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Or(k){let a,u,t,f;return a=new y({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
print(outputs.logits)`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),t=new y({props:{code:`tf.Tensor(
[[ 1.5693681  -1.3894582 ]
 [ 0.5803021  -0.41252586]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor(
[[ <span class="hljs-number">1.5693681</span>  -<span class="hljs-number">1.3894582</span> ]
 [ <span class="hljs-number">0.5803021</span>  -<span class="hljs-number">0.41252586</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Br(k){let a,u,t,f;return a=new y({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]

attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, tokenizer.pad_token_id],
]

attention_mask = [
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
<span class="hljs-built_in">print</span>(outputs.logits)`}}),t=new y({props:{code:`tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[ <span class="hljs-number">1.5694</span>, -<span class="hljs-number">1.3895</span>],
        [ <span class="hljs-number">0.5803</span>, -<span class="hljs-number">0.4125</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){g(a.$$.fragment),u=h(),g(t.$$.fragment)},l(l){q(a.$$.fragment,l),u=_(l),q(t.$$.fragment,l)},m(l,b){j(a,l,b),d(l,u,b),j(t,l,b),f=!0},i(l){f||($(a.$$.fragment,l),$(t.$$.fragment,l),f=!0)},o(l){v(a.$$.fragment,l),v(t.$$.fragment,l),f=!1},d(l){w(a,l),l&&s(u),w(t,l)}}}function Ur(k){let a,u,t,f,l,b,T,I;return{c(){a=p("p"),u=o("\u270F\uFE0F "),t=p("strong"),f=o("Essayez !"),l=o(" Appliquez la tok\xE9nisation manuellement sur les deux phrases utilis\xE9es dans la section 2 (\xAB I\u2019ve been waiting for a HuggingFace course my whole life.  \xBB et \xAB I hate this so much! \xBB). Passez-les dans le mod\xE8le et v\xE9rifiez que vous obtenez les m\xEAmes logits que dans la section 2. Ensuite regroupez-les en utilisant le jeton de "),b=p("em"),T=o("padding"),I=o(" et cr\xE9ez le masque d\u2019attention appropri\xE9. V\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats qu\u2019en passant par le mod\xE8le !")},l(M){a=c(M,"P",{});var z=m(a);u=i(z,"\u270F\uFE0F "),t=c(z,"STRONG",{});var le=m(t);f=i(le,"Essayez !"),le.forEach(s),l=i(z," Appliquez la tok\xE9nisation manuellement sur les deux phrases utilis\xE9es dans la section 2 (\xAB I\u2019ve been waiting for a HuggingFace course my whole life.  \xBB et \xAB I hate this so much! \xBB). Passez-les dans le mod\xE8le et v\xE9rifiez que vous obtenez les m\xEAmes logits que dans la section 2. Ensuite regroupez-les en utilisant le jeton de "),b=c(z,"EM",{});var ee=m(b);T=i(ee,"padding"),ee.forEach(s),I=i(z," et cr\xE9ez le masque d\u2019attention appropri\xE9. V\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats qu\u2019en passant par le mod\xE8le !"),z.forEach(s)},m(M,z){d(M,a,z),n(a,u),n(a,t),n(t,f),n(a,l),n(a,b),n(b,T),n(a,I)},d(M){M&&s(a)}}}function Vr(k){let a,u,t,f,l,b,T,I,M,z,le,ee,N,D,Qe,H,O,Ke,We,ot,dn,F,js,it,ut,ye,pt,ws,ct,mt,dt,Es,ft,ht,ys,_t,fn,ue,bt,zs,vt,$t,hn,ae,pe,As,ze,kt,xs,gt,_n,Xe,qt,bn,B,U,Ze,es,jt,vn,S,wt,Is,Et,yt,Ms,zt,At,Ps,xt,It,$n,V,J,ss,ns,Mt,kn,R,G,ts,ls,Pt,gn,Y,Q,as,ce,Tt,Ts,Ft,St,qn,Ae,jn,rs,Ct,wn,me,En,C,Lt,Fs,Nt,Dt,Ss,Ht,Ot,Cs,Bt,Ut,yn,re,de,Ls,xe,Vt,Ns,Jt,zn,os,Rt,An,Ie,xn,P,Gt,Ds,Yt,Qt,Hs,Kt,Wt,Os,Xt,Zt,Bs,el,sl,In,Me,Mn,fe,nl,Us,tl,ll,Pn,K,W,is,us,al,Tn,A,rl,Vs,ol,il,Js,ul,pl,Rs,cl,ml,Gs,dl,fl,Ys,hl,_l,Qs,bl,vl,Ks,$l,kl,Ws,gl,ql,Fn,oe,he,Xs,Pe,jl,Zs,wl,Sn,ps,El,Cn,_e,Te,yl,en,zl,Al,xl,Fe,Il,sn,Ml,Pl,Ln,cs,Tl,Nn,X,Z,ms,ds,Fl,Dn,be,Sl,nn,Cl,Ll,Hn,ve,On,ie,$e,tn,Se,Nl,ln,Dl,Bn,se,Hl,an,Ol,Bl,rn,Ul,Vl,Un,ke,on,Jl,Rl,un,Gl,Vn,ne,Yl,Ce,Ql,Kl,Le,Wl,Xl,Jn,ge,Zl,pn,ea,sa,Rn,Ne,Gn;t=new Er({props:{fw:k[0]}}),I=new mn({});const na=[zr,yr],De=[];function ta(e,r){return e[0]==="pt"?0:1}N=ta(k),D=De[N]=na[N](k);const la=[xr,Ar],He=[];function aa(e,r){return e[0]==="pt"?0:1}H=aa(k),O=He[H]=la[H](k),ze=new mn({});const ra=[Mr,Ir],Oe=[];function oa(e,r){return e[0]==="pt"?0:1}B=oa(k),U=Oe[B]=ra[B](k);const ia=[Tr,Pr],Be=[];function ua(e,r){return e[0]==="pt"?0:1}V=ua(k),J=Be[V]=ia[V](k);const pa=[Sr,Fr],Ue=[];function ca(e,r){return e[0]==="pt"?0:1}R=ca(k),G=Ue[R]=pa[R](k);const ma=[Lr,Cr],Ve=[];function da(e,r){return e[0]==="pt"?0:1}Y=da(k),Q=Ve[Y]=ma[Y](k),Ae=new y({props:{code:"batched_ids = [ids, ids]",highlighted:'<span class="hljs-attr">batched_ids</span> = [ids, ids]'}}),me=new br({props:{$$slots:{default:[Nr]},$$scope:{ctx:k}}}),xe=new mn({}),Ie=new y({props:{code:`batched_ids = [
    [200, 200, 200],
    [200, 200]
]`,highlighted:`batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>]
]`}}),Me=new y({props:{code:`padding_id = 100

batched_ids = [
    [200, 200, 200],
    [200, 200, padding_id],
]`,highlighted:`padding_id = <span class="hljs-number">100</span>

batched_ids = [
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, <span class="hljs-number">200</span>],
    [<span class="hljs-number">200</span>, <span class="hljs-number">200</span>, padding_id],
]`}});const fa=[Hr,Dr],Je=[];function ha(e,r){return e[0]==="pt"?0:1}K=ha(k),W=Je[K]=fa[K](k),Pe=new mn({});const _a=[Br,Or],Re=[];function ba(e,r){return e[0]==="pt"?0:1}return X=ba(k),Z=Re[X]=_a[X](k),ve=new br({props:{$$slots:{default:[Ur]},$$scope:{ctx:k}}}),Se=new mn({}),Ne=new y({props:{code:"sequence = sequence[:max_sequence_length]",highlighted:"sequence = sequence[:max_sequence_length]"}}),{c(){a=p("meta"),u=h(),g(t.$$.fragment),f=h(),l=p("h1"),b=p("a"),T=p("span"),g(I.$$.fragment),M=h(),z=p("span"),le=o("Manipulation de plusieurs s\xE9quences"),ee=h(),D.c(),Qe=h(),O.c(),Ke=h(),We=p("p"),ot=o("Dans la section pr\xE9c\xE9dente, nous avons explor\xE9 le cas d\u2019utilisation le plus simple : faire une inf\xE9rence sur une seule s\xE9quence de petite longueur. Cependant, certaines questions \xE9mergent d\xE9j\xE0 :"),dn=h(),F=p("ul"),js=p("li"),it=o("comment g\xE9rer de plusieurs s\xE9quences ?"),ut=h(),ye=p("li"),pt=o("comment g\xE9rer de plusieurs s\xE9quences "),ws=p("em"),ct=o("de longueurs diff\xE9rentes"),mt=o(" ?"),dt=h(),Es=p("li"),ft=o("les indices du vocabulaire sont-ils les seules entr\xE9es qui permettent \xE0 un mod\xE8le de bien fonctionner ?"),ht=h(),ys=p("li"),_t=o("existe-t-il une s\xE9quence trop longue ?"),fn=h(),ue=p("p"),bt=o("Voyons quels types de probl\xE8mes ces questions posent et comment nous pouvons les r\xE9soudre en utilisant l\u2019API \u{1F917} "),zs=p("em"),vt=o("Transformers"),$t=o("."),hn=h(),ae=p("h2"),pe=p("a"),As=p("span"),g(ze.$$.fragment),kt=h(),xs=p("span"),gt=o("Les mod\xE8les attendent un batch d'entr\xE9es"),_n=h(),Xe=p("p"),qt=o(`Dans l\u2019exercice pr\xE9c\xE9dent, vous avez vu comment les s\xE9quences sont traduites en listes de nombres.
Convertissons cette liste de nombres en un tenseur et envoyons-le au mod\xE8le :`),bn=h(),U.c(),Ze=h(),es=p("p"),jt=o("Pourquoi cela a \xE9chou\xE9 ? Nous avons suivi les \xE9tapes du pipeline de la section 2."),vn=h(),S=p("p"),wt=o("Le probl\xE8me est que nous avons envoy\xE9 une seule s\xE9quence au mod\xE8le, alors que les mod\xE8les de l\u2019API \u{1F917} "),Is=p("em"),Et=o("Transformers"),yt=o(" attendent plusieurs phrases par d\xE9faut. Ici, nous avons essay\xE9 de faire ce que le "),Ms=p("em"),zt=o("tokenizer"),At=o(" fait en coulisses lorsque nous l\u2019avons appliqu\xE9 \xE0 une "),Ps=p("code"),xt=o("s\xE9quence"),It=o(". Cependant si vous regardez de pr\xE8s, vous verrez qu\u2019il n\u2019a pas seulement converti la liste des identifiants d\u2019entr\xE9e en un tenseur mais aussi ajout\xE9 une dimension par-dessus :"),$n=h(),J.c(),ss=h(),ns=p("p"),Mt=o("Essayons \xE0 nouveau en ajoutant une nouvelle dimension :"),kn=h(),G.c(),ts=h(),ls=p("p"),Pt=o("Nous affichons les identifiants d\u2019entr\xE9e ainsi que les logits r\xE9sultants. Voici la sortie :"),gn=h(),Q.c(),as=h(),ce=p("p"),Tt=o("Le \xAB "),Ts=p("em"),Ft=o("batching"),St=o(" \xBB est l\u2019acte d\u2019envoyer plusieurs phrases \xE0 travers le mod\xE8le, toutes en m\xEAme temps. Si vous n\u2019avez qu\u2019une seule phrase, vous pouvez simplement construire un batch avec une seule s\xE9quence :"),qn=h(),g(Ae.$$.fragment),jn=h(),rs=p("p"),Ct=o("Il s\u2019agit d\u2019un batch de deux s\xE9quences identiques !"),wn=h(),g(me.$$.fragment),En=h(),C=p("p"),Lt=o("Utiliser des "),Fs=p("em"),Nt=o("batchs"),Dt=o(" permet au mod\xE8le de fonctionner lorsque vous lui donnez plusieurs s\xE9quences. Utiliser plusieurs s\xE9quences est aussi simple que de construire un batch avec une seule s\xE9quence. Il y a cependant un deuxi\xE8me probl\xE8me. Lorsque vous essayez de regrouper deux phrases (ou plus), elles peuvent \xEAtre de longueurs diff\xE9rentes. Si vous avez d\xE9j\xE0 travaill\xE9 avec des tenseurs, vous savez qu\u2019ils doivent \xEAtre de forme rectangulaire. Vous ne pourrez donc pas convertir directement la liste des identifiants d\u2019entr\xE9e en un tenseur. Pour contourner ce probl\xE8me, nous avons l\u2019habitude de "),Ss=p("em"),Ht=o("rembourrer"),Ot=o(" (le "),Cs=p("em"),Bt=o("padding"),Ut=o(" en anglais) les entr\xE9es."),yn=h(),re=p("h2"),de=p("a"),Ls=p("span"),g(xe.$$.fragment),Vt=h(),Ns=p("span"),Jt=o("*Padding* des entr\xE9es"),zn=h(),os=p("p"),Rt=o("La liste de listes suivante ne peut pas \xEAtre convertie en un tenseur :"),An=h(),g(Ie.$$.fragment),xn=h(),P=p("p"),Gt=o("Afin de contourner ce probl\xE8me, nous utilisons le "),Ds=p("em"),Yt=o("padding"),Qt=o(" pour que nos tenseurs aient une forme rectangulaire. Le "),Hs=p("em"),Kt=o("padding"),Wt=o(" permet de s\u2019assurer que toutes nos phrases ont la m\xEAme longueur en ajoutant un mot sp\xE9cial appel\xE9 "),Os=p("em"),Xt=o("padding token"),Zt=o(" aux phrases ayant moins de valeurs. Par exemple, si vous avez 10 phrases de 10 mots et 1 phrase de 20 mots, le "),Bs=p("em"),el=o("padding"),sl=o(" fait en sorte que toutes les phrases aient 20 mots. Dans notre exemple, le tenseur r\xE9sultant ressemble \xE0 ceci :"),In=h(),g(Me.$$.fragment),Mn=h(),fe=p("p"),nl=o("L\u2019identifiant du jeton de padding peut \xEAtre trouv\xE9 dans "),Us=p("code"),tl=o("tokenizer.pad_token_id"),ll=o(". Utilisons-le et envoyons nos deux phrases \xE0 travers le mod\xE8le premi\xE8rement individuellement puis en \xE9tant mises dans un m\xEAme batch :"),Pn=h(),W.c(),is=h(),us=p("p"),al=o("Il y a quelque chose qui ne va pas avec les logits de notre pr\xE9diction avec les s\xE9quences mises dans un m\xEAme batch. La deuxi\xE8me ligne devrait \xEAtre la m\xEAme que les logits pour la deuxi\xE8me phrase, mais nous avons des valeurs compl\xE8tement diff\xE9rentes !"),Tn=h(),A=p("p"),rl=o("C\u2019est parce que dans un "),Vs=p("em"),ol=o("transformer"),il=o(" les couches d\u2019attention "),Js=p("em"),ul=o("contextualisent"),pl=o(" chaque "),Rs=p("em"),cl=o("token"),ml=o(". Celles-ci prennent en compte les "),Gs=p("em"),dl=o("tokens"),fl=o(" de "),Ys=p("em"),hl=o("padding"),_l=o(" puisqu\u2019elles analysent tous les "),Qs=p("em"),bl=o("tokens"),vl=o(" d\u2019une s\xE9quence. Pour obtenir le m\xEAme r\xE9sultat lorsque l\u2019on passe dans notre mod\xE8le des phrases individuelles de diff\xE9rentes longueurs ou un batch compos\xE9 de m\xEAmes phrases avec "),Ks=p("em"),$l=o("padding"),kl=o(", nous devons dire \xE0 ces couches d\u2019attention d\u2019ignorer les jetons de "),Ws=p("em"),gl=o("padding"),ql=o(". Ceci est fait en utilisant un masque d\u2019attention."),Fn=h(),oe=p("h2"),he=p("a"),Xs=p("span"),g(Pe.$$.fragment),jl=h(),Zs=p("span"),wl=o("Masques d'attention"),Sn=h(),ps=p("p"),El=o("Les masques d\u2019attention sont des tenseurs ayant exactement la m\xEAme forme que le tenseur d\u2019identifiants d\u2019entr\xE9e, remplis de 0 et de 1 :"),Cn=h(),_e=p("ul"),Te=p("li"),yl=o("1 indique que les "),en=p("em"),zl=o("tokens"),Al=o(" correspondants doivent \xEAtre analys\xE9s"),xl=h(),Fe=p("li"),Il=o("0 indique que les "),sn=p("em"),Ml=o("tokens"),Pl=o(" correspondants ne doivent pas \xEAtre analys\xE9s (c\u2019est-\xE0-dire qu\u2019ils doivent \xEAtre ignor\xE9s par les couches d\u2019attention du mod\xE8le)."),Ln=h(),cs=p("p"),Tl=o("Compl\xE9tons l\u2019exemple pr\xE9c\xE9dent avec un masque d\u2019attention :"),Nn=h(),Z.c(),ms=h(),ds=p("p"),Fl=o("Nous obtenons maintenant les m\xEAmes logits pour la deuxi\xE8me phrase du batch."),Dn=h(),be=p("p"),Sl=o("Remarquez comment la derni\xE8re valeur de la deuxi\xE8me s\xE9quence est un identifiant de "),nn=p("em"),Cl=o("padding"),Ll=o(" valant 0 dans le masque d\u2019attention."),Hn=h(),g(ve.$$.fragment),On=h(),ie=p("h2"),$e=p("a"),tn=p("span"),g(Se.$$.fragment),Nl=h(),ln=p("span"),Dl=o("S\xE9quences plus longues"),Bn=h(),se=p("p"),Hl=o("Les "),an=p("em"),Ol=o("transformers"),Bl=o(" acceptent en entr\xE9e que des s\xE9quences d\u2019une longueur limit\xE9e. La plupart des mod\xE8les traitent des s\xE9quences allant jusqu\u2019\xE0 512 ou 1024 "),rn=p("em"),Ul=o("tokens"),Vl=o(" et plantent lorsqu\u2019on leur demande de traiter des s\xE9quences plus longues. Il existe deux solutions \xE0 ce probl\xE8me :"),Un=h(),ke=p("ul"),on=p("li"),Jl=o("utiliser un mod\xE8le avec une longueur de s\xE9quence support\xE9e plus longue,"),Rl=h(),un=p("li"),Gl=o("tronquer les s\xE9quences."),Vn=h(),ne=p("p"),Yl=o("Certains mod\xE8les sont sp\xE9cialis\xE9s dans le traitement de tr\xE8s longues s\xE9quences comme par exemple le "),Ce=p("a"),Ql=o("Longformer"),Kl=o(" ou le "),Le=p("a"),Wl=o("LED"),Xl=o(". Si vous travaillez sur une t\xE2che qui n\xE9cessite de tr\xE8s longues s\xE9quences, nous vous recommandons de jeter un coup d\u2019\u0153il \xE0 ces mod\xE8les."),Jn=h(),ge=p("p"),Zl=o("Sinon, nous vous recommandons de tronquer vos s\xE9quences en sp\xE9cifiant le param\xE8tre "),pn=p("code"),ea=o("max_sequence_length"),sa=o(" :"),Rn=h(),g(Ne.$$.fragment),this.h()},l(e){const r=jr('[data-svelte="svelte-1phssyn"]',document.head);a=c(r,"META",{name:!0,content:!0}),r.forEach(s),u=_(e),q(t.$$.fragment,e),f=_(e),l=c(e,"H1",{class:!0});var Ge=m(l);b=c(Ge,"A",{id:!0,class:!0,href:!0});var fs=m(b);T=c(fs,"SPAN",{});var hs=m(T);q(I.$$.fragment,hs),hs.forEach(s),fs.forEach(s),M=_(Ge),z=c(Ge,"SPAN",{});var _s=m(z);le=i(_s,"Manipulation de plusieurs s\xE9quences"),_s.forEach(s),Ge.forEach(s),ee=_(e),D.l(e),Qe=_(e),O.l(e),Ke=_(e),We=c(e,"P",{});var bs=m(We);ot=i(bs,"Dans la section pr\xE9c\xE9dente, nous avons explor\xE9 le cas d\u2019utilisation le plus simple : faire une inf\xE9rence sur une seule s\xE9quence de petite longueur. Cependant, certaines questions \xE9mergent d\xE9j\xE0 :"),bs.forEach(s),dn=_(e),F=c(e,"UL",{});var L=m(F);js=c(L,"LI",{});var vs=m(js);it=i(vs,"comment g\xE9rer de plusieurs s\xE9quences ?"),vs.forEach(s),ut=_(L),ye=c(L,"LI",{});var Ye=m(ye);pt=i(Ye,"comment g\xE9rer de plusieurs s\xE9quences "),ws=c(Ye,"EM",{});var $s=m(ws);ct=i($s,"de longueurs diff\xE9rentes"),$s.forEach(s),mt=i(Ye," ?"),Ye.forEach(s),dt=_(L),Es=c(L,"LI",{});var ks=m(Es);ft=i(ks,"les indices du vocabulaire sont-ils les seules entr\xE9es qui permettent \xE0 un mod\xE8le de bien fonctionner ?"),ks.forEach(s),ht=_(L),ys=c(L,"LI",{});var cn=m(ys);_t=i(cn,"existe-t-il une s\xE9quence trop longue ?"),cn.forEach(s),L.forEach(s),fn=_(e),ue=c(e,"P",{});var Yn=m(ue);bt=i(Yn,"Voyons quels types de probl\xE8mes ces questions posent et comment nous pouvons les r\xE9soudre en utilisant l\u2019API \u{1F917} "),zs=c(Yn,"EM",{});var va=m(zs);vt=i(va,"Transformers"),va.forEach(s),$t=i(Yn,"."),Yn.forEach(s),hn=_(e),ae=c(e,"H2",{class:!0});var Qn=m(ae);pe=c(Qn,"A",{id:!0,class:!0,href:!0});var $a=m(pe);As=c($a,"SPAN",{});var ka=m(As);q(ze.$$.fragment,ka),ka.forEach(s),$a.forEach(s),kt=_(Qn),xs=c(Qn,"SPAN",{});var ga=m(xs);gt=i(ga,"Les mod\xE8les attendent un batch d'entr\xE9es"),ga.forEach(s),Qn.forEach(s),_n=_(e),Xe=c(e,"P",{});var qa=m(Xe);qt=i(qa,`Dans l\u2019exercice pr\xE9c\xE9dent, vous avez vu comment les s\xE9quences sont traduites en listes de nombres.
Convertissons cette liste de nombres en un tenseur et envoyons-le au mod\xE8le :`),qa.forEach(s),bn=_(e),U.l(e),Ze=_(e),es=c(e,"P",{});var ja=m(es);jt=i(ja,"Pourquoi cela a \xE9chou\xE9 ? Nous avons suivi les \xE9tapes du pipeline de la section 2."),ja.forEach(s),vn=_(e),S=c(e,"P",{});var qe=m(S);wt=i(qe,"Le probl\xE8me est que nous avons envoy\xE9 une seule s\xE9quence au mod\xE8le, alors que les mod\xE8les de l\u2019API \u{1F917} "),Is=c(qe,"EM",{});var wa=m(Is);Et=i(wa,"Transformers"),wa.forEach(s),yt=i(qe," attendent plusieurs phrases par d\xE9faut. Ici, nous avons essay\xE9 de faire ce que le "),Ms=c(qe,"EM",{});var Ea=m(Ms);zt=i(Ea,"tokenizer"),Ea.forEach(s),At=i(qe," fait en coulisses lorsque nous l\u2019avons appliqu\xE9 \xE0 une "),Ps=c(qe,"CODE",{});var ya=m(Ps);xt=i(ya,"s\xE9quence"),ya.forEach(s),It=i(qe,". Cependant si vous regardez de pr\xE8s, vous verrez qu\u2019il n\u2019a pas seulement converti la liste des identifiants d\u2019entr\xE9e en un tenseur mais aussi ajout\xE9 une dimension par-dessus :"),qe.forEach(s),$n=_(e),J.l(e),ss=_(e),ns=c(e,"P",{});var za=m(ns);Mt=i(za,"Essayons \xE0 nouveau en ajoutant une nouvelle dimension :"),za.forEach(s),kn=_(e),G.l(e),ts=_(e),ls=c(e,"P",{});var Aa=m(ls);Pt=i(Aa,"Nous affichons les identifiants d\u2019entr\xE9e ainsi que les logits r\xE9sultants. Voici la sortie :"),Aa.forEach(s),gn=_(e),Q.l(e),as=_(e),ce=c(e,"P",{});var Kn=m(ce);Tt=i(Kn,"Le \xAB "),Ts=c(Kn,"EM",{});var xa=m(Ts);Ft=i(xa,"batching"),xa.forEach(s),St=i(Kn," \xBB est l\u2019acte d\u2019envoyer plusieurs phrases \xE0 travers le mod\xE8le, toutes en m\xEAme temps. Si vous n\u2019avez qu\u2019une seule phrase, vous pouvez simplement construire un batch avec une seule s\xE9quence :"),Kn.forEach(s),qn=_(e),q(Ae.$$.fragment,e),jn=_(e),rs=c(e,"P",{});var Ia=m(rs);Ct=i(Ia,"Il s\u2019agit d\u2019un batch de deux s\xE9quences identiques !"),Ia.forEach(s),wn=_(e),q(me.$$.fragment,e),En=_(e),C=c(e,"P",{});var je=m(C);Lt=i(je,"Utiliser des "),Fs=c(je,"EM",{});var Ma=m(Fs);Nt=i(Ma,"batchs"),Ma.forEach(s),Dt=i(je," permet au mod\xE8le de fonctionner lorsque vous lui donnez plusieurs s\xE9quences. Utiliser plusieurs s\xE9quences est aussi simple que de construire un batch avec une seule s\xE9quence. Il y a cependant un deuxi\xE8me probl\xE8me. Lorsque vous essayez de regrouper deux phrases (ou plus), elles peuvent \xEAtre de longueurs diff\xE9rentes. Si vous avez d\xE9j\xE0 travaill\xE9 avec des tenseurs, vous savez qu\u2019ils doivent \xEAtre de forme rectangulaire. Vous ne pourrez donc pas convertir directement la liste des identifiants d\u2019entr\xE9e en un tenseur. Pour contourner ce probl\xE8me, nous avons l\u2019habitude de "),Ss=c(je,"EM",{});var Pa=m(Ss);Ht=i(Pa,"rembourrer"),Pa.forEach(s),Ot=i(je," (le "),Cs=c(je,"EM",{});var Ta=m(Cs);Bt=i(Ta,"padding"),Ta.forEach(s),Ut=i(je," en anglais) les entr\xE9es."),je.forEach(s),yn=_(e),re=c(e,"H2",{class:!0});var Wn=m(re);de=c(Wn,"A",{id:!0,class:!0,href:!0});var Fa=m(de);Ls=c(Fa,"SPAN",{});var Sa=m(Ls);q(xe.$$.fragment,Sa),Sa.forEach(s),Fa.forEach(s),Vt=_(Wn),Ns=c(Wn,"SPAN",{});var Ca=m(Ns);Jt=i(Ca,"*Padding* des entr\xE9es"),Ca.forEach(s),Wn.forEach(s),zn=_(e),os=c(e,"P",{});var La=m(os);Rt=i(La,"La liste de listes suivante ne peut pas \xEAtre convertie en un tenseur :"),La.forEach(s),An=_(e),q(Ie.$$.fragment,e),xn=_(e),P=c(e,"P",{});var te=m(P);Gt=i(te,"Afin de contourner ce probl\xE8me, nous utilisons le "),Ds=c(te,"EM",{});var Na=m(Ds);Yt=i(Na,"padding"),Na.forEach(s),Qt=i(te," pour que nos tenseurs aient une forme rectangulaire. Le "),Hs=c(te,"EM",{});var Da=m(Hs);Kt=i(Da,"padding"),Da.forEach(s),Wt=i(te," permet de s\u2019assurer que toutes nos phrases ont la m\xEAme longueur en ajoutant un mot sp\xE9cial appel\xE9 "),Os=c(te,"EM",{});var Ha=m(Os);Xt=i(Ha,"padding token"),Ha.forEach(s),Zt=i(te," aux phrases ayant moins de valeurs. Par exemple, si vous avez 10 phrases de 10 mots et 1 phrase de 20 mots, le "),Bs=c(te,"EM",{});var Oa=m(Bs);el=i(Oa,"padding"),Oa.forEach(s),sl=i(te," fait en sorte que toutes les phrases aient 20 mots. Dans notre exemple, le tenseur r\xE9sultant ressemble \xE0 ceci :"),te.forEach(s),In=_(e),q(Me.$$.fragment,e),Mn=_(e),fe=c(e,"P",{});var Xn=m(fe);nl=i(Xn,"L\u2019identifiant du jeton de padding peut \xEAtre trouv\xE9 dans "),Us=c(Xn,"CODE",{});var Ba=m(Us);tl=i(Ba,"tokenizer.pad_token_id"),Ba.forEach(s),ll=i(Xn,". Utilisons-le et envoyons nos deux phrases \xE0 travers le mod\xE8le premi\xE8rement individuellement puis en \xE9tant mises dans un m\xEAme batch :"),Xn.forEach(s),Pn=_(e),W.l(e),is=_(e),us=c(e,"P",{});var Ua=m(us);al=i(Ua,"Il y a quelque chose qui ne va pas avec les logits de notre pr\xE9diction avec les s\xE9quences mises dans un m\xEAme batch. La deuxi\xE8me ligne devrait \xEAtre la m\xEAme que les logits pour la deuxi\xE8me phrase, mais nous avons des valeurs compl\xE8tement diff\xE9rentes !"),Ua.forEach(s),Tn=_(e),A=c(e,"P",{});var x=m(A);rl=i(x,"C\u2019est parce que dans un "),Vs=c(x,"EM",{});var Va=m(Vs);ol=i(Va,"transformer"),Va.forEach(s),il=i(x," les couches d\u2019attention "),Js=c(x,"EM",{});var Ja=m(Js);ul=i(Ja,"contextualisent"),Ja.forEach(s),pl=i(x," chaque "),Rs=c(x,"EM",{});var Ra=m(Rs);cl=i(Ra,"token"),Ra.forEach(s),ml=i(x,". Celles-ci prennent en compte les "),Gs=c(x,"EM",{});var Ga=m(Gs);dl=i(Ga,"tokens"),Ga.forEach(s),fl=i(x," de "),Ys=c(x,"EM",{});var Ya=m(Ys);hl=i(Ya,"padding"),Ya.forEach(s),_l=i(x," puisqu\u2019elles analysent tous les "),Qs=c(x,"EM",{});var Qa=m(Qs);bl=i(Qa,"tokens"),Qa.forEach(s),vl=i(x," d\u2019une s\xE9quence. Pour obtenir le m\xEAme r\xE9sultat lorsque l\u2019on passe dans notre mod\xE8le des phrases individuelles de diff\xE9rentes longueurs ou un batch compos\xE9 de m\xEAmes phrases avec "),Ks=c(x,"EM",{});var Ka=m(Ks);$l=i(Ka,"padding"),Ka.forEach(s),kl=i(x,", nous devons dire \xE0 ces couches d\u2019attention d\u2019ignorer les jetons de "),Ws=c(x,"EM",{});var Wa=m(Ws);gl=i(Wa,"padding"),Wa.forEach(s),ql=i(x,". Ceci est fait en utilisant un masque d\u2019attention."),x.forEach(s),Fn=_(e),oe=c(e,"H2",{class:!0});var Zn=m(oe);he=c(Zn,"A",{id:!0,class:!0,href:!0});var Xa=m(he);Xs=c(Xa,"SPAN",{});var Za=m(Xs);q(Pe.$$.fragment,Za),Za.forEach(s),Xa.forEach(s),jl=_(Zn),Zs=c(Zn,"SPAN",{});var er=m(Zs);wl=i(er,"Masques d'attention"),er.forEach(s),Zn.forEach(s),Sn=_(e),ps=c(e,"P",{});var sr=m(ps);El=i(sr,"Les masques d\u2019attention sont des tenseurs ayant exactement la m\xEAme forme que le tenseur d\u2019identifiants d\u2019entr\xE9e, remplis de 0 et de 1 :"),sr.forEach(s),Cn=_(e),_e=c(e,"UL",{});var et=m(_e);Te=c(et,"LI",{});var st=m(Te);yl=i(st,"1 indique que les "),en=c(st,"EM",{});var nr=m(en);zl=i(nr,"tokens"),nr.forEach(s),Al=i(st," correspondants doivent \xEAtre analys\xE9s"),st.forEach(s),xl=_(et),Fe=c(et,"LI",{});var nt=m(Fe);Il=i(nt,"0 indique que les "),sn=c(nt,"EM",{});var tr=m(sn);Ml=i(tr,"tokens"),tr.forEach(s),Pl=i(nt," correspondants ne doivent pas \xEAtre analys\xE9s (c\u2019est-\xE0-dire qu\u2019ils doivent \xEAtre ignor\xE9s par les couches d\u2019attention du mod\xE8le)."),nt.forEach(s),et.forEach(s),Ln=_(e),cs=c(e,"P",{});var lr=m(cs);Tl=i(lr,"Compl\xE9tons l\u2019exemple pr\xE9c\xE9dent avec un masque d\u2019attention :"),lr.forEach(s),Nn=_(e),Z.l(e),ms=_(e),ds=c(e,"P",{});var ar=m(ds);Fl=i(ar,"Nous obtenons maintenant les m\xEAmes logits pour la deuxi\xE8me phrase du batch."),ar.forEach(s),Dn=_(e),be=c(e,"P",{});var tt=m(be);Sl=i(tt,"Remarquez comment la derni\xE8re valeur de la deuxi\xE8me s\xE9quence est un identifiant de "),nn=c(tt,"EM",{});var rr=m(nn);Cl=i(rr,"padding"),rr.forEach(s),Ll=i(tt," valant 0 dans le masque d\u2019attention."),tt.forEach(s),Hn=_(e),q(ve.$$.fragment,e),On=_(e),ie=c(e,"H2",{class:!0});var lt=m(ie);$e=c(lt,"A",{id:!0,class:!0,href:!0});var or=m($e);tn=c(or,"SPAN",{});var ir=m(tn);q(Se.$$.fragment,ir),ir.forEach(s),or.forEach(s),Nl=_(lt),ln=c(lt,"SPAN",{});var ur=m(ln);Dl=i(ur,"S\xE9quences plus longues"),ur.forEach(s),lt.forEach(s),Bn=_(e),se=c(e,"P",{});var gs=m(se);Hl=i(gs,"Les "),an=c(gs,"EM",{});var pr=m(an);Ol=i(pr,"transformers"),pr.forEach(s),Bl=i(gs," acceptent en entr\xE9e que des s\xE9quences d\u2019une longueur limit\xE9e. La plupart des mod\xE8les traitent des s\xE9quences allant jusqu\u2019\xE0 512 ou 1024 "),rn=c(gs,"EM",{});var cr=m(rn);Ul=i(cr,"tokens"),cr.forEach(s),Vl=i(gs," et plantent lorsqu\u2019on leur demande de traiter des s\xE9quences plus longues. Il existe deux solutions \xE0 ce probl\xE8me :"),gs.forEach(s),Un=_(e),ke=c(e,"UL",{});var at=m(ke);on=c(at,"LI",{});var mr=m(on);Jl=i(mr,"utiliser un mod\xE8le avec une longueur de s\xE9quence support\xE9e plus longue,"),mr.forEach(s),Rl=_(at),un=c(at,"LI",{});var dr=m(un);Gl=i(dr,"tronquer les s\xE9quences."),dr.forEach(s),at.forEach(s),Vn=_(e),ne=c(e,"P",{});var qs=m(ne);Yl=i(qs,"Certains mod\xE8les sont sp\xE9cialis\xE9s dans le traitement de tr\xE8s longues s\xE9quences comme par exemple le "),Ce=c(qs,"A",{href:!0,rel:!0});var fr=m(Ce);Ql=i(fr,"Longformer"),fr.forEach(s),Kl=i(qs," ou le "),Le=c(qs,"A",{href:!0,rel:!0});var hr=m(Le);Wl=i(hr,"LED"),hr.forEach(s),Xl=i(qs,". Si vous travaillez sur une t\xE2che qui n\xE9cessite de tr\xE8s longues s\xE9quences, nous vous recommandons de jeter un coup d\u2019\u0153il \xE0 ces mod\xE8les."),qs.forEach(s),Jn=_(e),ge=c(e,"P",{});var rt=m(ge);Zl=i(rt,"Sinon, nous vous recommandons de tronquer vos s\xE9quences en sp\xE9cifiant le param\xE8tre "),pn=c(rt,"CODE",{});var _r=m(pn);ea=i(_r,"max_sequence_length"),_r.forEach(s),sa=i(rt," :"),rt.forEach(s),Rn=_(e),q(Ne.$$.fragment,e),this.h()},h(){E(a,"name","hf:doc:metadata"),E(a,"content",JSON.stringify(Jr)),E(b,"id","manipulation-de-plusieurs-squences"),E(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(b,"href","#manipulation-de-plusieurs-squences"),E(l,"class","relative group"),E(pe,"id","les-modles-attendent-un-batch-dentres"),E(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(pe,"href","#les-modles-attendent-un-batch-dentres"),E(ae,"class","relative group"),E(de,"id","padding-des-entres"),E(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(de,"href","#padding-des-entres"),E(re,"class","relative group"),E(he,"id","masques-dattention"),E(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(he,"href","#masques-dattention"),E(oe,"class","relative group"),E($e,"id","squences-plus-longues"),E($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E($e,"href","#squences-plus-longues"),E(ie,"class","relative group"),E(Ce,"href","https://huggingface.co/transformers/model_doc/longformer.html"),E(Ce,"rel","nofollow"),E(Le,"href","https://huggingface.co/transformers/model_doc/led.html"),E(Le,"rel","nofollow")},m(e,r){n(document.head,a),d(e,u,r),j(t,e,r),d(e,f,r),d(e,l,r),n(l,b),n(b,T),j(I,T,null),n(l,M),n(l,z),n(z,le),d(e,ee,r),De[N].m(e,r),d(e,Qe,r),He[H].m(e,r),d(e,Ke,r),d(e,We,r),n(We,ot),d(e,dn,r),d(e,F,r),n(F,js),n(js,it),n(F,ut),n(F,ye),n(ye,pt),n(ye,ws),n(ws,ct),n(ye,mt),n(F,dt),n(F,Es),n(Es,ft),n(F,ht),n(F,ys),n(ys,_t),d(e,fn,r),d(e,ue,r),n(ue,bt),n(ue,zs),n(zs,vt),n(ue,$t),d(e,hn,r),d(e,ae,r),n(ae,pe),n(pe,As),j(ze,As,null),n(ae,kt),n(ae,xs),n(xs,gt),d(e,_n,r),d(e,Xe,r),n(Xe,qt),d(e,bn,r),Oe[B].m(e,r),d(e,Ze,r),d(e,es,r),n(es,jt),d(e,vn,r),d(e,S,r),n(S,wt),n(S,Is),n(Is,Et),n(S,yt),n(S,Ms),n(Ms,zt),n(S,At),n(S,Ps),n(Ps,xt),n(S,It),d(e,$n,r),Be[V].m(e,r),d(e,ss,r),d(e,ns,r),n(ns,Mt),d(e,kn,r),Ue[R].m(e,r),d(e,ts,r),d(e,ls,r),n(ls,Pt),d(e,gn,r),Ve[Y].m(e,r),d(e,as,r),d(e,ce,r),n(ce,Tt),n(ce,Ts),n(Ts,Ft),n(ce,St),d(e,qn,r),j(Ae,e,r),d(e,jn,r),d(e,rs,r),n(rs,Ct),d(e,wn,r),j(me,e,r),d(e,En,r),d(e,C,r),n(C,Lt),n(C,Fs),n(Fs,Nt),n(C,Dt),n(C,Ss),n(Ss,Ht),n(C,Ot),n(C,Cs),n(Cs,Bt),n(C,Ut),d(e,yn,r),d(e,re,r),n(re,de),n(de,Ls),j(xe,Ls,null),n(re,Vt),n(re,Ns),n(Ns,Jt),d(e,zn,r),d(e,os,r),n(os,Rt),d(e,An,r),j(Ie,e,r),d(e,xn,r),d(e,P,r),n(P,Gt),n(P,Ds),n(Ds,Yt),n(P,Qt),n(P,Hs),n(Hs,Kt),n(P,Wt),n(P,Os),n(Os,Xt),n(P,Zt),n(P,Bs),n(Bs,el),n(P,sl),d(e,In,r),j(Me,e,r),d(e,Mn,r),d(e,fe,r),n(fe,nl),n(fe,Us),n(Us,tl),n(fe,ll),d(e,Pn,r),Je[K].m(e,r),d(e,is,r),d(e,us,r),n(us,al),d(e,Tn,r),d(e,A,r),n(A,rl),n(A,Vs),n(Vs,ol),n(A,il),n(A,Js),n(Js,ul),n(A,pl),n(A,Rs),n(Rs,cl),n(A,ml),n(A,Gs),n(Gs,dl),n(A,fl),n(A,Ys),n(Ys,hl),n(A,_l),n(A,Qs),n(Qs,bl),n(A,vl),n(A,Ks),n(Ks,$l),n(A,kl),n(A,Ws),n(Ws,gl),n(A,ql),d(e,Fn,r),d(e,oe,r),n(oe,he),n(he,Xs),j(Pe,Xs,null),n(oe,jl),n(oe,Zs),n(Zs,wl),d(e,Sn,r),d(e,ps,r),n(ps,El),d(e,Cn,r),d(e,_e,r),n(_e,Te),n(Te,yl),n(Te,en),n(en,zl),n(Te,Al),n(_e,xl),n(_e,Fe),n(Fe,Il),n(Fe,sn),n(sn,Ml),n(Fe,Pl),d(e,Ln,r),d(e,cs,r),n(cs,Tl),d(e,Nn,r),Re[X].m(e,r),d(e,ms,r),d(e,ds,r),n(ds,Fl),d(e,Dn,r),d(e,be,r),n(be,Sl),n(be,nn),n(nn,Cl),n(be,Ll),d(e,Hn,r),j(ve,e,r),d(e,On,r),d(e,ie,r),n(ie,$e),n($e,tn),j(Se,tn,null),n(ie,Nl),n(ie,ln),n(ln,Dl),d(e,Bn,r),d(e,se,r),n(se,Hl),n(se,an),n(an,Ol),n(se,Bl),n(se,rn),n(rn,Ul),n(se,Vl),d(e,Un,r),d(e,ke,r),n(ke,on),n(on,Jl),n(ke,Rl),n(ke,un),n(un,Gl),d(e,Vn,r),d(e,ne,r),n(ne,Yl),n(ne,Ce),n(Ce,Ql),n(ne,Kl),n(ne,Le),n(Le,Wl),n(ne,Xl),d(e,Jn,r),d(e,ge,r),n(ge,Zl),n(ge,pn),n(pn,ea),n(ge,sa),d(e,Rn,r),j(Ne,e,r),Gn=!0},p(e,[r]){const Ge={};r&1&&(Ge.fw=e[0]),t.$set(Ge);let fs=N;N=ta(e),N!==fs&&(Ee(),v(De[fs],1,1,()=>{De[fs]=null}),we(),D=De[N],D||(D=De[N]=na[N](e),D.c()),$(D,1),D.m(Qe.parentNode,Qe));let hs=H;H=aa(e),H!==hs&&(Ee(),v(He[hs],1,1,()=>{He[hs]=null}),we(),O=He[H],O||(O=He[H]=la[H](e),O.c()),$(O,1),O.m(Ke.parentNode,Ke));let _s=B;B=oa(e),B!==_s&&(Ee(),v(Oe[_s],1,1,()=>{Oe[_s]=null}),we(),U=Oe[B],U||(U=Oe[B]=ra[B](e),U.c()),$(U,1),U.m(Ze.parentNode,Ze));let bs=V;V=ua(e),V!==bs&&(Ee(),v(Be[bs],1,1,()=>{Be[bs]=null}),we(),J=Be[V],J||(J=Be[V]=ia[V](e),J.c()),$(J,1),J.m(ss.parentNode,ss));let L=R;R=ca(e),R!==L&&(Ee(),v(Ue[L],1,1,()=>{Ue[L]=null}),we(),G=Ue[R],G||(G=Ue[R]=pa[R](e),G.c()),$(G,1),G.m(ts.parentNode,ts));let vs=Y;Y=da(e),Y!==vs&&(Ee(),v(Ve[vs],1,1,()=>{Ve[vs]=null}),we(),Q=Ve[Y],Q||(Q=Ve[Y]=ma[Y](e),Q.c()),$(Q,1),Q.m(as.parentNode,as));const Ye={};r&2&&(Ye.$$scope={dirty:r,ctx:e}),me.$set(Ye);let $s=K;K=ha(e),K!==$s&&(Ee(),v(Je[$s],1,1,()=>{Je[$s]=null}),we(),W=Je[K],W||(W=Je[K]=fa[K](e),W.c()),$(W,1),W.m(is.parentNode,is));let ks=X;X=ba(e),X!==ks&&(Ee(),v(Re[ks],1,1,()=>{Re[ks]=null}),we(),Z=Re[X],Z||(Z=Re[X]=_a[X](e),Z.c()),$(Z,1),Z.m(ms.parentNode,ms));const cn={};r&2&&(cn.$$scope={dirty:r,ctx:e}),ve.$set(cn)},i(e){Gn||($(t.$$.fragment,e),$(I.$$.fragment,e),$(D),$(O),$(ze.$$.fragment,e),$(U),$(J),$(G),$(Q),$(Ae.$$.fragment,e),$(me.$$.fragment,e),$(xe.$$.fragment,e),$(Ie.$$.fragment,e),$(Me.$$.fragment,e),$(W),$(Pe.$$.fragment,e),$(Z),$(ve.$$.fragment,e),$(Se.$$.fragment,e),$(Ne.$$.fragment,e),Gn=!0)},o(e){v(t.$$.fragment,e),v(I.$$.fragment,e),v(D),v(O),v(ze.$$.fragment,e),v(U),v(J),v(G),v(Q),v(Ae.$$.fragment,e),v(me.$$.fragment,e),v(xe.$$.fragment,e),v(Ie.$$.fragment,e),v(Me.$$.fragment,e),v(W),v(Pe.$$.fragment,e),v(Z),v(ve.$$.fragment,e),v(Se.$$.fragment,e),v(Ne.$$.fragment,e),Gn=!1},d(e){s(a),e&&s(u),w(t,e),e&&s(f),e&&s(l),w(I),e&&s(ee),De[N].d(e),e&&s(Qe),He[H].d(e),e&&s(Ke),e&&s(We),e&&s(dn),e&&s(F),e&&s(fn),e&&s(ue),e&&s(hn),e&&s(ae),w(ze),e&&s(_n),e&&s(Xe),e&&s(bn),Oe[B].d(e),e&&s(Ze),e&&s(es),e&&s(vn),e&&s(S),e&&s($n),Be[V].d(e),e&&s(ss),e&&s(ns),e&&s(kn),Ue[R].d(e),e&&s(ts),e&&s(ls),e&&s(gn),Ve[Y].d(e),e&&s(as),e&&s(ce),e&&s(qn),w(Ae,e),e&&s(jn),e&&s(rs),e&&s(wn),w(me,e),e&&s(En),e&&s(C),e&&s(yn),e&&s(re),w(xe),e&&s(zn),e&&s(os),e&&s(An),w(Ie,e),e&&s(xn),e&&s(P),e&&s(In),w(Me,e),e&&s(Mn),e&&s(fe),e&&s(Pn),Je[K].d(e),e&&s(is),e&&s(us),e&&s(Tn),e&&s(A),e&&s(Fn),e&&s(oe),w(Pe),e&&s(Sn),e&&s(ps),e&&s(Cn),e&&s(_e),e&&s(Ln),e&&s(cs),e&&s(Nn),Re[X].d(e),e&&s(ms),e&&s(ds),e&&s(Dn),e&&s(be),e&&s(Hn),w(ve,e),e&&s(On),e&&s(ie),w(Se),e&&s(Bn),e&&s(se),e&&s(Un),e&&s(ke),e&&s(Vn),e&&s(ne),e&&s(Jn),e&&s(ge),e&&s(Rn),w(Ne,e)}}}const Jr={local:"manipulation-de-plusieurs-squences",sections:[{local:"les-modles-attendent-un-batch-dentres",title:"Les mod\xE8les attendent un batch d'entr\xE9es"},{local:"padding-des-entres",title:"*Padding* des entr\xE9es"},{local:"masques-dattention",title:"Masques d'attention"},{local:"squences-plus-longues",title:"S\xE9quences plus longues"}],title:"Manipulation de plusieurs s\xE9quences"};function Rr(k,a,u){let t="pt";return wr(()=>{const f=new URLSearchParams(window.location.search);u(0,t=f.get("fw")||"pt")}),[t]}class eo extends kr{constructor(a){super();gr(this,a,Rr,Vr,qr,{})}}export{eo as default,Jr as metadata};
