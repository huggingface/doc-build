import{S as Jp,i as Up,s as Bp,e as o,k as d,w as y,t as l,M as Rp,c as i,d as t,m as f,x,a as u,h as a,b as E,N as ll,G as s,g as m,y as P,o as b,p as ke,q as $,B as T,v as Qp,n as Ee}from"../../chunks/vendor-hf-doc-builder.js";import{T as Vp}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Gp}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Et}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as N}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Hp}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Yp}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Wp(g){let n,c;return n=new Hp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_tf.ipynb"}]}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function Xp(g){let n,c;return n=new Hp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section2_pt.ipynb"}]}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function Kp(g){let n;return{c(){n=l("Il s'agit de la premi\xE8re section dont le contenu est l\xE9g\xE8rement diff\xE9rent selon que vous utilisez PyTorch ou TensorFlow. Cliquez sur le bouton situ\xE9 au-dessus du titre pour s\xE9lectionner la plateforme que vous pr\xE9f\xE9rez !")},l(c){n=a(c,"Il s'agit de la premi\xE8re section dont le contenu est l\xE9g\xE8rement diff\xE9rent selon que vous utilisez PyTorch ou TensorFlow. Cliquez sur le bouton situ\xE9 au-dessus du titre pour s\xE9lectionner la plateforme que vous pr\xE9f\xE9rez !")},m(c,r){m(c,n,r)},d(c){c&&t(n)}}}function Zp(g){let n,c;return n=new Gp({props:{id:"wVN12smEvqg"}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function ec(g){let n,c;return n=new Gp({props:{id:"1pedAIvTWXk"}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function sc(g){let n,c;return n=new N({props:{code:`raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    # J'ai attendu un cours de HuggingFace toute ma vie.
    "I hate this so much!",  # Je d\xE9teste tellement \xE7a !
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)`,highlighted:`raw_inputs = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
    <span class="hljs-string">&quot;I hate this so much!&quot;</span>,  <span class="hljs-comment"># Je d\xE9teste tellement \xE7a !</span>
]
inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(inputs)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function tc(g){let n,c;return n=new N({props:{code:`raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    # J'ai attendu un cours de HuggingFace toute ma vie.
    "I hate this so much!",  # Je d\xE9teste tellement \xE7a !
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)`,highlighted:`raw_inputs = [
    <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
    <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
    <span class="hljs-string">&quot;I hate this so much!&quot;</span>,  <span class="hljs-comment"># Je d\xE9teste tellement \xE7a !</span>
]
inputs = tokenizer(raw_inputs, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function nc(g){let n,c,r,h,v;return h=new N({props:{code:`{
    'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
            [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
        ], dtype=int32)>, 
    'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
        array([
            [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
        ], dtype=int32)>
}`,highlighted:`{
    <span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
        array([
            [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>,  <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>],
            [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">5223</span>,  <span class="hljs-number">2023</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2172</span>,   <span class="hljs-number">999</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]
        ], dtype=int32)&gt;, 
    <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>), dtype=int32, numpy=
        array([
            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
            [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
        ], dtype=int32)&gt;
}`}}),{c(){n=o("p"),c=l("Voici \xE0 quoi ressemblent les r\xE9sultats sous forme de tenseurs TensorFlow :"),r=d(),y(h.$$.fragment)},l(_){n=i(_,"P",{});var M=u(n);c=a(M,"Voici \xE0 quoi ressemblent les r\xE9sultats sous forme de tenseurs TensorFlow :"),M.forEach(t),r=f(_),x(h.$$.fragment,_)},m(_,M){m(_,n,M),s(n,c),m(_,r,M),P(h,_,M),v=!0},i(_){v||($(h.$$.fragment,_),v=!0)},o(_){b(h.$$.fragment,_),v=!1},d(_){_&&t(n),_&&t(r),T(h,_)}}}function rc(g){let n,c,r,h,v;return h=new N({props:{code:`{
    'input_ids': tensor([
        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],
        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
    ]), 
    'attention_mask': tensor([
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}`,highlighted:`{
    <span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([
        [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">1005</span>,  <span class="hljs-number">2310</span>,  <span class="hljs-number">2042</span>,  <span class="hljs-number">3403</span>,  <span class="hljs-number">2005</span>,  <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>,  <span class="hljs-number">2026</span>,  <span class="hljs-number">2878</span>,  <span class="hljs-number">2166</span>,  <span class="hljs-number">1012</span>,   <span class="hljs-number">102</span>],
        [  <span class="hljs-number">101</span>,  <span class="hljs-number">1045</span>,  <span class="hljs-number">5223</span>,  <span class="hljs-number">2023</span>,  <span class="hljs-number">2061</span>,  <span class="hljs-number">2172</span>,   <span class="hljs-number">999</span>,   <span class="hljs-number">102</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]
    ]), 
    <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
    ])
}`}}),{c(){n=o("p"),c=l("Voici \xE0 quoi ressemblent les r\xE9sultats sous forme de tenseurs PyTorch :"),r=d(),y(h.$$.fragment)},l(_){n=i(_,"P",{});var M=u(n);c=a(M,"Voici \xE0 quoi ressemblent les r\xE9sultats sous forme de tenseurs PyTorch :"),M.forEach(t),r=f(_),x(h.$$.fragment,_)},m(_,M){m(_,n,M),s(n,c),m(_,r,M),P(h,_,M),v=!0},i(_){v||($(h.$$.fragment,_),v=!0)},o(_){b(h.$$.fragment,_),v=!1},d(_){_&&t(n),_&&t(r),T(h,_)}}}function lc(g){let n,c,r,h,v,_,M,A,C,z,F,k,q,I,D,S,U;return S=new N({props:{code:`from transformers import TFAutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModel.from_pretrained(checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = TFAutoModel.from_pretrained(checkpoint)`}}),{c(){n=o("p"),c=l("Nous pouvons t\xE9l\xE9charger notre mod\xE8le pr\xE9tra\xEEn\xE9 de la m\xEAme mani\xE8re que nous l\u2019avons fait avec notre "),r=o("em"),h=l("tokenizer"),v=l(". \u{1F917} "),_=o("em"),M=l("Transformers"),A=l(" fournit une classe "),C=o("code"),z=l("TFAutoModel"),F=l(" qui poss\xE8de \xE9galement une m\xE9thode "),k=o("code"),q=l("from_pretrained()"),I=l(" :"),D=d(),y(S.$$.fragment)},l(j){n=i(j,"P",{});var w=u(n);c=a(w,"Nous pouvons t\xE9l\xE9charger notre mod\xE8le pr\xE9tra\xEEn\xE9 de la m\xEAme mani\xE8re que nous l\u2019avons fait avec notre "),r=i(w,"EM",{});var X=u(r);h=a(X,"tokenizer"),X.forEach(t),v=a(w,". \u{1F917} "),_=i(w,"EM",{});var B=u(_);M=a(B,"Transformers"),B.forEach(t),A=a(w," fournit une classe "),C=i(w,"CODE",{});var ze=u(C);z=a(ze,"TFAutoModel"),ze.forEach(t),F=a(w," qui poss\xE8de \xE9galement une m\xE9thode "),k=i(w,"CODE",{});var K=u(k);q=a(K,"from_pretrained()"),K.forEach(t),I=a(w," :"),w.forEach(t),D=f(j),x(S.$$.fragment,j)},m(j,w){m(j,n,w),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A),s(n,C),s(C,z),s(n,F),s(n,k),s(k,q),s(n,I),m(j,D,w),P(S,j,w),U=!0},i(j){U||($(S.$$.fragment,j),U=!0)},o(j){b(S.$$.fragment,j),U=!1},d(j){j&&t(n),j&&t(D),T(S,j)}}}function ac(g){let n,c,r,h,v,_,M,A,C,z,F,k,q,I,D,S,U;return S=new N({props:{code:`from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModel.from_pretrained(checkpoint)`}}),{c(){n=o("p"),c=l("Nous pouvons t\xE9l\xE9charger notre mod\xE8le pr\xE9-entra\xEEn\xE9 de la m\xEAme mani\xE8re que nous l\u2019avons fait avec notre "),r=o("em"),h=l("tokenizer"),v=l(". \u{1F917} "),_=o("em"),M=l("Transformers"),A=l(" fournit une classe "),C=o("code"),z=l("AutoModel"),F=l(" qui poss\xE8de \xE9galement une m\xE9thode "),k=o("code"),q=l("from_pretrained()"),I=l(" :"),D=d(),y(S.$$.fragment)},l(j){n=i(j,"P",{});var w=u(n);c=a(w,"Nous pouvons t\xE9l\xE9charger notre mod\xE8le pr\xE9-entra\xEEn\xE9 de la m\xEAme mani\xE8re que nous l\u2019avons fait avec notre "),r=i(w,"EM",{});var X=u(r);h=a(X,"tokenizer"),X.forEach(t),v=a(w,". \u{1F917} "),_=i(w,"EM",{});var B=u(_);M=a(B,"Transformers"),B.forEach(t),A=a(w," fournit une classe "),C=i(w,"CODE",{});var ze=u(C);z=a(ze,"AutoModel"),ze.forEach(t),F=a(w," qui poss\xE8de \xE9galement une m\xE9thode "),k=i(w,"CODE",{});var K=u(k);q=a(K,"from_pretrained()"),K.forEach(t),I=a(w," :"),w.forEach(t),D=f(j),x(S.$$.fragment,j)},m(j,w){m(j,n,w),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A),s(n,C),s(C,z),s(n,F),s(n,k),s(k,q),s(n,I),m(j,D,w),P(S,j,w),U=!0},i(j){U||($(S.$$.fragment,j),U=!0)},o(j){b(S.$$.fragment,j),U=!1},d(j){j&&t(n),j&&t(D),T(S,j)}}}function oc(g){let n,c,r,h;return n=new N({props:{code:`outputs = model(inputs)
print(outputs.last_hidden_state.shape)`,highlighted:`outputs = model(inputs)
<span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)`}}),r=new N({props:{code:"(2, 16, 768)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">768</span>)'}}),{c(){y(n.$$.fragment),c=d(),y(r.$$.fragment)},l(v){x(n.$$.fragment,v),c=f(v),x(r.$$.fragment,v)},m(v,_){P(n,v,_),m(v,c,_),P(r,v,_),h=!0},i(v){h||($(n.$$.fragment,v),$(r.$$.fragment,v),h=!0)},o(v){b(n.$$.fragment,v),b(r.$$.fragment,v),h=!1},d(v){T(n,v),v&&t(c),T(r,v)}}}function ic(g){let n,c,r,h;return n=new N({props:{code:`outputs = model(**inputs)
print(outputs.last_hidden_state.shape)`,highlighted:`outputs = model(**inputs)
<span class="hljs-built_in">print</span>(outputs.last_hidden_state.shape)`}}),r=new N({props:{code:"torch.Size([2, 16, 768])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">16</span>, <span class="hljs-number">768</span>])'}}),{c(){y(n.$$.fragment),c=d(),y(r.$$.fragment)},l(v){x(n.$$.fragment,v),c=f(v),x(r.$$.fragment,v)},m(v,_){P(n,v,_),m(v,c,_),P(r,v,_),h=!0},i(v){h||($(n.$$.fragment,v),$(r.$$.fragment,v),h=!0)},o(v){b(n.$$.fragment,v),b(r.$$.fragment,v),h=!1},d(v){T(n,v),v&&t(c),T(r,v)}}}function uc(g){let n,c,r,h,v,_,M,A,C,z,F;return z=new N({props:{code:`from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)`}}),{c(){n=o("p"),c=l("Pour notre exemple, nous avons besoin d\u2019un mod\xE8le avec une t\xEAte de classification de s\xE9quence (pour pouvoir classer les phrases comme positives ou n\xE9gatives). Donc, nous n\u2019utilisons pas r\xE9ellement la classe "),r=o("code"),h=l("TFAutoModel"),v=l(" mais plut\xF4t "),_=o("code"),M=l("TFAutoModelForSequenceClassification"),A=l(" :"),C=d(),y(z.$$.fragment)},l(k){n=i(k,"P",{});var q=u(n);c=a(q,"Pour notre exemple, nous avons besoin d\u2019un mod\xE8le avec une t\xEAte de classification de s\xE9quence (pour pouvoir classer les phrases comme positives ou n\xE9gatives). Donc, nous n\u2019utilisons pas r\xE9ellement la classe "),r=i(q,"CODE",{});var I=u(r);h=a(I,"TFAutoModel"),I.forEach(t),v=a(q," mais plut\xF4t "),_=i(q,"CODE",{});var D=u(_);M=a(D,"TFAutoModelForSequenceClassification"),D.forEach(t),A=a(q," :"),q.forEach(t),C=f(k),x(z.$$.fragment,k)},m(k,q){m(k,n,q),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A),m(k,C,q),P(z,k,q),F=!0},i(k){F||($(z.$$.fragment,k),F=!0)},o(k){b(z.$$.fragment,k),F=!1},d(k){k&&t(n),k&&t(C),T(z,k)}}}function pc(g){let n,c,r,h,v,_,M,A,C,z,F;return z=new N({props:{code:`from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)`}}),{c(){n=o("p"),c=l("Pour notre exemple, nous avons besoin d\u2019un mod\xE8le avec une t\xEAte de classification de s\xE9quence (pour pouvoir classer les phrases comme positives ou n\xE9gatives). Donc, nous n\u2019utilisons pas r\xE9ellement la classe "),r=o("code"),h=l("AutoModel"),v=l(" mais plut\xF4t "),_=o("code"),M=l("AutoModelForSequenceClassification"),A=l(" :"),C=d(),y(z.$$.fragment)},l(k){n=i(k,"P",{});var q=u(n);c=a(q,"Pour notre exemple, nous avons besoin d\u2019un mod\xE8le avec une t\xEAte de classification de s\xE9quence (pour pouvoir classer les phrases comme positives ou n\xE9gatives). Donc, nous n\u2019utilisons pas r\xE9ellement la classe "),r=i(q,"CODE",{});var I=u(r);h=a(I,"AutoModel"),I.forEach(t),v=a(q," mais plut\xF4t "),_=i(q,"CODE",{});var D=u(_);M=a(D,"AutoModelForSequenceClassification"),D.forEach(t),A=a(q," :"),q.forEach(t),C=f(k),x(z.$$.fragment,k)},m(k,q){m(k,n,q),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A),m(k,C,q),P(z,k,q),F=!0},i(k){F||($(z.$$.fragment,k),F=!0)},o(k){b(z.$$.fragment,k),F=!1},d(k){k&&t(n),k&&t(C),T(z,k)}}}function cc(g){let n,c;return n=new N({props:{code:"(2, 2)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)'}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function mc(g){let n,c;return n=new N({props:{code:"torch.Size([2, 2])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])'}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function dc(g){let n,c;return n=new N({props:{code:`<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
    array([[-1.5606991,  1.6122842],
           [ 4.169231 , -3.3464472]], dtype=float32)>`,highlighted:`&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
    array([[-<span class="hljs-number">1.5606991</span>,  <span class="hljs-number">1.6122842</span>],
           [ <span class="hljs-number">4.169231</span> , -<span class="hljs-number">3.3464472</span>]], dtype=float32)&gt;`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function fc(g){let n,c;return n=new N({props:{code:`tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)`,highlighted:`tensor([[-<span class="hljs-number">1.5607</span>,  <span class="hljs-number">1.6123</span>],
        [ <span class="hljs-number">4.1692</span>, -<span class="hljs-number">3.3464</span>]], grad_fn=&lt;AddmmBackward&gt;)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function hc(g){let n,c;return n=new N({props:{code:`import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

predictions = tf.math.softmax(outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function _c(g){let n,c;return n=new N({props:{code:`import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)`,highlighted:`<span class="hljs-keyword">import</span> torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function vc(g){let n,c;return n=new N({props:{code:`tf.Tensor(
[[4.01951671e-02 9.59804833e-01]
 [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)`,highlighted:`tf.Tensor(
[[<span class="hljs-number">4.01951671e-02</span> <span class="hljs-number">9.59804833e-01</span>]
 [<span class="hljs-number">9.9945587e-01</span> <span class="hljs-number">5.4418424e-04</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function bc(g){let n,c;return n=new N({props:{code:`tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)`,highlighted:`tensor([[<span class="hljs-number">4.0195e-02</span>, <span class="hljs-number">9.5980e-01</span>],
        [<span class="hljs-number">9.9946e-01</span>, <span class="hljs-number">5.4418e-04</span>]], grad_fn=&lt;SoftmaxBackward&gt;)`}}),{c(){y(n.$$.fragment)},l(r){x(n.$$.fragment,r)},m(r,h){P(n,r,h),c=!0},i(r){c||($(n.$$.fragment,r),c=!0)},o(r){b(n.$$.fragment,r),c=!1},d(r){T(n,r)}}}function $c(g){let n,c,r,h,v,_,M,A;return{c(){n=o("p"),c=l("\u270F\uFE0F "),r=o("strong"),h=l("Essayez !"),v=l(" Choisissez deux (ou plus) textes de votre choix (en anglais) et faites-les passer par le pipeline "),_=o("code"),M=l("sentiment-analysis"),A=l(". Reproduisez ensuite vous-m\xEAme les \xE9tapes vues ici et v\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats !")},l(C){n=i(C,"P",{});var z=u(n);c=a(z,"\u270F\uFE0F "),r=i(z,"STRONG",{});var F=u(r);h=a(F,"Essayez !"),F.forEach(t),v=a(z," Choisissez deux (ou plus) textes de votre choix (en anglais) et faites-les passer par le pipeline "),_=i(z,"CODE",{});var k=u(_);M=a(k,"sentiment-analysis"),k.forEach(t),A=a(z,". Reproduisez ensuite vous-m\xEAme les \xE9tapes vues ici et v\xE9rifiez que vous obtenez les m\xEAmes r\xE9sultats !"),z.forEach(t)},m(C,z){m(C,n,z),s(n,c),s(n,r),s(r,h),s(n,v),s(n,_),s(_,M),s(n,A)},d(C){C&&t(n)}}}function gc(g){let n,c,r,h,v,_,M,A,C,z,F,k,q,I,D,S,U,j,w,X,B,ze,K,al,ol,Wn,ls,Xn,Ds,il,Kn,as,Zn,Le,ul,Ls,pl,cl,er,Ce,os,zi,ml,is,Ci,sr,Os,dl,tr,Ae,Oe,jt,us,fl,Vs,hl,qt,_l,nr,je,vl,wt,bl,$l,yt,gl,kl,rr,qe,ps,El,xt,jl,ql,wl,cs,yl,Pt,xl,Pl,Tl,Tt,Ml,lr,V,zl,ms,Mt,Cl,Al,zt,Nl,Il,Ct,Fl,Sl,At,Dl,Ll,Nt,Ol,Vl,ar,R,Gl,It,Hl,Jl,Ft,Ul,Bl,St,Rl,Ql,ds,Yl,Wl,or,fs,ir,Ve,Xl,Dt,Kl,Zl,ur,O,ea,Lt,sa,ta,Ot,na,ra,Vt,la,aa,Gt,oa,ia,Ht,ua,pa,Jt,ca,ma,pr,Ge,da,Ut,fa,ha,cr,ne,re,Gs,He,_a,Bt,va,ba,mr,le,ae,Hs,G,$a,Rt,ga,ka,Qt,Ea,ja,Yt,qa,wa,Wt,ya,xa,Xt,Pa,Ta,dr,Ne,Je,Kt,hs,Ma,Zt,za,fr,oe,ie,Js,Ue,Ca,en,Aa,Na,hr,Q,Ia,sn,Fa,Sa,tn,Da,La,nn,Oa,Va,Us,Ga,rn,Ha,Ja,_r,Bs,Ua,vr,we,Ba,ln,Ra,Qa,Rs,Ya,Wa,br,Ie,Be,an,_s,Xa,on,Ka,$r,Re,Za,un,eo,so,gr,ye,Qs,pn,to,no,ro,Ys,cn,lo,ao,oo,Ws,mn,io,uo,kr,Xs,po,Er,Ks,co,jr,ue,pe,Zs,Y,mo,dn,fo,ho,fn,_o,vo,hn,bo,$o,_n,go,ko,qr,Fe,Qe,vn,vs,Eo,bn,jo,wr,Se,bs,Ai,qo,$s,Ni,yr,Z,wo,$n,yo,xo,gn,Po,To,kn,Mo,zo,xr,L,et,En,Co,Ao,No,jn,qn,Io,Fo,wn,yn,So,Do,xn,Pn,Lo,Oo,Tn,Mn,Vo,Go,zn,Cn,Ho,Jo,An,Nn,Uo,Bo,In,Ro,Pr,ce,me,st,tt,Qo,Tr,gs,Mr,de,fe,nt,rt,Yo,zr,De,Ye,Fn,ks,Wo,Sn,Xo,Cr,lt,Ko,Ar,Es,Nr,he,_e,at,H,Zo,Dn,ei,si,Ln,ti,ni,On,ri,li,js,ai,oi,Vn,ii,ui,Ir,ve,be,ot,$e,ge,it,xe,pi,Gn,ci,mi,Hn,di,fi,Fr,We,hi,Jn,_i,vi,Sr,qs,Dr,ws,Lr,ut,bi,Or,Xe,Un,$i,gi,Bn,ki,Vr,Ke,Ei,Rn,ji,qi,Gr,Ze,Hr;r=new Yp({props:{fw:g[0]}}),A=new Et({});const Ii=[Xp,Wp],ys=[];function Fi(e,p){return e[0]==="pt"?0:1}q=Fi(g),I=ys[q]=Ii[q](g),S=new Vp({props:{$$slots:{default:[Kp]},$$scope:{ctx:g}}});const Si=[ec,Zp],xs=[];function Di(e,p){return e[0]==="pt"?0:1}j=Di(g),w=xs[j]=Si[j](g),ls=new N({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        # J'ai attendu un cours de HuggingFace toute ma vie.
        "I hate this so much!",  # Je d\xE9teste tellement \xE7a !
    ]
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)
classifier(
    [
        <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,
        <span class="hljs-comment"># J&#x27;ai attendu un cours de HuggingFace toute ma vie.</span>
        <span class="hljs-string">&quot;I hate this so much!&quot;</span>,  <span class="hljs-comment"># Je d\xE9teste tellement \xE7a !</span>
    ]
)`}}),as=new N({props:{code:`[{'label': 'POSITIVE', 'score': 0.9598047137260437},
 {'label': 'NEGATIVE', 'score': 0.9994558095932007}]`,highlighted:`[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9598047137260437</span>},
 {<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9994558095932007</span>}]`}}),us=new Et({}),fs=new N({props:{code:`from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)`}});const Li=[tc,sc],Ps=[];function Oi(e,p){return e[0]==="pt"?0:1}ne=Oi(g),re=Ps[ne]=Li[ne](g);const Vi=[rc,nc],Ts=[];function Gi(e,p){return e[0]==="pt"?0:1}le=Gi(g),ae=Ts[le]=Vi[le](g),hs=new Et({});const Hi=[ac,lc],Ms=[];function Ji(e,p){return e[0]==="pt"?0:1}oe=Ji(g),ie=Ms[oe]=Hi[oe](g),_s=new Et({});const Ui=[ic,oc],zs=[];function Bi(e,p){return e[0]==="pt"?0:1}ue=Bi(g),pe=zs[ue]=Ui[ue](g),vs=new Et({});const Ri=[pc,uc],Cs=[];function Qi(e,p){return e[0]==="pt"?0:1}ce=Qi(g),me=Cs[ce]=Ri[ce](g),gs=new N({props:{code:"print(outputs.logits.shape)",highlighted:'<span class="hljs-built_in">print</span>(outputs.logits.shape)'}});const Yi=[mc,cc],As=[];function Wi(e,p){return e[0]==="pt"?0:1}de=Wi(g),fe=As[de]=Yi[de](g),ks=new Et({}),Es=new N({props:{code:"print(outputs.logits)",highlighted:'<span class="hljs-built_in">print</span>(outputs.logits)'}});const Xi=[fc,dc],Ns=[];function Ki(e,p){return e[0]==="pt"?0:1}he=Ki(g),_e=Ns[he]=Xi[he](g);const Zi=[_c,hc],Is=[];function eu(e,p){return e[0]==="pt"?0:1}ve=eu(g),be=Is[ve]=Zi[ve](g);const su=[bc,vc],Fs=[];function tu(e,p){return e[0]==="pt"?0:1}return $e=tu(g),ge=Fs[$e]=su[$e](g),qs=new N({props:{code:"model.config.id2label",highlighted:"model.config.id2label"}}),ws=new N({props:{code:"{0: 'NEGATIVE', 1: 'POSITIVE'}",highlighted:'{<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;NEGATIVE&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>}'}}),Ze=new Vp({props:{$$slots:{default:[$c]},$$scope:{ctx:g}}}),{c(){n=o("meta"),c=d(),y(r.$$.fragment),h=d(),v=o("h1"),_=o("a"),M=o("span"),y(A.$$.fragment),C=d(),z=o("span"),F=l("Derri\xE8re le pipeline"),k=d(),I.c(),D=d(),y(S.$$.fragment),U=d(),w.c(),X=d(),B=o("p"),ze=l("Commen\xE7ons par un exemple complet en regardant ce qui s\u2019est pass\xE9 en coulisses lorsque nous avons ex\xE9cut\xE9 le code suivant dans le "),K=o("a"),al=l("chapitre 1"),ol=l(" :"),Wn=d(),y(ls.$$.fragment),Xn=d(),Ds=o("p"),il=l("la sortie :"),Kn=d(),y(as.$$.fragment),Zn=d(),Le=o("p"),ul=l("Comme nous l\u2019avons vu dans le "),Ls=o("a"),pl=l("chapitre 1"),cl=l(", ce pipeline regroupe trois \xE9tapes : le pr\xE9traitement, le passage des entr\xE9es dans le mod\xE8le et le post-traitement."),er=d(),Ce=o("div"),os=o("img"),ml=d(),is=o("img"),sr=d(),Os=o("p"),dl=l("Passons rapidement en revue chacun de ces \xE9l\xE9ments."),tr=d(),Ae=o("h2"),Oe=o("a"),jt=o("span"),y(us.$$.fragment),fl=d(),Vs=o("span"),hl=l("Pr\xE9traitement avec un "),qt=o("i"),_l=l("tokenizer"),nr=d(),je=o("p"),vl=l("Comme d\u2019autres r\xE9seaux de neurones, les "),wt=o("em"),bl=l("transformers"),$l=l(" ne peuvent pas traiter directement le texte brut, donc la premi\xE8re \xE9tape de notre pipeline est de convertir les entr\xE9es textuelles en nombres afin que le mod\xE8le puisse les comprendre. Pour ce faire, nous utilisons un "),yt=o("em"),gl=l("tokenizer"),kl=l(", qui sera responsable de :"),rr=d(),qe=o("ul"),ps=o("li"),El=l("diviser l\u2019entr\xE9e en mots, sous-mots, ou symboles (comme la ponctuation) qui sont appel\xE9s "),xt=o("em"),jl=l("tokens"),ql=l(","),wl=d(),cs=o("li"),yl=l("associer chaque "),Pt=o("em"),xl=l("token"),Pl=l(" \xE0 un nombre entier,"),Tl=d(),Tt=o("li"),Ml=l("ajouter des entr\xE9es suppl\xE9mentaires qui peuvent \xEAtre utiles au mod\xE8le."),lr=d(),V=o("p"),zl=l("Tout ce pr\xE9traitement doit \xEAtre effectu\xE9 exactement de la m\xEAme mani\xE8re que celui appliqu\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le. Nous devons donc d\u2019abord t\xE9l\xE9charger ces informations depuis le "),ms=o("a"),Mt=o("em"),Cl=l("Hub"),Al=l(". Pour ce faire, nous utilisons la classe "),zt=o("code"),Nl=l("AutoTokenizer"),Il=l(" et sa m\xE9thode "),Ct=o("code"),Fl=l("from_pretrained()"),Sl=l(". En utilisant le nom du "),At=o("em"),Dl=l("checkpoint"),Ll=l(" de notre mod\xE8le, elle va automatiquement r\xE9cup\xE9rer les donn\xE9es associ\xE9es au "),Nt=o("em"),Ol=l("tokenizer"),Vl=l(" du mod\xE8le et les mettre en cache (afin qu\u2019elles ne soient t\xE9l\xE9charg\xE9es que la premi\xE8re fois que vous ex\xE9cutez le code ci-dessous)."),ar=d(),R=o("p"),Gl=l("Puisque le "),It=o("em"),Hl=l("checkpoint"),Jl=l(" par d\xE9faut du pipeline "),Ft=o("code"),Ul=l("sentiment-analysis"),Bl=l(" (analyse de sentiment) est "),St=o("code"),Rl=l("distilbert-base-uncased-finetuned-sst-2-english"),Ql=l(" (vous pouvez voir la carte de ce mod\xE8le "),ds=o("a"),Yl=l("ici"),Wl=l("), nous ex\xE9cutons ce qui suit :"),or=d(),y(fs.$$.fragment),ir=d(),Ve=o("p"),Xl=l("Une fois que nous avons le "),Dt=o("em"),Kl=l("tokenizer"),Zl=l(" nous pouvons lui passer directement nos phrases et obtenir un dictionnaire pr\xEAt \xE0 \xEAtre donn\xE9 \xE0 notre mod\xE8le ! La seule chose qui reste \xE0 faire est de convertir en tenseurs la liste des identifiants d\u2019entr\xE9e."),ur=d(),O=o("p"),ea=l("Vous pouvez utiliser \u{1F917} "),Lt=o("em"),sa=l("Transformers"),ta=l(" sans avoir \xE0 vous soucier du "),Ot=o("em"),na=l("framework"),ra=l(" utilis\xE9 comme "),Vt=o("em"),la=l("backend"),aa=l(". Il peut s\u2019agir de PyTorch, de TensorFlow ou de Flax pour certains mod\xE8les. Cependant, les "),Gt=o("em"),oa=l("transformers"),ia=l(" n\u2019acceptent que les "),Ht=o("em"),ua=l("tenseurs"),pa=l(" en entr\xE9e. Si c\u2019est la premi\xE8re fois que vous entendez parler de tenseurs, vous pouvez les consid\xE9rer comme des tableaux NumPy. Un tableau NumPy peut \xEAtre un scalaire (0D), un vecteur (1D), une matrice (2D), ou avoir davantage de dimensions. Les tenseurs des autres "),Jt=o("em"),ca=l("frameworks"),ma=l(" d\u2019apprentissage machine se comportent de mani\xE8re similaire et sont g\xE9n\xE9ralement aussi simples \xE0 instancier que les tableaux NumPy."),pr=d(),Ge=o("p"),da=l("Pour sp\xE9cifier le type de tenseurs que nous voulons r\xE9cup\xE9rer (PyTorch, TensorFlow, ou simplement NumPy), nous utilisons l\u2019argument "),Ut=o("code"),fa=l("return_tensors"),ha=l(" :"),cr=d(),re.c(),Gs=d(),He=o("p"),_a=l("Ne vous pr\xE9occupez pas encore du remplissage ("),Bt=o("em"),va=l("padding"),ba=l(") et de la troncature, nous les expliquerons plus tard. Les principales choses \xE0 retenir ici sont que vous pouvez passer une phrase ou une liste de phrases, ainsi que sp\xE9cifier le type de tenseurs que vous voulez r\xE9cup\xE9rer (si aucun type n\u2019est pass\xE9, par d\xE9faut vous obtiendrez une liste de listes comme r\xE9sultat)."),mr=d(),ae.c(),Hs=d(),G=o("p"),$a=l("La sortie elle-m\xEAme est un dictionnaire contenant deux cl\xE9s : "),Rt=o("code"),ga=l("input_ids"),ka=l(" et "),Qt=o("code"),Ea=l("attention_mask"),ja=l(". "),Yt=o("code"),qa=l("input_ids"),wa=l(" contient deux lignes d\u2019entiers (une pour chaque phrase) qui sont les identifiants uniques des "),Wt=o("em"),ya=l("tokens"),xa=l(" dans chaque phrase. Nous expliquerons ce qu\u2019est l\u2019"),Xt=o("code"),Pa=l("attention_mask"),Ta=l(" plus tard dans ce chapitre."),dr=d(),Ne=o("h2"),Je=o("a"),Kt=o("span"),y(hs.$$.fragment),Ma=d(),Zt=o("span"),za=l("Passage au mod\xE8le"),fr=d(),ie.c(),Js=d(),Ue=o("p"),Ca=l("Dans cet extrait de code, nous avons t\xE9l\xE9charg\xE9 le m\xEAme "),en=o("em"),Aa=l("checkpoint"),Na=l(" que nous avons utilis\xE9 dans notre pipeline auparavant (il devrait en fait avoir d\xE9j\xE0 \xE9t\xE9 mis en cache) et instanci\xE9 un mod\xE8le avec lui."),hr=d(),Q=o("p"),Ia=l("Cette architecture ne contient que le module de "),sn=o("em"),Fa=l("transformer"),Sa=l(" de base : \xE9tant donn\xE9 certaines entr\xE9es, il produit ce que nous appellerons des "),tn=o("em"),Da=l("\xE9tats cach\xE9s"),La=l(", \xE9galement connus sous le nom de "),nn=o("em"),Oa=l("caract\xE9ristiques"),Va=l(`.
Pour chaque entr\xE9e du mod\xE8le, nous r\xE9cup\xE9rons un vecteur en grande dimension repr\xE9sentant la `),Us=o("strong"),Ga=l("compr\xE9hension contextuelle de cette entr\xE9e par le "),rn=o("em"),Ha=l("transformer"),Ja=l("."),_r=d(),Bs=o("p"),Ua=l("Si cela ne fait pas sens, ne vous inqui\xE9tez pas. Nous expliquons tout plus tard."),vr=d(),we=o("p"),Ba=l("Bien que ces \xE9tats cach\xE9s puissent \xEAtre utiles en eux-m\xEAmes, ils sont g\xE9n\xE9ralement les entr\xE9es d\u2019une autre partie du mod\xE8le, connue sous le nom de "),ln=o("em"),Ra=l("t\xEAte"),Qa=l(". Dans le "),Rs=o("a"),Ya=l("chapitre 1"),Wa=l(", les diff\xE9rentes t\xE2ches auraient pu \xEAtre r\xE9alis\xE9es avec la m\xEAme architecture mais en ayant chacune d\u2019elles une t\xEAte diff\xE9rente."),br=d(),Ie=o("h3"),Be=o("a"),an=o("span"),y(_s.$$.fragment),Xa=d(),on=o("span"),Ka=l("Un vecteur de grande dimension ?"),$r=d(),Re=o("p"),Za=l("Le vecteur produit en sortie par le "),un=o("em"),eo=l("transformer"),so=l(" est g\xE9n\xE9ralement de grande dimension. Il a g\xE9n\xE9ralement trois dimensions :"),gr=d(),ye=o("ul"),Qs=o("li"),pn=o("strong"),to=l("la taille du lot"),no=l(" : le nombre de s\xE9quences trait\xE9es \xE0 la fois (2 dans notre exemple),"),ro=d(),Ys=o("li"),cn=o("strong"),lo=l("la longueur de la s\xE9quence"),ao=l(" : la longueur de la repr\xE9sentation num\xE9rique de la s\xE9quence (16 dans notre exemple),"),oo=d(),Ws=o("li"),mn=o("strong"),io=l("la taille cach\xE9e"),uo=l(" : la dimension du vecteur de chaque entr\xE9e du mod\xE8le."),kr=d(),Xs=o("p"),po=l("On dit qu\u2019il est de \xAB grande dimension \xBB en raison de la derni\xE8re valeur. La taille cach\xE9e peut \xEAtre tr\xE8s grande (g\xE9n\xE9ralement 768 pour les petits mod\xE8les et pour les grands mod\xE8les cela peut atteindre 3072 voire plus)."),Er=d(),Ks=o("p"),co=l("Nous pouvons le constater si nous alimentons notre mod\xE8le avec les entr\xE9es que nous avons pr\xE9trait\xE9es :"),jr=d(),pe.c(),Zs=d(),Y=o("p"),mo=l("Notez que les sorties des mod\xE8les de la biblioth\xE8que \u{1F917} "),dn=o("em"),fo=l("Transformers"),ho=l(" se comportent comme des "),fn=o("code"),_o=l("namedtuples"),vo=l(" ou des dictionnaires. Vous pouvez acc\xE9der aux \xE9l\xE9ments par attributs (comme nous l\u2019avons fait), par cl\xE9 ("),hn=o("code"),bo=l('outputs["last_hidden_state"]'),$o=l("), ou m\xEAme par l\u2019index si vous savez exactement o\xF9 se trouve la chose que vous cherchez ("),_n=o("code"),go=l("outputs[0]"),ko=l(")."),qr=d(),Fe=o("h3"),Qe=o("a"),vn=o("span"),y(vs.$$.fragment),Eo=d(),bn=o("span"),jo=l("Les t\xEAtes des mod\xE8les : donner du sens aux chiffres"),wr=l(`

Les t\xEAtes des mod\xE8les prennent en entr\xE9e le vecteur de grande dimension des \xE9tats cach\xE9s et le projettent sur une autre dimension. Elles sont g\xE9n\xE9ralement compos\xE9es d'une ou de quelques couches lin\xE9aires :
`),Se=o("div"),bs=o("img"),qo=d(),$s=o("img"),yr=d(),Z=o("p"),wo=l("La sortie du "),$n=o("em"),yo=l("transformer"),xo=l(` est envoy\xE9e directement \xE0 la t\xEAte du mod\xE8le pour \xEAtre trait\xE9e.
Dans ce diagramme, le mod\xE8le est repr\xE9sent\xE9 par sa couche d\u2019ench\xE2ssement et les couches suivantes. La couche d\u2019ench\xE2ssement convertit chaque identifiant d\u2019entr\xE9e dans l\u2019entr\xE9e tokenis\xE9e en un vecteur qui repr\xE9sente le `),gn=o("em"),Po=l("token"),To=l(` associ\xE9. Les couches suivantes manipulent ces vecteurs en utilisant le m\xE9canisme d\u2019attention pour produire la repr\xE9sentation finale des phrases.
Il existe de nombreuses architectures diff\xE9rentes disponibles dans la biblioth\xE8que \u{1F917} `),kn=o("em"),Mo=l("Transformers"),zo=l(", chacune \xE9tant con\xE7ue autour de la prise en charge d\u2019une t\xE2che sp\xE9cifique. En voici une liste non exhaustive :"),xr=d(),L=o("ul"),et=o("li"),En=o("code"),Co=l("*Model"),Ao=l(" (r\xE9cup\xE9rer les \xE9tats cach\xE9s)"),No=d(),jn=o("li"),qn=o("code"),Io=l("*ForCausalLM"),Fo=d(),wn=o("li"),yn=o("code"),So=l("*ForMaskedLM"),Do=d(),xn=o("li"),Pn=o("code"),Lo=l("*ForMultipleChoice"),Oo=d(),Tn=o("li"),Mn=o("code"),Vo=l("*ForQuestionAnswering"),Go=d(),zn=o("li"),Cn=o("code"),Ho=l("*ForSequenceClassification"),Jo=d(),An=o("li"),Nn=o("code"),Uo=l("*ForTokenClassification"),Bo=d(),In=o("li"),Ro=l("et autres \u{1F917}"),Pr=d(),me.c(),st=d(),tt=o("p"),Qo=l("Maintenant, si nous examinons la forme de nos entr\xE9es, la dimensionnalit\xE9 est beaucoup plus faible. La t\xEAte du mod\xE8le prend en entr\xE9e les vecteurs de grande dimension que nous avons vus pr\xE9c\xE9demment et elle produit des vecteurs contenant deux valeurs (une par \xE9tiquette) :"),Tr=d(),y(gs.$$.fragment),Mr=d(),fe.c(),nt=d(),rt=o("p"),Yo=l("Comme nous n\u2019avons que deux phrases et deux \xE9tiquettes, le r\xE9sultat que nous obtenons est de forme 2 x 2"),zr=d(),De=o("h2"),Ye=o("a"),Fn=o("span"),y(ks.$$.fragment),Wo=d(),Sn=o("span"),Xo=l("Post-traitement de la sortie"),Cr=d(),lt=o("p"),Ko=l("Les valeurs que nous obtenons en sortie de notre mod\xE8le n\u2019ont pas n\xE9cessairement de sens en elles-m\xEAmes. Jetons-y un coup d\u2019\u0153il  :"),Ar=d(),y(Es.$$.fragment),Nr=d(),_e.c(),at=d(),H=o("p"),Zo=l("Notre mod\xE8le a pr\xE9dit "),Dn=o("code"),ei=l("[-1.5607, 1.6123]"),si=l(" pour la premi\xE8re phrase et "),Ln=o("code"),ti=l("[ 4.1692, -3.3464]"),ni=l(" pour la seconde. Ce ne sont pas des probabilit\xE9s mais des "),On=o("em"),ri=l("logits"),li=l(", les scores bruts, non normalis\xE9s, produits par la derni\xE8re couche du mod\xE8le. Pour \xEAtre convertis en probabilit\xE9s, ils doivent passer par une couche "),js=o("a"),ai=l("SoftMax"),oi=l(" (tous les mod\xE8les de la biblioth\xE8que \u{1F917} "),Vn=o("em"),ii=l("Transformers"),ui=l(" sortent les logits car la fonction de perte de l\u2019entra\xEEnement fusionne g\xE9n\xE9ralement la derni\xE8re fonction d\u2019activation, comme la SoftMax, avec la fonction de perte r\xE9elle, comme l\u2019entropie crois\xE9e) :"),Ir=d(),be.c(),ot=d(),ge.c(),it=d(),xe=o("p"),pi=l("Maintenant nous pouvons voir que le mod\xE8le a pr\xE9dit "),Gn=o("code"),ci=l("[0.0402, 0.9598]"),mi=l(" pour la premi\xE8re phrase et "),Hn=o("code"),di=l("[0.9995, 0.0005]"),fi=l(" pour la seconde. Ce sont des scores de probabilit\xE9 reconnaissables."),Fr=d(),We=o("p"),hi=l("Pour obtenir les \xE9tiquettes correspondant \xE0 chaque position, nous pouvons inspecter l\u2019attribut "),Jn=o("code"),_i=l("id2label"),vi=l(" de la configuration du mod\xE8le (plus de d\xE9tails dans la section suivante) :"),Sr=d(),y(qs.$$.fragment),Dr=d(),y(ws.$$.fragment),Lr=d(),ut=o("p"),bi=l("Nous pouvons maintenant conclure que le mod\xE8le a pr\xE9dit ce qui suit :"),Or=d(),Xe=o("ul"),Un=o("li"),$i=l("premi\xE8re phrase : NEGATIVE: 0.0402, POSITIVE: 0.9598"),gi=d(),Bn=o("li"),ki=l("deuxi\xE8me phrase : NEGATIVE: 0.9995, POSITIVE: 0.0005"),Vr=d(),Ke=o("p"),Ei=l("Nous avons reproduit avec succ\xE8s les trois \xE9tapes du pipeline : pr\xE9traitement avec les "),Rn=o("em"),ji=l("tokenizers"),qi=l(", passage des entr\xE9es dans le mod\xE8le et post-traitement ! Prenons maintenant le temps de nous plonger plus profond\xE9ment dans chacune de ces \xE9tapes."),Gr=d(),y(Ze.$$.fragment),this.h()},l(e){const p=Rp('[data-svelte="svelte-1phssyn"]',document.head);n=i(p,"META",{name:!0,content:!0}),p.forEach(t),c=f(e),x(r.$$.fragment,e),h=f(e),v=i(e,"H1",{class:!0});var Ss=u(v);_=i(Ss,"A",{id:!0,class:!0,href:!0});var pt=u(_);M=i(pt,"SPAN",{});var Qn=u(M);x(A.$$.fragment,Qn),Qn.forEach(t),pt.forEach(t),C=f(Ss),z=i(Ss,"SPAN",{});var ct=u(z);F=a(ct,"Derri\xE8re le pipeline"),ct.forEach(t),Ss.forEach(t),k=f(e),I.l(e),D=f(e),x(S.$$.fragment,e),U=f(e),w.l(e),X=f(e),B=i(e,"P",{});var es=u(B);ze=a(es,"Commen\xE7ons par un exemple complet en regardant ce qui s\u2019est pass\xE9 en coulisses lorsque nous avons ex\xE9cut\xE9 le code suivant dans le "),K=i(es,"A",{href:!0});var mt=u(K);al=a(mt,"chapitre 1"),mt.forEach(t),ol=a(es," :"),es.forEach(t),Wn=f(e),x(ls.$$.fragment,e),Xn=f(e),Ds=i(e,"P",{});var dt=u(Ds);il=a(dt,"la sortie :"),dt.forEach(t),Kn=f(e),x(as.$$.fragment,e),Zn=f(e),Le=i(e,"P",{});var ss=u(Le);ul=a(ss,"Comme nous l\u2019avons vu dans le "),Ls=i(ss,"A",{href:!0});var ft=u(Ls);pl=a(ft,"chapitre 1"),ft.forEach(t),cl=a(ss,", ce pipeline regroupe trois \xE9tapes : le pr\xE9traitement, le passage des entr\xE9es dans le mod\xE8le et le post-traitement."),ss.forEach(t),er=f(e),Ce=i(e,"DIV",{class:!0});var ts=u(Ce);os=i(ts,"IMG",{class:!0,src:!0,alt:!0}),ml=f(ts),is=i(ts,"IMG",{class:!0,src:!0,alt:!0}),ts.forEach(t),sr=f(e),Os=i(e,"P",{});var ht=u(Os);dl=a(ht,"Passons rapidement en revue chacun de ces \xE9l\xE9ments."),ht.forEach(t),tr=f(e),Ae=i(e,"H2",{class:!0});var ns=u(Ae);Oe=i(ns,"A",{id:!0,class:!0,href:!0});var _t=u(Oe);jt=i(_t,"SPAN",{});var Yn=u(jt);x(us.$$.fragment,Yn),Yn.forEach(t),_t.forEach(t),fl=f(ns),Vs=i(ns,"SPAN",{});var wi=u(Vs);hl=a(wi,"Pr\xE9traitement avec un "),qt=i(wi,"I",{});var nu=u(qt);_l=a(nu,"tokenizer"),nu.forEach(t),wi.forEach(t),ns.forEach(t),nr=f(e),je=i(e,"P",{});var vt=u(je);vl=a(vt,"Comme d\u2019autres r\xE9seaux de neurones, les "),wt=i(vt,"EM",{});var ru=u(wt);bl=a(ru,"transformers"),ru.forEach(t),$l=a(vt," ne peuvent pas traiter directement le texte brut, donc la premi\xE8re \xE9tape de notre pipeline est de convertir les entr\xE9es textuelles en nombres afin que le mod\xE8le puisse les comprendre. Pour ce faire, nous utilisons un "),yt=i(vt,"EM",{});var lu=u(yt);gl=a(lu,"tokenizer"),lu.forEach(t),kl=a(vt,", qui sera responsable de :"),vt.forEach(t),rr=f(e),qe=i(e,"UL",{});var bt=u(qe);ps=i(bt,"LI",{});var Jr=u(ps);El=a(Jr,"diviser l\u2019entr\xE9e en mots, sous-mots, ou symboles (comme la ponctuation) qui sont appel\xE9s "),xt=i(Jr,"EM",{});var au=u(xt);jl=a(au,"tokens"),au.forEach(t),ql=a(Jr,","),Jr.forEach(t),wl=f(bt),cs=i(bt,"LI",{});var Ur=u(cs);yl=a(Ur,"associer chaque "),Pt=i(Ur,"EM",{});var ou=u(Pt);xl=a(ou,"token"),ou.forEach(t),Pl=a(Ur," \xE0 un nombre entier,"),Ur.forEach(t),Tl=f(bt),Tt=i(bt,"LI",{});var iu=u(Tt);Ml=a(iu,"ajouter des entr\xE9es suppl\xE9mentaires qui peuvent \xEAtre utiles au mod\xE8le."),iu.forEach(t),bt.forEach(t),lr=f(e),V=i(e,"P",{});var ee=u(V);zl=a(ee,"Tout ce pr\xE9traitement doit \xEAtre effectu\xE9 exactement de la m\xEAme mani\xE8re que celui appliqu\xE9 lors du pr\xE9-entra\xEEnement du mod\xE8le. Nous devons donc d\u2019abord t\xE9l\xE9charger ces informations depuis le "),ms=i(ee,"A",{href:!0,rel:!0});var uu=u(ms);Mt=i(uu,"EM",{});var pu=u(Mt);Cl=a(pu,"Hub"),pu.forEach(t),uu.forEach(t),Al=a(ee,". Pour ce faire, nous utilisons la classe "),zt=i(ee,"CODE",{});var cu=u(zt);Nl=a(cu,"AutoTokenizer"),cu.forEach(t),Il=a(ee," et sa m\xE9thode "),Ct=i(ee,"CODE",{});var mu=u(Ct);Fl=a(mu,"from_pretrained()"),mu.forEach(t),Sl=a(ee,". En utilisant le nom du "),At=i(ee,"EM",{});var du=u(At);Dl=a(du,"checkpoint"),du.forEach(t),Ll=a(ee," de notre mod\xE8le, elle va automatiquement r\xE9cup\xE9rer les donn\xE9es associ\xE9es au "),Nt=i(ee,"EM",{});var fu=u(Nt);Ol=a(fu,"tokenizer"),fu.forEach(t),Vl=a(ee," du mod\xE8le et les mettre en cache (afin qu\u2019elles ne soient t\xE9l\xE9charg\xE9es que la premi\xE8re fois que vous ex\xE9cutez le code ci-dessous)."),ee.forEach(t),ar=f(e),R=i(e,"P",{});var Pe=u(R);Gl=a(Pe,"Puisque le "),It=i(Pe,"EM",{});var hu=u(It);Hl=a(hu,"checkpoint"),hu.forEach(t),Jl=a(Pe," par d\xE9faut du pipeline "),Ft=i(Pe,"CODE",{});var _u=u(Ft);Ul=a(_u,"sentiment-analysis"),_u.forEach(t),Bl=a(Pe," (analyse de sentiment) est "),St=i(Pe,"CODE",{});var vu=u(St);Rl=a(vu,"distilbert-base-uncased-finetuned-sst-2-english"),vu.forEach(t),Ql=a(Pe," (vous pouvez voir la carte de ce mod\xE8le "),ds=i(Pe,"A",{href:!0,rel:!0});var bu=u(ds);Yl=a(bu,"ici"),bu.forEach(t),Wl=a(Pe,"), nous ex\xE9cutons ce qui suit :"),Pe.forEach(t),or=f(e),x(fs.$$.fragment,e),ir=f(e),Ve=i(e,"P",{});var Br=u(Ve);Xl=a(Br,"Une fois que nous avons le "),Dt=i(Br,"EM",{});var $u=u(Dt);Kl=a($u,"tokenizer"),$u.forEach(t),Zl=a(Br," nous pouvons lui passer directement nos phrases et obtenir un dictionnaire pr\xEAt \xE0 \xEAtre donn\xE9 \xE0 notre mod\xE8le ! La seule chose qui reste \xE0 faire est de convertir en tenseurs la liste des identifiants d\u2019entr\xE9e."),Br.forEach(t),ur=f(e),O=i(e,"P",{});var W=u(O);ea=a(W,"Vous pouvez utiliser \u{1F917} "),Lt=i(W,"EM",{});var gu=u(Lt);sa=a(gu,"Transformers"),gu.forEach(t),ta=a(W," sans avoir \xE0 vous soucier du "),Ot=i(W,"EM",{});var ku=u(Ot);na=a(ku,"framework"),ku.forEach(t),ra=a(W," utilis\xE9 comme "),Vt=i(W,"EM",{});var Eu=u(Vt);la=a(Eu,"backend"),Eu.forEach(t),aa=a(W,". Il peut s\u2019agir de PyTorch, de TensorFlow ou de Flax pour certains mod\xE8les. Cependant, les "),Gt=i(W,"EM",{});var ju=u(Gt);oa=a(ju,"transformers"),ju.forEach(t),ia=a(W," n\u2019acceptent que les "),Ht=i(W,"EM",{});var qu=u(Ht);ua=a(qu,"tenseurs"),qu.forEach(t),pa=a(W," en entr\xE9e. Si c\u2019est la premi\xE8re fois que vous entendez parler de tenseurs, vous pouvez les consid\xE9rer comme des tableaux NumPy. Un tableau NumPy peut \xEAtre un scalaire (0D), un vecteur (1D), une matrice (2D), ou avoir davantage de dimensions. Les tenseurs des autres "),Jt=i(W,"EM",{});var wu=u(Jt);ca=a(wu,"frameworks"),wu.forEach(t),ma=a(W," d\u2019apprentissage machine se comportent de mani\xE8re similaire et sont g\xE9n\xE9ralement aussi simples \xE0 instancier que les tableaux NumPy."),W.forEach(t),pr=f(e),Ge=i(e,"P",{});var Rr=u(Ge);da=a(Rr,"Pour sp\xE9cifier le type de tenseurs que nous voulons r\xE9cup\xE9rer (PyTorch, TensorFlow, ou simplement NumPy), nous utilisons l\u2019argument "),Ut=i(Rr,"CODE",{});var yu=u(Ut);fa=a(yu,"return_tensors"),yu.forEach(t),ha=a(Rr," :"),Rr.forEach(t),cr=f(e),re.l(e),Gs=f(e),He=i(e,"P",{});var Qr=u(He);_a=a(Qr,"Ne vous pr\xE9occupez pas encore du remplissage ("),Bt=i(Qr,"EM",{});var xu=u(Bt);va=a(xu,"padding"),xu.forEach(t),ba=a(Qr,") et de la troncature, nous les expliquerons plus tard. Les principales choses \xE0 retenir ici sont que vous pouvez passer une phrase ou une liste de phrases, ainsi que sp\xE9cifier le type de tenseurs que vous voulez r\xE9cup\xE9rer (si aucun type n\u2019est pass\xE9, par d\xE9faut vous obtiendrez une liste de listes comme r\xE9sultat)."),Qr.forEach(t),mr=f(e),ae.l(e),Hs=f(e),G=i(e,"P",{});var se=u(G);$a=a(se,"La sortie elle-m\xEAme est un dictionnaire contenant deux cl\xE9s : "),Rt=i(se,"CODE",{});var Pu=u(Rt);ga=a(Pu,"input_ids"),Pu.forEach(t),ka=a(se," et "),Qt=i(se,"CODE",{});var Tu=u(Qt);Ea=a(Tu,"attention_mask"),Tu.forEach(t),ja=a(se,". "),Yt=i(se,"CODE",{});var Mu=u(Yt);qa=a(Mu,"input_ids"),Mu.forEach(t),wa=a(se," contient deux lignes d\u2019entiers (une pour chaque phrase) qui sont les identifiants uniques des "),Wt=i(se,"EM",{});var zu=u(Wt);ya=a(zu,"tokens"),zu.forEach(t),xa=a(se," dans chaque phrase. Nous expliquerons ce qu\u2019est l\u2019"),Xt=i(se,"CODE",{});var Cu=u(Xt);Pa=a(Cu,"attention_mask"),Cu.forEach(t),Ta=a(se," plus tard dans ce chapitre."),se.forEach(t),dr=f(e),Ne=i(e,"H2",{class:!0});var Yr=u(Ne);Je=i(Yr,"A",{id:!0,class:!0,href:!0});var Au=u(Je);Kt=i(Au,"SPAN",{});var Nu=u(Kt);x(hs.$$.fragment,Nu),Nu.forEach(t),Au.forEach(t),Ma=f(Yr),Zt=i(Yr,"SPAN",{});var Iu=u(Zt);za=a(Iu,"Passage au mod\xE8le"),Iu.forEach(t),Yr.forEach(t),fr=f(e),ie.l(e),Js=f(e),Ue=i(e,"P",{});var Wr=u(Ue);Ca=a(Wr,"Dans cet extrait de code, nous avons t\xE9l\xE9charg\xE9 le m\xEAme "),en=i(Wr,"EM",{});var Fu=u(en);Aa=a(Fu,"checkpoint"),Fu.forEach(t),Na=a(Wr," que nous avons utilis\xE9 dans notre pipeline auparavant (il devrait en fait avoir d\xE9j\xE0 \xE9t\xE9 mis en cache) et instanci\xE9 un mod\xE8le avec lui."),Wr.forEach(t),hr=f(e),Q=i(e,"P",{});var Te=u(Q);Ia=a(Te,"Cette architecture ne contient que le module de "),sn=i(Te,"EM",{});var Su=u(sn);Fa=a(Su,"transformer"),Su.forEach(t),Sa=a(Te," de base : \xE9tant donn\xE9 certaines entr\xE9es, il produit ce que nous appellerons des "),tn=i(Te,"EM",{});var Du=u(tn);Da=a(Du,"\xE9tats cach\xE9s"),Du.forEach(t),La=a(Te,", \xE9galement connus sous le nom de "),nn=i(Te,"EM",{});var Lu=u(nn);Oa=a(Lu,"caract\xE9ristiques"),Lu.forEach(t),Va=a(Te,`.
Pour chaque entr\xE9e du mod\xE8le, nous r\xE9cup\xE9rons un vecteur en grande dimension repr\xE9sentant la `),Us=i(Te,"STRONG",{});var yi=u(Us);Ga=a(yi,"compr\xE9hension contextuelle de cette entr\xE9e par le "),rn=i(yi,"EM",{});var Ou=u(rn);Ha=a(Ou,"transformer"),Ou.forEach(t),yi.forEach(t),Ja=a(Te,"."),Te.forEach(t),_r=f(e),Bs=i(e,"P",{});var Vu=u(Bs);Ua=a(Vu,"Si cela ne fait pas sens, ne vous inqui\xE9tez pas. Nous expliquons tout plus tard."),Vu.forEach(t),vr=f(e),we=i(e,"P",{});var $t=u(we);Ba=a($t,"Bien que ces \xE9tats cach\xE9s puissent \xEAtre utiles en eux-m\xEAmes, ils sont g\xE9n\xE9ralement les entr\xE9es d\u2019une autre partie du mod\xE8le, connue sous le nom de "),ln=i($t,"EM",{});var Gu=u(ln);Ra=a(Gu,"t\xEAte"),Gu.forEach(t),Qa=a($t,". Dans le "),Rs=i($t,"A",{href:!0});var Hu=u(Rs);Ya=a(Hu,"chapitre 1"),Hu.forEach(t),Wa=a($t,", les diff\xE9rentes t\xE2ches auraient pu \xEAtre r\xE9alis\xE9es avec la m\xEAme architecture mais en ayant chacune d\u2019elles une t\xEAte diff\xE9rente."),$t.forEach(t),br=f(e),Ie=i(e,"H3",{class:!0});var Xr=u(Ie);Be=i(Xr,"A",{id:!0,class:!0,href:!0});var Ju=u(Be);an=i(Ju,"SPAN",{});var Uu=u(an);x(_s.$$.fragment,Uu),Uu.forEach(t),Ju.forEach(t),Xa=f(Xr),on=i(Xr,"SPAN",{});var Bu=u(on);Ka=a(Bu,"Un vecteur de grande dimension ?"),Bu.forEach(t),Xr.forEach(t),$r=f(e),Re=i(e,"P",{});var Kr=u(Re);Za=a(Kr,"Le vecteur produit en sortie par le "),un=i(Kr,"EM",{});var Ru=u(un);eo=a(Ru,"transformer"),Ru.forEach(t),so=a(Kr," est g\xE9n\xE9ralement de grande dimension. Il a g\xE9n\xE9ralement trois dimensions :"),Kr.forEach(t),gr=f(e),ye=i(e,"UL",{});var gt=u(ye);Qs=i(gt,"LI",{});var xi=u(Qs);pn=i(xi,"STRONG",{});var Qu=u(pn);to=a(Qu,"la taille du lot"),Qu.forEach(t),no=a(xi," : le nombre de s\xE9quences trait\xE9es \xE0 la fois (2 dans notre exemple),"),xi.forEach(t),ro=f(gt),Ys=i(gt,"LI",{});var Pi=u(Ys);cn=i(Pi,"STRONG",{});var Yu=u(cn);lo=a(Yu,"la longueur de la s\xE9quence"),Yu.forEach(t),ao=a(Pi," : la longueur de la repr\xE9sentation num\xE9rique de la s\xE9quence (16 dans notre exemple),"),Pi.forEach(t),oo=f(gt),Ws=i(gt,"LI",{});var Ti=u(Ws);mn=i(Ti,"STRONG",{});var Wu=u(mn);io=a(Wu,"la taille cach\xE9e"),Wu.forEach(t),uo=a(Ti," : la dimension du vecteur de chaque entr\xE9e du mod\xE8le."),Ti.forEach(t),gt.forEach(t),kr=f(e),Xs=i(e,"P",{});var Xu=u(Xs);po=a(Xu,"On dit qu\u2019il est de \xAB grande dimension \xBB en raison de la derni\xE8re valeur. La taille cach\xE9e peut \xEAtre tr\xE8s grande (g\xE9n\xE9ralement 768 pour les petits mod\xE8les et pour les grands mod\xE8les cela peut atteindre 3072 voire plus)."),Xu.forEach(t),Er=f(e),Ks=i(e,"P",{});var Ku=u(Ks);co=a(Ku,"Nous pouvons le constater si nous alimentons notre mod\xE8le avec les entr\xE9es que nous avons pr\xE9trait\xE9es :"),Ku.forEach(t),jr=f(e),pe.l(e),Zs=f(e),Y=i(e,"P",{});var Me=u(Y);mo=a(Me,"Notez que les sorties des mod\xE8les de la biblioth\xE8que \u{1F917} "),dn=i(Me,"EM",{});var Zu=u(dn);fo=a(Zu,"Transformers"),Zu.forEach(t),ho=a(Me," se comportent comme des "),fn=i(Me,"CODE",{});var ep=u(fn);_o=a(ep,"namedtuples"),ep.forEach(t),vo=a(Me," ou des dictionnaires. Vous pouvez acc\xE9der aux \xE9l\xE9ments par attributs (comme nous l\u2019avons fait), par cl\xE9 ("),hn=i(Me,"CODE",{});var sp=u(hn);bo=a(sp,'outputs["last_hidden_state"]'),sp.forEach(t),$o=a(Me,"), ou m\xEAme par l\u2019index si vous savez exactement o\xF9 se trouve la chose que vous cherchez ("),_n=i(Me,"CODE",{});var tp=u(_n);go=a(tp,"outputs[0]"),tp.forEach(t),ko=a(Me,")."),Me.forEach(t),qr=f(e),Fe=i(e,"H3",{class:!0});var Zr=u(Fe);Qe=i(Zr,"A",{id:!0,class:!0,href:!0});var np=u(Qe);vn=i(np,"SPAN",{});var rp=u(vn);x(vs.$$.fragment,rp),rp.forEach(t),np.forEach(t),Eo=f(Zr),bn=i(Zr,"SPAN",{});var lp=u(bn);jo=a(lp,"Les t\xEAtes des mod\xE8les : donner du sens aux chiffres"),lp.forEach(t),Zr.forEach(t),wr=a(e,`

Les t\xEAtes des mod\xE8les prennent en entr\xE9e le vecteur de grande dimension des \xE9tats cach\xE9s et le projettent sur une autre dimension. Elles sont g\xE9n\xE9ralement compos\xE9es d'une ou de quelques couches lin\xE9aires :
`),Se=i(e,"DIV",{class:!0});var el=u(Se);bs=i(el,"IMG",{class:!0,src:!0,alt:!0}),qo=f(el),$s=i(el,"IMG",{class:!0,src:!0,alt:!0}),el.forEach(t),yr=f(e),Z=i(e,"P",{});var rs=u(Z);wo=a(rs,"La sortie du "),$n=i(rs,"EM",{});var ap=u($n);yo=a(ap,"transformer"),ap.forEach(t),xo=a(rs,` est envoy\xE9e directement \xE0 la t\xEAte du mod\xE8le pour \xEAtre trait\xE9e.
Dans ce diagramme, le mod\xE8le est repr\xE9sent\xE9 par sa couche d\u2019ench\xE2ssement et les couches suivantes. La couche d\u2019ench\xE2ssement convertit chaque identifiant d\u2019entr\xE9e dans l\u2019entr\xE9e tokenis\xE9e en un vecteur qui repr\xE9sente le `),gn=i(rs,"EM",{});var op=u(gn);Po=a(op,"token"),op.forEach(t),To=a(rs,` associ\xE9. Les couches suivantes manipulent ces vecteurs en utilisant le m\xE9canisme d\u2019attention pour produire la repr\xE9sentation finale des phrases.
Il existe de nombreuses architectures diff\xE9rentes disponibles dans la biblioth\xE8que \u{1F917} `),kn=i(rs,"EM",{});var ip=u(kn);Mo=a(ip,"Transformers"),ip.forEach(t),zo=a(rs,", chacune \xE9tant con\xE7ue autour de la prise en charge d\u2019une t\xE2che sp\xE9cifique. En voici une liste non exhaustive :"),rs.forEach(t),xr=f(e),L=i(e,"UL",{});var J=u(L);et=i(J,"LI",{});var Mi=u(et);En=i(Mi,"CODE",{});var up=u(En);Co=a(up,"*Model"),up.forEach(t),Ao=a(Mi," (r\xE9cup\xE9rer les \xE9tats cach\xE9s)"),Mi.forEach(t),No=f(J),jn=i(J,"LI",{});var pp=u(jn);qn=i(pp,"CODE",{});var cp=u(qn);Io=a(cp,"*ForCausalLM"),cp.forEach(t),pp.forEach(t),Fo=f(J),wn=i(J,"LI",{});var mp=u(wn);yn=i(mp,"CODE",{});var dp=u(yn);So=a(dp,"*ForMaskedLM"),dp.forEach(t),mp.forEach(t),Do=f(J),xn=i(J,"LI",{});var fp=u(xn);Pn=i(fp,"CODE",{});var hp=u(Pn);Lo=a(hp,"*ForMultipleChoice"),hp.forEach(t),fp.forEach(t),Oo=f(J),Tn=i(J,"LI",{});var _p=u(Tn);Mn=i(_p,"CODE",{});var vp=u(Mn);Vo=a(vp,"*ForQuestionAnswering"),vp.forEach(t),_p.forEach(t),Go=f(J),zn=i(J,"LI",{});var bp=u(zn);Cn=i(bp,"CODE",{});var $p=u(Cn);Ho=a($p,"*ForSequenceClassification"),$p.forEach(t),bp.forEach(t),Jo=f(J),An=i(J,"LI",{});var gp=u(An);Nn=i(gp,"CODE",{});var kp=u(Nn);Uo=a(kp,"*ForTokenClassification"),kp.forEach(t),gp.forEach(t),Bo=f(J),In=i(J,"LI",{});var Ep=u(In);Ro=a(Ep,"et autres \u{1F917}"),Ep.forEach(t),J.forEach(t),Pr=f(e),me.l(e),st=f(e),tt=i(e,"P",{});var jp=u(tt);Qo=a(jp,"Maintenant, si nous examinons la forme de nos entr\xE9es, la dimensionnalit\xE9 est beaucoup plus faible. La t\xEAte du mod\xE8le prend en entr\xE9e les vecteurs de grande dimension que nous avons vus pr\xE9c\xE9demment et elle produit des vecteurs contenant deux valeurs (une par \xE9tiquette) :"),jp.forEach(t),Tr=f(e),x(gs.$$.fragment,e),Mr=f(e),fe.l(e),nt=f(e),rt=i(e,"P",{});var qp=u(rt);Yo=a(qp,"Comme nous n\u2019avons que deux phrases et deux \xE9tiquettes, le r\xE9sultat que nous obtenons est de forme 2 x 2"),qp.forEach(t),zr=f(e),De=i(e,"H2",{class:!0});var sl=u(De);Ye=i(sl,"A",{id:!0,class:!0,href:!0});var wp=u(Ye);Fn=i(wp,"SPAN",{});var yp=u(Fn);x(ks.$$.fragment,yp),yp.forEach(t),wp.forEach(t),Wo=f(sl),Sn=i(sl,"SPAN",{});var xp=u(Sn);Xo=a(xp,"Post-traitement de la sortie"),xp.forEach(t),sl.forEach(t),Cr=f(e),lt=i(e,"P",{});var Pp=u(lt);Ko=a(Pp,"Les valeurs que nous obtenons en sortie de notre mod\xE8le n\u2019ont pas n\xE9cessairement de sens en elles-m\xEAmes. Jetons-y un coup d\u2019\u0153il  :"),Pp.forEach(t),Ar=f(e),x(Es.$$.fragment,e),Nr=f(e),_e.l(e),at=f(e),H=i(e,"P",{});var te=u(H);Zo=a(te,"Notre mod\xE8le a pr\xE9dit "),Dn=i(te,"CODE",{});var Tp=u(Dn);ei=a(Tp,"[-1.5607, 1.6123]"),Tp.forEach(t),si=a(te," pour la premi\xE8re phrase et "),Ln=i(te,"CODE",{});var Mp=u(Ln);ti=a(Mp,"[ 4.1692, -3.3464]"),Mp.forEach(t),ni=a(te," pour la seconde. Ce ne sont pas des probabilit\xE9s mais des "),On=i(te,"EM",{});var zp=u(On);ri=a(zp,"logits"),zp.forEach(t),li=a(te,", les scores bruts, non normalis\xE9s, produits par la derni\xE8re couche du mod\xE8le. Pour \xEAtre convertis en probabilit\xE9s, ils doivent passer par une couche "),js=i(te,"A",{href:!0,rel:!0});var Cp=u(js);ai=a(Cp,"SoftMax"),Cp.forEach(t),oi=a(te," (tous les mod\xE8les de la biblioth\xE8que \u{1F917} "),Vn=i(te,"EM",{});var Ap=u(Vn);ii=a(Ap,"Transformers"),Ap.forEach(t),ui=a(te," sortent les logits car la fonction de perte de l\u2019entra\xEEnement fusionne g\xE9n\xE9ralement la derni\xE8re fonction d\u2019activation, comme la SoftMax, avec la fonction de perte r\xE9elle, comme l\u2019entropie crois\xE9e) :"),te.forEach(t),Ir=f(e),be.l(e),ot=f(e),ge.l(e),it=f(e),xe=i(e,"P",{});var kt=u(xe);pi=a(kt,"Maintenant nous pouvons voir que le mod\xE8le a pr\xE9dit "),Gn=i(kt,"CODE",{});var Np=u(Gn);ci=a(Np,"[0.0402, 0.9598]"),Np.forEach(t),mi=a(kt," pour la premi\xE8re phrase et "),Hn=i(kt,"CODE",{});var Ip=u(Hn);di=a(Ip,"[0.9995, 0.0005]"),Ip.forEach(t),fi=a(kt," pour la seconde. Ce sont des scores de probabilit\xE9 reconnaissables."),kt.forEach(t),Fr=f(e),We=i(e,"P",{});var tl=u(We);hi=a(tl,"Pour obtenir les \xE9tiquettes correspondant \xE0 chaque position, nous pouvons inspecter l\u2019attribut "),Jn=i(tl,"CODE",{});var Fp=u(Jn);_i=a(Fp,"id2label"),Fp.forEach(t),vi=a(tl," de la configuration du mod\xE8le (plus de d\xE9tails dans la section suivante) :"),tl.forEach(t),Sr=f(e),x(qs.$$.fragment,e),Dr=f(e),x(ws.$$.fragment,e),Lr=f(e),ut=i(e,"P",{});var Sp=u(ut);bi=a(Sp,"Nous pouvons maintenant conclure que le mod\xE8le a pr\xE9dit ce qui suit :"),Sp.forEach(t),Or=f(e),Xe=i(e,"UL",{});var nl=u(Xe);Un=i(nl,"LI",{});var Dp=u(Un);$i=a(Dp,"premi\xE8re phrase : NEGATIVE: 0.0402, POSITIVE: 0.9598"),Dp.forEach(t),gi=f(nl),Bn=i(nl,"LI",{});var Lp=u(Bn);ki=a(Lp,"deuxi\xE8me phrase : NEGATIVE: 0.9995, POSITIVE: 0.0005"),Lp.forEach(t),nl.forEach(t),Vr=f(e),Ke=i(e,"P",{});var rl=u(Ke);Ei=a(rl,"Nous avons reproduit avec succ\xE8s les trois \xE9tapes du pipeline : pr\xE9traitement avec les "),Rn=i(rl,"EM",{});var Op=u(Rn);ji=a(Op,"tokenizers"),Op.forEach(t),qi=a(rl,", passage des entr\xE9es dans le mod\xE8le et post-traitement ! Prenons maintenant le temps de nous plonger plus profond\xE9ment dans chacune de ces \xE9tapes."),rl.forEach(t),Gr=f(e),x(Ze.$$.fragment,e),this.h()},h(){E(n,"name","hf:doc:metadata"),E(n,"content",JSON.stringify(kc)),E(_,"id","derrire-le-pipeline"),E(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(_,"href","#derrire-le-pipeline"),E(v,"class","relative group"),E(K,"href","/course/chapter1"),E(Ls,"href","/course/fr/chapter1"),E(os,"class","block dark:hidden"),ll(os.src,zi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg")||E(os,"src",zi),E(os,"alt","The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."),E(is,"class","hidden dark:block"),ll(is.src,Ci="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg")||E(is,"src",Ci),E(is,"alt","The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head."),E(Ce,"class","flex justify-center"),E(Oe,"id","prtraitement-avec-un-itokenizeri"),E(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Oe,"href","#prtraitement-avec-un-itokenizeri"),E(Ae,"class","relative group"),E(ms,"href","https://huggingface.co/models"),E(ms,"rel","nofollow"),E(ds,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),E(ds,"rel","nofollow"),E(Je,"id","passage-au-modle"),E(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Je,"href","#passage-au-modle"),E(Ne,"class","relative group"),E(Rs,"href","/course/fr/chapter1"),E(Be,"id","un-vecteur-de-grande-dimension"),E(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Be,"href","#un-vecteur-de-grande-dimension"),E(Ie,"class","relative group"),E(Qe,"id","les-ttes-des-modles-donner-du-sens-aux-chiffres"),E(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Qe,"href","#les-ttes-des-modles-donner-du-sens-aux-chiffres"),E(Fe,"class","relative group"),E(bs,"class","block dark:hidden"),ll(bs.src,Ai="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head.svg")||E(bs,"src",Ai),E(bs,"alt","A Transformer network alongside its head."),E($s,"class","hidden dark:block"),ll($s.src,Ni="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg")||E($s,"src",Ni),E($s,"alt","A Transformer network alongside its head."),E(Se,"class","flex justify-center"),E(Ye,"id","posttraitement-de-la-sortie"),E(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Ye,"href","#posttraitement-de-la-sortie"),E(De,"class","relative group"),E(js,"href","https://fr.wikipedia.org/wiki/Fonction_softmax"),E(js,"rel","nofollow")},m(e,p){s(document.head,n),m(e,c,p),P(r,e,p),m(e,h,p),m(e,v,p),s(v,_),s(_,M),P(A,M,null),s(v,C),s(v,z),s(z,F),m(e,k,p),ys[q].m(e,p),m(e,D,p),P(S,e,p),m(e,U,p),xs[j].m(e,p),m(e,X,p),m(e,B,p),s(B,ze),s(B,K),s(K,al),s(B,ol),m(e,Wn,p),P(ls,e,p),m(e,Xn,p),m(e,Ds,p),s(Ds,il),m(e,Kn,p),P(as,e,p),m(e,Zn,p),m(e,Le,p),s(Le,ul),s(Le,Ls),s(Ls,pl),s(Le,cl),m(e,er,p),m(e,Ce,p),s(Ce,os),s(Ce,ml),s(Ce,is),m(e,sr,p),m(e,Os,p),s(Os,dl),m(e,tr,p),m(e,Ae,p),s(Ae,Oe),s(Oe,jt),P(us,jt,null),s(Ae,fl),s(Ae,Vs),s(Vs,hl),s(Vs,qt),s(qt,_l),m(e,nr,p),m(e,je,p),s(je,vl),s(je,wt),s(wt,bl),s(je,$l),s(je,yt),s(yt,gl),s(je,kl),m(e,rr,p),m(e,qe,p),s(qe,ps),s(ps,El),s(ps,xt),s(xt,jl),s(ps,ql),s(qe,wl),s(qe,cs),s(cs,yl),s(cs,Pt),s(Pt,xl),s(cs,Pl),s(qe,Tl),s(qe,Tt),s(Tt,Ml),m(e,lr,p),m(e,V,p),s(V,zl),s(V,ms),s(ms,Mt),s(Mt,Cl),s(V,Al),s(V,zt),s(zt,Nl),s(V,Il),s(V,Ct),s(Ct,Fl),s(V,Sl),s(V,At),s(At,Dl),s(V,Ll),s(V,Nt),s(Nt,Ol),s(V,Vl),m(e,ar,p),m(e,R,p),s(R,Gl),s(R,It),s(It,Hl),s(R,Jl),s(R,Ft),s(Ft,Ul),s(R,Bl),s(R,St),s(St,Rl),s(R,Ql),s(R,ds),s(ds,Yl),s(R,Wl),m(e,or,p),P(fs,e,p),m(e,ir,p),m(e,Ve,p),s(Ve,Xl),s(Ve,Dt),s(Dt,Kl),s(Ve,Zl),m(e,ur,p),m(e,O,p),s(O,ea),s(O,Lt),s(Lt,sa),s(O,ta),s(O,Ot),s(Ot,na),s(O,ra),s(O,Vt),s(Vt,la),s(O,aa),s(O,Gt),s(Gt,oa),s(O,ia),s(O,Ht),s(Ht,ua),s(O,pa),s(O,Jt),s(Jt,ca),s(O,ma),m(e,pr,p),m(e,Ge,p),s(Ge,da),s(Ge,Ut),s(Ut,fa),s(Ge,ha),m(e,cr,p),Ps[ne].m(e,p),m(e,Gs,p),m(e,He,p),s(He,_a),s(He,Bt),s(Bt,va),s(He,ba),m(e,mr,p),Ts[le].m(e,p),m(e,Hs,p),m(e,G,p),s(G,$a),s(G,Rt),s(Rt,ga),s(G,ka),s(G,Qt),s(Qt,Ea),s(G,ja),s(G,Yt),s(Yt,qa),s(G,wa),s(G,Wt),s(Wt,ya),s(G,xa),s(G,Xt),s(Xt,Pa),s(G,Ta),m(e,dr,p),m(e,Ne,p),s(Ne,Je),s(Je,Kt),P(hs,Kt,null),s(Ne,Ma),s(Ne,Zt),s(Zt,za),m(e,fr,p),Ms[oe].m(e,p),m(e,Js,p),m(e,Ue,p),s(Ue,Ca),s(Ue,en),s(en,Aa),s(Ue,Na),m(e,hr,p),m(e,Q,p),s(Q,Ia),s(Q,sn),s(sn,Fa),s(Q,Sa),s(Q,tn),s(tn,Da),s(Q,La),s(Q,nn),s(nn,Oa),s(Q,Va),s(Q,Us),s(Us,Ga),s(Us,rn),s(rn,Ha),s(Q,Ja),m(e,_r,p),m(e,Bs,p),s(Bs,Ua),m(e,vr,p),m(e,we,p),s(we,Ba),s(we,ln),s(ln,Ra),s(we,Qa),s(we,Rs),s(Rs,Ya),s(we,Wa),m(e,br,p),m(e,Ie,p),s(Ie,Be),s(Be,an),P(_s,an,null),s(Ie,Xa),s(Ie,on),s(on,Ka),m(e,$r,p),m(e,Re,p),s(Re,Za),s(Re,un),s(un,eo),s(Re,so),m(e,gr,p),m(e,ye,p),s(ye,Qs),s(Qs,pn),s(pn,to),s(Qs,no),s(ye,ro),s(ye,Ys),s(Ys,cn),s(cn,lo),s(Ys,ao),s(ye,oo),s(ye,Ws),s(Ws,mn),s(mn,io),s(Ws,uo),m(e,kr,p),m(e,Xs,p),s(Xs,po),m(e,Er,p),m(e,Ks,p),s(Ks,co),m(e,jr,p),zs[ue].m(e,p),m(e,Zs,p),m(e,Y,p),s(Y,mo),s(Y,dn),s(dn,fo),s(Y,ho),s(Y,fn),s(fn,_o),s(Y,vo),s(Y,hn),s(hn,bo),s(Y,$o),s(Y,_n),s(_n,go),s(Y,ko),m(e,qr,p),m(e,Fe,p),s(Fe,Qe),s(Qe,vn),P(vs,vn,null),s(Fe,Eo),s(Fe,bn),s(bn,jo),m(e,wr,p),m(e,Se,p),s(Se,bs),s(Se,qo),s(Se,$s),m(e,yr,p),m(e,Z,p),s(Z,wo),s(Z,$n),s($n,yo),s(Z,xo),s(Z,gn),s(gn,Po),s(Z,To),s(Z,kn),s(kn,Mo),s(Z,zo),m(e,xr,p),m(e,L,p),s(L,et),s(et,En),s(En,Co),s(et,Ao),s(L,No),s(L,jn),s(jn,qn),s(qn,Io),s(L,Fo),s(L,wn),s(wn,yn),s(yn,So),s(L,Do),s(L,xn),s(xn,Pn),s(Pn,Lo),s(L,Oo),s(L,Tn),s(Tn,Mn),s(Mn,Vo),s(L,Go),s(L,zn),s(zn,Cn),s(Cn,Ho),s(L,Jo),s(L,An),s(An,Nn),s(Nn,Uo),s(L,Bo),s(L,In),s(In,Ro),m(e,Pr,p),Cs[ce].m(e,p),m(e,st,p),m(e,tt,p),s(tt,Qo),m(e,Tr,p),P(gs,e,p),m(e,Mr,p),As[de].m(e,p),m(e,nt,p),m(e,rt,p),s(rt,Yo),m(e,zr,p),m(e,De,p),s(De,Ye),s(Ye,Fn),P(ks,Fn,null),s(De,Wo),s(De,Sn),s(Sn,Xo),m(e,Cr,p),m(e,lt,p),s(lt,Ko),m(e,Ar,p),P(Es,e,p),m(e,Nr,p),Ns[he].m(e,p),m(e,at,p),m(e,H,p),s(H,Zo),s(H,Dn),s(Dn,ei),s(H,si),s(H,Ln),s(Ln,ti),s(H,ni),s(H,On),s(On,ri),s(H,li),s(H,js),s(js,ai),s(H,oi),s(H,Vn),s(Vn,ii),s(H,ui),m(e,Ir,p),Is[ve].m(e,p),m(e,ot,p),Fs[$e].m(e,p),m(e,it,p),m(e,xe,p),s(xe,pi),s(xe,Gn),s(Gn,ci),s(xe,mi),s(xe,Hn),s(Hn,di),s(xe,fi),m(e,Fr,p),m(e,We,p),s(We,hi),s(We,Jn),s(Jn,_i),s(We,vi),m(e,Sr,p),P(qs,e,p),m(e,Dr,p),P(ws,e,p),m(e,Lr,p),m(e,ut,p),s(ut,bi),m(e,Or,p),m(e,Xe,p),s(Xe,Un),s(Un,$i),s(Xe,gi),s(Xe,Bn),s(Bn,ki),m(e,Vr,p),m(e,Ke,p),s(Ke,Ei),s(Ke,Rn),s(Rn,ji),s(Ke,qi),m(e,Gr,p),P(Ze,e,p),Hr=!0},p(e,[p]){const Ss={};p&1&&(Ss.fw=e[0]),r.$set(Ss);let pt=q;q=Fi(e),q!==pt&&(Ee(),b(ys[pt],1,1,()=>{ys[pt]=null}),ke(),I=ys[q],I||(I=ys[q]=Ii[q](e),I.c()),$(I,1),I.m(D.parentNode,D));const Qn={};p&2&&(Qn.$$scope={dirty:p,ctx:e}),S.$set(Qn);let ct=j;j=Di(e),j!==ct&&(Ee(),b(xs[ct],1,1,()=>{xs[ct]=null}),ke(),w=xs[j],w||(w=xs[j]=Si[j](e),w.c()),$(w,1),w.m(X.parentNode,X));let es=ne;ne=Oi(e),ne!==es&&(Ee(),b(Ps[es],1,1,()=>{Ps[es]=null}),ke(),re=Ps[ne],re||(re=Ps[ne]=Li[ne](e),re.c()),$(re,1),re.m(Gs.parentNode,Gs));let mt=le;le=Gi(e),le!==mt&&(Ee(),b(Ts[mt],1,1,()=>{Ts[mt]=null}),ke(),ae=Ts[le],ae||(ae=Ts[le]=Vi[le](e),ae.c()),$(ae,1),ae.m(Hs.parentNode,Hs));let dt=oe;oe=Ji(e),oe!==dt&&(Ee(),b(Ms[dt],1,1,()=>{Ms[dt]=null}),ke(),ie=Ms[oe],ie||(ie=Ms[oe]=Hi[oe](e),ie.c()),$(ie,1),ie.m(Js.parentNode,Js));let ss=ue;ue=Bi(e),ue!==ss&&(Ee(),b(zs[ss],1,1,()=>{zs[ss]=null}),ke(),pe=zs[ue],pe||(pe=zs[ue]=Ui[ue](e),pe.c()),$(pe,1),pe.m(Zs.parentNode,Zs));let ft=ce;ce=Qi(e),ce!==ft&&(Ee(),b(Cs[ft],1,1,()=>{Cs[ft]=null}),ke(),me=Cs[ce],me||(me=Cs[ce]=Ri[ce](e),me.c()),$(me,1),me.m(st.parentNode,st));let ts=de;de=Wi(e),de!==ts&&(Ee(),b(As[ts],1,1,()=>{As[ts]=null}),ke(),fe=As[de],fe||(fe=As[de]=Yi[de](e),fe.c()),$(fe,1),fe.m(nt.parentNode,nt));let ht=he;he=Ki(e),he!==ht&&(Ee(),b(Ns[ht],1,1,()=>{Ns[ht]=null}),ke(),_e=Ns[he],_e||(_e=Ns[he]=Xi[he](e),_e.c()),$(_e,1),_e.m(at.parentNode,at));let ns=ve;ve=eu(e),ve!==ns&&(Ee(),b(Is[ns],1,1,()=>{Is[ns]=null}),ke(),be=Is[ve],be||(be=Is[ve]=Zi[ve](e),be.c()),$(be,1),be.m(ot.parentNode,ot));let _t=$e;$e=tu(e),$e!==_t&&(Ee(),b(Fs[_t],1,1,()=>{Fs[_t]=null}),ke(),ge=Fs[$e],ge||(ge=Fs[$e]=su[$e](e),ge.c()),$(ge,1),ge.m(it.parentNode,it));const Yn={};p&2&&(Yn.$$scope={dirty:p,ctx:e}),Ze.$set(Yn)},i(e){Hr||($(r.$$.fragment,e),$(A.$$.fragment,e),$(I),$(S.$$.fragment,e),$(w),$(ls.$$.fragment,e),$(as.$$.fragment,e),$(us.$$.fragment,e),$(fs.$$.fragment,e),$(re),$(ae),$(hs.$$.fragment,e),$(ie),$(_s.$$.fragment,e),$(pe),$(vs.$$.fragment,e),$(me),$(gs.$$.fragment,e),$(fe),$(ks.$$.fragment,e),$(Es.$$.fragment,e),$(_e),$(be),$(ge),$(qs.$$.fragment,e),$(ws.$$.fragment,e),$(Ze.$$.fragment,e),Hr=!0)},o(e){b(r.$$.fragment,e),b(A.$$.fragment,e),b(I),b(S.$$.fragment,e),b(w),b(ls.$$.fragment,e),b(as.$$.fragment,e),b(us.$$.fragment,e),b(fs.$$.fragment,e),b(re),b(ae),b(hs.$$.fragment,e),b(ie),b(_s.$$.fragment,e),b(pe),b(vs.$$.fragment,e),b(me),b(gs.$$.fragment,e),b(fe),b(ks.$$.fragment,e),b(Es.$$.fragment,e),b(_e),b(be),b(ge),b(qs.$$.fragment,e),b(ws.$$.fragment,e),b(Ze.$$.fragment,e),Hr=!1},d(e){t(n),e&&t(c),T(r,e),e&&t(h),e&&t(v),T(A),e&&t(k),ys[q].d(e),e&&t(D),T(S,e),e&&t(U),xs[j].d(e),e&&t(X),e&&t(B),e&&t(Wn),T(ls,e),e&&t(Xn),e&&t(Ds),e&&t(Kn),T(as,e),e&&t(Zn),e&&t(Le),e&&t(er),e&&t(Ce),e&&t(sr),e&&t(Os),e&&t(tr),e&&t(Ae),T(us),e&&t(nr),e&&t(je),e&&t(rr),e&&t(qe),e&&t(lr),e&&t(V),e&&t(ar),e&&t(R),e&&t(or),T(fs,e),e&&t(ir),e&&t(Ve),e&&t(ur),e&&t(O),e&&t(pr),e&&t(Ge),e&&t(cr),Ps[ne].d(e),e&&t(Gs),e&&t(He),e&&t(mr),Ts[le].d(e),e&&t(Hs),e&&t(G),e&&t(dr),e&&t(Ne),T(hs),e&&t(fr),Ms[oe].d(e),e&&t(Js),e&&t(Ue),e&&t(hr),e&&t(Q),e&&t(_r),e&&t(Bs),e&&t(vr),e&&t(we),e&&t(br),e&&t(Ie),T(_s),e&&t($r),e&&t(Re),e&&t(gr),e&&t(ye),e&&t(kr),e&&t(Xs),e&&t(Er),e&&t(Ks),e&&t(jr),zs[ue].d(e),e&&t(Zs),e&&t(Y),e&&t(qr),e&&t(Fe),T(vs),e&&t(wr),e&&t(Se),e&&t(yr),e&&t(Z),e&&t(xr),e&&t(L),e&&t(Pr),Cs[ce].d(e),e&&t(st),e&&t(tt),e&&t(Tr),T(gs,e),e&&t(Mr),As[de].d(e),e&&t(nt),e&&t(rt),e&&t(zr),e&&t(De),T(ks),e&&t(Cr),e&&t(lt),e&&t(Ar),T(Es,e),e&&t(Nr),Ns[he].d(e),e&&t(at),e&&t(H),e&&t(Ir),Is[ve].d(e),e&&t(ot),Fs[$e].d(e),e&&t(it),e&&t(xe),e&&t(Fr),e&&t(We),e&&t(Sr),T(qs,e),e&&t(Dr),T(ws,e),e&&t(Lr),e&&t(ut),e&&t(Or),e&&t(Xe),e&&t(Vr),e&&t(Ke),e&&t(Gr),T(Ze,e)}}}const kc={local:"derrire-le-pipeline",sections:[{local:"prtraitement-avec-un-itokenizeri",title:"Pr\xE9traitement avec un <i>tokenizer</i>"},{local:"passage-au-modle",sections:[{local:"un-vecteur-de-grande-dimension",title:"Un vecteur de grande dimension ?"},{local:"les-ttes-des-modles-donner-du-sens-aux-chiffres",title:"Les t\xEAtes des mod\xE8les : donner du sens aux chiffres"}],title:"Passage au mod\xE8le"},{local:"posttraitement-de-la-sortie",title:"Post-traitement de la sortie"}],title:"Derri\xE8re le pipeline"};function Ec(g,n,c){let r="pt";return Qp(()=>{const h=new URLSearchParams(window.location.search);c(0,r=h.get("fw")||"pt")}),[r]}class Mc extends Jp{constructor(n){super();Up(this,n,Ec,gc,Bp,{})}}export{Mc as default,kc as metadata};
