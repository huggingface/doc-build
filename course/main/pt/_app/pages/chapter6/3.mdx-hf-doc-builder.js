import{S as Iu,i as Du,s as Tu,e as t,k as d,w as j,t as o,M as Su,c as r,d as a,m as c,x as b,a as l,h as n,b as $,N as Cu,G as e,g as i,y as v,o as h,p as $r,q as x,B as _,v as Ru,n as kr}from"../../chunks/vendor-hf-doc-builder.js";import{T as ec}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Er}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ta}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as E}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Pu}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Au}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function Bu(I){let m,g;return m=new Pu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_tf.ipynb"}]}}),{c(){j(m.$$.fragment)},l(u){b(m.$$.fragment,u)},m(u,k){v(m,u,k),g=!0},i(u){g||(x(m.$$.fragment,u),g=!0)},o(u){h(m.$$.fragment,u),g=!1},d(u){_(m,u)}}}function Fu(I){let m,g;return m=new Pu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3_pt.ipynb"}]}}),{c(){j(m.$$.fragment)},l(u){b(m.$$.fragment,u)},m(u,k){v(m,u,k),g=!0},i(u){g||(x(m.$$.fragment,u),g=!0)},o(u){h(m.$$.fragment,u),g=!1},d(u){_(m,u)}}}function Nu(I){let m,g;return{c(){m=t("p"),g=o("\u26A0\uFE0F Ao tokenizar uma \xFAnica frase, voc\xEA nem sempre ver\xE1 uma diferen\xE7a de velocidade entre as vers\xF5es lenta e r\xE1pida do mesmo tokenizador. Na verdade, a vers\xE3o r\xE1pida pode ser mais lenta! \xC9 somente ao tokenizar muitos textos em paralelo ao mesmo tempo que voc\xEA poder\xE1 ver a diferen\xE7a com maior nitidez.")},l(u){m=r(u,"P",{});var k=l(m);g=n(k,"\u26A0\uFE0F Ao tokenizar uma \xFAnica frase, voc\xEA nem sempre ver\xE1 uma diferen\xE7a de velocidade entre as vers\xF5es lenta e r\xE1pida do mesmo tokenizador. Na verdade, a vers\xE3o r\xE1pida pode ser mais lenta! \xC9 somente ao tokenizar muitos textos em paralelo ao mesmo tempo que voc\xEA poder\xE1 ver a diferen\xE7a com maior nitidez."),k.forEach(a)},m(u,k){i(u,m,k),e(m,g)},d(u){u&&a(m)}}}function Xu(I){let m,g,u,k,D,q,S,F,M,T,U,O,B,C;return{c(){m=t("p"),g=o("A no\xE7\xE3o do que \xE9 uma palavra \xE9 complicada. Por exemplo, \u201Cd\u2019\xE1gua\u201D (uma contra\xE7\xE3o de \u201Cda \xE1gua\u201D) conta como uma ou duas palavras? Na verdade, depende do tokenizador e da opera\xE7\xE3o de pr\xE9-tokeniza\xE7\xE3o que \xE9 aplicada. Alguns tokenizadores apenas dividem em espa\xE7os, ent\xE3o eles considerar\xE3o isso como uma palavra. Outros usam pontua\xE7\xE3o em cima dos espa\xE7os, ent\xE3o considerar\xE3o duas palavras."),u=d(),k=t("p"),D=o("\u270F\uFE0F "),q=t("strong"),S=o("Experimente!"),F=o(" Crie um tokenizador a partir dos checkpoints de "),M=t("code"),T=o("bert-base-cased "),U=o("e "),O=t("code"),B=o("roberta-base"),C=o(" e tokenize \u201D81s\u201D com eles. O que voc\xEA observa? Quais s\xE3o os IDs das palavras?")},l(P){m=r(P,"P",{});var y=l(m);g=n(y,"A no\xE7\xE3o do que \xE9 uma palavra \xE9 complicada. Por exemplo, \u201Cd\u2019\xE1gua\u201D (uma contra\xE7\xE3o de \u201Cda \xE1gua\u201D) conta como uma ou duas palavras? Na verdade, depende do tokenizador e da opera\xE7\xE3o de pr\xE9-tokeniza\xE7\xE3o que \xE9 aplicada. Alguns tokenizadores apenas dividem em espa\xE7os, ent\xE3o eles considerar\xE3o isso como uma palavra. Outros usam pontua\xE7\xE3o em cima dos espa\xE7os, ent\xE3o considerar\xE3o duas palavras."),y.forEach(a),u=c(P),k=r(P,"P",{});var N=l(k);D=n(N,"\u270F\uFE0F "),q=r(N,"STRONG",{});var R=l(q);S=n(R,"Experimente!"),R.forEach(a),F=n(N," Crie um tokenizador a partir dos checkpoints de "),M=r(N,"CODE",{});var Y=l(M);T=n(Y,"bert-base-cased "),Y.forEach(a),U=n(N,"e "),O=r(N,"CODE",{});var X=l(O);B=n(X,"roberta-base"),X.forEach(a),C=n(N," e tokenize \u201D81s\u201D com eles. O que voc\xEA observa? Quais s\xE3o os IDs das palavras?"),N.forEach(a)},m(P,y){i(P,m,y),e(m,g),i(P,u,y),i(P,k,y),e(k,D),e(k,q),e(q,S),e(k,F),e(k,M),e(M,T),e(k,U),e(k,O),e(O,B),e(k,C)},d(P){P&&a(m),P&&a(u),P&&a(k)}}}function Hu(I){let m,g,u,k,D;return{c(){m=t("p"),g=o("\u270F\uFE0F "),u=t("strong"),k=o("Experimente!"),D=o(" Crie seu pr\xF3prio texto de exemplo e veja se voc\xEA consegue entender quais tokens est\xE3o associados ao ID da palavra e tamb\xE9m como extrair os intervalos de caracteres para uma \xFAnica palavra. Como b\xF4nus, tente usar duas frases como entrada e veja se os IDs das frases fazem sentido para voc\xEA.")},l(q){m=r(q,"P",{});var S=l(m);g=n(S,"\u270F\uFE0F "),u=r(S,"STRONG",{});var F=l(u);k=n(F,"Experimente!"),F.forEach(a),D=n(S," Crie seu pr\xF3prio texto de exemplo e veja se voc\xEA consegue entender quais tokens est\xE3o associados ao ID da palavra e tamb\xE9m como extrair os intervalos de caracteres para uma \xFAnica palavra. Como b\xF4nus, tente usar duas frases como entrada e veja se os IDs das frases fazem sentido para voc\xEA."),S.forEach(a)},m(q,S){i(q,m,S),e(m,g),e(m,u),e(u,k),e(m,D)},d(q){q&&a(m)}}}function Mu(I){let m,g;return m=new Er({props:{id:"PrX4CjrVnNc"}}),{c(){j(m.$$.fragment)},l(u){b(m.$$.fragment,u)},m(u,k){v(m,u,k),g=!0},i(u){g||(x(m.$$.fragment,u),g=!0)},o(u){h(m.$$.fragment,u),g=!1},d(u){_(m,u)}}}function Gu(I){let m,g;return m=new Er({props:{id:"0E7ltQB7fM8"}}),{c(){j(m.$$.fragment)},l(u){b(m.$$.fragment,u)},m(u,k){v(m,u,k),g=!0},i(u){g||(x(m.$$.fragment,u),g=!0)},o(u){h(m.$$.fragment,u),g=!1},d(u){_(m,u)}}}function Lu(I){let m,g,u,k,D,q,S,F,M,T,U,O,B,C,P,y,N,R,Y,X,Z;return T=new E({props:{code:`from transformers import AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="tf")
outputs = model(**inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForTokenClassification

model_checkpoint = <span class="hljs-string">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
inputs = tokenizer(example, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
outputs = model(**inputs)`}}),R=new E({props:{code:`print(inputs["input_ids"].shape)
print(outputs.logits.shape)`,highlighted:`<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)
<span class="hljs-built_in">print</span>(outputs.logits.shape)`}}),X=new E({props:{code:`(1, 19)
(1, 19, 9)`,highlighted:`(<span class="hljs-number">1</span>, <span class="hljs-number">19</span>)
(<span class="hljs-number">1</span>, <span class="hljs-number">19</span>, <span class="hljs-number">9</span>)`}}),{c(){m=t("p"),g=o("Primeiro, precisamos tokenizar nossa entrada e pass\xE1-la pelo modelo. Isso \xE9 feito exatamente como no "),u=t("a"),k=o("Cap\xEDtulo 2"),D=o("; instanciamos o tokenizador e o modelo usando as classes "),q=t("code"),S=o("TFAutoXxx"),F=o(" e depois as usamos em nosso exemplo:"),M=d(),j(T.$$.fragment),U=d(),O=t("p"),B=o("Como estamos usando "),C=t("code"),P=o("TFAutoModelForTokenClassification"),y=o(" neste caso, obtemos um conjunto de logits para cada token na sequ\xEAncia de entrada:"),N=d(),j(R.$$.fragment),Y=d(),j(X.$$.fragment),this.h()},l(f){m=r(f,"P",{});var w=l(m);g=n(w,"Primeiro, precisamos tokenizar nossa entrada e pass\xE1-la pelo modelo. Isso \xE9 feito exatamente como no "),u=r(w,"A",{href:!0});var fs=l(u);k=n(fs,"Cap\xEDtulo 2"),fs.forEach(a),D=n(w,"; instanciamos o tokenizador e o modelo usando as classes "),q=r(w,"CODE",{});var ys=l(q);S=n(ys,"TFAutoXxx"),ys.forEach(a),F=n(w," e depois as usamos em nosso exemplo:"),w.forEach(a),M=c(f),b(T.$$.fragment,f),U=c(f),O=r(f,"P",{});var ts=l(O);B=n(ts,"Como estamos usando "),C=r(ts,"CODE",{});var ls=l(C);P=n(ls,"TFAutoModelForTokenClassification"),ls.forEach(a),y=n(ts," neste caso, obtemos um conjunto de logits para cada token na sequ\xEAncia de entrada:"),ts.forEach(a),N=c(f),b(R.$$.fragment,f),Y=c(f),b(X.$$.fragment,f),this.h()},h(){$(u,"href","/course/chapter2")},m(f,w){i(f,m,w),e(m,g),e(m,u),e(u,k),e(m,D),e(m,q),e(q,S),e(m,F),i(f,M,w),v(T,f,w),i(f,U,w),i(f,O,w),e(O,B),e(O,C),e(C,P),e(O,y),i(f,N,w),v(R,f,w),i(f,Y,w),v(X,f,w),Z=!0},i(f){Z||(x(T.$$.fragment,f),x(R.$$.fragment,f),x(X.$$.fragment,f),Z=!0)},o(f){h(T.$$.fragment,f),h(R.$$.fragment,f),h(X.$$.fragment,f),Z=!1},d(f){f&&a(m),f&&a(M),_(T,f),f&&a(U),f&&a(O),f&&a(N),_(R,f),f&&a(Y),_(X,f)}}}function Vu(I){let m,g,u,k,D,q,S,F,M,T,U,O,B,C,P,y,N,R,Y,X,Z;return T=new E({props:{code:`from transformers import AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
inputs = tokenizer(example, return_tensors="pt")
outputs = model(**inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForTokenClassification

model_checkpoint = <span class="hljs-string">&quot;dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)

example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
inputs = tokenizer(example, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)`}}),R=new E({props:{code:`print(inputs["input_ids"].shape)
print(outputs.logits.shape)`,highlighted:`<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)
<span class="hljs-built_in">print</span>(outputs.logits.shape)`}}),X=new E({props:{code:`torch.Size([1, 19])
torch.Size([1, 19, 9])`,highlighted:`torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">19</span>])
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">19</span>, <span class="hljs-number">9</span>])`}}),{c(){m=t("p"),g=o("Primeiro, precisamos tokenizar nossa entrada e pass\xE1-la pelo modelo. Isso \xE9 feito exatamente como no "),u=t("a"),k=o("Cap\xEDtulo 2"),D=o("; instanciamos o tokenizador e o modelo usando as classes "),q=t("code"),S=o("AutoXxx"),F=o(" e depois as usamos em nosso exemplo:"),M=d(),j(T.$$.fragment),U=d(),O=t("p"),B=o("Como estamos usando "),C=t("code"),P=o("AutoModelForTokenClassification"),y=o(" neste caso, obtemos um conjunto de logits para cada token na sequ\xEAncia de entrada:"),N=d(),j(R.$$.fragment),Y=d(),j(X.$$.fragment),this.h()},l(f){m=r(f,"P",{});var w=l(m);g=n(w,"Primeiro, precisamos tokenizar nossa entrada e pass\xE1-la pelo modelo. Isso \xE9 feito exatamente como no "),u=r(w,"A",{href:!0});var fs=l(u);k=n(fs,"Cap\xEDtulo 2"),fs.forEach(a),D=n(w,"; instanciamos o tokenizador e o modelo usando as classes "),q=r(w,"CODE",{});var ys=l(q);S=n(ys,"AutoXxx"),ys.forEach(a),F=n(w," e depois as usamos em nosso exemplo:"),w.forEach(a),M=c(f),b(T.$$.fragment,f),U=c(f),O=r(f,"P",{});var ts=l(O);B=n(ts,"Como estamos usando "),C=r(ts,"CODE",{});var ls=l(C);P=n(ls,"AutoModelForTokenClassification"),ls.forEach(a),y=n(ts," neste caso, obtemos um conjunto de logits para cada token na sequ\xEAncia de entrada:"),ts.forEach(a),N=c(f),b(R.$$.fragment,f),Y=c(f),b(X.$$.fragment,f),this.h()},h(){$(u,"href","/course/chapter3")},m(f,w){i(f,m,w),e(m,g),e(m,u),e(u,k),e(m,D),e(m,q),e(q,S),e(m,F),i(f,M,w),v(T,f,w),i(f,U,w),i(f,O,w),e(O,B),e(O,C),e(C,P),e(O,y),i(f,N,w),v(R,f,w),i(f,Y,w),v(X,f,w),Z=!0},i(f){Z||(x(T.$$.fragment,f),x(R.$$.fragment,f),x(X.$$.fragment,f),Z=!0)},o(f){h(T.$$.fragment,f),h(R.$$.fragment,f),h(X.$$.fragment,f),Z=!1},d(f){f&&a(m),f&&a(M),_(T,f),f&&a(U),f&&a(O),f&&a(N),_(R,f),f&&a(Y),_(X,f)}}}function Qu(I){let m,g;return m=new E({props:{code:`import tensorflow as tf

probabilities = tf.math.softmax(outputs.logits, axis=-1)[0]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-1)[0]
predictions = predictions.numpy().tolist()
print(predictions)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

probabilities = tf.math.softmax(outputs.logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
probabilities = probabilities.numpy().tolist()
predictions = tf.math.argmax(outputs.logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
predictions = predictions.numpy().tolist()
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){j(m.$$.fragment)},l(u){b(m.$$.fragment,u)},m(u,k){v(m,u,k),g=!0},i(u){g||(x(m.$$.fragment,u),g=!0)},o(u){h(m.$$.fragment,u),g=!1},d(u){_(m,u)}}}function Uu(I){let m,g;return m=new E({props:{code:`import torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()
predictions = outputs.logits.argmax(dim=-1)[0].tolist()
print(predictions)`,highlighted:`<span class="hljs-keyword">import</span> torch

probabilities = torch.nn.functional.softmax(outputs.logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].tolist()
predictions = outputs.logits.argmax(dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].tolist()
<span class="hljs-built_in">print</span>(predictions)`}}),{c(){j(m.$$.fragment)},l(u){b(m.$$.fragment,u)},m(u,k){v(m,u,k),g=!0},i(u){g||(x(m.$$.fragment,u),g=!0)},o(u){h(m.$$.fragment,u),g=!1},d(u){_(m,u)}}}function Yu(I){let m,g,u,k,D,q,S,F,M,T,U,O,B,C,P,y,N,R,Y,X,Z,f,w,fs,ys,ts,ls,wr,qr,Tn,le,Sn,As,yr,sa,Or,zr,Rn,Bs,Sa,Os,ea,Cr,Pr,aa,Ir,Dr,Ra,Tr,pe,zs,oa,Aa,Sr,Rr,na,Ar,Br,ta,Fr,Nr,Cs,ra,Ba,Xr,Hr,la,Mr,Gr,pa,Lr,An,Fs,Bn,Ps,Ns,Fa,ie,Vr,Na,Qr,Fn,de,Nn,Xs,Ur,Xa,Yr,Jr,Xn,Hs,Kr,Ha,Wr,Zr,Hn,ia,sl,Mn,ce,Gn,Ms,el,Ma,al,ol,Ln,me,Vn,ps,nl,Ga,tl,rl,La,ll,pl,Va,il,dl,Qn,ue,Un,fe,Yn,Gs,cl,Qa,ml,ul,Jn,he,Kn,xe,Wn,da,fl,Zn,ge,st,je,et,$s,hl,Ua,xl,gl,Ya,jl,bl,at,be,ot,ve,nt,J,vl,Ja,_l,$l,Ka,kl,El,Wa,wl,ql,Za,yl,Ol,so,zl,Cl,tt,Ls,rt,ks,Pl,eo,Il,Dl,ao,Tl,Sl,lt,G,Rl,oo,Al,Bl,no,Fl,Nl,to,Xl,Hl,ro,Ml,Gl,lo,Ll,Vl,po,Ql,Ul,pt,_e,it,$e,dt,Es,Yl,io,Jl,Kl,co,Wl,Zl,ct,Vs,mt,Is,Qs,mo,ke,sp,ca,ep,uo,ap,ut,ss,op,ma,np,tp,fo,rp,lp,ua,pp,ip,ho,dp,cp,ft,hs,xs,fa,Ds,Us,xo,Ee,mp,go,up,ht,Ys,fp,we,jo,hp,xp,xt,qe,gt,ye,jt,ha,gp,bt,Oe,vt,ze,_t,L,jp,bo,bp,vp,vo,_p,$p,_o,kp,Ep,$o,wp,qp,ko,yp,Op,Eo,zp,Cp,$t,ws,Js,wo,Pp,Ip,qo,Dp,Tp,Sp,Ks,yo,Rp,Ap,Oo,Bp,Fp,Np,es,zo,Xp,Hp,Co,Mp,Gp,Po,Lp,Vp,Io,Qp,Up,Do,Yp,Jp,kt,Ws,Kp,To,Wp,Zp,Et,Ts,Zs,So,Ce,si,Ro,ei,wt,gs,js,xa,ga,ai,qt,bs,vs,ja,Pe,yt,se,oi,Ao,ni,ti,Ot,Ie,zt,De,Ct,z,ri,Bo,li,pi,Fo,ii,di,No,ci,mi,Xo,ui,fi,Ho,hi,xi,Mo,gi,ji,Go,bi,vi,Lo,_i,$i,Vo,ki,Ei,Qo,wi,qi,Uo,yi,Oi,Pt,H,zi,Yo,Ci,Pi,Jo,Ii,Di,Ko,Ti,Si,Wo,Ri,Ai,Zo,Bi,Fi,sn,Ni,Xi,en,Hi,Mi,an,Gi,Li,It,Ss,Te,ac,Vi,Se,oc,Dt,ee,Qi,on,Ui,Yi,Tt,Re,St,Ae,Rt,is,Ji,nn,Ki,Wi,tn,Zi,sd,rn,ed,ad,At,Be,Bt,Fe,Ft,ds,od,ln,nd,td,pn,rd,ld,dn,pd,id,Nt,Ne,Xt,ae,dd,cn,cd,md,Ht,Xe,Mt,ba,ud,Gt,He,Lt,Me,Vt,va,fd,Qt,Rs,oe,mn,Ge,hd,un,xd,Ut,V,gd,fn,jd,bd,hn,vd,_d,xn,$d,kd,gn,Ed,wd,jn,qd,yd,bn,Od,zd,Yt,K,Cd,vn,Pd,Id,_n,Dd,Td,$n,Sd,Rd,kn,Ad,Bd,En,Fd,Nd,Jt,Le,Kt,Ve,Wt,W,Xd,wn,Hd,Md,qn,Gd,Ld,yn,Vd,Qd,On,Ud,Yd,zn,Jd,Kd,Zt,Qe,sr,_a,Wd,er,Ue,ar,$a,Zd,or;u=new Au({props:{fw:I[0]}}),F=new Ta({});const nc=[Fu,Bu],Ye=[];function tc(s,p){return s[0]==="pt"?0:1}B=tc(I),C=Ye[B]=nc[B](I),le=new Er({props:{id:"g8quOxoqhHQ"}}),Fs=new ec({props:{warning:!0,$$slots:{default:[Nu]},$$scope:{ctx:I}}}),ie=new Ta({}),de=new Er({props:{id:"3umI3tm27Vw"}}),ce=new E({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
example = "My name is Sylvain and I work at Hugging Face in Brooklyn."
encoding = tokenizer(example)
print(type(encoding))`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
example = <span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>
encoding = tokenizer(example)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(encoding))`}}),me=new E({props:{code:"<class 'transformers.tokenization_utils_base.BatchEncoding'>",highlighted:'&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;transformers.tokenization_utils_base.BatchEncoding&#x27;</span>&gt;'}}),ue=new E({props:{code:"tokenizer.is_fast",highlighted:"tokenizer.is_fast"}}),fe=new E({props:{code:"True",highlighted:'<span class="hljs-literal">True</span>'}}),he=new E({props:{code:"encoding.is_fast",highlighted:"encoding.is_fast"}}),xe=new E({props:{code:"True",highlighted:'<span class="hljs-literal">True</span>'}}),ge=new E({props:{code:"encoding.tokens()",highlighted:"encoding.tokens()"}}),je=new E({props:{code:`['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
 'Brooklyn', '.', '[SEP]']`,highlighted:`[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;My&#x27;</span>, <span class="hljs-string">&#x27;name&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;and&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;work&#x27;</span>, <span class="hljs-string">&#x27;at&#x27;</span>, <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;in&#x27;</span>,
 <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]`}}),be=new E({props:{code:"encoding.word_ids()",highlighted:"encoding.word_ids()"}}),ve=new E({props:{code:"[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]",highlighted:'[<span class="hljs-literal">None</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-literal">None</span>]'}}),Ls=new ec({props:{$$slots:{default:[Xu]},$$scope:{ctx:I}}}),_e=new E({props:{code:`start, end = encoding.word_to_chars(3)
example[start:end]`,highlighted:`start, end = encoding.word_to_chars(<span class="hljs-number">3</span>)
example[start:end]`}}),$e=new E({props:{code:"Sylvain",highlighted:"Sylvain"}}),Vs=new ec({props:{$$slots:{default:[Hu]},$$scope:{ctx:I}}}),ke=new Ta({});const rc=[Gu,Mu],Je=[];function lc(s,p){return s[0]==="pt"?0:1}hs=lc(I),xs=Je[hs]=rc[hs](I),Ee=new Ta({}),qe=new E({props:{code:`from transformers import pipeline

token_classifier = pipeline("token-classification")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

token_classifier = pipeline(<span class="hljs-string">&quot;token-classification&quot;</span>)
token_classifier(<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)`}}),ye=new E({props:{code:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">12</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">14</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">16</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">35</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">40</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">41</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),Oe=new E({props:{code:`from transformers import pipeline

token_classifier = pipeline("token-classification", aggregation_strategy="simple")
token_classifier("My name is Sylvain and I work at Hugging Face in Brooklyn.")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

token_classifier = pipeline(<span class="hljs-string">&quot;token-classification&quot;</span>, aggregation_strategy=<span class="hljs-string">&quot;simple&quot;</span>)
token_classifier(<span class="hljs-string">&quot;My name is Sylvain and I work at Hugging Face in Brooklyn.&quot;</span>)`}}),ze=new E({props:{code:`[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9981694</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Sylvain&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97960204</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),Ce=new Ta({});const pc=[Vu,Lu],Ke=[];function ic(s,p){return s[0]==="pt"?0:1}gs=ic(I),js=Ke[gs]=pc[gs](I);const dc=[Uu,Qu],We=[];function cc(s,p){return s[0]==="pt"?0:1}return bs=cc(I),vs=We[bs]=dc[bs](I),Pe=new E({props:{code:"[0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0]",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]'}}),Ie=new E({props:{code:"model.config.id2label",highlighted:"model.config.id2label"}}),De=new E({props:{code:`{0: 'O',
 1: 'B-MISC',
 2: 'I-MISC',
 3: 'B-PER',
 4: 'I-PER',
 5: 'B-ORG',
 6: 'I-ORG',
 7: 'B-LOC',
 8: 'I-LOC'}`,highlighted:`{<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;O&#x27;</span>,
 <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;B-MISC&#x27;</span>,
 <span class="hljs-number">2</span>: <span class="hljs-string">&#x27;I-MISC&#x27;</span>,
 <span class="hljs-number">3</span>: <span class="hljs-string">&#x27;B-PER&#x27;</span>,
 <span class="hljs-number">4</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>,
 <span class="hljs-number">5</span>: <span class="hljs-string">&#x27;B-ORG&#x27;</span>,
 <span class="hljs-number">6</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>,
 <span class="hljs-number">7</span>: <span class="hljs-string">&#x27;B-LOC&#x27;</span>,
 <span class="hljs-number">8</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>}`}}),Re=new E({props:{code:`results = []
tokens = inputs.tokens()

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        results.append(
            {"entity": label, "score": probabilities[idx][pred], "word": tokens[idx]}
        )

print(results)`,highlighted:`results = []
tokens = inputs.tokens()

<span class="hljs-keyword">for</span> idx, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predictions):
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        results.append(
            {<span class="hljs-string">&quot;entity&quot;</span>: label, <span class="hljs-string">&quot;score&quot;</span>: probabilities[idx][pred], <span class="hljs-string">&quot;word&quot;</span>: tokens[idx]}
        )

<span class="hljs-built_in">print</span>(results)`}}),Ae=new E({props:{code:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S'},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl'},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va'},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in'},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu'},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging'},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face'},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn'}]`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>}]`}}),Be=new E({props:{code:`inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
inputs_with_offsets["offset_mapping"]`,highlighted:`inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]`}}),Fe=new E({props:{code:`[(0, 0), (0, 2), (3, 7), (8, 10), (11, 12), (12, 14), (14, 16), (16, 18), (19, 22), (23, 24), (25, 29), (30, 32),
 (33, 35), (35, 40), (41, 45), (46, 48), (49, 57), (57, 58), (0, 0)]`,highlighted:`[(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">2</span>), (<span class="hljs-number">3</span>, <span class="hljs-number">7</span>), (<span class="hljs-number">8</span>, <span class="hljs-number">10</span>), (<span class="hljs-number">11</span>, <span class="hljs-number">12</span>), (<span class="hljs-number">12</span>, <span class="hljs-number">14</span>), (<span class="hljs-number">14</span>, <span class="hljs-number">16</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">18</span>), (<span class="hljs-number">19</span>, <span class="hljs-number">22</span>), (<span class="hljs-number">23</span>, <span class="hljs-number">24</span>), (<span class="hljs-number">25</span>, <span class="hljs-number">29</span>), (<span class="hljs-number">30</span>, <span class="hljs-number">32</span>),
 (<span class="hljs-number">33</span>, <span class="hljs-number">35</span>), (<span class="hljs-number">35</span>, <span class="hljs-number">40</span>), (<span class="hljs-number">41</span>, <span class="hljs-number">45</span>), (<span class="hljs-number">46</span>, <span class="hljs-number">48</span>), (<span class="hljs-number">49</span>, <span class="hljs-number">57</span>), (<span class="hljs-number">57</span>, <span class="hljs-number">58</span>), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>)]`}}),Ne=new E({props:{code:"example[12:14]",highlighted:'example[<span class="hljs-number">12</span>:<span class="hljs-number">14</span>]'}}),Xe=new E({props:{code:"yl",highlighted:"yl"}}),He=new E({props:{code:`results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

for idx, pred in enumerate(predictions):
    label = model.config.id2label[pred]
    if label != "O":
        start, end = offsets[idx]
        results.append(
            {
                "entity": label,
                "score": probabilities[idx][pred],
                "word": tokens[idx],
                "start": start,
                "end": end,
            }
        )

print(results)`,highlighted:`results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

<span class="hljs-keyword">for</span> idx, pred <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(predictions):
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        start, end = offsets[idx]
        results.append(
            {
                <span class="hljs-string">&quot;entity&quot;</span>: label,
                <span class="hljs-string">&quot;score&quot;</span>: probabilities[idx][pred],
                <span class="hljs-string">&quot;word&quot;</span>: tokens[idx],
                <span class="hljs-string">&quot;start&quot;</span>: start,
                <span class="hljs-string">&quot;end&quot;</span>: end,
            }
        )

<span class="hljs-built_in">print</span>(results)`}}),Me=new E({props:{code:`[{'entity': 'I-PER', 'score': 0.9993828, 'index': 4, 'word': 'S', 'start': 11, 'end': 12},
 {'entity': 'I-PER', 'score': 0.99815476, 'index': 5, 'word': '##yl', 'start': 12, 'end': 14},
 {'entity': 'I-PER', 'score': 0.99590725, 'index': 6, 'word': '##va', 'start': 14, 'end': 16},
 {'entity': 'I-PER', 'score': 0.9992327, 'index': 7, 'word': '##in', 'start': 16, 'end': 18},
 {'entity': 'I-ORG', 'score': 0.97389334, 'index': 12, 'word': 'Hu', 'start': 33, 'end': 35},
 {'entity': 'I-ORG', 'score': 0.976115, 'index': 13, 'word': '##gging', 'start': 35, 'end': 40},
 {'entity': 'I-ORG', 'score': 0.98879766, 'index': 14, 'word': 'Face', 'start': 41, 'end': 45},
 {'entity': 'I-LOC', 'score': 0.99321055, 'index': 16, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9993828</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;S&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">12</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99815476</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">5</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##yl&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">14</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99590725</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##va&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">16</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9992327</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##in&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97389334</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">12</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hu&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">35</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.976115</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;##gging&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">35</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">40</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.98879766</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">14</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">41</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity&#x27;</span>: <span class="hljs-string">&#x27;I-LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;index&#x27;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),Ge=new Ta({}),Le=new E({props:{code:"example[33:45]",highlighted:'example[<span class="hljs-number">33</span>:<span class="hljs-number">45</span>]'}}),Ve=new E({props:{code:"Hugging Face",highlighted:"Hugging Face"}}),Qe=new E({props:{code:`import numpy as np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets["offset_mapping"]

idx = 0
while idx < len(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    if label != "O":
        # Removendo o B- ou I-
        label = label[2:]
        start, _ = offsets[idx]

        # Vamos pegar todos os tokens rotulados com I-
        all_scores = []
        while (
            idx < len(predictions)
            and model.config.id2label[predictions[idx]] == f"I-{label}"
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += 1

        # A pontua\xE7\xE3o \xE9 a m\xE9dia de todas as pontua\xE7\xF5es dos tokens da entidade agrupada
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                "entity_group": label,
                "score": score,
                "word": word,
                "start": start,
                "end": end,
            }
        )
    idx += 1

print(results)`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

results = []
inputs_with_offsets = tokenizer(example, return_offsets_mapping=<span class="hljs-literal">True</span>)
tokens = inputs_with_offsets.tokens()
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

idx = <span class="hljs-number">0</span>
<span class="hljs-keyword">while</span> idx &lt; <span class="hljs-built_in">len</span>(predictions):
    pred = predictions[idx]
    label = model.config.id2label[pred]
    <span class="hljs-keyword">if</span> label != <span class="hljs-string">&quot;O&quot;</span>:
        <span class="hljs-comment"># Removendo o B- ou I-</span>
        label = label[<span class="hljs-number">2</span>:]
        start, _ = offsets[idx]

        <span class="hljs-comment"># Vamos pegar todos os tokens rotulados com I-</span>
        all_scores = []
        <span class="hljs-keyword">while</span> (
            idx &lt; <span class="hljs-built_in">len</span>(predictions)
            <span class="hljs-keyword">and</span> model.config.id2label[predictions[idx]] == <span class="hljs-string">f&quot;I-<span class="hljs-subst">{label}</span>&quot;</span>
        ):
            all_scores.append(probabilities[idx][pred])
            _, end = offsets[idx]
            idx += <span class="hljs-number">1</span>

        <span class="hljs-comment"># A pontua\xE7\xE3o \xE9 a m\xE9dia de todas as pontua\xE7\xF5es dos tokens da entidade agrupada</span>
        score = np.mean(all_scores).item()
        word = example[start:end]
        results.append(
            {
                <span class="hljs-string">&quot;entity_group&quot;</span>: label,
                <span class="hljs-string">&quot;score&quot;</span>: score,
                <span class="hljs-string">&quot;word&quot;</span>: word,
                <span class="hljs-string">&quot;start&quot;</span>: start,
                <span class="hljs-string">&quot;end&quot;</span>: end,
            }
        )
    idx += <span class="hljs-number">1</span>

<span class="hljs-built_in">print</span>(results)`}}),Ue=new E({props:{code:`[{'entity_group': 'PER', 'score': 0.9981694, 'word': 'Sylvain', 'start': 11, 'end': 18},
 {'entity_group': 'ORG', 'score': 0.97960204, 'word': 'Hugging Face', 'start': 33, 'end': 45},
 {'entity_group': 'LOC', 'score': 0.99321055, 'word': 'Brooklyn', 'start': 49, 'end': 57}]`,highlighted:`[{<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;PER&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9981694</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Sylvain&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">18</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97960204</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Hugging Face&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">33</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">45</span>},
 {<span class="hljs-string">&#x27;entity_group&#x27;</span>: <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.99321055</span>, <span class="hljs-string">&#x27;word&#x27;</span>: <span class="hljs-string">&#x27;Brooklyn&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">49</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">57</span>}]`}}),{c(){m=t("meta"),g=d(),j(u.$$.fragment),k=d(),D=t("h1"),q=t("a"),S=t("span"),j(F.$$.fragment),M=d(),T=t("span"),U=o("Os poderes especiais dos tokenizadores r\xE1pidos"),O=d(),C.c(),P=d(),y=t("p"),N=o("Nesta se\xE7\xE3o, examinaremos mais de perto os recursos dos tokenizadores em \u{1F917} Transformers. At\xE9 agora, s\xF3 os usamos para tokenizar entradas ou decodificar IDs de volta em texto, mas tokenizadores - especialmente aqueles apoiados pela biblioteca \u{1F917} Tokenizers - podem fazer muito mais. Para ilustrar esses recursos adicionais, exploraremos como reproduzir os resultados dos pipelines "),R=t("code"),Y=o("token-classification"),X=o(" (que chamamos de "),Z=t("code"),f=o("ner"),w=o(") e "),fs=t("code"),ys=o("question-answering"),ts=o(" que encontramos pela primeira vez no "),ls=t("a"),wr=o("Cap\xEDtulo 1"),qr=o("."),Tn=d(),j(le.$$.fragment),Sn=d(),As=t("p"),yr=o("Na discuss\xE3o a seguir, muitas vezes faremos a distin\xE7\xE3o entre tokenizadores \u201Clentos\u201D e \u201Cr\xE1pidos\u201D. Tokenizadores lentos s\xE3o aqueles escritos em Python dentro da biblioteca \u{1F917} Transformers, enquanto as vers\xF5es r\xE1pidas s\xE3o aquelas fornecidas por \u{1F917} Tokenizers, que s\xE3o escritos em Rust. Se voc\xEA se lembrar da tabela do "),sa=t("a"),Or=o("Cap\xEDtulo 5"),zr=o(" que informava quanto tempo levou um tokenizador r\xE1pido e um lento para tokenizar o conjunto de dados de revis\xE3o de medicamentos, voc\xEA deve ter uma ideia do motivo pelo qual os chamamos de r\xE1pido e lento:"),Rn=d(),Bs=t("table"),Sa=t("thead"),Os=t("tr"),ea=t("th"),Cr=o("Fast tokenizer"),Pr=d(),aa=t("th"),Ir=o("Slow tokenizer"),Dr=d(),Ra=t("th"),Tr=d(),pe=t("tbody"),zs=t("tr"),oa=t("td"),Aa=t("code"),Sr=o("batched=True"),Rr=d(),na=t("td"),Ar=o("10.8s"),Br=d(),ta=t("td"),Fr=o("4min41s"),Nr=d(),Cs=t("tr"),ra=t("td"),Ba=t("code"),Xr=o("batched=False"),Hr=d(),la=t("td"),Mr=o("59.2s"),Gr=d(),pa=t("td"),Lr=o("5min3s"),An=d(),j(Fs.$$.fragment),Bn=d(),Ps=t("h2"),Ns=t("a"),Fa=t("span"),j(ie.$$.fragment),Vr=d(),Na=t("span"),Qr=o("Codifica\xE7\xE3o em lote"),Fn=d(),j(de.$$.fragment),Nn=d(),Xs=t("p"),Ur=o("A sa\xEDda de um tokenizador n\xE3o \xE9 um simples dicion\xE1rio em Python; o que obtemos \xE9, na verdade, um objeto especial chamado "),Xa=t("code"),Yr=o("BatchEncoding"),Jr=o(". Este objeto \xE9 uma subclasse de um dicion\xE1rio (e \xE9 por isso que conseguimos indexar esse resultado sem nenhum problema antes), mas com m\xE9todos adicionais que s\xE3o usados \u200B\u200Bprincipalmente por tokenizadores r\xE1pidos."),Xn=d(),Hs=t("p"),Kr=o("Al\xE9m de seus recursos de paraleliza\xE7\xE3o, uma funcionalidade importante dos tokenizadores r\xE1pidos \xE9 que eles sempre acompanham o intervalo original de textos dos quais os tokens finais v\xEAm - um recurso que chamamos de "),Ha=t("em"),Wr=o("mapeamento de offset"),Zr=o(". Isso, por sua vez, desbloqueia recursos como o mapeamento de cada palavra para os tokens gerados ou mapeamento de cada caractere do texto original para o token que est\xE1 dentro e vice-versa."),Hn=d(),ia=t("p"),sl=o("Vamos analisar um exemplo:"),Mn=d(),j(ce.$$.fragment),Gn=d(),Ms=t("p"),el=o("Como mencionado anteriormente, n\xF3s obtemos um objeto "),Ma=t("code"),al=o("BatchEncoding"),ol=o(" na sa\xEDda do tokenizador:"),Ln=d(),j(me.$$.fragment),Vn=d(),ps=t("p"),nl=o("Como a classe "),Ga=t("code"),tl=o("AutoTokenizer"),rl=o(" escolhe o tokenizador r\xE1pido como padr\xE3o, podemos usar os m\xE9todos adicionais que o objeto "),La=t("code"),ll=o("BatchEncoding"),pl=o(" fornece. Temos duas formas de verificar se o nosso tokenizador \xE9 r\xE1pido ou lento. Podemos, por exemplo, avaliar o atributo "),Va=t("code"),il=o("is_fast"),dl=o(" do tokenizador:"),Qn=d(),j(ue.$$.fragment),Un=d(),j(fe.$$.fragment),Yn=d(),Gs=t("p"),cl=o("ou checar o mesmo atributo do nosso "),Qa=t("code"),ml=o("encoding"),ul=o(":"),Jn=d(),j(he.$$.fragment),Kn=d(),j(xe.$$.fragment),Wn=d(),da=t("p"),fl=o("Vejamos o que um tokenizador r\xE1pido nos permite fazer. Primeiro, podemos acessar os tokens sem precisar converter os IDs de volta em tokens:"),Zn=d(),j(ge.$$.fragment),st=d(),j(je.$$.fragment),et=d(),$s=t("p"),hl=o("No caso, o token no \xEDndice 5 \xE9 "),Ua=t("code"),xl=o("##yl"),gl=o(", que faz parte da palavra \u201CSylvain\u201D na senten\xE7a original. N\xF3s podemos tamb\xE9m usar o metodo "),Ya=t("code"),jl=o("words_ids()"),bl=o(" para obter o \xEDndice da palavra de onde cada palavra vem:"),at=d(),j(be.$$.fragment),ot=d(),j(ve.$$.fragment),nt=d(),J=t("p"),vl=o("Podemos observar que as palavras especiais do tokenizador "),Ja=t("code"),_l=o("[CLS]"),$l=o(" e "),Ka=t("code"),kl=o("[SEP]"),El=o(" s\xE3o mapeados para "),Wa=t("code"),wl=o("None"),ql=o(", e ent\xE3o cada token \xE9 mapeada para a palavra de onde se origina. Isso \xE9 especialmente \xFAtil para determinar se um token est\xE1 no in\xEDcio da palavra ou se dois tokens est\xE3o em uma mesma palavra. Poder\xEDamos contar com o prefix "),Za=t("code"),yl=o("##"),Ol=o(" para isso, mas apenas para tokenizadores do tipo BERT; este m\xE9todo funciona para qualquer tipo de tokenizador, desde que seja do tipo r\xE1pido. No pr\xF3ximo cap\xEDtulo, n\xF3s veremos como podemos usar esse recurso para aplicar os r\xF3tulos que temos para cada palavra adequadamente aos tokens em tarefas como reconhecimento de entidade nomeada (em ingl\xEAs, Named Entity Recognition, ou NER) e marca\xE7\xE3o de parte da fala (em ingl\xEAs, part-of-speech, ou POS). Tamb\xE9m podemos us\xE1-lo para mascarar todos os tokens provenientes da mesma palavra na modelagem de linguagem mascarada (uma t\xE9cnica chamada "),so=t("em"),zl=o("mascaramento da palavra inteira"),Cl=o(")"),tt=d(),j(Ls.$$.fragment),rt=d(),ks=t("p"),Pl=o("Da mesma forma, existe um m\xE9todo "),eo=t("code"),Il=o("sentence_ids()"),Dl=o(" que podemos usar para mapear um token para a senten\xE7a de onde veio (embora, neste caso, o "),ao=t("code"),Tl=o("token_type_ids"),Sl=o(" retornado pelo tokenizador possa nos dar a mesma informa\xE7\xE3o)."),lt=d(),G=t("p"),Rl=o("Por fim, podemos mapear qualquer palavra ou token para caracteres no texto original (e vice-versa) atrav\xE9s dos m\xE9todos "),oo=t("code"),Al=o("word_to_chars()"),Bl=o(" ou "),no=t("code"),Fl=o("token_to_chars()"),Nl=o(" e "),to=t("code"),Xl=o("char_to_word()"),Hl=o(" ou "),ro=t("code"),Ml=o("char_to_token()"),Gl=o(". Por exemplo, o m\xE9todo "),lo=t("code"),Ll=o("word_ids()"),Vl=o(" nos diz que "),po=t("code"),Ql=o("##yl"),Ul=o(" \xE9 parte da palavra no \xEDndice 3, mas qual palavra est\xE1 na frase? Podemos descobrir da seguinte forma:"),pt=d(),j(_e.$$.fragment),it=d(),j($e.$$.fragment),dt=d(),Es=t("p"),Yl=o("Como mencionamos anteriormente, isso \xE9 apoiado pelo fato de que o tokenizador r\xE1pido acompanha o intervalo de texto de cada token em uma lista de "),io=t("em"),Jl=o("offsets"),Kl=o(". Para ilustrar seu uso, mostraremos a seguir como replicar manualmente os resultados do pipeline "),co=t("code"),Wl=o("token-classification"),Zl=o("."),ct=d(),j(Vs.$$.fragment),mt=d(),Is=t("h2"),Qs=t("a"),mo=t("span"),j(ke.$$.fragment),sp=d(),ca=t("span"),ep=o("Dentro do pipeline "),uo=t("code"),ap=o("token-classification"),ut=d(),ss=t("p"),op=o("No "),ma=t("a"),np=o("Cap\xEDtulo 1"),tp=o(" tivemos o primeiro gosto de aplicar o NER \u2014 onde a tarefa \xE9 identificar quais partes do texto correspondem a entidades como pessoas, locais ou organiza\xE7\xF5es \u2014 com a fun\xE7\xE3o do \u{1F917} Transformers "),fo=t("code"),rp=o("pipeline()"),lp=o(". Ent\xE3o, no "),ua=t("a"),pp=o("Cap\xEDtulo 2"),ip=o(", vimos como um pipeline agrupa os tr\xEAs est\xE1gios necess\xE1rios para obter as previs\xF5es de um texto: tokeniza\xE7\xE3o, passagem das entradas pelo modelo e p\xF3s-processamento. As duas primeiras etapas do pipeline "),ho=t("code"),dp=o("token-classification"),cp=o(" s\xE3o as mesmas de qualquer outro pipeline, mas o p\xF3s-processamento \xE9 um pouco mais complexo \u2014 vejamos como!"),ft=d(),xs.c(),fa=d(),Ds=t("h3"),Us=t("a"),xo=t("span"),j(Ee.$$.fragment),mp=d(),go=t("span"),up=o("Obtendo os resultados b\xE1sicos com o pipeline"),ht=d(),Ys=t("p"),fp=o("Primeiro, vamos usar um pipeline de classifica\xE7\xE3o de token para que possamos obter alguns resultados para comparar manualmente. O modelo usado por padr\xE3o \xE9 "),we=t("a"),jo=t("code"),hp=o("dbmdz/bert-large-cased-finetuned-conll03-english"),xp=o("; ele executa NER em frases:"),xt=d(),j(qe.$$.fragment),gt=d(),j(ye.$$.fragment),jt=d(),ha=t("p"),gp=o("O modelo identificou corretamente cada token gerado por \u201CSylvain\u201D como uma pessoa, cada token gerado por \u201CHugging Face\u201D como uma organiza\xE7\xE3o e o token \u201CBrooklyn\u201D como um local. Tamb\xE9m podemos pedir ao pipeline para agrupar os tokens que correspondem \xE0 mesma entidade:"),bt=d(),j(Oe.$$.fragment),vt=d(),j(ze.$$.fragment),_t=d(),L=t("p"),jp=o("O par\xE2metro "),bo=t("code"),bp=o("aggregation_strategy"),vp=o(" escolhido mudar\xE1 as pontua\xE7\xF5es calculadas para cada entidade agrupada. Com o valor "),vo=t("code"),_p=o('"simple"'),$p=o(", a pontua\xE7\xE3o \xE9 apenas a m\xE9dia das pontua\xE7\xF5es de cada token na entidade dada: por exemplo, a pontua\xE7\xE3o de \u201CSylvain\u201D \xE9 a m\xE9dia das pontua\xE7\xF5es que vimos no exemplo anterior para os tokens "),_o=t("code"),kp=o("S"),Ep=o(", "),$o=t("code"),wp=o("##yl"),qp=o(", "),ko=t("code"),yp=o("##va"),Op=o(", e "),Eo=t("code"),zp=o("##in"),Cp=o(". Outras estrat\xE9gias dispon\xEDveis s\xE3o:"),$t=d(),ws=t("ul"),Js=t("li"),wo=t("code"),Pp=o('"first"'),Ip=o(", onde a pontua\xE7\xE3o de cada entidade \xE9 a pontua\xE7\xE3o do primeiro token dessa entidade (portanto, para \u201CSylvain\u201D seria 0.993828, a pontua\xE7\xE3o do token "),qo=t("code"),Dp=o("S"),Tp=o(")"),Sp=d(),Ks=t("li"),yo=t("code"),Rp=o('"max"'),Ap=o(", onde a pontua\xE7\xE3o de cada entidade \xE9 a pontua\xE7\xE3o m\xE1xima dos tokens naquela entidade (portanto, para \u201CHugging Face\u201D seria 0.98879766, a pontua\xE7\xE3o do token "),Oo=t("code"),Bp=o('"Face"'),Fp=o(")"),Np=d(),es=t("li"),zo=t("code"),Xp=o('"average"'),Hp=o(", onde a pontua\xE7\xE3o de cada entidade \xE9 a m\xE9dia das pontua\xE7\xF5es das palavras que comp\xF5em aquela entidade (assim para \u201CSylvain\u201D n\xE3o haveria diferen\xE7a da estrat\xE9gia "),Co=t("code"),Mp=o('"simple"'),Gp=o(", mas "),Po=t("code"),Lp=o('"Hugging Face"'),Vp=o(" teria uma pontua\xE7\xE3o de 0.9819, a m\xE9dia das pontua\xE7\xF5es para "),Io=t("code"),Qp=o('"Hugging"'),Up=o(", 0.975, e "),Do=t("code"),Yp=o('"Face"'),Jp=o(", 0.98879)"),kt=d(),Ws=t("p"),Kp=o("Agora vejamos como obter esses resultados sem usar a fun\xE7\xE3o "),To=t("code"),Wp=o("pipeline()"),Zp=o("!"),Et=d(),Ts=t("h3"),Zs=t("a"),So=t("span"),j(Ce.$$.fragment),si=d(),Ro=t("span"),ei=o("Das entradas \xE0s previs\xF5es"),wt=d(),js.c(),xa=d(),ga=t("p"),ai=o("Temos um lote com 1 sequ\xEAncia de 19 tokens e o modelo tem 9 r\xF3tulos diferentes, ent\xE3o a sa\xEDda do modelo tem um tamanho de 1 x 19 x 9. Assim como para o pipeline de classifica\xE7\xE3o de texto, usamos uma fun\xE7\xE3o softmax para converter esses logits para probabilidades, e pegamos o argmax para obter previs\xF5es (note que podemos pegar o argmax nos logits porque o softmax n\xE3o altera a ordem):"),qt=d(),vs.c(),ja=d(),j(Pe.$$.fragment),yt=d(),se=t("p"),oi=o("O atributo "),Ao=t("code"),ni=o("model.config.id2label"),ti=o(" cont\xE9m o mapeamento de \xEDndices para r\xF3tulos que podemos usar para entender as previs\xF5es:"),Ot=d(),j(Ie.$$.fragment),zt=d(),j(De.$$.fragment),Ct=d(),z=t("p"),ri=o("Como vimos anteriormente, existem 9 r\xF3tulos: "),Bo=t("code"),li=o("O"),pi=o(" \xE9 o r\xF3tulo para os tokens que n\xE3o est\xE3o em nenhuma entidade nomeada, e ent\xE3o temos dois r\xF3tulos para cada tipo de entidade (miscel\xE2nia, pessoa, organiza\xE7\xE3o e localiza\xE7\xE3o). O r\xF3tulo "),Fo=t("code"),ii=o("B-XXX"),di=o(" indica que o token est\xE1 no in\xEDcio de uma entidade "),No=t("code"),ci=o("XXX"),mi=o(" e o r\xF3tulo "),Xo=t("code"),ui=o("I-XXX"),fi=o(" indica que o token est\xE1 dentro da entidade "),Ho=t("code"),hi=o("XXX"),xi=o(". No caso do exemplo atual, esperar\xEDamos que o nosso modelo classificasse o token "),Mo=t("code"),gi=o("S"),ji=o(" como "),Go=t("code"),bi=o("B-PER"),vi=o(" (in\xEDcio de uma entidade pessoa) e os tokens "),Lo=t("code"),_i=o("##yl"),$i=o(", "),Vo=t("code"),ki=o("##va"),Ei=o(" e "),Qo=t("code"),wi=o("##in"),qi=o(" como "),Uo=t("code"),yi=o("I-PER"),Oi=o(" (dentro da entidade pessoa)."),Pt=d(),H=t("p"),zi=o("Voc\xEA pode pensar que o modelo estava errado neste caso, pois deu o r\xF3tulo "),Yo=t("code"),Ci=o("I-PER"),Pi=o(" a todos esses quatro tokens, mas isso n\xE3o \xE9 totalmente verdade. Na realidade, existem dois formatos para esses r\xF3tulos: "),Jo=t("code"),Ii=o("B-"),Di=o(" e "),Ko=t("code"),Ti=o("I-"),Si=o(": "),Wo=t("em"),Ri=o("IOB1"),Ai=o(" e "),Zo=t("em"),Bi=o("IOB2"),Fi=o(". O formato IOB2 (em rosa abaixo), \xE9 o que introduzimos, enquanto que no formato IOB1 (em azul), os r\xF3tulos que come\xE7am com "),sn=t("code"),Ni=o("B-"),Xi=o(" s\xE3o usados apenas para separar duas entidades adjacentes do mesmo tipo. O modelo que estamos usando foi ajustado em um conjunto de dados usando esse formato, e \xE9 por isso que ele atribui o r\xF3tulo "),en=t("code"),Hi=o("I-PER"),Mi=o(" ao token "),an=t("code"),Gi=o("S"),Li=o("."),It=d(),Ss=t("div"),Te=t("img"),Vi=d(),Se=t("img"),Dt=d(),ee=t("p"),Qi=o("Com este mapa, estamos prontos para reproduzir (quase inteiramente) os resultados do primeiro pipeline \u2014 podemos apenas pegar a pontua\xE7\xE3o e o r\xF3tulo de cada token que n\xE3o foi classificado como "),on=t("code"),Ui=o("O"),Yi=o(":"),Tt=d(),j(Re.$$.fragment),St=d(),j(Ae.$$.fragment),Rt=d(),is=t("p"),Ji=o("Isso \xE9 muito parecido com o que t\xEDnhamos antes, com uma exce\xE7\xE3o: o pipeline tamb\xE9m nos dava informa\xE7\xF5es sobre o "),nn=t("code"),Ki=o("start"),Wi=o(" e "),tn=t("code"),Zi=o("end"),sd=o(" de cada entidade na frase original. \xC9 aqui que nosso mapeamento de offset entrar\xE1 em a\xE7\xE3o. Para obter tais offsets, basta definir "),rn=t("code"),ed=o("return_offsets_mapping=True"),ad=o(" quando aplicamos o tokenizador \xE0s nossas entradas:"),At=d(),j(Be.$$.fragment),Bt=d(),j(Fe.$$.fragment),Ft=d(),ds=t("p"),od=o("Cada tupla \xE9 o intervalo de texto correspondente a cada token, onde "),ln=t("code"),nd=o("(0, 0)"),td=o(" \xE9 reservado para os tokens especiais. Vimos antes que o token no \xEDndice 5 \xE9 "),pn=t("code"),rd=o("##yl"),ld=o(", que tem "),dn=t("code"),pd=o("(12, 14)"),id=o(" como offset aqui. Se pegarmos a fatia correspondente em nosso exemplo:"),Nt=d(),j(Ne.$$.fragment),Xt=d(),ae=t("p"),dd=o("obtemos o intervalo adequado de texto sem o "),cn=t("code"),cd=o("##"),md=o(":"),Ht=d(),j(Xe.$$.fragment),Mt=d(),ba=t("p"),ud=o("Usando isso, agora podemos completar os resultados anteriores:"),Gt=d(),j(He.$$.fragment),Lt=d(),j(Me.$$.fragment),Vt=d(),va=t("p"),fd=o("Este \xE9 o mesmo resultado que obtivemos no primeiro pipeline!"),Qt=d(),Rs=t("h3"),oe=t("a"),mn=t("span"),j(Ge.$$.fragment),hd=d(),un=t("span"),xd=o("Agrupando entidades"),Ut=d(),V=t("p"),gd=o("Usar os offsets para determinar as chaves inicial e final de cada entidade \xE9 \xFAtil, mas essa informa\xE7\xE3o n\xE3o \xE9 estritamente necess\xE1ria. Quando queremos agrupar as entidades, no entanto, os offsets nos poupar\xE3o muito c\xF3digo confuso. Por exemplo, se quisermos agrupar os tokens "),fn=t("code"),jd=o("Hu"),bd=o(", "),hn=t("code"),vd=o("##gging"),_d=o(" e "),xn=t("code"),$d=o("Face"),kd=o(", podemos fazer regras especiais que digam que os dois primeiros devem ser anexados e removido o "),gn=t("code"),Ed=o("##"),wd=o(", e o "),jn=t("code"),qd=o("Face"),yd=o(" deve ser adicionado com um espa\xE7o, pois n\xE3o come\xE7a com "),bn=t("code"),Od=o("##"),zd=o(" \u2014 mas isso s\xF3 funcionaria para esse tipo espec\xEDfico de tokenizador. Ter\xEDamos que escrever outro conjunto de regras para um tokenizador SentencePiece ou Byte-Pair-Encoding (discutido mais adiante neste cap\xEDtulo)."),Yt=d(),K=t("p"),Cd=o("Com os offsets, todo esse c\xF3digo personalizado desaparece: podemos apenas pegar o intervalo no texto original que come\xE7a com o primeiro token e termina com o \xFAltimo token. Ent\xE3o, no caso dos tokens "),vn=t("code"),Pd=o("Hu"),Id=o(", "),_n=t("code"),Dd=o("##ging"),Td=o(" e "),$n=t("code"),Sd=o("Face"),Rd=o(", devemos come\xE7ar no caractere 33 (o in\xEDcio de "),kn=t("code"),Ad=o("Hu"),Bd=o(") e terminar antes do caractere 45 (o final de "),En=t("code"),Fd=o("Face"),Nd=o("):"),Jt=d(),j(Le.$$.fragment),Kt=d(),j(Ve.$$.fragment),Wt=d(),W=t("p"),Xd=o("Para escrever o c\xF3digo para o p\xF3s-processamento das previs\xF5es ao agrupar entidades, agruparemos entidades consecutivas e rotuladas com "),wn=t("code"),Hd=o("I-XXX"),Md=o(", excento a primeira, que pode ser rotulada como "),qn=t("code"),Gd=o("B-XXX"),Ld=o(" ou "),yn=t("code"),Vd=o("I-XXX"),Qd=o(" (portanto, paramos de agrupar uma entidade quando obtemos um "),On=t("code"),Ud=o("O"),Yd=o(", um novo tipo de entidade ou um "),zn=t("code"),Jd=o("B-XXX"),Kd=o(" que nos informa que uma entidade do mesmo tipo est\xE1 iniciando):"),Zt=d(),j(Qe.$$.fragment),sr=d(),_a=t("p"),Wd=o("E obtemos os mesmos resultados do nosso segundo pipeline!"),er=d(),j(Ue.$$.fragment),ar=d(),$a=t("p"),Zd=o("Outro exemplo de uma tarefa onde esses offsets s\xE3o extremamente \xFAteis \xE9 a resposta a perguntas. O conhecimento deste pipeline, que faremos na pr\xF3xima se\xE7\xE3o, tamb\xE9m nos permitir\xE1 dar uma olhada em um \xFAltimo recurso dos tokenizadores na biblioteca \u{1F917} Transformers: lidar com tokens em excesso quando truncamos uma entrada em um determinado comprimento."),this.h()},l(s){const p=Su('[data-svelte="svelte-1phssyn"]',document.head);m=r(p,"META",{name:!0,content:!0}),p.forEach(a),g=c(s),b(u.$$.fragment,s),k=c(s),D=r(s,"H1",{class:!0});var Ze=l(D);q=r(Ze,"A",{id:!0,class:!0,href:!0});var ka=l(q);S=r(ka,"SPAN",{});var Cn=l(S);b(F.$$.fragment,Cn),Cn.forEach(a),ka.forEach(a),M=c(Ze),T=r(Ze,"SPAN",{});var Pn=l(T);U=n(Pn,"Os poderes especiais dos tokenizadores r\xE1pidos"),Pn.forEach(a),Ze.forEach(a),O=c(s),C.l(s),P=c(s),y=r(s,"P",{});var rs=l(y);N=n(rs,"Nesta se\xE7\xE3o, examinaremos mais de perto os recursos dos tokenizadores em \u{1F917} Transformers. At\xE9 agora, s\xF3 os usamos para tokenizar entradas ou decodificar IDs de volta em texto, mas tokenizadores - especialmente aqueles apoiados pela biblioteca \u{1F917} Tokenizers - podem fazer muito mais. Para ilustrar esses recursos adicionais, exploraremos como reproduzir os resultados dos pipelines "),R=r(rs,"CODE",{});var Ea=l(R);Y=n(Ea,"token-classification"),Ea.forEach(a),X=n(rs," (que chamamos de "),Z=r(rs,"CODE",{});var wa=l(Z);f=n(wa,"ner"),wa.forEach(a),w=n(rs,") e "),fs=r(rs,"CODE",{});var qa=l(fs);ys=n(qa,"question-answering"),qa.forEach(a),ts=n(rs," que encontramos pela primeira vez no "),ls=r(rs,"A",{href:!0});var mc=l(ls);wr=n(mc,"Cap\xEDtulo 1"),mc.forEach(a),qr=n(rs,"."),rs.forEach(a),Tn=c(s),b(le.$$.fragment,s),Sn=c(s),As=r(s,"P",{});var nr=l(As);yr=n(nr,"Na discuss\xE3o a seguir, muitas vezes faremos a distin\xE7\xE3o entre tokenizadores \u201Clentos\u201D e \u201Cr\xE1pidos\u201D. Tokenizadores lentos s\xE3o aqueles escritos em Python dentro da biblioteca \u{1F917} Transformers, enquanto as vers\xF5es r\xE1pidas s\xE3o aquelas fornecidas por \u{1F917} Tokenizers, que s\xE3o escritos em Rust. Se voc\xEA se lembrar da tabela do "),sa=r(nr,"A",{href:!0});var uc=l(sa);Or=n(uc,"Cap\xEDtulo 5"),uc.forEach(a),zr=n(nr," que informava quanto tempo levou um tokenizador r\xE1pido e um lento para tokenizar o conjunto de dados de revis\xE3o de medicamentos, voc\xEA deve ter uma ideia do motivo pelo qual os chamamos de r\xE1pido e lento:"),nr.forEach(a),Rn=c(s),Bs=r(s,"TABLE",{});var tr=l(Bs);Sa=r(tr,"THEAD",{});var fc=l(Sa);Os=r(fc,"TR",{});var ya=l(Os);ea=r(ya,"TH",{align:!0});var hc=l(ea);Cr=n(hc,"Fast tokenizer"),hc.forEach(a),Pr=c(ya),aa=r(ya,"TH",{align:!0});var xc=l(aa);Ir=n(xc,"Slow tokenizer"),xc.forEach(a),Dr=c(ya),Ra=r(ya,"TH",{align:!0}),l(Ra).forEach(a),ya.forEach(a),fc.forEach(a),Tr=c(tr),pe=r(tr,"TBODY",{});var rr=l(pe);zs=r(rr,"TR",{});var Oa=l(zs);oa=r(Oa,"TD",{align:!0});var gc=l(oa);Aa=r(gc,"CODE",{});var jc=l(Aa);Sr=n(jc,"batched=True"),jc.forEach(a),gc.forEach(a),Rr=c(Oa),na=r(Oa,"TD",{align:!0});var bc=l(na);Ar=n(bc,"10.8s"),bc.forEach(a),Br=c(Oa),ta=r(Oa,"TD",{align:!0});var vc=l(ta);Fr=n(vc,"4min41s"),vc.forEach(a),Oa.forEach(a),Nr=c(rr),Cs=r(rr,"TR",{});var za=l(Cs);ra=r(za,"TD",{align:!0});var _c=l(ra);Ba=r(_c,"CODE",{});var $c=l(Ba);Xr=n($c,"batched=False"),$c.forEach(a),_c.forEach(a),Hr=c(za),la=r(za,"TD",{align:!0});var kc=l(la);Mr=n(kc,"59.2s"),kc.forEach(a),Gr=c(za),pa=r(za,"TD",{align:!0});var Ec=l(pa);Lr=n(Ec,"5min3s"),Ec.forEach(a),za.forEach(a),rr.forEach(a),tr.forEach(a),An=c(s),b(Fs.$$.fragment,s),Bn=c(s),Ps=r(s,"H2",{class:!0});var lr=l(Ps);Ns=r(lr,"A",{id:!0,class:!0,href:!0});var wc=l(Ns);Fa=r(wc,"SPAN",{});var qc=l(Fa);b(ie.$$.fragment,qc),qc.forEach(a),wc.forEach(a),Vr=c(lr),Na=r(lr,"SPAN",{});var yc=l(Na);Qr=n(yc,"Codifica\xE7\xE3o em lote"),yc.forEach(a),lr.forEach(a),Fn=c(s),b(de.$$.fragment,s),Nn=c(s),Xs=r(s,"P",{});var pr=l(Xs);Ur=n(pr,"A sa\xEDda de um tokenizador n\xE3o \xE9 um simples dicion\xE1rio em Python; o que obtemos \xE9, na verdade, um objeto especial chamado "),Xa=r(pr,"CODE",{});var Oc=l(Xa);Yr=n(Oc,"BatchEncoding"),Oc.forEach(a),Jr=n(pr,". Este objeto \xE9 uma subclasse de um dicion\xE1rio (e \xE9 por isso que conseguimos indexar esse resultado sem nenhum problema antes), mas com m\xE9todos adicionais que s\xE3o usados \u200B\u200Bprincipalmente por tokenizadores r\xE1pidos."),pr.forEach(a),Xn=c(s),Hs=r(s,"P",{});var ir=l(Hs);Kr=n(ir,"Al\xE9m de seus recursos de paraleliza\xE7\xE3o, uma funcionalidade importante dos tokenizadores r\xE1pidos \xE9 que eles sempre acompanham o intervalo original de textos dos quais os tokens finais v\xEAm - um recurso que chamamos de "),Ha=r(ir,"EM",{});var zc=l(Ha);Wr=n(zc,"mapeamento de offset"),zc.forEach(a),Zr=n(ir,". Isso, por sua vez, desbloqueia recursos como o mapeamento de cada palavra para os tokens gerados ou mapeamento de cada caractere do texto original para o token que est\xE1 dentro e vice-versa."),ir.forEach(a),Hn=c(s),ia=r(s,"P",{});var Cc=l(ia);sl=n(Cc,"Vamos analisar um exemplo:"),Cc.forEach(a),Mn=c(s),b(ce.$$.fragment,s),Gn=c(s),Ms=r(s,"P",{});var dr=l(Ms);el=n(dr,"Como mencionado anteriormente, n\xF3s obtemos um objeto "),Ma=r(dr,"CODE",{});var Pc=l(Ma);al=n(Pc,"BatchEncoding"),Pc.forEach(a),ol=n(dr," na sa\xEDda do tokenizador:"),dr.forEach(a),Ln=c(s),b(me.$$.fragment,s),Vn=c(s),ps=r(s,"P",{});var ne=l(ps);nl=n(ne,"Como a classe "),Ga=r(ne,"CODE",{});var Ic=l(Ga);tl=n(Ic,"AutoTokenizer"),Ic.forEach(a),rl=n(ne," escolhe o tokenizador r\xE1pido como padr\xE3o, podemos usar os m\xE9todos adicionais que o objeto "),La=r(ne,"CODE",{});var Dc=l(La);ll=n(Dc,"BatchEncoding"),Dc.forEach(a),pl=n(ne," fornece. Temos duas formas de verificar se o nosso tokenizador \xE9 r\xE1pido ou lento. Podemos, por exemplo, avaliar o atributo "),Va=r(ne,"CODE",{});var Tc=l(Va);il=n(Tc,"is_fast"),Tc.forEach(a),dl=n(ne," do tokenizador:"),ne.forEach(a),Qn=c(s),b(ue.$$.fragment,s),Un=c(s),b(fe.$$.fragment,s),Yn=c(s),Gs=r(s,"P",{});var cr=l(Gs);cl=n(cr,"ou checar o mesmo atributo do nosso "),Qa=r(cr,"CODE",{});var Sc=l(Qa);ml=n(Sc,"encoding"),Sc.forEach(a),ul=n(cr,":"),cr.forEach(a),Jn=c(s),b(he.$$.fragment,s),Kn=c(s),b(xe.$$.fragment,s),Wn=c(s),da=r(s,"P",{});var Rc=l(da);fl=n(Rc,"Vejamos o que um tokenizador r\xE1pido nos permite fazer. Primeiro, podemos acessar os tokens sem precisar converter os IDs de volta em tokens:"),Rc.forEach(a),Zn=c(s),b(ge.$$.fragment,s),st=c(s),b(je.$$.fragment,s),et=c(s),$s=r(s,"P",{});var Ca=l($s);hl=n(Ca,"No caso, o token no \xEDndice 5 \xE9 "),Ua=r(Ca,"CODE",{});var Ac=l(Ua);xl=n(Ac,"##yl"),Ac.forEach(a),gl=n(Ca,", que faz parte da palavra \u201CSylvain\u201D na senten\xE7a original. N\xF3s podemos tamb\xE9m usar o metodo "),Ya=r(Ca,"CODE",{});var Bc=l(Ya);jl=n(Bc,"words_ids()"),Bc.forEach(a),bl=n(Ca," para obter o \xEDndice da palavra de onde cada palavra vem:"),Ca.forEach(a),at=c(s),b(be.$$.fragment,s),ot=c(s),b(ve.$$.fragment,s),nt=c(s),J=r(s,"P",{});var cs=l(J);vl=n(cs,"Podemos observar que as palavras especiais do tokenizador "),Ja=r(cs,"CODE",{});var Fc=l(Ja);_l=n(Fc,"[CLS]"),Fc.forEach(a),$l=n(cs," e "),Ka=r(cs,"CODE",{});var Nc=l(Ka);kl=n(Nc,"[SEP]"),Nc.forEach(a),El=n(cs," s\xE3o mapeados para "),Wa=r(cs,"CODE",{});var Xc=l(Wa);wl=n(Xc,"None"),Xc.forEach(a),ql=n(cs,", e ent\xE3o cada token \xE9 mapeada para a palavra de onde se origina. Isso \xE9 especialmente \xFAtil para determinar se um token est\xE1 no in\xEDcio da palavra ou se dois tokens est\xE3o em uma mesma palavra. Poder\xEDamos contar com o prefix "),Za=r(cs,"CODE",{});var Hc=l(Za);yl=n(Hc,"##"),Hc.forEach(a),Ol=n(cs," para isso, mas apenas para tokenizadores do tipo BERT; este m\xE9todo funciona para qualquer tipo de tokenizador, desde que seja do tipo r\xE1pido. No pr\xF3ximo cap\xEDtulo, n\xF3s veremos como podemos usar esse recurso para aplicar os r\xF3tulos que temos para cada palavra adequadamente aos tokens em tarefas como reconhecimento de entidade nomeada (em ingl\xEAs, Named Entity Recognition, ou NER) e marca\xE7\xE3o de parte da fala (em ingl\xEAs, part-of-speech, ou POS). Tamb\xE9m podemos us\xE1-lo para mascarar todos os tokens provenientes da mesma palavra na modelagem de linguagem mascarada (uma t\xE9cnica chamada "),so=r(cs,"EM",{});var Mc=l(so);zl=n(Mc,"mascaramento da palavra inteira"),Mc.forEach(a),Cl=n(cs,")"),cs.forEach(a),tt=c(s),b(Ls.$$.fragment,s),rt=c(s),ks=r(s,"P",{});var Pa=l(ks);Pl=n(Pa,"Da mesma forma, existe um m\xE9todo "),eo=r(Pa,"CODE",{});var Gc=l(eo);Il=n(Gc,"sentence_ids()"),Gc.forEach(a),Dl=n(Pa," que podemos usar para mapear um token para a senten\xE7a de onde veio (embora, neste caso, o "),ao=r(Pa,"CODE",{});var Lc=l(ao);Tl=n(Lc,"token_type_ids"),Lc.forEach(a),Sl=n(Pa," retornado pelo tokenizador possa nos dar a mesma informa\xE7\xE3o)."),Pa.forEach(a),lt=c(s),G=r(s,"P",{});var as=l(G);Rl=n(as,"Por fim, podemos mapear qualquer palavra ou token para caracteres no texto original (e vice-versa) atrav\xE9s dos m\xE9todos "),oo=r(as,"CODE",{});var Vc=l(oo);Al=n(Vc,"word_to_chars()"),Vc.forEach(a),Bl=n(as," ou "),no=r(as,"CODE",{});var Qc=l(no);Fl=n(Qc,"token_to_chars()"),Qc.forEach(a),Nl=n(as," e "),to=r(as,"CODE",{});var Uc=l(to);Xl=n(Uc,"char_to_word()"),Uc.forEach(a),Hl=n(as," ou "),ro=r(as,"CODE",{});var Yc=l(ro);Ml=n(Yc,"char_to_token()"),Yc.forEach(a),Gl=n(as,". Por exemplo, o m\xE9todo "),lo=r(as,"CODE",{});var Jc=l(lo);Ll=n(Jc,"word_ids()"),Jc.forEach(a),Vl=n(as," nos diz que "),po=r(as,"CODE",{});var Kc=l(po);Ql=n(Kc,"##yl"),Kc.forEach(a),Ul=n(as," \xE9 parte da palavra no \xEDndice 3, mas qual palavra est\xE1 na frase? Podemos descobrir da seguinte forma:"),as.forEach(a),pt=c(s),b(_e.$$.fragment,s),it=c(s),b($e.$$.fragment,s),dt=c(s),Es=r(s,"P",{});var Ia=l(Es);Yl=n(Ia,"Como mencionamos anteriormente, isso \xE9 apoiado pelo fato de que o tokenizador r\xE1pido acompanha o intervalo de texto de cada token em uma lista de "),io=r(Ia,"EM",{});var Wc=l(io);Jl=n(Wc,"offsets"),Wc.forEach(a),Kl=n(Ia,". Para ilustrar seu uso, mostraremos a seguir como replicar manualmente os resultados do pipeline "),co=r(Ia,"CODE",{});var Zc=l(co);Wl=n(Zc,"token-classification"),Zc.forEach(a),Zl=n(Ia,"."),Ia.forEach(a),ct=c(s),b(Vs.$$.fragment,s),mt=c(s),Is=r(s,"H2",{class:!0});var mr=l(Is);Qs=r(mr,"A",{id:!0,class:!0,href:!0});var sm=l(Qs);mo=r(sm,"SPAN",{});var em=l(mo);b(ke.$$.fragment,em),em.forEach(a),sm.forEach(a),sp=c(mr),ca=r(mr,"SPAN",{});var sc=l(ca);ep=n(sc,"Dentro do pipeline "),uo=r(sc,"CODE",{});var am=l(uo);ap=n(am,"token-classification"),am.forEach(a),sc.forEach(a),mr.forEach(a),ut=c(s),ss=r(s,"P",{});var qs=l(ss);op=n(qs,"No "),ma=r(qs,"A",{href:!0});var om=l(ma);np=n(om,"Cap\xEDtulo 1"),om.forEach(a),tp=n(qs," tivemos o primeiro gosto de aplicar o NER \u2014 onde a tarefa \xE9 identificar quais partes do texto correspondem a entidades como pessoas, locais ou organiza\xE7\xF5es \u2014 com a fun\xE7\xE3o do \u{1F917} Transformers "),fo=r(qs,"CODE",{});var nm=l(fo);rp=n(nm,"pipeline()"),nm.forEach(a),lp=n(qs,". Ent\xE3o, no "),ua=r(qs,"A",{href:!0});var tm=l(ua);pp=n(tm,"Cap\xEDtulo 2"),tm.forEach(a),ip=n(qs,", vimos como um pipeline agrupa os tr\xEAs est\xE1gios necess\xE1rios para obter as previs\xF5es de um texto: tokeniza\xE7\xE3o, passagem das entradas pelo modelo e p\xF3s-processamento. As duas primeiras etapas do pipeline "),ho=r(qs,"CODE",{});var rm=l(ho);dp=n(rm,"token-classification"),rm.forEach(a),cp=n(qs," s\xE3o as mesmas de qualquer outro pipeline, mas o p\xF3s-processamento \xE9 um pouco mais complexo \u2014 vejamos como!"),qs.forEach(a),ft=c(s),xs.l(s),fa=c(s),Ds=r(s,"H3",{class:!0});var ur=l(Ds);Us=r(ur,"A",{id:!0,class:!0,href:!0});var lm=l(Us);xo=r(lm,"SPAN",{});var pm=l(xo);b(Ee.$$.fragment,pm),pm.forEach(a),lm.forEach(a),mp=c(ur),go=r(ur,"SPAN",{});var im=l(go);up=n(im,"Obtendo os resultados b\xE1sicos com o pipeline"),im.forEach(a),ur.forEach(a),ht=c(s),Ys=r(s,"P",{});var fr=l(Ys);fp=n(fr,"Primeiro, vamos usar um pipeline de classifica\xE7\xE3o de token para que possamos obter alguns resultados para comparar manualmente. O modelo usado por padr\xE3o \xE9 "),we=r(fr,"A",{href:!0,rel:!0});var dm=l(we);jo=r(dm,"CODE",{});var cm=l(jo);hp=n(cm,"dbmdz/bert-large-cased-finetuned-conll03-english"),cm.forEach(a),dm.forEach(a),xp=n(fr,"; ele executa NER em frases:"),fr.forEach(a),xt=c(s),b(qe.$$.fragment,s),gt=c(s),b(ye.$$.fragment,s),jt=c(s),ha=r(s,"P",{});var mm=l(ha);gp=n(mm,"O modelo identificou corretamente cada token gerado por \u201CSylvain\u201D como uma pessoa, cada token gerado por \u201CHugging Face\u201D como uma organiza\xE7\xE3o e o token \u201CBrooklyn\u201D como um local. Tamb\xE9m podemos pedir ao pipeline para agrupar os tokens que correspondem \xE0 mesma entidade:"),mm.forEach(a),bt=c(s),b(Oe.$$.fragment,s),vt=c(s),b(ze.$$.fragment,s),_t=c(s),L=r(s,"P",{});var os=l(L);jp=n(os,"O par\xE2metro "),bo=r(os,"CODE",{});var um=l(bo);bp=n(um,"aggregation_strategy"),um.forEach(a),vp=n(os," escolhido mudar\xE1 as pontua\xE7\xF5es calculadas para cada entidade agrupada. Com o valor "),vo=r(os,"CODE",{});var fm=l(vo);_p=n(fm,'"simple"'),fm.forEach(a),$p=n(os,", a pontua\xE7\xE3o \xE9 apenas a m\xE9dia das pontua\xE7\xF5es de cada token na entidade dada: por exemplo, a pontua\xE7\xE3o de \u201CSylvain\u201D \xE9 a m\xE9dia das pontua\xE7\xF5es que vimos no exemplo anterior para os tokens "),_o=r(os,"CODE",{});var hm=l(_o);kp=n(hm,"S"),hm.forEach(a),Ep=n(os,", "),$o=r(os,"CODE",{});var xm=l($o);wp=n(xm,"##yl"),xm.forEach(a),qp=n(os,", "),ko=r(os,"CODE",{});var gm=l(ko);yp=n(gm,"##va"),gm.forEach(a),Op=n(os,", e "),Eo=r(os,"CODE",{});var jm=l(Eo);zp=n(jm,"##in"),jm.forEach(a),Cp=n(os,". Outras estrat\xE9gias dispon\xEDveis s\xE3o:"),os.forEach(a),$t=c(s),ws=r(s,"UL",{});var Da=l(ws);Js=r(Da,"LI",{});var In=l(Js);wo=r(In,"CODE",{});var bm=l(wo);Pp=n(bm,'"first"'),bm.forEach(a),Ip=n(In,", onde a pontua\xE7\xE3o de cada entidade \xE9 a pontua\xE7\xE3o do primeiro token dessa entidade (portanto, para \u201CSylvain\u201D seria 0.993828, a pontua\xE7\xE3o do token "),qo=r(In,"CODE",{});var vm=l(qo);Dp=n(vm,"S"),vm.forEach(a),Tp=n(In,")"),In.forEach(a),Sp=c(Da),Ks=r(Da,"LI",{});var Dn=l(Ks);yo=r(Dn,"CODE",{});var _m=l(yo);Rp=n(_m,'"max"'),_m.forEach(a),Ap=n(Dn,", onde a pontua\xE7\xE3o de cada entidade \xE9 a pontua\xE7\xE3o m\xE1xima dos tokens naquela entidade (portanto, para \u201CHugging Face\u201D seria 0.98879766, a pontua\xE7\xE3o do token "),Oo=r(Dn,"CODE",{});var $m=l(Oo);Bp=n($m,'"Face"'),$m.forEach(a),Fp=n(Dn,")"),Dn.forEach(a),Np=c(Da),es=r(Da,"LI",{});var _s=l(es);zo=r(_s,"CODE",{});var km=l(zo);Xp=n(km,'"average"'),km.forEach(a),Hp=n(_s,", onde a pontua\xE7\xE3o de cada entidade \xE9 a m\xE9dia das pontua\xE7\xF5es das palavras que comp\xF5em aquela entidade (assim para \u201CSylvain\u201D n\xE3o haveria diferen\xE7a da estrat\xE9gia "),Co=r(_s,"CODE",{});var Em=l(Co);Mp=n(Em,'"simple"'),Em.forEach(a),Gp=n(_s,", mas "),Po=r(_s,"CODE",{});var wm=l(Po);Lp=n(wm,'"Hugging Face"'),wm.forEach(a),Vp=n(_s," teria uma pontua\xE7\xE3o de 0.9819, a m\xE9dia das pontua\xE7\xF5es para "),Io=r(_s,"CODE",{});var qm=l(Io);Qp=n(qm,'"Hugging"'),qm.forEach(a),Up=n(_s,", 0.975, e "),Do=r(_s,"CODE",{});var ym=l(Do);Yp=n(ym,'"Face"'),ym.forEach(a),Jp=n(_s,", 0.98879)"),_s.forEach(a),Da.forEach(a),kt=c(s),Ws=r(s,"P",{});var hr=l(Ws);Kp=n(hr,"Agora vejamos como obter esses resultados sem usar a fun\xE7\xE3o "),To=r(hr,"CODE",{});var Om=l(To);Wp=n(Om,"pipeline()"),Om.forEach(a),Zp=n(hr,"!"),hr.forEach(a),Et=c(s),Ts=r(s,"H3",{class:!0});var xr=l(Ts);Zs=r(xr,"A",{id:!0,class:!0,href:!0});var zm=l(Zs);So=r(zm,"SPAN",{});var Cm=l(So);b(Ce.$$.fragment,Cm),Cm.forEach(a),zm.forEach(a),si=c(xr),Ro=r(xr,"SPAN",{});var Pm=l(Ro);ei=n(Pm,"Das entradas \xE0s previs\xF5es"),Pm.forEach(a),xr.forEach(a),wt=c(s),js.l(s),xa=c(s),ga=r(s,"P",{});var Im=l(ga);ai=n(Im,"Temos um lote com 1 sequ\xEAncia de 19 tokens e o modelo tem 9 r\xF3tulos diferentes, ent\xE3o a sa\xEDda do modelo tem um tamanho de 1 x 19 x 9. Assim como para o pipeline de classifica\xE7\xE3o de texto, usamos uma fun\xE7\xE3o softmax para converter esses logits para probabilidades, e pegamos o argmax para obter previs\xF5es (note que podemos pegar o argmax nos logits porque o softmax n\xE3o altera a ordem):"),Im.forEach(a),qt=c(s),vs.l(s),ja=c(s),b(Pe.$$.fragment,s),yt=c(s),se=r(s,"P",{});var gr=l(se);oi=n(gr,"O atributo "),Ao=r(gr,"CODE",{});var Dm=l(Ao);ni=n(Dm,"model.config.id2label"),Dm.forEach(a),ti=n(gr," cont\xE9m o mapeamento de \xEDndices para r\xF3tulos que podemos usar para entender as previs\xF5es:"),gr.forEach(a),Ot=c(s),b(Ie.$$.fragment,s),zt=c(s),b(De.$$.fragment,s),Ct=c(s),z=r(s,"P",{});var A=l(z);ri=n(A,"Como vimos anteriormente, existem 9 r\xF3tulos: "),Bo=r(A,"CODE",{});var Tm=l(Bo);li=n(Tm,"O"),Tm.forEach(a),pi=n(A," \xE9 o r\xF3tulo para os tokens que n\xE3o est\xE3o em nenhuma entidade nomeada, e ent\xE3o temos dois r\xF3tulos para cada tipo de entidade (miscel\xE2nia, pessoa, organiza\xE7\xE3o e localiza\xE7\xE3o). O r\xF3tulo "),Fo=r(A,"CODE",{});var Sm=l(Fo);ii=n(Sm,"B-XXX"),Sm.forEach(a),di=n(A," indica que o token est\xE1 no in\xEDcio de uma entidade "),No=r(A,"CODE",{});var Rm=l(No);ci=n(Rm,"XXX"),Rm.forEach(a),mi=n(A," e o r\xF3tulo "),Xo=r(A,"CODE",{});var Am=l(Xo);ui=n(Am,"I-XXX"),Am.forEach(a),fi=n(A," indica que o token est\xE1 dentro da entidade "),Ho=r(A,"CODE",{});var Bm=l(Ho);hi=n(Bm,"XXX"),Bm.forEach(a),xi=n(A,". No caso do exemplo atual, esperar\xEDamos que o nosso modelo classificasse o token "),Mo=r(A,"CODE",{});var Fm=l(Mo);gi=n(Fm,"S"),Fm.forEach(a),ji=n(A," como "),Go=r(A,"CODE",{});var Nm=l(Go);bi=n(Nm,"B-PER"),Nm.forEach(a),vi=n(A," (in\xEDcio de uma entidade pessoa) e os tokens "),Lo=r(A,"CODE",{});var Xm=l(Lo);_i=n(Xm,"##yl"),Xm.forEach(a),$i=n(A,", "),Vo=r(A,"CODE",{});var Hm=l(Vo);ki=n(Hm,"##va"),Hm.forEach(a),Ei=n(A," e "),Qo=r(A,"CODE",{});var Mm=l(Qo);wi=n(Mm,"##in"),Mm.forEach(a),qi=n(A," como "),Uo=r(A,"CODE",{});var Gm=l(Uo);yi=n(Gm,"I-PER"),Gm.forEach(a),Oi=n(A," (dentro da entidade pessoa)."),A.forEach(a),Pt=c(s),H=r(s,"P",{});var Q=l(H);zi=n(Q,"Voc\xEA pode pensar que o modelo estava errado neste caso, pois deu o r\xF3tulo "),Yo=r(Q,"CODE",{});var Lm=l(Yo);Ci=n(Lm,"I-PER"),Lm.forEach(a),Pi=n(Q," a todos esses quatro tokens, mas isso n\xE3o \xE9 totalmente verdade. Na realidade, existem dois formatos para esses r\xF3tulos: "),Jo=r(Q,"CODE",{});var Vm=l(Jo);Ii=n(Vm,"B-"),Vm.forEach(a),Di=n(Q," e "),Ko=r(Q,"CODE",{});var Qm=l(Ko);Ti=n(Qm,"I-"),Qm.forEach(a),Si=n(Q,": "),Wo=r(Q,"EM",{});var Um=l(Wo);Ri=n(Um,"IOB1"),Um.forEach(a),Ai=n(Q," e "),Zo=r(Q,"EM",{});var Ym=l(Zo);Bi=n(Ym,"IOB2"),Ym.forEach(a),Fi=n(Q,". O formato IOB2 (em rosa abaixo), \xE9 o que introduzimos, enquanto que no formato IOB1 (em azul), os r\xF3tulos que come\xE7am com "),sn=r(Q,"CODE",{});var Jm=l(sn);Ni=n(Jm,"B-"),Jm.forEach(a),Xi=n(Q," s\xE3o usados apenas para separar duas entidades adjacentes do mesmo tipo. O modelo que estamos usando foi ajustado em um conjunto de dados usando esse formato, e \xE9 por isso que ele atribui o r\xF3tulo "),en=r(Q,"CODE",{});var Km=l(en);Hi=n(Km,"I-PER"),Km.forEach(a),Mi=n(Q," ao token "),an=r(Q,"CODE",{});var Wm=l(an);Gi=n(Wm,"S"),Wm.forEach(a),Li=n(Q,"."),Q.forEach(a),It=c(s),Ss=r(s,"DIV",{class:!0});var jr=l(Ss);Te=r(jr,"IMG",{class:!0,src:!0,alt:!0}),Vi=c(jr),Se=r(jr,"IMG",{class:!0,src:!0,alt:!0}),jr.forEach(a),Dt=c(s),ee=r(s,"P",{});var br=l(ee);Qi=n(br,"Com este mapa, estamos prontos para reproduzir (quase inteiramente) os resultados do primeiro pipeline \u2014 podemos apenas pegar a pontua\xE7\xE3o e o r\xF3tulo de cada token que n\xE3o foi classificado como "),on=r(br,"CODE",{});var Zm=l(on);Ui=n(Zm,"O"),Zm.forEach(a),Yi=n(br,":"),br.forEach(a),Tt=c(s),b(Re.$$.fragment,s),St=c(s),b(Ae.$$.fragment,s),Rt=c(s),is=r(s,"P",{});var te=l(is);Ji=n(te,"Isso \xE9 muito parecido com o que t\xEDnhamos antes, com uma exce\xE7\xE3o: o pipeline tamb\xE9m nos dava informa\xE7\xF5es sobre o "),nn=r(te,"CODE",{});var su=l(nn);Ki=n(su,"start"),su.forEach(a),Wi=n(te," e "),tn=r(te,"CODE",{});var eu=l(tn);Zi=n(eu,"end"),eu.forEach(a),sd=n(te," de cada entidade na frase original. \xC9 aqui que nosso mapeamento de offset entrar\xE1 em a\xE7\xE3o. Para obter tais offsets, basta definir "),rn=r(te,"CODE",{});var au=l(rn);ed=n(au,"return_offsets_mapping=True"),au.forEach(a),ad=n(te," quando aplicamos o tokenizador \xE0s nossas entradas:"),te.forEach(a),At=c(s),b(Be.$$.fragment,s),Bt=c(s),b(Fe.$$.fragment,s),Ft=c(s),ds=r(s,"P",{});var re=l(ds);od=n(re,"Cada tupla \xE9 o intervalo de texto correspondente a cada token, onde "),ln=r(re,"CODE",{});var ou=l(ln);nd=n(ou,"(0, 0)"),ou.forEach(a),td=n(re," \xE9 reservado para os tokens especiais. Vimos antes que o token no \xEDndice 5 \xE9 "),pn=r(re,"CODE",{});var nu=l(pn);rd=n(nu,"##yl"),nu.forEach(a),ld=n(re,", que tem "),dn=r(re,"CODE",{});var tu=l(dn);pd=n(tu,"(12, 14)"),tu.forEach(a),id=n(re," como offset aqui. Se pegarmos a fatia correspondente em nosso exemplo:"),re.forEach(a),Nt=c(s),b(Ne.$$.fragment,s),Xt=c(s),ae=r(s,"P",{});var vr=l(ae);dd=n(vr,"obtemos o intervalo adequado de texto sem o "),cn=r(vr,"CODE",{});var ru=l(cn);cd=n(ru,"##"),ru.forEach(a),md=n(vr,":"),vr.forEach(a),Ht=c(s),b(Xe.$$.fragment,s),Mt=c(s),ba=r(s,"P",{});var lu=l(ba);ud=n(lu,"Usando isso, agora podemos completar os resultados anteriores:"),lu.forEach(a),Gt=c(s),b(He.$$.fragment,s),Lt=c(s),b(Me.$$.fragment,s),Vt=c(s),va=r(s,"P",{});var pu=l(va);fd=n(pu,"Este \xE9 o mesmo resultado que obtivemos no primeiro pipeline!"),pu.forEach(a),Qt=c(s),Rs=r(s,"H3",{class:!0});var _r=l(Rs);oe=r(_r,"A",{id:!0,class:!0,href:!0});var iu=l(oe);mn=r(iu,"SPAN",{});var du=l(mn);b(Ge.$$.fragment,du),du.forEach(a),iu.forEach(a),hd=c(_r),un=r(_r,"SPAN",{});var cu=l(un);xd=n(cu,"Agrupando entidades"),cu.forEach(a),_r.forEach(a),Ut=c(s),V=r(s,"P",{});var ns=l(V);gd=n(ns,"Usar os offsets para determinar as chaves inicial e final de cada entidade \xE9 \xFAtil, mas essa informa\xE7\xE3o n\xE3o \xE9 estritamente necess\xE1ria. Quando queremos agrupar as entidades, no entanto, os offsets nos poupar\xE3o muito c\xF3digo confuso. Por exemplo, se quisermos agrupar os tokens "),fn=r(ns,"CODE",{});var mu=l(fn);jd=n(mu,"Hu"),mu.forEach(a),bd=n(ns,", "),hn=r(ns,"CODE",{});var uu=l(hn);vd=n(uu,"##gging"),uu.forEach(a),_d=n(ns," e "),xn=r(ns,"CODE",{});var fu=l(xn);$d=n(fu,"Face"),fu.forEach(a),kd=n(ns,", podemos fazer regras especiais que digam que os dois primeiros devem ser anexados e removido o "),gn=r(ns,"CODE",{});var hu=l(gn);Ed=n(hu,"##"),hu.forEach(a),wd=n(ns,", e o "),jn=r(ns,"CODE",{});var xu=l(jn);qd=n(xu,"Face"),xu.forEach(a),yd=n(ns," deve ser adicionado com um espa\xE7o, pois n\xE3o come\xE7a com "),bn=r(ns,"CODE",{});var gu=l(bn);Od=n(gu,"##"),gu.forEach(a),zd=n(ns," \u2014 mas isso s\xF3 funcionaria para esse tipo espec\xEDfico de tokenizador. Ter\xEDamos que escrever outro conjunto de regras para um tokenizador SentencePiece ou Byte-Pair-Encoding (discutido mais adiante neste cap\xEDtulo)."),ns.forEach(a),Yt=c(s),K=r(s,"P",{});var ms=l(K);Cd=n(ms,"Com os offsets, todo esse c\xF3digo personalizado desaparece: podemos apenas pegar o intervalo no texto original que come\xE7a com o primeiro token e termina com o \xFAltimo token. Ent\xE3o, no caso dos tokens "),vn=r(ms,"CODE",{});var ju=l(vn);Pd=n(ju,"Hu"),ju.forEach(a),Id=n(ms,", "),_n=r(ms,"CODE",{});var bu=l(_n);Dd=n(bu,"##ging"),bu.forEach(a),Td=n(ms," e "),$n=r(ms,"CODE",{});var vu=l($n);Sd=n(vu,"Face"),vu.forEach(a),Rd=n(ms,", devemos come\xE7ar no caractere 33 (o in\xEDcio de "),kn=r(ms,"CODE",{});var _u=l(kn);Ad=n(_u,"Hu"),_u.forEach(a),Bd=n(ms,") e terminar antes do caractere 45 (o final de "),En=r(ms,"CODE",{});var $u=l(En);Fd=n($u,"Face"),$u.forEach(a),Nd=n(ms,"):"),ms.forEach(a),Jt=c(s),b(Le.$$.fragment,s),Kt=c(s),b(Ve.$$.fragment,s),Wt=c(s),W=r(s,"P",{});var us=l(W);Xd=n(us,"Para escrever o c\xF3digo para o p\xF3s-processamento das previs\xF5es ao agrupar entidades, agruparemos entidades consecutivas e rotuladas com "),wn=r(us,"CODE",{});var ku=l(wn);Hd=n(ku,"I-XXX"),ku.forEach(a),Md=n(us,", excento a primeira, que pode ser rotulada como "),qn=r(us,"CODE",{});var Eu=l(qn);Gd=n(Eu,"B-XXX"),Eu.forEach(a),Ld=n(us," ou "),yn=r(us,"CODE",{});var wu=l(yn);Vd=n(wu,"I-XXX"),wu.forEach(a),Qd=n(us," (portanto, paramos de agrupar uma entidade quando obtemos um "),On=r(us,"CODE",{});var qu=l(On);Ud=n(qu,"O"),qu.forEach(a),Yd=n(us,", um novo tipo de entidade ou um "),zn=r(us,"CODE",{});var yu=l(zn);Jd=n(yu,"B-XXX"),yu.forEach(a),Kd=n(us," que nos informa que uma entidade do mesmo tipo est\xE1 iniciando):"),us.forEach(a),Zt=c(s),b(Qe.$$.fragment,s),sr=c(s),_a=r(s,"P",{});var Ou=l(_a);Wd=n(Ou,"E obtemos os mesmos resultados do nosso segundo pipeline!"),Ou.forEach(a),er=c(s),b(Ue.$$.fragment,s),ar=c(s),$a=r(s,"P",{});var zu=l($a);Zd=n(zu,"Outro exemplo de uma tarefa onde esses offsets s\xE3o extremamente \xFAteis \xE9 a resposta a perguntas. O conhecimento deste pipeline, que faremos na pr\xF3xima se\xE7\xE3o, tamb\xE9m nos permitir\xE1 dar uma olhada em um \xFAltimo recurso dos tokenizadores na biblioteca \u{1F917} Transformers: lidar com tokens em excesso quando truncamos uma entrada em um determinado comprimento."),zu.forEach(a),this.h()},h(){$(m,"name","hf:doc:metadata"),$(m,"content",JSON.stringify(Ju)),$(q,"id","os-poderes-especiais-dos-tokenizadores-rpidos"),$(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(q,"href","#os-poderes-especiais-dos-tokenizadores-rpidos"),$(D,"class","relative group"),$(ls,"href","/course/chapter1"),$(sa,"href","/course/chapter5/3"),$(ea,"align","center"),$(aa,"align","center"),$(Ra,"align","center"),$(oa,"align","center"),$(na,"align","center"),$(ta,"align","center"),$(ra,"align","center"),$(la,"align","center"),$(pa,"align","center"),$(Ns,"id","codificao-em-lote"),$(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ns,"href","#codificao-em-lote"),$(Ps,"class","relative group"),$(Qs,"id","dentro-do-pipeline-tokenclassification"),$(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Qs,"href","#dentro-do-pipeline-tokenclassification"),$(Is,"class","relative group"),$(ma,"href","/course/chapter1"),$(ua,"href","/course/chapter2"),$(Us,"id","obtendo-os-resultados-bsicos-com-o-pipeline"),$(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Us,"href","#obtendo-os-resultados-bsicos-com-o-pipeline"),$(Ds,"class","relative group"),$(we,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),$(we,"rel","nofollow"),$(Zs,"id","das-entradas-s-previses"),$(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Zs,"href","#das-entradas-s-previses"),$(Ts,"class","relative group"),$(Te,"class","block dark:hidden"),Cu(Te.src,ac="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions.svg")||$(Te,"src",ac),$(Te,"alt","IOB1 vs IOB2 format"),$(Se,"class","hidden dark:block"),Cu(Se.src,oc="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/IOB_versions-dark.svg")||$(Se,"src",oc),$(Se,"alt","IOB1 vs IOB2 format"),$(Ss,"class","flex justify-center"),$(oe,"id","agrupando-entidades"),$(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(oe,"href","#agrupando-entidades"),$(Rs,"class","relative group")},m(s,p){e(document.head,m),i(s,g,p),v(u,s,p),i(s,k,p),i(s,D,p),e(D,q),e(q,S),v(F,S,null),e(D,M),e(D,T),e(T,U),i(s,O,p),Ye[B].m(s,p),i(s,P,p),i(s,y,p),e(y,N),e(y,R),e(R,Y),e(y,X),e(y,Z),e(Z,f),e(y,w),e(y,fs),e(fs,ys),e(y,ts),e(y,ls),e(ls,wr),e(y,qr),i(s,Tn,p),v(le,s,p),i(s,Sn,p),i(s,As,p),e(As,yr),e(As,sa),e(sa,Or),e(As,zr),i(s,Rn,p),i(s,Bs,p),e(Bs,Sa),e(Sa,Os),e(Os,ea),e(ea,Cr),e(Os,Pr),e(Os,aa),e(aa,Ir),e(Os,Dr),e(Os,Ra),e(Bs,Tr),e(Bs,pe),e(pe,zs),e(zs,oa),e(oa,Aa),e(Aa,Sr),e(zs,Rr),e(zs,na),e(na,Ar),e(zs,Br),e(zs,ta),e(ta,Fr),e(pe,Nr),e(pe,Cs),e(Cs,ra),e(ra,Ba),e(Ba,Xr),e(Cs,Hr),e(Cs,la),e(la,Mr),e(Cs,Gr),e(Cs,pa),e(pa,Lr),i(s,An,p),v(Fs,s,p),i(s,Bn,p),i(s,Ps,p),e(Ps,Ns),e(Ns,Fa),v(ie,Fa,null),e(Ps,Vr),e(Ps,Na),e(Na,Qr),i(s,Fn,p),v(de,s,p),i(s,Nn,p),i(s,Xs,p),e(Xs,Ur),e(Xs,Xa),e(Xa,Yr),e(Xs,Jr),i(s,Xn,p),i(s,Hs,p),e(Hs,Kr),e(Hs,Ha),e(Ha,Wr),e(Hs,Zr),i(s,Hn,p),i(s,ia,p),e(ia,sl),i(s,Mn,p),v(ce,s,p),i(s,Gn,p),i(s,Ms,p),e(Ms,el),e(Ms,Ma),e(Ma,al),e(Ms,ol),i(s,Ln,p),v(me,s,p),i(s,Vn,p),i(s,ps,p),e(ps,nl),e(ps,Ga),e(Ga,tl),e(ps,rl),e(ps,La),e(La,ll),e(ps,pl),e(ps,Va),e(Va,il),e(ps,dl),i(s,Qn,p),v(ue,s,p),i(s,Un,p),v(fe,s,p),i(s,Yn,p),i(s,Gs,p),e(Gs,cl),e(Gs,Qa),e(Qa,ml),e(Gs,ul),i(s,Jn,p),v(he,s,p),i(s,Kn,p),v(xe,s,p),i(s,Wn,p),i(s,da,p),e(da,fl),i(s,Zn,p),v(ge,s,p),i(s,st,p),v(je,s,p),i(s,et,p),i(s,$s,p),e($s,hl),e($s,Ua),e(Ua,xl),e($s,gl),e($s,Ya),e(Ya,jl),e($s,bl),i(s,at,p),v(be,s,p),i(s,ot,p),v(ve,s,p),i(s,nt,p),i(s,J,p),e(J,vl),e(J,Ja),e(Ja,_l),e(J,$l),e(J,Ka),e(Ka,kl),e(J,El),e(J,Wa),e(Wa,wl),e(J,ql),e(J,Za),e(Za,yl),e(J,Ol),e(J,so),e(so,zl),e(J,Cl),i(s,tt,p),v(Ls,s,p),i(s,rt,p),i(s,ks,p),e(ks,Pl),e(ks,eo),e(eo,Il),e(ks,Dl),e(ks,ao),e(ao,Tl),e(ks,Sl),i(s,lt,p),i(s,G,p),e(G,Rl),e(G,oo),e(oo,Al),e(G,Bl),e(G,no),e(no,Fl),e(G,Nl),e(G,to),e(to,Xl),e(G,Hl),e(G,ro),e(ro,Ml),e(G,Gl),e(G,lo),e(lo,Ll),e(G,Vl),e(G,po),e(po,Ql),e(G,Ul),i(s,pt,p),v(_e,s,p),i(s,it,p),v($e,s,p),i(s,dt,p),i(s,Es,p),e(Es,Yl),e(Es,io),e(io,Jl),e(Es,Kl),e(Es,co),e(co,Wl),e(Es,Zl),i(s,ct,p),v(Vs,s,p),i(s,mt,p),i(s,Is,p),e(Is,Qs),e(Qs,mo),v(ke,mo,null),e(Is,sp),e(Is,ca),e(ca,ep),e(ca,uo),e(uo,ap),i(s,ut,p),i(s,ss,p),e(ss,op),e(ss,ma),e(ma,np),e(ss,tp),e(ss,fo),e(fo,rp),e(ss,lp),e(ss,ua),e(ua,pp),e(ss,ip),e(ss,ho),e(ho,dp),e(ss,cp),i(s,ft,p),Je[hs].m(s,p),i(s,fa,p),i(s,Ds,p),e(Ds,Us),e(Us,xo),v(Ee,xo,null),e(Ds,mp),e(Ds,go),e(go,up),i(s,ht,p),i(s,Ys,p),e(Ys,fp),e(Ys,we),e(we,jo),e(jo,hp),e(Ys,xp),i(s,xt,p),v(qe,s,p),i(s,gt,p),v(ye,s,p),i(s,jt,p),i(s,ha,p),e(ha,gp),i(s,bt,p),v(Oe,s,p),i(s,vt,p),v(ze,s,p),i(s,_t,p),i(s,L,p),e(L,jp),e(L,bo),e(bo,bp),e(L,vp),e(L,vo),e(vo,_p),e(L,$p),e(L,_o),e(_o,kp),e(L,Ep),e(L,$o),e($o,wp),e(L,qp),e(L,ko),e(ko,yp),e(L,Op),e(L,Eo),e(Eo,zp),e(L,Cp),i(s,$t,p),i(s,ws,p),e(ws,Js),e(Js,wo),e(wo,Pp),e(Js,Ip),e(Js,qo),e(qo,Dp),e(Js,Tp),e(ws,Sp),e(ws,Ks),e(Ks,yo),e(yo,Rp),e(Ks,Ap),e(Ks,Oo),e(Oo,Bp),e(Ks,Fp),e(ws,Np),e(ws,es),e(es,zo),e(zo,Xp),e(es,Hp),e(es,Co),e(Co,Mp),e(es,Gp),e(es,Po),e(Po,Lp),e(es,Vp),e(es,Io),e(Io,Qp),e(es,Up),e(es,Do),e(Do,Yp),e(es,Jp),i(s,kt,p),i(s,Ws,p),e(Ws,Kp),e(Ws,To),e(To,Wp),e(Ws,Zp),i(s,Et,p),i(s,Ts,p),e(Ts,Zs),e(Zs,So),v(Ce,So,null),e(Ts,si),e(Ts,Ro),e(Ro,ei),i(s,wt,p),Ke[gs].m(s,p),i(s,xa,p),i(s,ga,p),e(ga,ai),i(s,qt,p),We[bs].m(s,p),i(s,ja,p),v(Pe,s,p),i(s,yt,p),i(s,se,p),e(se,oi),e(se,Ao),e(Ao,ni),e(se,ti),i(s,Ot,p),v(Ie,s,p),i(s,zt,p),v(De,s,p),i(s,Ct,p),i(s,z,p),e(z,ri),e(z,Bo),e(Bo,li),e(z,pi),e(z,Fo),e(Fo,ii),e(z,di),e(z,No),e(No,ci),e(z,mi),e(z,Xo),e(Xo,ui),e(z,fi),e(z,Ho),e(Ho,hi),e(z,xi),e(z,Mo),e(Mo,gi),e(z,ji),e(z,Go),e(Go,bi),e(z,vi),e(z,Lo),e(Lo,_i),e(z,$i),e(z,Vo),e(Vo,ki),e(z,Ei),e(z,Qo),e(Qo,wi),e(z,qi),e(z,Uo),e(Uo,yi),e(z,Oi),i(s,Pt,p),i(s,H,p),e(H,zi),e(H,Yo),e(Yo,Ci),e(H,Pi),e(H,Jo),e(Jo,Ii),e(H,Di),e(H,Ko),e(Ko,Ti),e(H,Si),e(H,Wo),e(Wo,Ri),e(H,Ai),e(H,Zo),e(Zo,Bi),e(H,Fi),e(H,sn),e(sn,Ni),e(H,Xi),e(H,en),e(en,Hi),e(H,Mi),e(H,an),e(an,Gi),e(H,Li),i(s,It,p),i(s,Ss,p),e(Ss,Te),e(Ss,Vi),e(Ss,Se),i(s,Dt,p),i(s,ee,p),e(ee,Qi),e(ee,on),e(on,Ui),e(ee,Yi),i(s,Tt,p),v(Re,s,p),i(s,St,p),v(Ae,s,p),i(s,Rt,p),i(s,is,p),e(is,Ji),e(is,nn),e(nn,Ki),e(is,Wi),e(is,tn),e(tn,Zi),e(is,sd),e(is,rn),e(rn,ed),e(is,ad),i(s,At,p),v(Be,s,p),i(s,Bt,p),v(Fe,s,p),i(s,Ft,p),i(s,ds,p),e(ds,od),e(ds,ln),e(ln,nd),e(ds,td),e(ds,pn),e(pn,rd),e(ds,ld),e(ds,dn),e(dn,pd),e(ds,id),i(s,Nt,p),v(Ne,s,p),i(s,Xt,p),i(s,ae,p),e(ae,dd),e(ae,cn),e(cn,cd),e(ae,md),i(s,Ht,p),v(Xe,s,p),i(s,Mt,p),i(s,ba,p),e(ba,ud),i(s,Gt,p),v(He,s,p),i(s,Lt,p),v(Me,s,p),i(s,Vt,p),i(s,va,p),e(va,fd),i(s,Qt,p),i(s,Rs,p),e(Rs,oe),e(oe,mn),v(Ge,mn,null),e(Rs,hd),e(Rs,un),e(un,xd),i(s,Ut,p),i(s,V,p),e(V,gd),e(V,fn),e(fn,jd),e(V,bd),e(V,hn),e(hn,vd),e(V,_d),e(V,xn),e(xn,$d),e(V,kd),e(V,gn),e(gn,Ed),e(V,wd),e(V,jn),e(jn,qd),e(V,yd),e(V,bn),e(bn,Od),e(V,zd),i(s,Yt,p),i(s,K,p),e(K,Cd),e(K,vn),e(vn,Pd),e(K,Id),e(K,_n),e(_n,Dd),e(K,Td),e(K,$n),e($n,Sd),e(K,Rd),e(K,kn),e(kn,Ad),e(K,Bd),e(K,En),e(En,Fd),e(K,Nd),i(s,Jt,p),v(Le,s,p),i(s,Kt,p),v(Ve,s,p),i(s,Wt,p),i(s,W,p),e(W,Xd),e(W,wn),e(wn,Hd),e(W,Md),e(W,qn),e(qn,Gd),e(W,Ld),e(W,yn),e(yn,Vd),e(W,Qd),e(W,On),e(On,Ud),e(W,Yd),e(W,zn),e(zn,Jd),e(W,Kd),i(s,Zt,p),v(Qe,s,p),i(s,sr,p),i(s,_a,p),e(_a,Wd),i(s,er,p),v(Ue,s,p),i(s,ar,p),i(s,$a,p),e($a,Zd),or=!0},p(s,[p]){const Ze={};p&1&&(Ze.fw=s[0]),u.$set(Ze);let ka=B;B=tc(s),B!==ka&&(kr(),h(Ye[ka],1,1,()=>{Ye[ka]=null}),$r(),C=Ye[B],C||(C=Ye[B]=nc[B](s),C.c()),x(C,1),C.m(P.parentNode,P));const Cn={};p&2&&(Cn.$$scope={dirty:p,ctx:s}),Fs.$set(Cn);const Pn={};p&2&&(Pn.$$scope={dirty:p,ctx:s}),Ls.$set(Pn);const rs={};p&2&&(rs.$$scope={dirty:p,ctx:s}),Vs.$set(rs);let Ea=hs;hs=lc(s),hs!==Ea&&(kr(),h(Je[Ea],1,1,()=>{Je[Ea]=null}),$r(),xs=Je[hs],xs||(xs=Je[hs]=rc[hs](s),xs.c()),x(xs,1),xs.m(fa.parentNode,fa));let wa=gs;gs=ic(s),gs!==wa&&(kr(),h(Ke[wa],1,1,()=>{Ke[wa]=null}),$r(),js=Ke[gs],js||(js=Ke[gs]=pc[gs](s),js.c()),x(js,1),js.m(xa.parentNode,xa));let qa=bs;bs=cc(s),bs!==qa&&(kr(),h(We[qa],1,1,()=>{We[qa]=null}),$r(),vs=We[bs],vs||(vs=We[bs]=dc[bs](s),vs.c()),x(vs,1),vs.m(ja.parentNode,ja))},i(s){or||(x(u.$$.fragment,s),x(F.$$.fragment,s),x(C),x(le.$$.fragment,s),x(Fs.$$.fragment,s),x(ie.$$.fragment,s),x(de.$$.fragment,s),x(ce.$$.fragment,s),x(me.$$.fragment,s),x(ue.$$.fragment,s),x(fe.$$.fragment,s),x(he.$$.fragment,s),x(xe.$$.fragment,s),x(ge.$$.fragment,s),x(je.$$.fragment,s),x(be.$$.fragment,s),x(ve.$$.fragment,s),x(Ls.$$.fragment,s),x(_e.$$.fragment,s),x($e.$$.fragment,s),x(Vs.$$.fragment,s),x(ke.$$.fragment,s),x(xs),x(Ee.$$.fragment,s),x(qe.$$.fragment,s),x(ye.$$.fragment,s),x(Oe.$$.fragment,s),x(ze.$$.fragment,s),x(Ce.$$.fragment,s),x(js),x(vs),x(Pe.$$.fragment,s),x(Ie.$$.fragment,s),x(De.$$.fragment,s),x(Re.$$.fragment,s),x(Ae.$$.fragment,s),x(Be.$$.fragment,s),x(Fe.$$.fragment,s),x(Ne.$$.fragment,s),x(Xe.$$.fragment,s),x(He.$$.fragment,s),x(Me.$$.fragment,s),x(Ge.$$.fragment,s),x(Le.$$.fragment,s),x(Ve.$$.fragment,s),x(Qe.$$.fragment,s),x(Ue.$$.fragment,s),or=!0)},o(s){h(u.$$.fragment,s),h(F.$$.fragment,s),h(C),h(le.$$.fragment,s),h(Fs.$$.fragment,s),h(ie.$$.fragment,s),h(de.$$.fragment,s),h(ce.$$.fragment,s),h(me.$$.fragment,s),h(ue.$$.fragment,s),h(fe.$$.fragment,s),h(he.$$.fragment,s),h(xe.$$.fragment,s),h(ge.$$.fragment,s),h(je.$$.fragment,s),h(be.$$.fragment,s),h(ve.$$.fragment,s),h(Ls.$$.fragment,s),h(_e.$$.fragment,s),h($e.$$.fragment,s),h(Vs.$$.fragment,s),h(ke.$$.fragment,s),h(xs),h(Ee.$$.fragment,s),h(qe.$$.fragment,s),h(ye.$$.fragment,s),h(Oe.$$.fragment,s),h(ze.$$.fragment,s),h(Ce.$$.fragment,s),h(js),h(vs),h(Pe.$$.fragment,s),h(Ie.$$.fragment,s),h(De.$$.fragment,s),h(Re.$$.fragment,s),h(Ae.$$.fragment,s),h(Be.$$.fragment,s),h(Fe.$$.fragment,s),h(Ne.$$.fragment,s),h(Xe.$$.fragment,s),h(He.$$.fragment,s),h(Me.$$.fragment,s),h(Ge.$$.fragment,s),h(Le.$$.fragment,s),h(Ve.$$.fragment,s),h(Qe.$$.fragment,s),h(Ue.$$.fragment,s),or=!1},d(s){a(m),s&&a(g),_(u,s),s&&a(k),s&&a(D),_(F),s&&a(O),Ye[B].d(s),s&&a(P),s&&a(y),s&&a(Tn),_(le,s),s&&a(Sn),s&&a(As),s&&a(Rn),s&&a(Bs),s&&a(An),_(Fs,s),s&&a(Bn),s&&a(Ps),_(ie),s&&a(Fn),_(de,s),s&&a(Nn),s&&a(Xs),s&&a(Xn),s&&a(Hs),s&&a(Hn),s&&a(ia),s&&a(Mn),_(ce,s),s&&a(Gn),s&&a(Ms),s&&a(Ln),_(me,s),s&&a(Vn),s&&a(ps),s&&a(Qn),_(ue,s),s&&a(Un),_(fe,s),s&&a(Yn),s&&a(Gs),s&&a(Jn),_(he,s),s&&a(Kn),_(xe,s),s&&a(Wn),s&&a(da),s&&a(Zn),_(ge,s),s&&a(st),_(je,s),s&&a(et),s&&a($s),s&&a(at),_(be,s),s&&a(ot),_(ve,s),s&&a(nt),s&&a(J),s&&a(tt),_(Ls,s),s&&a(rt),s&&a(ks),s&&a(lt),s&&a(G),s&&a(pt),_(_e,s),s&&a(it),_($e,s),s&&a(dt),s&&a(Es),s&&a(ct),_(Vs,s),s&&a(mt),s&&a(Is),_(ke),s&&a(ut),s&&a(ss),s&&a(ft),Je[hs].d(s),s&&a(fa),s&&a(Ds),_(Ee),s&&a(ht),s&&a(Ys),s&&a(xt),_(qe,s),s&&a(gt),_(ye,s),s&&a(jt),s&&a(ha),s&&a(bt),_(Oe,s),s&&a(vt),_(ze,s),s&&a(_t),s&&a(L),s&&a($t),s&&a(ws),s&&a(kt),s&&a(Ws),s&&a(Et),s&&a(Ts),_(Ce),s&&a(wt),Ke[gs].d(s),s&&a(xa),s&&a(ga),s&&a(qt),We[bs].d(s),s&&a(ja),_(Pe,s),s&&a(yt),s&&a(se),s&&a(Ot),_(Ie,s),s&&a(zt),_(De,s),s&&a(Ct),s&&a(z),s&&a(Pt),s&&a(H),s&&a(It),s&&a(Ss),s&&a(Dt),s&&a(ee),s&&a(Tt),_(Re,s),s&&a(St),_(Ae,s),s&&a(Rt),s&&a(is),s&&a(At),_(Be,s),s&&a(Bt),_(Fe,s),s&&a(Ft),s&&a(ds),s&&a(Nt),_(Ne,s),s&&a(Xt),s&&a(ae),s&&a(Ht),_(Xe,s),s&&a(Mt),s&&a(ba),s&&a(Gt),_(He,s),s&&a(Lt),_(Me,s),s&&a(Vt),s&&a(va),s&&a(Qt),s&&a(Rs),_(Ge),s&&a(Ut),s&&a(V),s&&a(Yt),s&&a(K),s&&a(Jt),_(Le,s),s&&a(Kt),_(Ve,s),s&&a(Wt),s&&a(W),s&&a(Zt),_(Qe,s),s&&a(sr),s&&a(_a),s&&a(er),_(Ue,s),s&&a(ar),s&&a($a)}}}const Ju={local:"os-poderes-especiais-dos-tokenizadores-rpidos",sections:[{local:"codificao-em-lote",title:"Codifica\xE7\xE3o em lote"},{local:"dentro-do-pipeline-tokenclassification",sections:[{local:"obtendo-os-resultados-bsicos-com-o-pipeline",title:"Obtendo os resultados b\xE1sicos com o pipeline "},{local:"das-entradas-s-previses",title:"Das entradas \xE0s previs\xF5es"},{local:"agrupando-entidades",title:"Agrupando entidades "}],title:"Dentro do pipeline `token-classification`"}],title:"Os poderes especiais dos tokenizadores r\xE1pidos"};function Ku(I,m,g){let u="pt";return Ru(()=>{const k=new URLSearchParams(window.location.search);g(0,u=k.get("fw")||"pt")}),[u]}class tf extends Iu{constructor(m){super();Du(this,m,Ku,Yu,Tu,{})}}export{tf as default,Ju as metadata};
