import{S as Qr,i as Wr,s as Yr,e as r,k as c,w as Ge,t as n,M as Kr,c as s,d as o,m as d,a as t,x as Re,h as i,b as l,N as Jr,G as a,g as m,y as je,L as Vr,q as Be,o as Ue,B as Je,v as Xr}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Zr}from"../../chunks/Youtube-hf-doc-builder.js";import{I as ya}from"../../chunks/IconCopyLink-hf-doc-builder.js";function es(Xo){let b,Qe,E,T,ge,M,Aa,be,Na,We,_,$,Ee,C,Ta,_e,$a,Ye,I,Ke,p,Sa,F,Ha,za,D,ka,Ma,O,Ca,Ia,x,Fa,Da,G,Oa,xa,R,Ga,Ra,Ve,w,S,we,j,ja,Le,Ba,Xe,me,Ua,Ze,L,B,Zo,Ja,U,er,ea,H,J,Qa,Q,Wa,Ya,Ka,qe,Va,aa,ce,Xa,oa,h,Pe,Za,eo,v,ao,W,oo,ro,Y,so,to,K,no,io,q,lo,V,uo,mo,X,co,po,ra,z,fo,Z,ho,vo,sa,P,k,ye,ee,go,Ae,bo,ta,de,Eo,na,ae,Ne,_o,wo,ia,oe,Te,Lo,qo,la,y,$e,Po,yo,Se,re,Ao,No,ua,se,He,To,$o,ma,te,ze,So,Ho,ca,A,ke,zo,ko,ne,Mo,Co,da,N,Me,Io,Fo,ie,Do,Oo,pa,pe,xo,fa,g,le,Go,Ce,Ro,jo,Bo,Ie,Uo,Jo,Fe,Qo,ha;return M=new ya({}),C=new ya({}),I=new Zr({props:{id:"00GKzGyWFEs"}}),j=new ya({}),ee=new ya({}),{c(){b=r("meta"),Qe=c(),E=r("h1"),T=r("a"),ge=r("span"),Ge(M.$$.fragment),Aa=c(),be=r("span"),Na=n("Introdu\xE7\xE3o"),We=c(),_=r("h2"),$=r("a"),Ee=r("span"),Ge(C.$$.fragment),Ta=c(),_e=r("span"),$a=n("Bem-vindo(a) ao Curso \u{1F917}!"),Ye=c(),Ge(I.$$.fragment),Ke=c(),p=r("p"),Sa=n("Esse curso te ensinar\xE1 sobre processamento de linguagem natural (PLN, ou NLP em ingl\xEAs) usando as bibliotecas do ecossistema "),F=r("a"),Ha=n("Hugging Face"),za=n(" \u2014 "),D=r("a"),ka=n("\u{1F917} Transformers"),Ma=n(", "),O=r("a"),Ca=n("\u{1F917} Datasets"),Ia=n(", "),x=r("a"),Fa=n("\u{1F917} Tokenizers"),Da=n(" e "),G=r("a"),Oa=n("\u{1F917} Accelerate"),xa=n(" \u2014 assim como a "),R=r("a"),Ga=n("Hugging Face Hub"),Ra=n(". \xC9 completamente gratuito e sem an\xFAncios!"),Ve=c(),w=r("h2"),S=r("a"),we=r("span"),Ge(j.$$.fragment),ja=c(),Le=r("span"),Ba=n("O que esperar?"),Xe=c(),me=r("p"),Ua=n("Aqui uma vis\xE3o geral do curso:"),Ze=c(),L=r("div"),B=r("img"),Ja=c(),U=r("img"),ea=c(),H=r("ul"),J=r("li"),Qa=n("Cap\xEDtulos 1 ao 4 d\xE1 uma introdu\xE7\xE3o para os principais conceitos da biblioteca Transformers \u{1F917}. No final dessa parte do curso, voc\xEA se tornar\xE1 familiar sobre como os modelos Transformers funcionam, saber\xE1 como usar o modelo da "),Q=r("a"),Wa=n("Hugging Face Hub"),Ya=n(", fazer um ajuste fino em um dataset e compartilhar os resultados no Hub!"),Ka=c(),qe=r("li"),Va=n("Cap\xEDtulo 5 ao 8 ensina o b\xE1sico dos \u{1F917} Datasets e \u{1F917} Tokenizadores antes de mergulhar nas tarefas cl\xE1ssicas de NLP. Ao final dessa parte, voc\xEA ser\xE1 capaz de resolver por conta pr\xF3pria os problemas mais comuns de NLP. Ao longo do caminho, voc\xEA aprender\xE1 como construir e compartilhar demonstra\xE7\xF5es de seus modelos e otimiza-los para ambientes de produ\xE7\xE3o. No final dessa parte, voc\xEA ser\xE1 capaz de aplicar a \u{1F917} Transformers para (quase) qualquer problema de aprendizagem de m\xE1quina!"),aa=c(),ce=r("p"),Xa=n("Esse curso:"),oa=c(),h=r("ul"),Pe=r("li"),Za=n("Requer um bom conhecimento de Python"),eo=c(),v=r("li"),ao=n("\xC9 melhor aproveitado depois de um curso de introdu\xE7\xE3o ao deep learning (aprendizagem profunda), como o da "),W=r("a"),oo=n("fast.ai"),ro=c(),Y=r("a"),so=n("Practical Deep Learning for Coders"),to=n("  ou um dos programas desenvolvidos pela "),K=r("a"),no=n("DeepLearning.AI"),io=c(),q=r("li"),lo=n("N\xE3o se espera nenhum conhecimento pr\xE9vio em "),V=r("a"),uo=n("PyTorch"),mo=n("  ou em "),X=r("a"),co=n("TensorFlow"),po=n(" , ainda que certa familiaridade com ambas as ferramentas seja proveitoso"),ra=c(),z=r("p"),fo=n("Depois de voc\xEA ter completado esse curso, n\xF3s recomendamos dar uma olhada na especializa\xE7\xE3o da DeepLearning.AI de "),Z=r("a"),ho=n("Processamento de Linguagem Natural"),vo=n(", que cobre uma grande gama de modelos de NLP como naive Bayes e LSTMs que valem bastante a pena ter conhecimento!"),sa=c(),P=r("h2"),k=r("a"),ye=r("span"),Ge(ee.$$.fragment),go=c(),Ae=r("span"),bo=n("Quem n\xF3s somos?"),ta=c(),de=r("p"),Eo=n("Sobre os autores:"),na=c(),ae=r("p"),Ne=r("strong"),_o=n("Matthew Carrigan"),wo=n(" \xE9 Engenheiro de Machine Learning na Hugging Face. Ele mora em Dublin na Irlanda e anteriormente trabalhou como Engenheiro de ML na Parse.ly e antes disso como pesquisador p\xF3s-doc na Trinity College Dublin. Ele n\xE3o acredita que chegaremos a um rendimento anual bruto corrigido escalando as arquiteturas existentes, mas de qualquer forma ele tem grandes esperan\xE7as na imortalidade das m\xE1quinas."),ia=c(),oe=r("p"),Te=r("strong"),Lo=n("Lysandre Debut"),qo=n(" \xE9 um Engenheiro de Machine Learning na Hugging Face e tem trabalhado para a biblioteca \u{1F917} Transformers desde seus est\xE1gios iniciais de desenvolvimento. Seu objetivo \xE9 fazer com que a \xE1rea de NLP se torne acess\xEDvel para qualquer pessoa atrav\xE9s do desenvolvimento de ferramentas com uma API simples."),la=c(),y=r("p"),$e=r("strong"),Po=n("Sylvain Gugger"),yo=n(" \xE9 um Engenheiro Pesquisador na Hugging Face e um dos principais mantenedores da biblioteca \u{1F917} Transformers. Anteriormente ele foi Cientista Pesquisador na fast.ai, e co-escreveu "),Se=r("em"),re=r("a"),Ao=n("Deep Learning for Coders with fastai and PyTorch"),No=n(" com Jeremy Howard. O principal foco de sua pesquisa est\xE1 em fazer o deep learning mais acess\xEDvel, atrav\xE9s de desenhos e t\xE9cnicas de aprimoramento que permitam que modelos sejam treinados mais r\xE1pidos e com recursos limitados."),ua=c(),se=r("p"),He=r("strong"),To=n("Merve Noyan"),$o=n(" \xE9 um desenvolvedor e evangelista na Hugging Face, trabalhando no desenvolvimento e constru\xE7\xE3o de conte\xFAdos envolta da tem\xE1tica de democratiza\xE7\xE3o do Machine Learning para todas as pessoas."),ma=c(),te=r("p"),ze=r("strong"),So=n("Lucile Saulnier"),Ho=n(" \xE9 uma Engenheira de Machine Learning na Hugging Face, desenvolvendo e apoiando o uso de ferramentas de c\xF3digo aberto. Ela tamb\xE9m \xE9 ativamente envolvida em muitos projetos de pesquisa no campo do Processamento de Linguagem natural assim como em treinamentos colaborativos e BigScience."),ca=c(),A=r("p"),ke=r("strong"),zo=n("Lewis Tunstall"),ko=n("  \xE9 um Engenheiro de Machine Learning na Hugging Face, focado no desenvolvimento de ferramentas open-source e em faz\xEA-las amplamente acess\xEDveis pela comunidade. Ele tamb\xE9m \xE9 co-autor do livro que est\xE1 pra lan\xE7ar "),ne=r("a"),Mo=n("O\u2019Reilly book on Transformers"),Co=n("."),da=c(),N=r("p"),Me=r("strong"),Io=n("Leandro von Werra"),Fo=n("  \xE9 um Engenheiro de Machine Learning no time de open-source na Hugging Face e tamb\xE9m co-autor do livro "),ie=r("a"),Do=n("O\u2019Reilly book on Transformers"),Oo=n(". Ele tem muitos anos de experi\xEAncia na ind\xFAstria trazendo projetos de NLP para produ\xE7\xE3o trabalhando com v\xE1rias stacks de Machine Learning."),pa=c(),pe=r("p"),xo=n("Est\xE1 pronto para seguir? Nesse cap\xEDtulo, voc\xEA aprender\xE1:"),fa=c(),g=r("ul"),le=r("li"),Go=n("Como usar a fun\xE7\xE3o "),Ce=r("code"),Ro=n("pipeline()"),jo=n("  para solucionar tarefas de NLP tais como gera\xE7\xE3o de texto e classifica\xE7\xE3o"),Bo=c(),Ie=r("li"),Uo=n("Sobre a arquitetura Transformer"),Jo=c(),Fe=r("li"),Qo=n("Como distinguir entre as arquiteturas encoder, decoder, encoder-decoder e seus casos de uso"),this.h()},l(e){const u=Kr('[data-svelte="svelte-1phssyn"]',document.head);b=s(u,"META",{name:!0,content:!0}),u.forEach(o),Qe=d(e),E=s(e,"H1",{class:!0});var va=t(E);T=s(va,"A",{id:!0,class:!0,href:!0});var ar=t(T);ge=s(ar,"SPAN",{});var or=t(ge);Re(M.$$.fragment,or),or.forEach(o),ar.forEach(o),Aa=d(va),be=s(va,"SPAN",{});var rr=t(be);Na=i(rr,"Introdu\xE7\xE3o"),rr.forEach(o),va.forEach(o),We=d(e),_=s(e,"H2",{class:!0});var ga=t(_);$=s(ga,"A",{id:!0,class:!0,href:!0});var sr=t($);Ee=s(sr,"SPAN",{});var tr=t(Ee);Re(C.$$.fragment,tr),tr.forEach(o),sr.forEach(o),Ta=d(ga),_e=s(ga,"SPAN",{});var nr=t(_e);$a=i(nr,"Bem-vindo(a) ao Curso \u{1F917}!"),nr.forEach(o),ga.forEach(o),Ye=d(e),Re(I.$$.fragment,e),Ke=d(e),p=s(e,"P",{});var f=t(p);Sa=i(f,"Esse curso te ensinar\xE1 sobre processamento de linguagem natural (PLN, ou NLP em ingl\xEAs) usando as bibliotecas do ecossistema "),F=s(f,"A",{href:!0,rel:!0});var ir=t(F);Ha=i(ir,"Hugging Face"),ir.forEach(o),za=i(f," \u2014 "),D=s(f,"A",{href:!0,rel:!0});var lr=t(D);ka=i(lr,"\u{1F917} Transformers"),lr.forEach(o),Ma=i(f,", "),O=s(f,"A",{href:!0,rel:!0});var ur=t(O);Ca=i(ur,"\u{1F917} Datasets"),ur.forEach(o),Ia=i(f,", "),x=s(f,"A",{href:!0,rel:!0});var mr=t(x);Fa=i(mr,"\u{1F917} Tokenizers"),mr.forEach(o),Da=i(f," e "),G=s(f,"A",{href:!0,rel:!0});var cr=t(G);Oa=i(cr,"\u{1F917} Accelerate"),cr.forEach(o),xa=i(f," \u2014 assim como a "),R=s(f,"A",{href:!0,rel:!0});var dr=t(R);Ga=i(dr,"Hugging Face Hub"),dr.forEach(o),Ra=i(f,". \xC9 completamente gratuito e sem an\xFAncios!"),f.forEach(o),Ve=d(e),w=s(e,"H2",{class:!0});var ba=t(w);S=s(ba,"A",{id:!0,class:!0,href:!0});var pr=t(S);we=s(pr,"SPAN",{});var fr=t(we);Re(j.$$.fragment,fr),fr.forEach(o),pr.forEach(o),ja=d(ba),Le=s(ba,"SPAN",{});var hr=t(Le);Ba=i(hr,"O que esperar?"),hr.forEach(o),ba.forEach(o),Xe=d(e),me=s(e,"P",{});var vr=t(me);Ua=i(vr,"Aqui uma vis\xE3o geral do curso:"),vr.forEach(o),Ze=d(e),L=s(e,"DIV",{class:!0});var Ea=t(L);B=s(Ea,"IMG",{class:!0,src:!0,alt:!0}),Ja=d(Ea),U=s(Ea,"IMG",{class:!0,src:!0,alt:!0}),Ea.forEach(o),ea=d(e),H=s(e,"UL",{});var _a=t(H);J=s(_a,"LI",{});var wa=t(J);Qa=i(wa,"Cap\xEDtulos 1 ao 4 d\xE1 uma introdu\xE7\xE3o para os principais conceitos da biblioteca Transformers \u{1F917}. No final dessa parte do curso, voc\xEA se tornar\xE1 familiar sobre como os modelos Transformers funcionam, saber\xE1 como usar o modelo da "),Q=s(wa,"A",{href:!0,rel:!0});var gr=t(Q);Wa=i(gr,"Hugging Face Hub"),gr.forEach(o),Ya=i(wa,", fazer um ajuste fino em um dataset e compartilhar os resultados no Hub!"),wa.forEach(o),Ka=d(_a),qe=s(_a,"LI",{});var br=t(qe);Va=i(br,"Cap\xEDtulo 5 ao 8 ensina o b\xE1sico dos \u{1F917} Datasets e \u{1F917} Tokenizadores antes de mergulhar nas tarefas cl\xE1ssicas de NLP. Ao final dessa parte, voc\xEA ser\xE1 capaz de resolver por conta pr\xF3pria os problemas mais comuns de NLP. Ao longo do caminho, voc\xEA aprender\xE1 como construir e compartilhar demonstra\xE7\xF5es de seus modelos e otimiza-los para ambientes de produ\xE7\xE3o. No final dessa parte, voc\xEA ser\xE1 capaz de aplicar a \u{1F917} Transformers para (quase) qualquer problema de aprendizagem de m\xE1quina!"),br.forEach(o),_a.forEach(o),aa=d(e),ce=s(e,"P",{});var Er=t(ce);Xa=i(Er,"Esse curso:"),Er.forEach(o),oa=d(e),h=s(e,"UL",{});var fe=t(h);Pe=s(fe,"LI",{});var _r=t(Pe);Za=i(_r,"Requer um bom conhecimento de Python"),_r.forEach(o),eo=d(fe),v=s(fe,"LI",{});var ue=t(v);ao=i(ue,"\xC9 melhor aproveitado depois de um curso de introdu\xE7\xE3o ao deep learning (aprendizagem profunda), como o da "),W=s(ue,"A",{href:!0,rel:!0});var wr=t(W);oo=i(wr,"fast.ai"),wr.forEach(o),ro=d(ue),Y=s(ue,"A",{href:!0,rel:!0});var Lr=t(Y);so=i(Lr,"Practical Deep Learning for Coders"),Lr.forEach(o),to=i(ue,"  ou um dos programas desenvolvidos pela "),K=s(ue,"A",{href:!0,rel:!0});var qr=t(K);no=i(qr,"DeepLearning.AI"),qr.forEach(o),ue.forEach(o),io=d(fe),q=s(fe,"LI",{});var he=t(q);lo=i(he,"N\xE3o se espera nenhum conhecimento pr\xE9vio em "),V=s(he,"A",{href:!0,rel:!0});var Pr=t(V);uo=i(Pr,"PyTorch"),Pr.forEach(o),mo=i(he,"  ou em "),X=s(he,"A",{href:!0,rel:!0});var yr=t(X);co=i(yr,"TensorFlow"),yr.forEach(o),po=i(he," , ainda que certa familiaridade com ambas as ferramentas seja proveitoso"),he.forEach(o),fe.forEach(o),ra=d(e),z=s(e,"P",{});var La=t(z);fo=i(La,"Depois de voc\xEA ter completado esse curso, n\xF3s recomendamos dar uma olhada na especializa\xE7\xE3o da DeepLearning.AI de "),Z=s(La,"A",{href:!0,rel:!0});var Ar=t(Z);ho=i(Ar,"Processamento de Linguagem Natural"),Ar.forEach(o),vo=i(La,", que cobre uma grande gama de modelos de NLP como naive Bayes e LSTMs que valem bastante a pena ter conhecimento!"),La.forEach(o),sa=d(e),P=s(e,"H2",{class:!0});var qa=t(P);k=s(qa,"A",{id:!0,class:!0,href:!0});var Nr=t(k);ye=s(Nr,"SPAN",{});var Tr=t(ye);Re(ee.$$.fragment,Tr),Tr.forEach(o),Nr.forEach(o),go=d(qa),Ae=s(qa,"SPAN",{});var $r=t(Ae);bo=i($r,"Quem n\xF3s somos?"),$r.forEach(o),qa.forEach(o),ta=d(e),de=s(e,"P",{});var Sr=t(de);Eo=i(Sr,"Sobre os autores:"),Sr.forEach(o),na=d(e),ae=s(e,"P",{});var Wo=t(ae);Ne=s(Wo,"STRONG",{});var Hr=t(Ne);_o=i(Hr,"Matthew Carrigan"),Hr.forEach(o),wo=i(Wo," \xE9 Engenheiro de Machine Learning na Hugging Face. Ele mora em Dublin na Irlanda e anteriormente trabalhou como Engenheiro de ML na Parse.ly e antes disso como pesquisador p\xF3s-doc na Trinity College Dublin. Ele n\xE3o acredita que chegaremos a um rendimento anual bruto corrigido escalando as arquiteturas existentes, mas de qualquer forma ele tem grandes esperan\xE7as na imortalidade das m\xE1quinas."),Wo.forEach(o),ia=d(e),oe=s(e,"P",{});var Yo=t(oe);Te=s(Yo,"STRONG",{});var zr=t(Te);Lo=i(zr,"Lysandre Debut"),zr.forEach(o),qo=i(Yo," \xE9 um Engenheiro de Machine Learning na Hugging Face e tem trabalhado para a biblioteca \u{1F917} Transformers desde seus est\xE1gios iniciais de desenvolvimento. Seu objetivo \xE9 fazer com que a \xE1rea de NLP se torne acess\xEDvel para qualquer pessoa atrav\xE9s do desenvolvimento de ferramentas com uma API simples."),Yo.forEach(o),la=d(e),y=s(e,"P",{});var De=t(y);$e=s(De,"STRONG",{});var kr=t($e);Po=i(kr,"Sylvain Gugger"),kr.forEach(o),yo=i(De," \xE9 um Engenheiro Pesquisador na Hugging Face e um dos principais mantenedores da biblioteca \u{1F917} Transformers. Anteriormente ele foi Cientista Pesquisador na fast.ai, e co-escreveu "),Se=s(De,"EM",{});var Mr=t(Se);re=s(Mr,"A",{href:!0,rel:!0});var Cr=t(re);Ao=i(Cr,"Deep Learning for Coders with fastai and PyTorch"),Cr.forEach(o),Mr.forEach(o),No=i(De," com Jeremy Howard. O principal foco de sua pesquisa est\xE1 em fazer o deep learning mais acess\xEDvel, atrav\xE9s de desenhos e t\xE9cnicas de aprimoramento que permitam que modelos sejam treinados mais r\xE1pidos e com recursos limitados."),De.forEach(o),ua=d(e),se=s(e,"P",{});var Ko=t(se);He=s(Ko,"STRONG",{});var Ir=t(He);To=i(Ir,"Merve Noyan"),Ir.forEach(o),$o=i(Ko," \xE9 um desenvolvedor e evangelista na Hugging Face, trabalhando no desenvolvimento e constru\xE7\xE3o de conte\xFAdos envolta da tem\xE1tica de democratiza\xE7\xE3o do Machine Learning para todas as pessoas."),Ko.forEach(o),ma=d(e),te=s(e,"P",{});var Vo=t(te);ze=s(Vo,"STRONG",{});var Fr=t(ze);So=i(Fr,"Lucile Saulnier"),Fr.forEach(o),Ho=i(Vo," \xE9 uma Engenheira de Machine Learning na Hugging Face, desenvolvendo e apoiando o uso de ferramentas de c\xF3digo aberto. Ela tamb\xE9m \xE9 ativamente envolvida em muitos projetos de pesquisa no campo do Processamento de Linguagem natural assim como em treinamentos colaborativos e BigScience."),Vo.forEach(o),ca=d(e),A=s(e,"P",{});var Oe=t(A);ke=s(Oe,"STRONG",{});var Dr=t(ke);zo=i(Dr,"Lewis Tunstall"),Dr.forEach(o),ko=i(Oe,"  \xE9 um Engenheiro de Machine Learning na Hugging Face, focado no desenvolvimento de ferramentas open-source e em faz\xEA-las amplamente acess\xEDveis pela comunidade. Ele tamb\xE9m \xE9 co-autor do livro que est\xE1 pra lan\xE7ar "),ne=s(Oe,"A",{href:!0,rel:!0});var Or=t(ne);Mo=i(Or,"O\u2019Reilly book on Transformers"),Or.forEach(o),Co=i(Oe,"."),Oe.forEach(o),da=d(e),N=s(e,"P",{});var xe=t(N);Me=s(xe,"STRONG",{});var xr=t(Me);Io=i(xr,"Leandro von Werra"),xr.forEach(o),Fo=i(xe,"  \xE9 um Engenheiro de Machine Learning no time de open-source na Hugging Face e tamb\xE9m co-autor do livro "),ie=s(xe,"A",{href:!0,rel:!0});var Gr=t(ie);Do=i(Gr,"O\u2019Reilly book on Transformers"),Gr.forEach(o),Oo=i(xe,". Ele tem muitos anos de experi\xEAncia na ind\xFAstria trazendo projetos de NLP para produ\xE7\xE3o trabalhando com v\xE1rias stacks de Machine Learning."),xe.forEach(o),pa=d(e),pe=s(e,"P",{});var Rr=t(pe);xo=i(Rr,"Est\xE1 pronto para seguir? Nesse cap\xEDtulo, voc\xEA aprender\xE1:"),Rr.forEach(o),fa=d(e),g=s(e,"UL",{});var ve=t(g);le=s(ve,"LI",{});var Pa=t(le);Go=i(Pa,"Como usar a fun\xE7\xE3o "),Ce=s(Pa,"CODE",{});var jr=t(Ce);Ro=i(jr,"pipeline()"),jr.forEach(o),jo=i(Pa,"  para solucionar tarefas de NLP tais como gera\xE7\xE3o de texto e classifica\xE7\xE3o"),Pa.forEach(o),Bo=d(ve),Ie=s(ve,"LI",{});var Br=t(Ie);Uo=i(Br,"Sobre a arquitetura Transformer"),Br.forEach(o),Jo=d(ve),Fe=s(ve,"LI",{});var Ur=t(Fe);Qo=i(Ur,"Como distinguir entre as arquiteturas encoder, decoder, encoder-decoder e seus casos de uso"),Ur.forEach(o),ve.forEach(o),this.h()},h(){l(b,"name","hf:doc:metadata"),l(b,"content",JSON.stringify(as)),l(T,"id","introduo"),l(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(T,"href","#introduo"),l(E,"class","relative group"),l($,"id","bemvindoa-ao-curso"),l($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($,"href","#bemvindoa-ao-curso"),l(_,"class","relative group"),l(F,"href","https://huggingface.co/"),l(F,"rel","nofollow"),l(D,"href","https://github.com/huggingface/transformers"),l(D,"rel","nofollow"),l(O,"href","https://github.com/huggingface/datasets"),l(O,"rel","nofollow"),l(x,"href","https://github.com/huggingface/tokenizers"),l(x,"rel","nofollow"),l(G,"href","https://github.com/huggingface/accelerate"),l(G,"rel","nofollow"),l(R,"href","https://huggingface.co/models"),l(R,"rel","nofollow"),l(S,"id","o-que-esperar"),l(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(S,"href","#o-que-esperar"),l(w,"class","relative group"),l(B,"class","block dark:hidden"),Jr(B.src,Zo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg")||l(B,"src",Zo),l(B,"alt","Brief overview of the chapters of the course."),l(U,"class","hidden dark:block"),Jr(U.src,er="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg")||l(U,"src",er),l(U,"alt","Brief overview of the chapters of the course."),l(L,"class","flex justify-center"),l(Q,"href","https://huggingface.co/models"),l(Q,"rel","nofollow"),l(W,"href","https://www.fast.ai/"),l(W,"rel","nofollow"),l(Y,"href","https://course.fast.ai/"),l(Y,"rel","nofollow"),l(K,"href","https://www.deeplearning.ai/"),l(K,"rel","nofollow"),l(V,"href","https://pytorch.org/"),l(V,"rel","nofollow"),l(X,"href","https://www.tensorflow.org/"),l(X,"rel","nofollow"),l(Z,"href","https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh"),l(Z,"rel","nofollow"),l(k,"id","quem-ns-somos"),l(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(k,"href","#quem-ns-somos"),l(P,"class","relative group"),l(re,"href","https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/"),l(re,"rel","nofollow"),l(ne,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),l(ne,"rel","nofollow"),l(ie,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098103231/"),l(ie,"rel","nofollow")},m(e,u){a(document.head,b),m(e,Qe,u),m(e,E,u),a(E,T),a(T,ge),je(M,ge,null),a(E,Aa),a(E,be),a(be,Na),m(e,We,u),m(e,_,u),a(_,$),a($,Ee),je(C,Ee,null),a(_,Ta),a(_,_e),a(_e,$a),m(e,Ye,u),je(I,e,u),m(e,Ke,u),m(e,p,u),a(p,Sa),a(p,F),a(F,Ha),a(p,za),a(p,D),a(D,ka),a(p,Ma),a(p,O),a(O,Ca),a(p,Ia),a(p,x),a(x,Fa),a(p,Da),a(p,G),a(G,Oa),a(p,xa),a(p,R),a(R,Ga),a(p,Ra),m(e,Ve,u),m(e,w,u),a(w,S),a(S,we),je(j,we,null),a(w,ja),a(w,Le),a(Le,Ba),m(e,Xe,u),m(e,me,u),a(me,Ua),m(e,Ze,u),m(e,L,u),a(L,B),a(L,Ja),a(L,U),m(e,ea,u),m(e,H,u),a(H,J),a(J,Qa),a(J,Q),a(Q,Wa),a(J,Ya),a(H,Ka),a(H,qe),a(qe,Va),m(e,aa,u),m(e,ce,u),a(ce,Xa),m(e,oa,u),m(e,h,u),a(h,Pe),a(Pe,Za),a(h,eo),a(h,v),a(v,ao),a(v,W),a(W,oo),a(v,ro),a(v,Y),a(Y,so),a(v,to),a(v,K),a(K,no),a(h,io),a(h,q),a(q,lo),a(q,V),a(V,uo),a(q,mo),a(q,X),a(X,co),a(q,po),m(e,ra,u),m(e,z,u),a(z,fo),a(z,Z),a(Z,ho),a(z,vo),m(e,sa,u),m(e,P,u),a(P,k),a(k,ye),je(ee,ye,null),a(P,go),a(P,Ae),a(Ae,bo),m(e,ta,u),m(e,de,u),a(de,Eo),m(e,na,u),m(e,ae,u),a(ae,Ne),a(Ne,_o),a(ae,wo),m(e,ia,u),m(e,oe,u),a(oe,Te),a(Te,Lo),a(oe,qo),m(e,la,u),m(e,y,u),a(y,$e),a($e,Po),a(y,yo),a(y,Se),a(Se,re),a(re,Ao),a(y,No),m(e,ua,u),m(e,se,u),a(se,He),a(He,To),a(se,$o),m(e,ma,u),m(e,te,u),a(te,ze),a(ze,So),a(te,Ho),m(e,ca,u),m(e,A,u),a(A,ke),a(ke,zo),a(A,ko),a(A,ne),a(ne,Mo),a(A,Co),m(e,da,u),m(e,N,u),a(N,Me),a(Me,Io),a(N,Fo),a(N,ie),a(ie,Do),a(N,Oo),m(e,pa,u),m(e,pe,u),a(pe,xo),m(e,fa,u),m(e,g,u),a(g,le),a(le,Go),a(le,Ce),a(Ce,Ro),a(le,jo),a(g,Bo),a(g,Ie),a(Ie,Uo),a(g,Jo),a(g,Fe),a(Fe,Qo),ha=!0},p:Vr,i(e){ha||(Be(M.$$.fragment,e),Be(C.$$.fragment,e),Be(I.$$.fragment,e),Be(j.$$.fragment,e),Be(ee.$$.fragment,e),ha=!0)},o(e){Ue(M.$$.fragment,e),Ue(C.$$.fragment,e),Ue(I.$$.fragment,e),Ue(j.$$.fragment,e),Ue(ee.$$.fragment,e),ha=!1},d(e){o(b),e&&o(Qe),e&&o(E),Je(M),e&&o(We),e&&o(_),Je(C),e&&o(Ye),Je(I,e),e&&o(Ke),e&&o(p),e&&o(Ve),e&&o(w),Je(j),e&&o(Xe),e&&o(me),e&&o(Ze),e&&o(L),e&&o(ea),e&&o(H),e&&o(aa),e&&o(ce),e&&o(oa),e&&o(h),e&&o(ra),e&&o(z),e&&o(sa),e&&o(P),Je(ee),e&&o(ta),e&&o(de),e&&o(na),e&&o(ae),e&&o(ia),e&&o(oe),e&&o(la),e&&o(y),e&&o(ua),e&&o(se),e&&o(ma),e&&o(te),e&&o(ca),e&&o(A),e&&o(da),e&&o(N),e&&o(pa),e&&o(pe),e&&o(fa),e&&o(g)}}}const as={local:"introduo",sections:[{local:"bemvindoa-ao-curso",title:"Bem-vindo(a) ao Curso \u{1F917}!"},{local:"o-que-esperar",title:"O que esperar?"},{local:"quem-ns-somos",title:"Quem n\xF3s somos?"}],title:"Introdu\xE7\xE3o"};function os(Xo){return Xr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ns extends Qr{constructor(b){super();Wr(this,b,os,es,Yr,{})}}export{ns as default,as as metadata};
