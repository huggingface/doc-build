import{S as Le,i as De,s as Ie,e as n,k as m,w as me,t as u,M as We,c as a,d as t,m as c,a as l,x as ce,h as d,b as i,G as r,g as o,y as he,L as Re,q as pe,o as ge,B as ve,v as je}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ce}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ne}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ze}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Fe(_e){let h,W,p,g,P,w,H,T,J,R,z,j,E,C,v,K,x,Q,X,N,_,ee,A,te,re,Z,q,ne,F,y,ae,O,f,B,S,se,le,L,$,ie,oe,D,k,ue,de,I,M,fe,U;return w=new Ne({}),z=new Ze({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),E=new Ce({props:{id:"0_4KEb08xrE"}}),{c(){h=n("meta"),W=m(),p=n("h1"),g=n("a"),P=n("span"),me(w.$$.fragment),H=m(),T=n("span"),J=u("Sequence-to-Sequence-Modelle"),R=m(),me(z.$$.fragment),j=m(),me(E.$$.fragment),C=m(),v=n("p"),K=u("Encoder-Decoder-Modelle (auch "),x=n("em"),Q=u("Sequence-to-Sequence-Modelle"),X=u(" genannt) verwenden beide Teile der Transformer-Architektur. Die Attention-Layer des Encoders k\xF6nnen in jedem Schritt auf alle W\xF6rter des Ausgangssatzes zugreifen, w\xE4hrend die Attention-Layer des Decoders nur auf die W\xF6rter zugreifen k\xF6nnen, die vor einem bestimmten Wort des Inputs stehen."),N=m(),_=n("p"),ee=u("Das Pretraining dieser Modelle kann wie das Pretraining von rein Encoder- oder Decoder-basierten Modellen erfolgen, ist aber in der Regel etwas komplexer. Beim Pretraining von "),A=n("a"),te=u("T5"),re=u(" werden zum Beispiel zuf\xE4llige Textabschnitte (die mehrere W\xF6rter enthalten k\xF6nnen) durch ein einzelnes spezielles Maskierungswort ersetzt, und das Ziel (engl. Pretraining Objective) besteht dann darin, den Text vorherzusagen, der durch dieses Maskierungswort ersetzt bzw. verdeckt wurde."),Z=m(),q=n("p"),ne=u("Sequence-to-Sequence-Modelle eignen sich am besten f\xFCr Aufgaben, bei denen es darum geht, neue S\xE4tze in Abh\xE4ngigkeit von einem bestimmten Input zu generieren, z. B. bei der Zusammenfassung, \xDCbersetzung oder generativen Frage-Antwort-Systemen."),F=m(),y=n("p"),ae=u("Vertreter dieser Modellfamilie sind u. a.:"),O=m(),f=n("ul"),B=n("li"),S=n("a"),se=u("BART"),le=m(),L=n("li"),$=n("a"),ie=u("mBART"),oe=m(),D=n("li"),k=n("a"),ue=u("Marian"),de=m(),I=n("li"),M=n("a"),fe=u("T5"),this.h()},l(e){const s=We('[data-svelte="svelte-1phssyn"]',document.head);h=a(s,"META",{name:!0,content:!0}),s.forEach(t),W=c(e),p=a(e,"H1",{class:!0});var V=l(p);g=a(V,"A",{id:!0,class:!0,href:!0});var be=l(g);P=a(be,"SPAN",{});var we=l(P);ce(w.$$.fragment,we),we.forEach(t),be.forEach(t),H=c(V),T=a(V,"SPAN",{});var ze=l(T);J=d(ze,"Sequence-to-Sequence-Modelle"),ze.forEach(t),V.forEach(t),R=c(e),ce(z.$$.fragment,e),j=c(e),ce(E.$$.fragment,e),C=c(e),v=a(e,"P",{});var Y=l(v);K=d(Y,"Encoder-Decoder-Modelle (auch "),x=a(Y,"EM",{});var Ee=l(x);Q=d(Ee,"Sequence-to-Sequence-Modelle"),Ee.forEach(t),X=d(Y," genannt) verwenden beide Teile der Transformer-Architektur. Die Attention-Layer des Encoders k\xF6nnen in jedem Schritt auf alle W\xF6rter des Ausgangssatzes zugreifen, w\xE4hrend die Attention-Layer des Decoders nur auf die W\xF6rter zugreifen k\xF6nnen, die vor einem bestimmten Wort des Inputs stehen."),Y.forEach(t),N=c(e),_=a(e,"P",{});var G=l(_);ee=d(G,"Das Pretraining dieser Modelle kann wie das Pretraining von rein Encoder- oder Decoder-basierten Modellen erfolgen, ist aber in der Regel etwas komplexer. Beim Pretraining von "),A=a(G,"A",{href:!0,rel:!0});var Ae=l(A);te=d(Ae,"T5"),Ae.forEach(t),re=d(G," werden zum Beispiel zuf\xE4llige Textabschnitte (die mehrere W\xF6rter enthalten k\xF6nnen) durch ein einzelnes spezielles Maskierungswort ersetzt, und das Ziel (engl. Pretraining Objective) besteht dann darin, den Text vorherzusagen, der durch dieses Maskierungswort ersetzt bzw. verdeckt wurde."),G.forEach(t),Z=c(e),q=a(e,"P",{});var Se=l(q);ne=d(Se,"Sequence-to-Sequence-Modelle eignen sich am besten f\xFCr Aufgaben, bei denen es darum geht, neue S\xE4tze in Abh\xE4ngigkeit von einem bestimmten Input zu generieren, z. B. bei der Zusammenfassung, \xDCbersetzung oder generativen Frage-Antwort-Systemen."),Se.forEach(t),F=c(e),y=a(e,"P",{});var $e=l(y);ae=d($e,"Vertreter dieser Modellfamilie sind u. a.:"),$e.forEach(t),O=c(e),f=a(e,"UL",{});var b=l(f);B=a(b,"LI",{});var ke=l(B);S=a(ke,"A",{href:!0,rel:!0});var Me=l(S);se=d(Me,"BART"),Me.forEach(t),ke.forEach(t),le=c(b),L=a(b,"LI",{});var qe=l(L);$=a(qe,"A",{href:!0,rel:!0});var ye=l($);ie=d(ye,"mBART"),ye.forEach(t),qe.forEach(t),oe=c(b),D=a(b,"LI",{});var Pe=l(D);k=a(Pe,"A",{href:!0,rel:!0});var Te=l(k);ue=d(Te,"Marian"),Te.forEach(t),Pe.forEach(t),de=c(b),I=a(b,"LI",{});var xe=l(I);M=a(xe,"A",{href:!0,rel:!0});var Be=l(M);fe=d(Be,"T5"),Be.forEach(t),xe.forEach(t),b.forEach(t),this.h()},h(){i(h,"name","hf:doc:metadata"),i(h,"content",JSON.stringify(Oe)),i(g,"id","sequencetosequencemodelle"),i(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(g,"href","#sequencetosequencemodelle"),i(p,"class","relative group"),i(A,"href","https://huggingface.co/t5-base"),i(A,"rel","nofollow"),i(S,"href","https://huggingface.co/transformers/model_doc/bart.html"),i(S,"rel","nofollow"),i($,"href","https://huggingface.co/transformers/model_doc/mbart.html"),i($,"rel","nofollow"),i(k,"href","https://huggingface.co/transformers/model_doc/marian.html"),i(k,"rel","nofollow"),i(M,"href","https://huggingface.co/transformers/model_doc/t5.html"),i(M,"rel","nofollow")},m(e,s){r(document.head,h),o(e,W,s),o(e,p,s),r(p,g),r(g,P),he(w,P,null),r(p,H),r(p,T),r(T,J),o(e,R,s),he(z,e,s),o(e,j,s),he(E,e,s),o(e,C,s),o(e,v,s),r(v,K),r(v,x),r(x,Q),r(v,X),o(e,N,s),o(e,_,s),r(_,ee),r(_,A),r(A,te),r(_,re),o(e,Z,s),o(e,q,s),r(q,ne),o(e,F,s),o(e,y,s),r(y,ae),o(e,O,s),o(e,f,s),r(f,B),r(B,S),r(S,se),r(f,le),r(f,L),r(L,$),r($,ie),r(f,oe),r(f,D),r(D,k),r(k,ue),r(f,de),r(f,I),r(I,M),r(M,fe),U=!0},p:Re,i(e){U||(pe(w.$$.fragment,e),pe(z.$$.fragment,e),pe(E.$$.fragment,e),U=!0)},o(e){ge(w.$$.fragment,e),ge(z.$$.fragment,e),ge(E.$$.fragment,e),U=!1},d(e){t(h),e&&t(W),e&&t(p),ve(w),e&&t(R),ve(z,e),e&&t(j),ve(E,e),e&&t(C),e&&t(v),e&&t(N),e&&t(_),e&&t(Z),e&&t(q),e&&t(F),e&&t(y),e&&t(O),e&&t(f)}}}const Oe={local:"sequencetosequencemodelle",title:"Sequence-to-Sequence-Modelle"};function Ue(_e){return je(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ke extends Le{constructor(h){super();De(this,h,Ue,Fe,Ie,{})}}export{Ke as default,Oe as metadata};
