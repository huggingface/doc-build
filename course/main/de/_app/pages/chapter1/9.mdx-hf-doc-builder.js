import{S as nt,i as at,s as rt,e as a,k as o,w as Ue,t as i,M as st,c as r,d as n,m as l,a as s,x as Ye,h as d,b as L,G as e,g as p,y as Qe,L as ot,q as Ve,o as et,B as tt,v as it}from"../../chunks/vendor-hf-doc-builder.js";import{I as lt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as dt}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function ut($e){let h,W,c,g,$,b,ne,M,ae,X,A,J,_,re,x,se,oe,U,k,ie,Y,w,P,f,S,le,de,C,ue,he,G,ce,fe,m,T,F,me,Te,H,Ee,ve,I,pe,ge,E,N,_e,we,K,be,Ae,q,ke,Re,v,O,Be,De,Z,ye,ze,j,Le,Q;return b=new lt({}),A=new dt({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),{c(){h=a("meta"),W=o(),c=a("h1"),g=a("a"),$=a("span"),Ue(b.$$.fragment),ne=o(),M=a("span"),ae=i("Zusammenfassung"),X=o(),Ue(A.$$.fragment),J=o(),_=a("p"),re=i("In diesem Kapitel hast du gelernt, wie du verschiedene CL-Aufgaben mit der High-Level-Funktion "),x=a("code"),se=i("pipeline()"),oe=i(" aus der \u{1F917} Transformers-Bibliothek angehen kannst. Du hast auch erfahren, wie du im Hub nach Modellen suchen und sie nutzen kannst, und wie du die Inference API verwenden kannst, um die Modelle direkt in deinem Browser zu testen."),U=o(),k=a("p"),ie=i("Wir haben besprochen, wie Transformer-Modelle im Gro\xDFen und Ganzen funktionieren, und haben die Bedeutung von Tranfer Learning und Feintuning erl\xE4utert. Ein wichtiger Aspekt ist, dass du entweder die gesamte Architektur, nur den Encoder oder auch nur den Decoder verwenden kannst - je nachdem, welche Art von Aufgabe du l\xF6sen willst. Die nachfolgende Tabelle gibt noch einmal einen guten \xDCberblick:"),Y=o(),w=a("table"),P=a("thead"),f=a("tr"),S=a("th"),le=i("Modell"),de=o(),C=a("th"),ue=i("Beispiele"),he=o(),G=a("th"),ce=i("Aufgaben (Tasks)"),fe=o(),m=a("tbody"),T=a("tr"),F=a("td"),me=i("Encoder"),Te=o(),H=a("td"),Ee=i("ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),ve=o(),I=a("td"),pe=i("Klassifizierung von S\xE4tzen, Eigennamenerkennung/NER, Extraktive Frage-Antwort-Systeme"),ge=o(),E=a("tr"),N=a("td"),_e=i("Decoder"),we=o(),K=a("td"),be=i("CTRL, GPT, GPT-2, Transformer XL"),Ae=o(),q=a("td"),ke=i("Textgenerierung"),Re=o(),v=a("tr"),O=a("td"),Be=i("Encoder-Decoder"),De=o(),Z=a("td"),ye=i("BART, T5, Marian, mBART"),ze=o(),j=a("td"),Le=i("Automatische Textzusammenfassung, Maschinelle \xDCbersetzung, Generative Frage-Antwort-Systeme"),this.h()},l(t){const u=st('[data-svelte="svelte-1phssyn"]',document.head);h=r(u,"META",{name:!0,content:!0}),u.forEach(n),W=l(t),c=r(t,"H1",{class:!0});var V=s(c);g=r(V,"A",{id:!0,class:!0,href:!0});var Me=s(g);$=r(Me,"SPAN",{});var xe=s($);Ye(b.$$.fragment,xe),xe.forEach(n),Me.forEach(n),ne=l(V),M=r(V,"SPAN",{});var Pe=s(M);ae=d(Pe,"Zusammenfassung"),Pe.forEach(n),V.forEach(n),X=l(t),Ye(A.$$.fragment,t),J=l(t),_=r(t,"P",{});var ee=s(_);re=d(ee,"In diesem Kapitel hast du gelernt, wie du verschiedene CL-Aufgaben mit der High-Level-Funktion "),x=r(ee,"CODE",{});var Se=s(x);se=d(Se,"pipeline()"),Se.forEach(n),oe=d(ee," aus der \u{1F917} Transformers-Bibliothek angehen kannst. Du hast auch erfahren, wie du im Hub nach Modellen suchen und sie nutzen kannst, und wie du die Inference API verwenden kannst, um die Modelle direkt in deinem Browser zu testen."),ee.forEach(n),U=l(t),k=r(t,"P",{});var Ce=s(k);ie=d(Ce,"Wir haben besprochen, wie Transformer-Modelle im Gro\xDFen und Ganzen funktionieren, und haben die Bedeutung von Tranfer Learning und Feintuning erl\xE4utert. Ein wichtiger Aspekt ist, dass du entweder die gesamte Architektur, nur den Encoder oder auch nur den Decoder verwenden kannst - je nachdem, welche Art von Aufgabe du l\xF6sen willst. Die nachfolgende Tabelle gibt noch einmal einen guten \xDCberblick:"),Ce.forEach(n),Y=l(t),w=r(t,"TABLE",{});var te=s(w);P=r(te,"THEAD",{});var Ge=s(P);f=r(Ge,"TR",{});var R=s(f);S=r(R,"TH",{});var Fe=s(S);le=d(Fe,"Modell"),Fe.forEach(n),de=l(R),C=r(R,"TH",{});var He=s(C);ue=d(He,"Beispiele"),He.forEach(n),he=l(R),G=r(R,"TH",{});var Ie=s(G);ce=d(Ie,"Aufgaben (Tasks)"),Ie.forEach(n),R.forEach(n),Ge.forEach(n),fe=l(te),m=r(te,"TBODY",{});var B=s(m);T=r(B,"TR",{});var D=s(T);F=r(D,"TD",{});var Ne=s(F);me=d(Ne,"Encoder"),Ne.forEach(n),Te=l(D),H=r(D,"TD",{});var Ke=s(H);Ee=d(Ke,"ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa"),Ke.forEach(n),ve=l(D),I=r(D,"TD",{});var qe=s(I);pe=d(qe,"Klassifizierung von S\xE4tzen, Eigennamenerkennung/NER, Extraktive Frage-Antwort-Systeme"),qe.forEach(n),D.forEach(n),ge=l(B),E=r(B,"TR",{});var y=s(E);N=r(y,"TD",{});var Oe=s(N);_e=d(Oe,"Decoder"),Oe.forEach(n),we=l(y),K=r(y,"TD",{});var Ze=s(K);be=d(Ze,"CTRL, GPT, GPT-2, Transformer XL"),Ze.forEach(n),Ae=l(y),q=r(y,"TD",{});var je=s(q);ke=d(je,"Textgenerierung"),je.forEach(n),y.forEach(n),Re=l(B),v=r(B,"TR",{});var z=s(v);O=r(z,"TD",{});var We=s(O);Be=d(We,"Encoder-Decoder"),We.forEach(n),De=l(z),Z=r(z,"TD",{});var Xe=s(Z);ye=d(Xe,"BART, T5, Marian, mBART"),Xe.forEach(n),ze=l(z),j=r(z,"TD",{});var Je=s(j);Le=d(Je,"Automatische Textzusammenfassung, Maschinelle \xDCbersetzung, Generative Frage-Antwort-Systeme"),Je.forEach(n),z.forEach(n),B.forEach(n),te.forEach(n),this.h()},h(){L(h,"name","hf:doc:metadata"),L(h,"content",JSON.stringify(ht)),L(g,"id","zusammenfassung"),L(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),L(g,"href","#zusammenfassung"),L(c,"class","relative group")},m(t,u){e(document.head,h),p(t,W,u),p(t,c,u),e(c,g),e(g,$),Qe(b,$,null),e(c,ne),e(c,M),e(M,ae),p(t,X,u),Qe(A,t,u),p(t,J,u),p(t,_,u),e(_,re),e(_,x),e(x,se),e(_,oe),p(t,U,u),p(t,k,u),e(k,ie),p(t,Y,u),p(t,w,u),e(w,P),e(P,f),e(f,S),e(S,le),e(f,de),e(f,C),e(C,ue),e(f,he),e(f,G),e(G,ce),e(w,fe),e(w,m),e(m,T),e(T,F),e(F,me),e(T,Te),e(T,H),e(H,Ee),e(T,ve),e(T,I),e(I,pe),e(m,ge),e(m,E),e(E,N),e(N,_e),e(E,we),e(E,K),e(K,be),e(E,Ae),e(E,q),e(q,ke),e(m,Re),e(m,v),e(v,O),e(O,Be),e(v,De),e(v,Z),e(Z,ye),e(v,ze),e(v,j),e(j,Le),Q=!0},p:ot,i(t){Q||(Ve(b.$$.fragment,t),Ve(A.$$.fragment,t),Q=!0)},o(t){et(b.$$.fragment,t),et(A.$$.fragment,t),Q=!1},d(t){n(h),t&&n(W),t&&n(c),tt(b),t&&n(X),tt(A,t),t&&n(J),t&&n(_),t&&n(U),t&&n(k),t&&n(Y),t&&n(w)}}}const ht={local:"zusammenfassung",title:"Zusammenfassung"};function ct($e){return it(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vt extends nt{constructor(h){super();at(this,h,ct,ut,rt,{})}}export{vt as default,ht as metadata};
