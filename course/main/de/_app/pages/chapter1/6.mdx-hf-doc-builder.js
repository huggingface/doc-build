import{S as Se,i as xe,s as ze,e as o,k as m,w as ie,t as u,M as We,c as n,d as t,m as d,a,x as fe,h,b as s,G as r,g as i,y as me,L as ke,q as de,o as ue,B as he,v as Ce}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ge}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ie}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Re}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function qe(ce){let c,G,p,_,A,$,Z,S,F,I,w,R,E,q,v,H,x,J,O,B,y,Q,N,L,K,j,T,ee,U,f,z,M,te,re,W,b,le,oe,k,D,ne,ae,C,P,se,V;return $=new Ie({}),w=new Re({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),E=new Ge({props:{id:"d_ixlCubqQw"}}),{c(){c=o("meta"),G=m(),p=o("h1"),_=o("a"),A=o("span"),ie($.$$.fragment),Z=m(),S=o("span"),F=u("Decoder-Modelle"),I=m(),ie(w.$$.fragment),R=m(),ie(E.$$.fragment),q=m(),v=o("p"),H=u("Decoder-Modelle verwenden nur den Decoder eines Transformer-Modells. Die Attention-Layer k\xF6nnen bei jedem Schritt hinsichtlich eines bestimmten Wortes nur auf die W\xF6rter zugreifen, die vor diesem Wort im Satz stehen. Diese Modelle werden oft als "),x=o("em"),J=u("autoregressive Modelle"),O=u(" bezeichnet."),B=m(),y=o("p"),Q=u("Beim Pretraining von Decoder-Modellen geht es in der Regel um die Vorhersage des n\xE4chsten Wortes im Satz."),N=m(),L=o("p"),K=u("Diese Modelle sind am besten f\xFCr Aufgaben geeignet, bei denen es um die Generierung von Texten geht."),j=m(),T=o("p"),ee=u("Zu dieser Modellfamilie geh\xF6ren unter anderem:"),U=m(),f=o("ul"),z=o("li"),M=o("a"),te=u("CTRL"),re=m(),W=o("li"),b=o("a"),le=u("GPT"),oe=m(),k=o("li"),D=o("a"),ne=u("GPT-2"),ae=m(),C=o("li"),P=o("a"),se=u("Transformer XL"),this.h()},l(e){const l=We('[data-svelte="svelte-1phssyn"]',document.head);c=n(l,"META",{name:!0,content:!0}),l.forEach(t),G=d(e),p=n(e,"H1",{class:!0});var X=a(p);_=n(X,"A",{id:!0,class:!0,href:!0});var pe=a(_);A=n(pe,"SPAN",{});var _e=a(A);fe($.$$.fragment,_e),_e.forEach(t),pe.forEach(t),Z=d(X),S=n(X,"SPAN",{});var ve=a(S);F=h(ve,"Decoder-Modelle"),ve.forEach(t),X.forEach(t),I=d(e),fe(w.$$.fragment,e),R=d(e),fe(E.$$.fragment,e),q=d(e),v=n(e,"P",{});var Y=a(v);H=h(Y,"Decoder-Modelle verwenden nur den Decoder eines Transformer-Modells. Die Attention-Layer k\xF6nnen bei jedem Schritt hinsichtlich eines bestimmten Wortes nur auf die W\xF6rter zugreifen, die vor diesem Wort im Satz stehen. Diese Modelle werden oft als "),x=n(Y,"EM",{});var ge=a(x);J=h(ge,"autoregressive Modelle"),ge.forEach(t),O=h(Y," bezeichnet."),Y.forEach(t),B=d(e),y=n(e,"P",{});var $e=a(y);Q=h($e,"Beim Pretraining von Decoder-Modellen geht es in der Regel um die Vorhersage des n\xE4chsten Wortes im Satz."),$e.forEach(t),N=d(e),L=n(e,"P",{});var we=a(L);K=h(we,"Diese Modelle sind am besten f\xFCr Aufgaben geeignet, bei denen es um die Generierung von Texten geht."),we.forEach(t),j=d(e),T=n(e,"P",{});var Ee=a(T);ee=h(Ee,"Zu dieser Modellfamilie geh\xF6ren unter anderem:"),Ee.forEach(t),U=d(e),f=n(e,"UL",{});var g=a(f);z=n(g,"LI",{});var Me=a(z);M=n(Me,"A",{href:!0,rel:!0});var be=a(M);te=h(be,"CTRL"),be.forEach(t),Me.forEach(t),re=d(g),W=n(g,"LI",{});var De=a(W);b=n(De,"A",{href:!0,rel:!0});var Pe=a(b);le=h(Pe,"GPT"),Pe.forEach(t),De.forEach(t),oe=d(g),k=n(g,"LI",{});var ye=a(k);D=n(ye,"A",{href:!0,rel:!0});var Le=a(D);ne=h(Le,"GPT-2"),Le.forEach(t),ye.forEach(t),ae=d(g),C=n(g,"LI",{});var Te=a(C);P=n(Te,"A",{href:!0,rel:!0});var Ae=a(P);se=h(Ae,"Transformer XL"),Ae.forEach(t),Te.forEach(t),g.forEach(t),this.h()},h(){s(c,"name","hf:doc:metadata"),s(c,"content",JSON.stringify(Be)),s(_,"id","decodermodelle"),s(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(_,"href","#decodermodelle"),s(p,"class","relative group"),s(M,"href","https://huggingface.co/transformers/model_doc/ctrl.html"),s(M,"rel","nofollow"),s(b,"href","https://huggingface.co/transformers/model_doc/gpt.html"),s(b,"rel","nofollow"),s(D,"href","https://huggingface.co/transformers/model_doc/gpt2.html"),s(D,"rel","nofollow"),s(P,"href","https://huggingface.co/transformers/model_doc/transformerxl.html"),s(P,"rel","nofollow")},m(e,l){r(document.head,c),i(e,G,l),i(e,p,l),r(p,_),r(_,A),me($,A,null),r(p,Z),r(p,S),r(S,F),i(e,I,l),me(w,e,l),i(e,R,l),me(E,e,l),i(e,q,l),i(e,v,l),r(v,H),r(v,x),r(x,J),r(v,O),i(e,B,l),i(e,y,l),r(y,Q),i(e,N,l),i(e,L,l),r(L,K),i(e,j,l),i(e,T,l),r(T,ee),i(e,U,l),i(e,f,l),r(f,z),r(z,M),r(M,te),r(f,re),r(f,W),r(W,b),r(b,le),r(f,oe),r(f,k),r(k,D),r(D,ne),r(f,ae),r(f,C),r(C,P),r(P,se),V=!0},p:ke,i(e){V||(de($.$$.fragment,e),de(w.$$.fragment,e),de(E.$$.fragment,e),V=!0)},o(e){ue($.$$.fragment,e),ue(w.$$.fragment,e),ue(E.$$.fragment,e),V=!1},d(e){t(c),e&&t(G),e&&t(p),he($),e&&t(I),he(w,e),e&&t(R),he(E,e),e&&t(q),e&&t(v),e&&t(B),e&&t(y),e&&t(N),e&&t(L),e&&t(j),e&&t(T),e&&t(U),e&&t(f)}}}const Be={local:"decodermodelle",title:"Decoder-Modelle"};function Ne(ce){return Ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ze extends Se{constructor(c){super();xe(this,c,Ne,qe,ze,{})}}export{Ze as default,Be as metadata};
