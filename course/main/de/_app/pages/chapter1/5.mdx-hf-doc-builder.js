import{S as Ie,i as Ce,s as De,e as n,k as f,w as me,t as m,M as Ne,c as l,d as t,m as u,a,x as ce,h as c,b as o,G as r,g as d,y as he,L as qe,q as pe,o as ge,B as Ee,v as Ke}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ze}from"../../chunks/Youtube-hf-doc-builder.js";import{I as je}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Fe}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Ue(ve){let p,D,g,E,S,_,G,L,H,N,z,q,w,K,v,O,T,Q,X,Z,R,ee,j,y,te,F,B,re,U,s,P,b,ne,le,x,M,ae,ie,W,$,oe,se,I,A,de,fe,C,k,ue,J;return _=new je({}),z=new Fe({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),w=new Ze({props:{id:"MUqNwgPjJvQ"}}),{c(){p=n("meta"),D=f(),g=n("h1"),E=n("a"),S=n("span"),me(_.$$.fragment),G=f(),L=n("span"),H=m("Encoder-Modelle"),N=f(),me(z.$$.fragment),q=f(),me(w.$$.fragment),K=f(),v=n("p"),O=m("Encoder-Modelle verwenden nur den Encoder eines Transformer-Modells. Die Attention-Layer k\xF6nnen zu jeder Zeit auf alle W\xF6rter des Ausgangssatzes zugreifen. Diese Modelle werden h\xE4ufig als Modelle mit \u201Cbidirektionaler\u201D (engl. bi-directional) Attention bezeichnet und oft "),T=n("em"),Q=m("Auto-Encoding-Modelle"),X=m(" genannt."),Z=f(),R=n("p"),ee=m("Beim Pretraining dieser Modelle geht es in der Regel darum, einen bestimmten Satz auf irgendeine Weise zu verf\xE4lschen (z. B. indem zuf\xE4llig W\xF6rter darin maskiert werden) und das Modell dann damit zu betrauen, den urspr\xFCnglichen Satz zu finden bzw. zu rekonstruieren."),j=f(),y=n("p"),te=m("Rein Encoder-basierte Modelle eignen sich am besten f\xFCr Aufgaben, die ein Verst\xE4ndnis des gesamten Satzes erfordern, wie z. B. die Klassifizierung von S\xE4tzen, die Eigennamenerkennung (bzw. allgemeiner die Klassifikation von W\xF6rtern) und extraktive Frage-Antwort-Systeme."),F=f(),B=n("p"),re=m("Zu dieser Modellfamilie geh\xF6ren unter anderem:"),U=f(),s=n("ul"),P=n("li"),b=n("a"),ne=m("ALBERT"),le=f(),x=n("li"),M=n("a"),ae=m("BERT"),ie=f(),W=n("li"),$=n("a"),oe=m("DistilBERT"),se=f(),I=n("li"),A=n("a"),de=m("ELECTRA"),fe=f(),C=n("li"),k=n("a"),ue=m("RoBERTa"),this.h()},l(e){const i=Ne('[data-svelte="svelte-1phssyn"]',document.head);p=l(i,"META",{name:!0,content:!0}),i.forEach(t),D=u(e),g=l(e,"H1",{class:!0});var V=a(g);E=l(V,"A",{id:!0,class:!0,href:!0});var _e=a(E);S=l(_e,"SPAN",{});var ze=a(S);ce(_.$$.fragment,ze),ze.forEach(t),_e.forEach(t),G=u(V),L=l(V,"SPAN",{});var we=a(L);H=c(we,"Encoder-Modelle"),we.forEach(t),V.forEach(t),N=u(e),ce(z.$$.fragment,e),q=u(e),ce(w.$$.fragment,e),K=u(e),v=l(e,"P",{});var Y=a(v);O=c(Y,"Encoder-Modelle verwenden nur den Encoder eines Transformer-Modells. Die Attention-Layer k\xF6nnen zu jeder Zeit auf alle W\xF6rter des Ausgangssatzes zugreifen. Diese Modelle werden h\xE4ufig als Modelle mit \u201Cbidirektionaler\u201D (engl. bi-directional) Attention bezeichnet und oft "),T=l(Y,"EM",{});var be=a(T);Q=c(be,"Auto-Encoding-Modelle"),be.forEach(t),X=c(Y," genannt."),Y.forEach(t),Z=u(e),R=l(e,"P",{});var Me=a(R);ee=c(Me,"Beim Pretraining dieser Modelle geht es in der Regel darum, einen bestimmten Satz auf irgendeine Weise zu verf\xE4lschen (z. B. indem zuf\xE4llig W\xF6rter darin maskiert werden) und das Modell dann damit zu betrauen, den urspr\xFCnglichen Satz zu finden bzw. zu rekonstruieren."),Me.forEach(t),j=u(e),y=l(e,"P",{});var $e=a(y);te=c($e,"Rein Encoder-basierte Modelle eignen sich am besten f\xFCr Aufgaben, die ein Verst\xE4ndnis des gesamten Satzes erfordern, wie z. B. die Klassifizierung von S\xE4tzen, die Eigennamenerkennung (bzw. allgemeiner die Klassifikation von W\xF6rtern) und extraktive Frage-Antwort-Systeme."),$e.forEach(t),F=u(e),B=l(e,"P",{});var Ae=a(B);re=c(Ae,"Zu dieser Modellfamilie geh\xF6ren unter anderem:"),Ae.forEach(t),U=u(e),s=l(e,"UL",{});var h=a(s);P=l(h,"LI",{});var ke=a(P);b=l(ke,"A",{href:!0,rel:!0});var Re=a(b);ne=c(Re,"ALBERT"),Re.forEach(t),ke.forEach(t),le=u(h),x=l(h,"LI",{});var ye=a(x);M=l(ye,"A",{href:!0,rel:!0});var Be=a(M);ae=c(Be,"BERT"),Be.forEach(t),ye.forEach(t),ie=u(h),W=l(h,"LI",{});var Se=a(W);$=l(Se,"A",{href:!0,rel:!0});var Le=a($);oe=c(Le,"DistilBERT"),Le.forEach(t),Se.forEach(t),se=u(h),I=l(h,"LI",{});var Te=a(I);A=l(Te,"A",{href:!0,rel:!0});var Pe=a(A);de=c(Pe,"ELECTRA"),Pe.forEach(t),Te.forEach(t),fe=u(h),C=l(h,"LI",{});var xe=a(C);k=l(xe,"A",{href:!0,rel:!0});var We=a(k);ue=c(We,"RoBERTa"),We.forEach(t),xe.forEach(t),h.forEach(t),this.h()},h(){o(p,"name","hf:doc:metadata"),o(p,"content",JSON.stringify(Je)),o(E,"id","encodermodelle"),o(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),o(E,"href","#encodermodelle"),o(g,"class","relative group"),o(b,"href","https://huggingface.co/transformers/model_doc/albert.html"),o(b,"rel","nofollow"),o(M,"href","https://huggingface.co/transformers/model_doc/bert.html"),o(M,"rel","nofollow"),o($,"href","https://huggingface.co/transformers/model_doc/distilbert.html"),o($,"rel","nofollow"),o(A,"href","https://huggingface.co/transformers/model_doc/electra.html"),o(A,"rel","nofollow"),o(k,"href","https://huggingface.co/transformers/model_doc/roberta.html"),o(k,"rel","nofollow")},m(e,i){r(document.head,p),d(e,D,i),d(e,g,i),r(g,E),r(E,S),he(_,S,null),r(g,G),r(g,L),r(L,H),d(e,N,i),he(z,e,i),d(e,q,i),he(w,e,i),d(e,K,i),d(e,v,i),r(v,O),r(v,T),r(T,Q),r(v,X),d(e,Z,i),d(e,R,i),r(R,ee),d(e,j,i),d(e,y,i),r(y,te),d(e,F,i),d(e,B,i),r(B,re),d(e,U,i),d(e,s,i),r(s,P),r(P,b),r(b,ne),r(s,le),r(s,x),r(x,M),r(M,ae),r(s,ie),r(s,W),r(W,$),r($,oe),r(s,se),r(s,I),r(I,A),r(A,de),r(s,fe),r(s,C),r(C,k),r(k,ue),J=!0},p:qe,i(e){J||(pe(_.$$.fragment,e),pe(z.$$.fragment,e),pe(w.$$.fragment,e),J=!0)},o(e){ge(_.$$.fragment,e),ge(z.$$.fragment,e),ge(w.$$.fragment,e),J=!1},d(e){t(p),e&&t(D),e&&t(g),Ee(_),e&&t(N),Ee(z,e),e&&t(q),Ee(w,e),e&&t(K),e&&t(v),e&&t(Z),e&&t(R),e&&t(j),e&&t(y),e&&t(F),e&&t(B),e&&t(U),e&&t(s)}}}const Je={local:"encodermodelle",title:"Encoder-Modelle"};function Ve(ve){return Ke(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xe extends Ie{constructor(p){super();Ce(this,p,Ve,Ue,De,{})}}export{Xe as default,Je as metadata};
