import{S as Zu,i as Ku,s as Ou,e as t,k as h,w as m,t as a,M as Cu,c as i,d as n,m as c,a as s,x as p,h as l,b as u,N as f,G as r,g as d,y as v,L as qu,q as b,o as w,B as k,v as Hu}from"../../chunks/vendor-hf-doc-builder.js";import{Y as _o}from"../../chunks/Youtube-hf-doc-builder.js";import{I}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Uu}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Yu(Mo){let G,bt,L,ee,Qr,Me,is,en,ss,wt,Ae,kt,vr,as,zt,B,re,rn,Te,ls,nn,os,Et,br,ds,_t,R,De,Ao,us,Se,To,Mt,ne,hs,$e,cs,fs,At,g,tn,te,sn,gs,ms,Pe,ps,vs,bs,an,ie,ln,ws,ks,We,zs,Es,_s,on,se,dn,Ms,As,ye,Ts,Ds,Ss,un,ae,hn,$s,Ps,Ie,Ws,ys,Is,cn,_,fn,Gs,Ls,Ge,Bs,Rs,Le,Fs,xs,Ns,gn,M,mn,Vs,js,Be,Zs,Ks,pn,Os,Cs,Tt,wr,qs,Dt,A,Re,Hs,vn,Us,Ys,Js,Fe,Xs,bn,Qs,ea,ra,xe,na,wn,ta,ia,St,kr,sa,$t,F,le,kn,Ne,aa,zn,la,Pt,oe,oa,En,da,ua,Wt,de,ha,_n,ca,fa,yt,T,ga,Mn,ma,pa,An,va,ba,It,x,Ve,Do,wa,je,So,Gt,ue,ka,Tn,za,Ea,Lt,N,Ze,$o,_a,Ke,Po,Bt,V,he,Dn,Oe,Ma,Sn,Aa,Rt,zr,Ta,Ft,Ce,qe,Wo,xt,Er,Da,Nt,j,He,yo,Sa,Ue,Io,Vt,Ye,jt,_r,$a,Zt,Mr,Pa,Kt,Ar,Wa,Ot,Z,ce,$n,Je,ya,Pn,Ia,Ct,Xe,qt,fe,Ga,Wn,La,Ba,Ht,K,Qe,Go,Ra,er,Lo,Ut,Tr,Fa,Yt,O,yn,xa,Na,In,Va,ja,Jt,D,Gn,Za,Ka,Ln,Oa,Ca,Bn,qa,Xt,ge,Ha,Rn,Ua,Ya,Qt,C,rr,Bo,Ja,nr,Ro,ei,Dr,Xa,ri,Sr,Qa,ni,q,me,Fn,tr,el,xn,rl,ti,$r,nl,ii,ir,si,H,pe,Nn,sr,tl,Vn,il,ai,Pr,sl,li,ve,Wr,jn,al,ll,ol,yr,Zn,dl,ul,oi,U,ar,Fo,hl,lr,xo,di,Ir,cl,ui,S,Gr,Kn,fl,gl,ml,Lr,On,pl,vl,bl,be,Cn,wl,kl,qn,zl,El,hi,Br,_l,ci,Y,we,Hn,or,Ml,Un,Al,fi,$,Tl,Yn,Dl,Sl,dr,$l,Pl,gi,Rr,Wl,mi,Fr,yl,pi,xr,Il,vi,J,ke,Jn,ur,Gl,Xn,Ll,bi,Nr,Bl,wi,Vr,Rl,ki,jr,Fl,zi,X,hr,No,xl,cr,Vo,Ei,Zr,Nl,_i,ze,Vl,Qn,jl,Zl,Mi,Q,Ee,et,fr,Kl,rt,Ol,Ai,z,Cl,nt,ql,Hl,tt,Ul,Yl,it,Jl,Xl,Ti,P,Kr,st,Ql,eo,ro,Or,at,no,to,io,W,lt,so,ao,ot,lo,oo,dt,uo,ho,Di,y,co,ut,fo,go,ht,mo,po,Si;return Me=new I({}),Ae=new Uu({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),Te=new I({}),Ne=new I({}),Oe=new I({}),Ye=new _o({props:{id:"ftWlj4FBHTg"}}),Je=new I({}),Xe=new _o({props:{id:"BqqfQnyjmgg"}}),tr=new I({}),ir=new _o({props:{id:"H39Z_720T5s"}}),sr=new I({}),or=new I({}),ur=new I({}),fr=new I({}),{c(){G=t("meta"),bt=h(),L=t("h1"),ee=t("a"),Qr=t("span"),m(Me.$$.fragment),is=h(),en=t("span"),ss=a("Wie funktionieren Transformer-Modelle?"),wt=h(),m(Ae.$$.fragment),kt=h(),vr=t("p"),as=a("In diesem Abschnitt werfen wir einen Blick auf die Architektur von Transformer-Modellen."),zt=h(),B=t("h2"),re=t("a"),rn=t("span"),m(Te.$$.fragment),ls=h(),nn=t("span"),os=a("Kurz zur Entwicklungsgeschichte der Transformer-Modelle"),Et=h(),br=t("p"),ds=a("Hier sind einige wichtige Meilensteine in der (kurzen) Geschichte der Transformer-Modelle:"),_t=h(),R=t("div"),De=t("img"),us=h(),Se=t("img"),Mt=h(),ne=t("p"),hs=a("Die "),$e=t("a"),cs=a("Transformer-Architektur"),fs=a(" wurde erstmals im Juni 2017 ver\xF6ffentlicht. Der Schwerpunkt der urspr\xFCnglichen Forschung lag auf \xDCbersetzungsaufgaben. In der Folge wurden mehrere einflussreiche Modelle ver\xF6ffentlicht, darunter:"),At=h(),g=t("ul"),tn=t("li"),te=t("p"),sn=t("strong"),gs=a("Juni 2018"),ms=a(": "),Pe=t("a"),ps=a("GPT"),vs=a(", das erste vortrainierte Transformer-Modell, wurde zum Feintuning f\xFCr verschiedene CL-Aufgaben eingesetzt und erzielte Ergebnisse, die dem neuesten Stand der Technik entsprachen."),bs=h(),an=t("li"),ie=t("p"),ln=t("strong"),ws=a("Oktober 2018"),ks=a(": "),We=t("a"),zs=a("BERT"),Es=a(", ein weiteres gro\xDFes vortrainiertes Modell, das dazu dient, bessere Zusammenfassungen von S\xE4tzen zu erstellen (mehr dazu im n\xE4chsten Kapitel!)"),_s=h(),on=t("li"),se=t("p"),dn=t("strong"),Ms=a("Februar 2019"),As=a(": "),ye=t("a"),Ts=a("GPT-2"),Ds=a(", eine verbesserte (und gr\xF6\xDFere) Version von GPT, die aus ethischen Erw\xE4gungen nicht sofort ver\xF6ffentlicht wurde"),Ss=h(),un=t("li"),ae=t("p"),hn=t("strong"),$s=a("Oktober 2019"),Ps=a(": "),Ie=t("a"),Ws=a("DistilBERT"),ys=a(", eine abgespeckte Version von BERT, die 60 % schneller ist, 40 % weniger Speicherplatz ben\xF6tigt und dennoch 97 % der Leistung von BERT erreicht"),Is=h(),cn=t("li"),_=t("p"),fn=t("strong"),Gs=a("Oktober 2019"),Ls=a(": "),Ge=t("a"),Bs=a("BART"),Rs=a(" und "),Le=t("a"),Fs=a("T5"),xs=a(", zwei gro\xDFe vortrainierte Modelle, die dieselbe Architektur wie das urspr\xFCngliche Transformer-Modell verwenden (die ersten, die dies getan haben)"),Ns=h(),gn=t("li"),M=t("p"),mn=t("strong"),Vs=a("Mai 2020"),js=a(", "),Be=t("a"),Zs=a("GPT-3"),Ks=a(", eine noch gr\xF6\xDFere Version von GPT-2, die in der Lage ist, bei einer Vielzahl von Aufgabenstellungen gute Leistungen zu erbringen, ohne dass ein Feintuning erforderlich ist (auch "),pn=t("em"),Os=a("Zero-Shot Learning"),Cs=a(" genannt)"),Tt=h(),wr=t("p"),qs=a("Diese Auflistung ist bei weitem nicht vollst\xE4ndig und soll nur einige der verschiedenen Arten von Transformer-Modellen aufzeigen. Sie lassen sich grob in drei Kategorien einteilen:"),Dt=h(),A=t("ul"),Re=t("li"),Hs=a("GPT-\xE4hnliche (auch "),vn=t("em"),Us=a("autoregressive"),Ys=a("-Transformer-Modelle genannt)"),Js=h(),Fe=t("li"),Xs=a("BERT-\xE4hnliche (auch "),bn=t("em"),Qs=a("Auto-Encoding"),ea=a("-Transformer-Modelle genannt)"),ra=h(),xe=t("li"),na=a("BART-/T5-\xE4hnliche (auch "),wn=t("em"),ta=a("Sequence-to-Sequence"),ia=a("-Transformer-Modelle genannt)"),St=h(),kr=t("p"),sa=a("Wir werden uns mit diesen unterschiedlichen Modellfamilien sp\xE4ter noch eingehender besch\xE4ftigen."),$t=h(),F=t("h2"),le=t("a"),kn=t("span"),m(Ne.$$.fragment),aa=h(),zn=t("span"),la=a("Transformer-Modelle sind Sprachmodelle"),Pt=h(),oe=t("p"),oa=a("Alle oben genannten Transformer-Modelle (GPT, BERT, BART, T5, etc.) wurden als "),En=t("em"),da=a("Sprachmodelle"),ua=a(" (engl. Language Models) trainiert. Das bedeutet, dass sie mit gro\xDFen Mengen an Rohtext auf selbst\xFCberwachte (engl. self-supervised) Weise trainiert wurden. Selbst\xFCberwachtes Lernen ist eine Form des Trainings, bei der die vorherzusagende Variable, die sog. Zielvariable (engl. Target), automatisch aus den Inputs des Modells berechnet wird. Das bedeutet, dass kein menschliches Zutun n\xF6tig ist, um die Daten zu labeln!"),Wt=h(),de=t("p"),ha=a("Diese Art von Modell entwickelt ein statistisches Verst\xE4ndnis der Sprache, auf die es trainiert wurde, ist aber f\xFCr spezifische praktische Aufgaben nicht sehr n\xFCtzlich. Aus diesem Grund durchl\xE4uft das allgemeine, vortrainierte Modell ein Vorgang namens "),_n=t("em"),ca=a("Transfer Learning"),fa=a(". W\xE4hrend dieses Vorgangs wird das Modell unter \xDCberwachung - d. h. mit Hilfe von durch Menschen bereitgestellte Labels - f\xFCr eine bestimmte Aufgabe feingetunt."),yt=h(),T=t("p"),ga=a("Ein Beispiel f\xFCr eine Aufgabe ist die Vorhersage des n\xE4chsten Wortes in einem Satz, nachdem man die "),Mn=t("em"),ma=a("n"),pa=a(" vorherigen W\xF6rter gelesen hat. Dies nennt sich "),An=t("em"),va=a("kausale Sprachmodellierung"),ba=a(" (engl. Causal Language Modeling), da der Output von den vergangenen und aktuellen Inputs abh\xE4ngt, aber nicht von den zuk\xFCnftigen."),It=h(),x=t("div"),Ve=t("img"),wa=h(),je=t("img"),Gt=h(),ue=t("p"),ka=a("Ein weiteres Beispiel ist die "),Tn=t("em"),za=a("maskierte Sprachmodellierung"),Ea=a(" (engl. Masked Language Modeling), bei der das Modell ein Wort im Satz, das maskiert ist, vorhersagt."),Lt=h(),N=t("div"),Ze=t("img"),_a=h(),Ke=t("img"),Bt=h(),V=t("h2"),he=t("a"),Dn=t("span"),m(Oe.$$.fragment),Ma=h(),Sn=t("span"),Aa=a("Transformer-Modelle sind gro\xDF"),Rt=h(),zr=t("p"),Ta=a("Abgesehen von einigen wenigen Ausrei\xDFern (wie DistilBERT) besteht die allgemeine Strategie, um eine bessere Leistung zu erzielen, darin, die Modelle zu vergr\xF6\xDFern und die Menge an Daten zu erh\xF6hen, auf denen sie vortrainiert werden."),Ft=h(),Ce=t("div"),qe=t("img"),xt=h(),Er=t("p"),Da=a("Leider erfordert das Training eines Modells, insbesondere eines gro\xDFen, eine gro\xDFe Menge an Daten. Das ist sehr kostspielig in Bezug auf Zeit und Rechenleistung. Es hat sogar Auswirkungen auf die Umwelt, wie in der folgenden Grafik zu sehen ist."),Nt=h(),j=t("div"),He=t("img"),Sa=h(),Ue=t("img"),Vt=h(),m(Ye.$$.fragment),jt=h(),_r=t("p"),$a=a("Hier ist ein Projekt zu sehen, bei dem ein Team gezielt versucht, die Umweltauswirkungen des Pretrainings (sehr gro\xDFer) Modelle zu reduzieren. Wenn man die vielen Versuche ber\xFCcksichtigt, die dazu n\xF6tig sind, die besten Hyperparameter zu finden, w\xE4ren die zu bemessenden \xF6kologischen Konsequenzen noch gr\xF6\xDFer."),Zt=h(),Mr=t("p"),Pa=a("Stell dir vor, dass jedes Mal, wenn ein Forschungsteam, eine Bildungseinrichtung oder ein Unternehmen ein Modell trainieren m\xF6chte, dies von Grund auf tun m\xFCsste. Das w\xFCrde zu enormen, unn\xF6tigen globalen Kosten f\xFChren!"),Kt=h(),Ar=t("p"),Wa=a("Deshalb ist die gemeinsame Nutzung von Sprachmodellen von gr\xF6\xDFter Bedeutung: trainierte Gewichtungen gemeinsam zu nutzen und auf bereits trainierten Gewichtungen aufzubauen, reduziert die gesamten Rechenkosten und den CO2-Fu\xDFabdruck der Community."),Ot=h(),Z=t("h2"),ce=t("a"),$n=t("span"),m(Je.$$.fragment),ya=h(),Pn=t("span"),Ia=a("Transfer Learning"),Ct=h(),m(Xe.$$.fragment),qt=h(),fe=t("p"),Ga=a("Beim "),Wn=t("em"),La=a("Pretraining"),Ba=a(" wird ein Modell von Grund auf neu trainiert: Die Gewichte werden nach dem Zufallsprinzip initialisiert und das Training beginnt ohne jegliches Vorwissen."),Ht=h(),K=t("div"),Qe=t("img"),Ra=h(),er=t("img"),Ut=h(),Tr=t("p"),Fa=a("Dieses Pretraining wird normalerweise mit sehr gro\xDFen Datenmengen durchgef\xFChrt. Daher wird ein sehr gro\xDFer Korpus an Daten ben\xF6tigt und das Training kann mehrere Wochen in Anspruch nehmen."),Yt=h(),O=t("p"),yn=t("em"),xa=a("Feintuning"),Na=a(" ist hingegen das Training, das "),In=t("strong"),Va=a("nach"),ja=a(" dem Pretraining eines Modells durchgef\xFChrt wird. F\xFCr das Feintuning nimmst du zun\xE4chst ein vortrainiertes Sprachmodell und trainierst es dann mit einem aufgabenspezifischen Datensatz nach. Moment - warum trainierst du das Modell nicht gleich f\xFCr die endg\xFCltige Aufgabe? Daf\xFCr gibt es mehrere Gr\xFCnde:"),Jt=h(),D=t("ul"),Gn=t("li"),Za=a("Das vortrainierte Modell wurde bereits auf einem Datensatz trainiert, der einige \xC4hnlichkeiten mit dem Datensatz, der f\xFCr das Feintuning verwendet wird, aufweist. Beim Feintuning kann also von dem Wissen profitiert werden, das das urspr\xFCngliche Modell w\xE4hrend des Pretrainings erlangt hat (bei CL-Problemstellungen verf\xFCgt das vortrainierte Modell zum Beispiel \xFCber eine Art statistisches Verst\xE4ndnis der Sprache, die du f\xFCr deine Aufgabe verwendest)."),Ka=h(),Ln=t("li"),Oa=a("Da das vortrainierte Modell bereits auf vielen Daten trainiert wurde, sind zum Feintuning bedeutend weniger Daten erforderlich, um brauchbare Ergebnisse erzielen zu k\xF6nnen."),Ca=h(),Bn=t("li"),qa=a("Aus demselben Grund sind der Zeitaufwand und die Ressourcen, die f\xFCr gute Ergebnisse ben\xF6tigt werden, bedeutend geringer."),Xt=h(),ge=t("p"),Ha=a("Man k\xF6nnte zum Beispiel ein auf Englisch trainiertes Modell nutzen und es dann auf einem arXiv-Korpus feintunen, um ein auf wissenschaftliche Sprache ausgerichtetes Modell zu erstellen. F\xFCr das Feintuning wird nur eine begrenzte Menge an Daten ben\xF6tigt: Das Wissen, das das vortrainierte Modell erworben hat, wird \u201C\xFCbertragen\u201D (engl. transferred), daher der Begriff "),Rn=t("em"),Ua=a("Transfer Learning"),Ya=a("."),Qt=h(),C=t("div"),rr=t("img"),Ja=h(),nr=t("img"),ei=h(),Dr=t("p"),Xa=a("Das Feintuning eines Modells ist daher mit geringeren Zeit-, Daten-, Umwelt- und finanziellen Kosten verbunden. Es ist auch schneller und einfacher, verschiedene Modelle f\xFCr das Feintuning auszuprobieren, da das Training mit geringeren Einschr\xE4nkungen einhergeht als ein vollst\xE4ndiges Pretraining."),ri=h(),Sr=t("p"),Qa=a("Dieser Ansatz f\xFChrt auch zu besseren Ergebnissen als ein Training von Grund auf (es sei denn, du hast viele Daten). Deshalb solltest du immer versuchen, ein vortrainiertes Modell zu nutzen - und zwar ein Modell, das so nah wie m\xF6glich an deiner Aufgabenstellung ist - und es f\xFCr das Feintuning verwenden."),ni=h(),q=t("h2"),me=t("a"),Fn=t("span"),m(tr.$$.fragment),el=h(),xn=t("span"),rl=a("Grundlegende Architektur"),ti=h(),$r=t("p"),nl=a("In diesem Abschnitt gehen wir auf die grundlegende Architektur des Transformer-Modells ein. Mach dir keine Sorgen, wenn du einige der Konzepte nicht verstehst. Im weiteren Verlauf folgen noch ausf\xFChrliche Abschnitte zu den einzelnen Komponenten."),ii=h(),m(ir.$$.fragment),si=h(),H=t("h2"),pe=t("a"),Nn=t("span"),m(sr.$$.fragment),tl=h(),Vn=t("span"),il=a("Einf\xFChrung"),ai=h(),Pr=t("p"),sl=a("Das Modell besteht haupts\xE4chlich aus zwei Bl\xF6cken:"),li=h(),ve=t("ul"),Wr=t("li"),jn=t("strong"),al=a("Encoder (links)"),ll=a(": Der Encoder, auch Kodierer genannt, empf\xE4ngt einen Input und erstellt eine numerische Darstellung bzw. Repr\xE4sentation des Inputs (seiner Features, im Deutschen auch als Merkmale bezeichnet). Das bedeutet, dass das Modell darauf optimiert ist, ein Verst\xE4ndnis vom Input zu erlangen."),ol=h(),yr=t("li"),Zn=t("strong"),dl=a("Decoder (rechts)"),ul=a(": Der Decoder, auch bekannt als Dekodierer, verwendet die Repr\xE4sentation des Encoders (Features) zusammen mit anderen Inputs, um eine Zielsequenz zu generieren. Das bedeutet, dass das Modell darauf optimiert ist, einen Output zu generieren."),oi=h(),U=t("div"),ar=t("img"),hl=h(),lr=t("img"),di=h(),Ir=t("p"),cl=a("Jede dieser Komponenten kann je nach Aufgabe unabh\xE4ngig voneinander verwendet werden:"),ui=h(),S=t("ul"),Gr=t("li"),Kn=t("strong"),fl=a("Rein Encoder-basierte Modelle"),gl=a(" (\u201CEncoder-only Models\u201D): Gut f\xFCr Aufgaben, die ein Verst\xE4ndnis des Inputs erfordern, wie z. B. bei der Klassifizierung von S\xE4tzen und der Eigennamenerkennung (NER)."),ml=h(),Lr=t("li"),On=t("strong"),pl=a("Rein Decoder-basierte Modelle"),vl=a(" (\u201CDecoder-only Models\u201D): Gut geeignet f\xFCr generative Aufgaben wie die Textgenerierung."),bl=h(),be=t("li"),Cn=t("strong"),wl=a("Encoder-Decoder-basierte Modelle"),kl=a(" bzw. "),qn=t("strong"),zl=a("Sequence-to-Sequence-Modelle"),El=a(": Gut f\xFCr generative Aufgaben, die einen Input erfordern, wie z. B. \xDCbersetzungen oder Zusammenfassungen."),hi=h(),Br=t("p"),_l=a("Wir werden diese Architekturen in sp\xE4teren Abschnitten noch gesondert behandeln."),ci=h(),Y=t("h2"),we=t("a"),Hn=t("span"),m(or.$$.fragment),Ml=h(),Un=t("span"),Al=a("Attention-Layer"),fi=h(),$=t("p"),Tl=a("Ein wesentliches Merkmal der Transformer-Modelle ist, dass sie mit speziellen Layern (im Deutschen auch als Schichten bezeichnet), den "),Yn=t("em"),Dl=a("Attention-Layern"),Sl=a(", aufgebaut sind. Der Titel des Forschungsbeitrags, in dem die Transformer-Architektur vorgestellt wurde, lautete sogar "),dr=t("a"),$l=a("\u201CAttention Is All You Need\u201D"),Pl=a("! Wir werden uns sp\xE4ter im Kurs mit den Details von Attention-Layern befassen. F\xFCr den Moment musst du nur wissen, dass dieser Layer dem Modell sagt, dass es bei der Repr\xE4sentation eines jeden Worts in einem Satz, den du ihm \xFCbergeben hast, bestimmten W\xF6rtern besondere Aufmerksamkeit schenken (und die anderen mehr oder weniger ignorieren) soll."),gi=h(),Rr=t("p"),Wl=a("Angenommen, du sollst einen Text aus dem Englischen ins Franz\xF6sische \xFCbersetzen. Bei dem Input \u201CYou like this course\u201D muss ein \xDCbersetzungsmodell auch das angrenzende Wort \u201CYou\u201D ber\xFCcksichtigen, um die richtige \xDCbersetzung f\xFCr das Wort \u201Clike\u201D zu erhalten, denn im Franz\xF6sischen wird das Verb \u201Clike\u201D je nach Subjekt unterschiedlich konjugiert. Der Rest des Satzes ist jedoch f\xFCr die \xDCbersetzung dieses Wortes nicht hilfreich. Genauso muss das Modell bei der \xDCbersetzung von \u201Cthis\u201D auf das Wort \u201Ccourse\u201D achten, denn \u201Cthis\u201D wird unterschiedlich \xFCbersetzt, je nachdem, ob das zugeh\xF6rige Substantiv m\xE4nnlich oder weiblich ist. Auch hier spielen die anderen W\xF6rter im Satz f\xFCr die \xDCbersetzung von \u201Cthis\u201D keine Rolle. Bei komplexeren S\xE4tzen (und komplexeren Grammatikregeln) muss das Modell besonders auf W\xF6rter achten, die weiter entfernt im Satz vorkommen, um jedes Wort richtig zu \xFCbersetzen."),mi=h(),Fr=t("p"),yl=a("Das gleiche Konzept gilt f\xFCr jede Aufgabenstellung, die mit nat\xFCrlicher Sprache zu tun hat: Ein Wort an sich hat eine Bedeutung, aber diese Bedeutung h\xE4ngt stark vom Kontext ab, der sich durch ein anderes Wort (oder W\xF6rter) vor oder nach dem untersuchten Wort ergibt."),pi=h(),xr=t("p"),Il=a("Nachdem du nun eine Vorstellung davon hast, worum es bei Attention-Layern geht, nehmen wir die Transformer-Architektur genauer unter die Lupe."),vi=h(),J=t("h2"),ke=t("a"),Jn=t("span"),m(ur.$$.fragment),Gl=h(),Xn=t("span"),Ll=a("Die urspr\xFCngliche Architektur"),bi=h(),Nr=t("p"),Bl=a("Die Transformer-Architektur wurde urspr\xFCnglich f\xFCr die maschinelle \xDCbersetzung entwickelt. Beim Training erh\xE4lt der Encoder Inputs (S\xE4tze) in einer bestimmten Sprache, w\xE4hrend der Decoder die gleichen S\xE4tze in der gew\xFCnschten Zielsprache erh\xE4lt. Im Encoder k\xF6nnen die Attention-Layer alle W\xF6rter eines Satzes verwenden (denn wie wir gerade gesehen haben, kann die \xDCbersetzung eines bestimmten Wortes sowohl von dem abh\xE4ngen, was nach, als auch von dem, was vor dem Wort im Satz steht). Der Decoder arbeitet hingegen sequentiell und kann nur die W\xF6rter im Satz ber\xFCcksichtigen, die er bereits \xFCbersetzt hat (also nur die W\xF6rter vor dem Wort, das gerade generiert wird). Wenn wir zum Beispiel die ersten drei W\xF6rter der \xFCbersetzten Zielsequenz vorhergesagt haben, geben wir sie an den Decoder weiter, der dann alle Inputs des Encoders verwendet, um das vierte Wort vorherzusagen."),wi=h(),Vr=t("p"),Rl=a("Um das Training zu beschleunigen (insofern das Modell Zugriff auf die Ziels\xE4tze hat), wird der Decoder mit dem gesamten (vorherzusagenden) Zielsatz gef\xFCttert, aber er darf keine nachfolgenden W\xF6rter verwenden (wenn er Zugriff zum Wort an Position 2 h\xE4tte, w\xE4hrend er versucht, das Wort an Position 2 vorherzusagen, w\xE4re die Aufgabe nicht sonderlich schwer!). Wenn er zum Beispiel versucht, das vierte Wort vorherzusagen, hat der Attention-Layer nur Zugriff zu den W\xF6rtern an den Positionen 1 bis 3."),ki=h(),jr=t("p"),Fl=a("Die urspr\xFCngliche Transformer-Architektur sah wie folgt aus - mit dem Encoder auf der linken und dem Decoder auf der rechten Seite:"),zi=h(),X=t("div"),hr=t("img"),xl=h(),cr=t("img"),Ei=h(),Zr=t("p"),Nl=a("Beachte, dass die Attention des ersten Attention-Layers in einem Decoder-Block alle (vorangegangenen) Inputs, die der Decoder erhalten hat, ber\xFCcksichtigt, w\xE4hrend der zweite Attention-Layer den Output des Encoders verwendet. Im Rahmen der Vorhersage des aktuellen Wortes kann er also auf den gesamten Input-Satz zugreifen. Das ist vor allem deshalb n\xFCtzlich, da es in den verschiedenen Sprachen unterschiedliche grammatikalische Regeln geben kann, wodurch die W\xF6rter in einer anderen Reihenfolge aneinandergereiht werden. Ebenso k\xF6nnte ein erst sp\xE4ter im Satz enthaltener Zusammenhang dabei hilfreich sein, die beste \xDCbersetzung eines bestimmten Wortes zu bestimmen."),_i=h(),ze=t("p"),Vl=a("Die "),Qn=t("em"),jl=a("Attention-Mask"),Zl=a(" kann auch im Encoder bzw. Decoder verwendet werden, um zu verhindern, dass das Modell bestimmte W\xF6rter beachtet - zum Beispiel das spezielle F\xFCllwort (engl. Padding Word), das verwendet wird, um alle Inputs auf die gleiche L\xE4nge zu bringen, wenn die S\xE4tze zu Batches zusammengef\xFChrt werden."),Mi=h(),Q=t("h2"),Ee=t("a"),et=t("span"),m(fr.$$.fragment),Kl=h(),rt=t("span"),Ol=a("Architekturen vs. Checkpoints"),Ai=h(),z=t("p"),Cl=a("Wenn wir uns in diesem Kurs mit Transformer-Modellen besch\xE4ftigen, wirst du auf "),nt=t("em"),ql=a("Architekturen"),Hl=a(", "),tt=t("em"),Ul=a("Checkpoints"),Yl=a(" und auch auf "),it=t("em"),Jl=a("Modelle"),Xl=a(" sto\xDFen. Diese Begriffe haben alle eine etwas unterschiedliche Bedeutung:"),Ti=h(),P=t("ul"),Kr=t("li"),st=t("strong"),Ql=a("Architektur"),eo=a(": Dies ist das Skelett des Modells - die Definition jedes Layers und jeder Operation, die innerhalb des Modells stattfindet."),ro=h(),Or=t("li"),at=t("strong"),no=a("Checkpoints"),to=a(": Dies ist die Gewichtung, die f\xFCr eine bestimmte Architektur geladen wird."),io=h(),W=t("li"),lt=t("strong"),so=a("Modell"),ao=a(": Dies ist ein Oberbegriff, der nicht so pr\xE4zise ist wie \u201CArchitektur\u201D oder \u201CCheckpoint\u201D: Er kann beides bedeuten. In diesem Kurs wird jeweils explizit spezifiziert, ob es sich um eine "),ot=t("em"),lo=a("Architektur"),oo=a(" oder um einen "),dt=t("em"),uo=a("Checkpoint"),ho=a(" handelt, um Zweideutigkeiten zu vermeiden."),Di=h(),y=t("p"),co=a("BERT ist zum Beispiel eine Architektur, w\xE4hrend "),ut=t("code"),fo=a("bert-base-cased"),go=a(" - ein Satz von Gewichten, der vom Google-Team f\xFCr die erste Version von BERT trainiert wurde - ein Checkpoint ist. Man kann aber auch \u201Cdas BERT-Modell\u201D oder \u201Cdas "),ht=t("code"),mo=a("bert-base-cased"),po=a("-Modell\u201D sagen."),this.h()},l(e){const o=Cu('[data-svelte="svelte-1phssyn"]',document.head);G=i(o,"META",{name:!0,content:!0}),o.forEach(n),bt=c(e),L=i(e,"H1",{class:!0});var $i=s(L);ee=i($i,"A",{id:!0,class:!0,href:!0});var jo=s(ee);Qr=i(jo,"SPAN",{});var Zo=s(Qr);p(Me.$$.fragment,Zo),Zo.forEach(n),jo.forEach(n),is=c($i),en=i($i,"SPAN",{});var Ko=s(en);ss=l(Ko,"Wie funktionieren Transformer-Modelle?"),Ko.forEach(n),$i.forEach(n),wt=c(e),p(Ae.$$.fragment,e),kt=c(e),vr=i(e,"P",{});var Oo=s(vr);as=l(Oo,"In diesem Abschnitt werfen wir einen Blick auf die Architektur von Transformer-Modellen."),Oo.forEach(n),zt=c(e),B=i(e,"H2",{class:!0});var Pi=s(B);re=i(Pi,"A",{id:!0,class:!0,href:!0});var Co=s(re);rn=i(Co,"SPAN",{});var qo=s(rn);p(Te.$$.fragment,qo),qo.forEach(n),Co.forEach(n),ls=c(Pi),nn=i(Pi,"SPAN",{});var Ho=s(nn);os=l(Ho,"Kurz zur Entwicklungsgeschichte der Transformer-Modelle"),Ho.forEach(n),Pi.forEach(n),Et=c(e),br=i(e,"P",{});var Uo=s(br);ds=l(Uo,"Hier sind einige wichtige Meilensteine in der (kurzen) Geschichte der Transformer-Modelle:"),Uo.forEach(n),_t=c(e),R=i(e,"DIV",{class:!0});var Wi=s(R);De=i(Wi,"IMG",{class:!0,src:!0,alt:!0}),us=c(Wi),Se=i(Wi,"IMG",{class:!0,src:!0,alt:!0}),Wi.forEach(n),Mt=c(e),ne=i(e,"P",{});var yi=s(ne);hs=l(yi,"Die "),$e=i(yi,"A",{href:!0,rel:!0});var Yo=s($e);cs=l(Yo,"Transformer-Architektur"),Yo.forEach(n),fs=l(yi," wurde erstmals im Juni 2017 ver\xF6ffentlicht. Der Schwerpunkt der urspr\xFCnglichen Forschung lag auf \xDCbersetzungsaufgaben. In der Folge wurden mehrere einflussreiche Modelle ver\xF6ffentlicht, darunter:"),yi.forEach(n),At=c(e),g=i(e,"UL",{});var E=s(g);tn=i(E,"LI",{});var Jo=s(tn);te=i(Jo,"P",{});var ct=s(te);sn=i(ct,"STRONG",{});var Xo=s(sn);gs=l(Xo,"Juni 2018"),Xo.forEach(n),ms=l(ct,": "),Pe=i(ct,"A",{href:!0,rel:!0});var Qo=s(Pe);ps=l(Qo,"GPT"),Qo.forEach(n),vs=l(ct,", das erste vortrainierte Transformer-Modell, wurde zum Feintuning f\xFCr verschiedene CL-Aufgaben eingesetzt und erzielte Ergebnisse, die dem neuesten Stand der Technik entsprachen."),ct.forEach(n),Jo.forEach(n),bs=c(E),an=i(E,"LI",{});var ed=s(an);ie=i(ed,"P",{});var ft=s(ie);ln=i(ft,"STRONG",{});var rd=s(ln);ws=l(rd,"Oktober 2018"),rd.forEach(n),ks=l(ft,": "),We=i(ft,"A",{href:!0,rel:!0});var nd=s(We);zs=l(nd,"BERT"),nd.forEach(n),Es=l(ft,", ein weiteres gro\xDFes vortrainiertes Modell, das dazu dient, bessere Zusammenfassungen von S\xE4tzen zu erstellen (mehr dazu im n\xE4chsten Kapitel!)"),ft.forEach(n),ed.forEach(n),_s=c(E),on=i(E,"LI",{});var td=s(on);se=i(td,"P",{});var gt=s(se);dn=i(gt,"STRONG",{});var id=s(dn);Ms=l(id,"Februar 2019"),id.forEach(n),As=l(gt,": "),ye=i(gt,"A",{href:!0,rel:!0});var sd=s(ye);Ts=l(sd,"GPT-2"),sd.forEach(n),Ds=l(gt,", eine verbesserte (und gr\xF6\xDFere) Version von GPT, die aus ethischen Erw\xE4gungen nicht sofort ver\xF6ffentlicht wurde"),gt.forEach(n),td.forEach(n),Ss=c(E),un=i(E,"LI",{});var ad=s(un);ae=i(ad,"P",{});var mt=s(ae);hn=i(mt,"STRONG",{});var ld=s(hn);$s=l(ld,"Oktober 2019"),ld.forEach(n),Ps=l(mt,": "),Ie=i(mt,"A",{href:!0,rel:!0});var od=s(Ie);Ws=l(od,"DistilBERT"),od.forEach(n),ys=l(mt,", eine abgespeckte Version von BERT, die 60 % schneller ist, 40 % weniger Speicherplatz ben\xF6tigt und dennoch 97 % der Leistung von BERT erreicht"),mt.forEach(n),ad.forEach(n),Is=c(E),cn=i(E,"LI",{});var dd=s(cn);_=i(dd,"P",{});var gr=s(_);fn=i(gr,"STRONG",{});var ud=s(fn);Gs=l(ud,"Oktober 2019"),ud.forEach(n),Ls=l(gr,": "),Ge=i(gr,"A",{href:!0,rel:!0});var hd=s(Ge);Bs=l(hd,"BART"),hd.forEach(n),Rs=l(gr," und "),Le=i(gr,"A",{href:!0,rel:!0});var cd=s(Le);Fs=l(cd,"T5"),cd.forEach(n),xs=l(gr,", zwei gro\xDFe vortrainierte Modelle, die dieselbe Architektur wie das urspr\xFCngliche Transformer-Modell verwenden (die ersten, die dies getan haben)"),gr.forEach(n),dd.forEach(n),Ns=c(E),gn=i(E,"LI",{});var fd=s(gn);M=i(fd,"P",{});var mr=s(M);mn=i(mr,"STRONG",{});var gd=s(mn);Vs=l(gd,"Mai 2020"),gd.forEach(n),js=l(mr,", "),Be=i(mr,"A",{href:!0,rel:!0});var md=s(Be);Zs=l(md,"GPT-3"),md.forEach(n),Ks=l(mr,", eine noch gr\xF6\xDFere Version von GPT-2, die in der Lage ist, bei einer Vielzahl von Aufgabenstellungen gute Leistungen zu erbringen, ohne dass ein Feintuning erforderlich ist (auch "),pn=i(mr,"EM",{});var pd=s(pn);Os=l(pd,"Zero-Shot Learning"),pd.forEach(n),Cs=l(mr," genannt)"),mr.forEach(n),fd.forEach(n),E.forEach(n),Tt=c(e),wr=i(e,"P",{});var vd=s(wr);qs=l(vd,"Diese Auflistung ist bei weitem nicht vollst\xE4ndig und soll nur einige der verschiedenen Arten von Transformer-Modellen aufzeigen. Sie lassen sich grob in drei Kategorien einteilen:"),vd.forEach(n),Dt=c(e),A=i(e,"UL",{});var Cr=s(A);Re=i(Cr,"LI",{});var Ii=s(Re);Hs=l(Ii,"GPT-\xE4hnliche (auch "),vn=i(Ii,"EM",{});var bd=s(vn);Us=l(bd,"autoregressive"),bd.forEach(n),Ys=l(Ii,"-Transformer-Modelle genannt)"),Ii.forEach(n),Js=c(Cr),Fe=i(Cr,"LI",{});var Gi=s(Fe);Xs=l(Gi,"BERT-\xE4hnliche (auch "),bn=i(Gi,"EM",{});var wd=s(bn);Qs=l(wd,"Auto-Encoding"),wd.forEach(n),ea=l(Gi,"-Transformer-Modelle genannt)"),Gi.forEach(n),ra=c(Cr),xe=i(Cr,"LI",{});var Li=s(xe);na=l(Li,"BART-/T5-\xE4hnliche (auch "),wn=i(Li,"EM",{});var kd=s(wn);ta=l(kd,"Sequence-to-Sequence"),kd.forEach(n),ia=l(Li,"-Transformer-Modelle genannt)"),Li.forEach(n),Cr.forEach(n),St=c(e),kr=i(e,"P",{});var zd=s(kr);sa=l(zd,"Wir werden uns mit diesen unterschiedlichen Modellfamilien sp\xE4ter noch eingehender besch\xE4ftigen."),zd.forEach(n),$t=c(e),F=i(e,"H2",{class:!0});var Bi=s(F);le=i(Bi,"A",{id:!0,class:!0,href:!0});var Ed=s(le);kn=i(Ed,"SPAN",{});var _d=s(kn);p(Ne.$$.fragment,_d),_d.forEach(n),Ed.forEach(n),aa=c(Bi),zn=i(Bi,"SPAN",{});var Md=s(zn);la=l(Md,"Transformer-Modelle sind Sprachmodelle"),Md.forEach(n),Bi.forEach(n),Pt=c(e),oe=i(e,"P",{});var Ri=s(oe);oa=l(Ri,"Alle oben genannten Transformer-Modelle (GPT, BERT, BART, T5, etc.) wurden als "),En=i(Ri,"EM",{});var Ad=s(En);da=l(Ad,"Sprachmodelle"),Ad.forEach(n),ua=l(Ri," (engl. Language Models) trainiert. Das bedeutet, dass sie mit gro\xDFen Mengen an Rohtext auf selbst\xFCberwachte (engl. self-supervised) Weise trainiert wurden. Selbst\xFCberwachtes Lernen ist eine Form des Trainings, bei der die vorherzusagende Variable, die sog. Zielvariable (engl. Target), automatisch aus den Inputs des Modells berechnet wird. Das bedeutet, dass kein menschliches Zutun n\xF6tig ist, um die Daten zu labeln!"),Ri.forEach(n),Wt=c(e),de=i(e,"P",{});var Fi=s(de);ha=l(Fi,"Diese Art von Modell entwickelt ein statistisches Verst\xE4ndnis der Sprache, auf die es trainiert wurde, ist aber f\xFCr spezifische praktische Aufgaben nicht sehr n\xFCtzlich. Aus diesem Grund durchl\xE4uft das allgemeine, vortrainierte Modell ein Vorgang namens "),_n=i(Fi,"EM",{});var Td=s(_n);ca=l(Td,"Transfer Learning"),Td.forEach(n),fa=l(Fi,". W\xE4hrend dieses Vorgangs wird das Modell unter \xDCberwachung - d. h. mit Hilfe von durch Menschen bereitgestellte Labels - f\xFCr eine bestimmte Aufgabe feingetunt."),Fi.forEach(n),yt=c(e),T=i(e,"P",{});var qr=s(T);ga=l(qr,"Ein Beispiel f\xFCr eine Aufgabe ist die Vorhersage des n\xE4chsten Wortes in einem Satz, nachdem man die "),Mn=i(qr,"EM",{});var Dd=s(Mn);ma=l(Dd,"n"),Dd.forEach(n),pa=l(qr," vorherigen W\xF6rter gelesen hat. Dies nennt sich "),An=i(qr,"EM",{});var Sd=s(An);va=l(Sd,"kausale Sprachmodellierung"),Sd.forEach(n),ba=l(qr," (engl. Causal Language Modeling), da der Output von den vergangenen und aktuellen Inputs abh\xE4ngt, aber nicht von den zuk\xFCnftigen."),qr.forEach(n),It=c(e),x=i(e,"DIV",{class:!0});var xi=s(x);Ve=i(xi,"IMG",{class:!0,src:!0,alt:!0}),wa=c(xi),je=i(xi,"IMG",{class:!0,src:!0,alt:!0}),xi.forEach(n),Gt=c(e),ue=i(e,"P",{});var Ni=s(ue);ka=l(Ni,"Ein weiteres Beispiel ist die "),Tn=i(Ni,"EM",{});var $d=s(Tn);za=l($d,"maskierte Sprachmodellierung"),$d.forEach(n),Ea=l(Ni," (engl. Masked Language Modeling), bei der das Modell ein Wort im Satz, das maskiert ist, vorhersagt."),Ni.forEach(n),Lt=c(e),N=i(e,"DIV",{class:!0});var Vi=s(N);Ze=i(Vi,"IMG",{class:!0,src:!0,alt:!0}),_a=c(Vi),Ke=i(Vi,"IMG",{class:!0,src:!0,alt:!0}),Vi.forEach(n),Bt=c(e),V=i(e,"H2",{class:!0});var ji=s(V);he=i(ji,"A",{id:!0,class:!0,href:!0});var Pd=s(he);Dn=i(Pd,"SPAN",{});var Wd=s(Dn);p(Oe.$$.fragment,Wd),Wd.forEach(n),Pd.forEach(n),Ma=c(ji),Sn=i(ji,"SPAN",{});var yd=s(Sn);Aa=l(yd,"Transformer-Modelle sind gro\xDF"),yd.forEach(n),ji.forEach(n),Rt=c(e),zr=i(e,"P",{});var Id=s(zr);Ta=l(Id,"Abgesehen von einigen wenigen Ausrei\xDFern (wie DistilBERT) besteht die allgemeine Strategie, um eine bessere Leistung zu erzielen, darin, die Modelle zu vergr\xF6\xDFern und die Menge an Daten zu erh\xF6hen, auf denen sie vortrainiert werden."),Id.forEach(n),Ft=c(e),Ce=i(e,"DIV",{class:!0});var Gd=s(Ce);qe=i(Gd,"IMG",{src:!0,alt:!0,width:!0}),Gd.forEach(n),xt=c(e),Er=i(e,"P",{});var Ld=s(Er);Da=l(Ld,"Leider erfordert das Training eines Modells, insbesondere eines gro\xDFen, eine gro\xDFe Menge an Daten. Das ist sehr kostspielig in Bezug auf Zeit und Rechenleistung. Es hat sogar Auswirkungen auf die Umwelt, wie in der folgenden Grafik zu sehen ist."),Ld.forEach(n),Nt=c(e),j=i(e,"DIV",{class:!0});var Zi=s(j);He=i(Zi,"IMG",{class:!0,src:!0,alt:!0}),Sa=c(Zi),Ue=i(Zi,"IMG",{class:!0,src:!0,alt:!0}),Zi.forEach(n),Vt=c(e),p(Ye.$$.fragment,e),jt=c(e),_r=i(e,"P",{});var Bd=s(_r);$a=l(Bd,"Hier ist ein Projekt zu sehen, bei dem ein Team gezielt versucht, die Umweltauswirkungen des Pretrainings (sehr gro\xDFer) Modelle zu reduzieren. Wenn man die vielen Versuche ber\xFCcksichtigt, die dazu n\xF6tig sind, die besten Hyperparameter zu finden, w\xE4ren die zu bemessenden \xF6kologischen Konsequenzen noch gr\xF6\xDFer."),Bd.forEach(n),Zt=c(e),Mr=i(e,"P",{});var Rd=s(Mr);Pa=l(Rd,"Stell dir vor, dass jedes Mal, wenn ein Forschungsteam, eine Bildungseinrichtung oder ein Unternehmen ein Modell trainieren m\xF6chte, dies von Grund auf tun m\xFCsste. Das w\xFCrde zu enormen, unn\xF6tigen globalen Kosten f\xFChren!"),Rd.forEach(n),Kt=c(e),Ar=i(e,"P",{});var Fd=s(Ar);Wa=l(Fd,"Deshalb ist die gemeinsame Nutzung von Sprachmodellen von gr\xF6\xDFter Bedeutung: trainierte Gewichtungen gemeinsam zu nutzen und auf bereits trainierten Gewichtungen aufzubauen, reduziert die gesamten Rechenkosten und den CO2-Fu\xDFabdruck der Community."),Fd.forEach(n),Ot=c(e),Z=i(e,"H2",{class:!0});var Ki=s(Z);ce=i(Ki,"A",{id:!0,class:!0,href:!0});var xd=s(ce);$n=i(xd,"SPAN",{});var Nd=s($n);p(Je.$$.fragment,Nd),Nd.forEach(n),xd.forEach(n),ya=c(Ki),Pn=i(Ki,"SPAN",{});var Vd=s(Pn);Ia=l(Vd,"Transfer Learning"),Vd.forEach(n),Ki.forEach(n),Ct=c(e),p(Xe.$$.fragment,e),qt=c(e),fe=i(e,"P",{});var Oi=s(fe);Ga=l(Oi,"Beim "),Wn=i(Oi,"EM",{});var jd=s(Wn);La=l(jd,"Pretraining"),jd.forEach(n),Ba=l(Oi," wird ein Modell von Grund auf neu trainiert: Die Gewichte werden nach dem Zufallsprinzip initialisiert und das Training beginnt ohne jegliches Vorwissen."),Oi.forEach(n),Ht=c(e),K=i(e,"DIV",{class:!0});var Ci=s(K);Qe=i(Ci,"IMG",{class:!0,src:!0,alt:!0}),Ra=c(Ci),er=i(Ci,"IMG",{class:!0,src:!0,alt:!0}),Ci.forEach(n),Ut=c(e),Tr=i(e,"P",{});var Zd=s(Tr);Fa=l(Zd,"Dieses Pretraining wird normalerweise mit sehr gro\xDFen Datenmengen durchgef\xFChrt. Daher wird ein sehr gro\xDFer Korpus an Daten ben\xF6tigt und das Training kann mehrere Wochen in Anspruch nehmen."),Zd.forEach(n),Yt=c(e),O=i(e,"P",{});var pt=s(O);yn=i(pt,"EM",{});var Kd=s(yn);xa=l(Kd,"Feintuning"),Kd.forEach(n),Na=l(pt," ist hingegen das Training, das "),In=i(pt,"STRONG",{});var Od=s(In);Va=l(Od,"nach"),Od.forEach(n),ja=l(pt," dem Pretraining eines Modells durchgef\xFChrt wird. F\xFCr das Feintuning nimmst du zun\xE4chst ein vortrainiertes Sprachmodell und trainierst es dann mit einem aufgabenspezifischen Datensatz nach. Moment - warum trainierst du das Modell nicht gleich f\xFCr die endg\xFCltige Aufgabe? Daf\xFCr gibt es mehrere Gr\xFCnde:"),pt.forEach(n),Jt=c(e),D=i(e,"UL",{});var Hr=s(D);Gn=i(Hr,"LI",{});var Cd=s(Gn);Za=l(Cd,"Das vortrainierte Modell wurde bereits auf einem Datensatz trainiert, der einige \xC4hnlichkeiten mit dem Datensatz, der f\xFCr das Feintuning verwendet wird, aufweist. Beim Feintuning kann also von dem Wissen profitiert werden, das das urspr\xFCngliche Modell w\xE4hrend des Pretrainings erlangt hat (bei CL-Problemstellungen verf\xFCgt das vortrainierte Modell zum Beispiel \xFCber eine Art statistisches Verst\xE4ndnis der Sprache, die du f\xFCr deine Aufgabe verwendest)."),Cd.forEach(n),Ka=c(Hr),Ln=i(Hr,"LI",{});var qd=s(Ln);Oa=l(qd,"Da das vortrainierte Modell bereits auf vielen Daten trainiert wurde, sind zum Feintuning bedeutend weniger Daten erforderlich, um brauchbare Ergebnisse erzielen zu k\xF6nnen."),qd.forEach(n),Ca=c(Hr),Bn=i(Hr,"LI",{});var Hd=s(Bn);qa=l(Hd,"Aus demselben Grund sind der Zeitaufwand und die Ressourcen, die f\xFCr gute Ergebnisse ben\xF6tigt werden, bedeutend geringer."),Hd.forEach(n),Hr.forEach(n),Xt=c(e),ge=i(e,"P",{});var qi=s(ge);Ha=l(qi,"Man k\xF6nnte zum Beispiel ein auf Englisch trainiertes Modell nutzen und es dann auf einem arXiv-Korpus feintunen, um ein auf wissenschaftliche Sprache ausgerichtetes Modell zu erstellen. F\xFCr das Feintuning wird nur eine begrenzte Menge an Daten ben\xF6tigt: Das Wissen, das das vortrainierte Modell erworben hat, wird \u201C\xFCbertragen\u201D (engl. transferred), daher der Begriff "),Rn=i(qi,"EM",{});var Ud=s(Rn);Ua=l(Ud,"Transfer Learning"),Ud.forEach(n),Ya=l(qi,"."),qi.forEach(n),Qt=c(e),C=i(e,"DIV",{class:!0});var Hi=s(C);rr=i(Hi,"IMG",{class:!0,src:!0,alt:!0}),Ja=c(Hi),nr=i(Hi,"IMG",{class:!0,src:!0,alt:!0}),Hi.forEach(n),ei=c(e),Dr=i(e,"P",{});var Yd=s(Dr);Xa=l(Yd,"Das Feintuning eines Modells ist daher mit geringeren Zeit-, Daten-, Umwelt- und finanziellen Kosten verbunden. Es ist auch schneller und einfacher, verschiedene Modelle f\xFCr das Feintuning auszuprobieren, da das Training mit geringeren Einschr\xE4nkungen einhergeht als ein vollst\xE4ndiges Pretraining."),Yd.forEach(n),ri=c(e),Sr=i(e,"P",{});var Jd=s(Sr);Qa=l(Jd,"Dieser Ansatz f\xFChrt auch zu besseren Ergebnissen als ein Training von Grund auf (es sei denn, du hast viele Daten). Deshalb solltest du immer versuchen, ein vortrainiertes Modell zu nutzen - und zwar ein Modell, das so nah wie m\xF6glich an deiner Aufgabenstellung ist - und es f\xFCr das Feintuning verwenden."),Jd.forEach(n),ni=c(e),q=i(e,"H2",{class:!0});var Ui=s(q);me=i(Ui,"A",{id:!0,class:!0,href:!0});var Xd=s(me);Fn=i(Xd,"SPAN",{});var Qd=s(Fn);p(tr.$$.fragment,Qd),Qd.forEach(n),Xd.forEach(n),el=c(Ui),xn=i(Ui,"SPAN",{});var eu=s(xn);rl=l(eu,"Grundlegende Architektur"),eu.forEach(n),Ui.forEach(n),ti=c(e),$r=i(e,"P",{});var ru=s($r);nl=l(ru,"In diesem Abschnitt gehen wir auf die grundlegende Architektur des Transformer-Modells ein. Mach dir keine Sorgen, wenn du einige der Konzepte nicht verstehst. Im weiteren Verlauf folgen noch ausf\xFChrliche Abschnitte zu den einzelnen Komponenten."),ru.forEach(n),ii=c(e),p(ir.$$.fragment,e),si=c(e),H=i(e,"H2",{class:!0});var Yi=s(H);pe=i(Yi,"A",{id:!0,class:!0,href:!0});var nu=s(pe);Nn=i(nu,"SPAN",{});var tu=s(Nn);p(sr.$$.fragment,tu),tu.forEach(n),nu.forEach(n),tl=c(Yi),Vn=i(Yi,"SPAN",{});var iu=s(Vn);il=l(iu,"Einf\xFChrung"),iu.forEach(n),Yi.forEach(n),ai=c(e),Pr=i(e,"P",{});var su=s(Pr);sl=l(su,"Das Modell besteht haupts\xE4chlich aus zwei Bl\xF6cken:"),su.forEach(n),li=c(e),ve=i(e,"UL",{});var Ji=s(ve);Wr=i(Ji,"LI",{});var vo=s(Wr);jn=i(vo,"STRONG",{});var au=s(jn);al=l(au,"Encoder (links)"),au.forEach(n),ll=l(vo,": Der Encoder, auch Kodierer genannt, empf\xE4ngt einen Input und erstellt eine numerische Darstellung bzw. Repr\xE4sentation des Inputs (seiner Features, im Deutschen auch als Merkmale bezeichnet). Das bedeutet, dass das Modell darauf optimiert ist, ein Verst\xE4ndnis vom Input zu erlangen."),vo.forEach(n),ol=c(Ji),yr=i(Ji,"LI",{});var bo=s(yr);Zn=i(bo,"STRONG",{});var lu=s(Zn);dl=l(lu,"Decoder (rechts)"),lu.forEach(n),ul=l(bo,": Der Decoder, auch bekannt als Dekodierer, verwendet die Repr\xE4sentation des Encoders (Features) zusammen mit anderen Inputs, um eine Zielsequenz zu generieren. Das bedeutet, dass das Modell darauf optimiert ist, einen Output zu generieren."),bo.forEach(n),Ji.forEach(n),oi=c(e),U=i(e,"DIV",{class:!0});var Xi=s(U);ar=i(Xi,"IMG",{class:!0,src:!0,alt:!0}),hl=c(Xi),lr=i(Xi,"IMG",{class:!0,src:!0,alt:!0}),Xi.forEach(n),di=c(e),Ir=i(e,"P",{});var ou=s(Ir);cl=l(ou,"Jede dieser Komponenten kann je nach Aufgabe unabh\xE4ngig voneinander verwendet werden:"),ou.forEach(n),ui=c(e),S=i(e,"UL",{});var Ur=s(S);Gr=i(Ur,"LI",{});var wo=s(Gr);Kn=i(wo,"STRONG",{});var du=s(Kn);fl=l(du,"Rein Encoder-basierte Modelle"),du.forEach(n),gl=l(wo," (\u201CEncoder-only Models\u201D): Gut f\xFCr Aufgaben, die ein Verst\xE4ndnis des Inputs erfordern, wie z. B. bei der Klassifizierung von S\xE4tzen und der Eigennamenerkennung (NER)."),wo.forEach(n),ml=c(Ur),Lr=i(Ur,"LI",{});var ko=s(Lr);On=i(ko,"STRONG",{});var uu=s(On);pl=l(uu,"Rein Decoder-basierte Modelle"),uu.forEach(n),vl=l(ko," (\u201CDecoder-only Models\u201D): Gut geeignet f\xFCr generative Aufgaben wie die Textgenerierung."),ko.forEach(n),bl=c(Ur),be=i(Ur,"LI",{});var vt=s(be);Cn=i(vt,"STRONG",{});var hu=s(Cn);wl=l(hu,"Encoder-Decoder-basierte Modelle"),hu.forEach(n),kl=l(vt," bzw. "),qn=i(vt,"STRONG",{});var cu=s(qn);zl=l(cu,"Sequence-to-Sequence-Modelle"),cu.forEach(n),El=l(vt,": Gut f\xFCr generative Aufgaben, die einen Input erfordern, wie z. B. \xDCbersetzungen oder Zusammenfassungen."),vt.forEach(n),Ur.forEach(n),hi=c(e),Br=i(e,"P",{});var fu=s(Br);_l=l(fu,"Wir werden diese Architekturen in sp\xE4teren Abschnitten noch gesondert behandeln."),fu.forEach(n),ci=c(e),Y=i(e,"H2",{class:!0});var Qi=s(Y);we=i(Qi,"A",{id:!0,class:!0,href:!0});var gu=s(we);Hn=i(gu,"SPAN",{});var mu=s(Hn);p(or.$$.fragment,mu),mu.forEach(n),gu.forEach(n),Ml=c(Qi),Un=i(Qi,"SPAN",{});var pu=s(Un);Al=l(pu,"Attention-Layer"),pu.forEach(n),Qi.forEach(n),fi=c(e),$=i(e,"P",{});var Yr=s($);Tl=l(Yr,"Ein wesentliches Merkmal der Transformer-Modelle ist, dass sie mit speziellen Layern (im Deutschen auch als Schichten bezeichnet), den "),Yn=i(Yr,"EM",{});var vu=s(Yn);Dl=l(vu,"Attention-Layern"),vu.forEach(n),Sl=l(Yr,", aufgebaut sind. Der Titel des Forschungsbeitrags, in dem die Transformer-Architektur vorgestellt wurde, lautete sogar "),dr=i(Yr,"A",{href:!0,rel:!0});var bu=s(dr);$l=l(bu,"\u201CAttention Is All You Need\u201D"),bu.forEach(n),Pl=l(Yr,"! Wir werden uns sp\xE4ter im Kurs mit den Details von Attention-Layern befassen. F\xFCr den Moment musst du nur wissen, dass dieser Layer dem Modell sagt, dass es bei der Repr\xE4sentation eines jeden Worts in einem Satz, den du ihm \xFCbergeben hast, bestimmten W\xF6rtern besondere Aufmerksamkeit schenken (und die anderen mehr oder weniger ignorieren) soll."),Yr.forEach(n),gi=c(e),Rr=i(e,"P",{});var wu=s(Rr);Wl=l(wu,"Angenommen, du sollst einen Text aus dem Englischen ins Franz\xF6sische \xFCbersetzen. Bei dem Input \u201CYou like this course\u201D muss ein \xDCbersetzungsmodell auch das angrenzende Wort \u201CYou\u201D ber\xFCcksichtigen, um die richtige \xDCbersetzung f\xFCr das Wort \u201Clike\u201D zu erhalten, denn im Franz\xF6sischen wird das Verb \u201Clike\u201D je nach Subjekt unterschiedlich konjugiert. Der Rest des Satzes ist jedoch f\xFCr die \xDCbersetzung dieses Wortes nicht hilfreich. Genauso muss das Modell bei der \xDCbersetzung von \u201Cthis\u201D auf das Wort \u201Ccourse\u201D achten, denn \u201Cthis\u201D wird unterschiedlich \xFCbersetzt, je nachdem, ob das zugeh\xF6rige Substantiv m\xE4nnlich oder weiblich ist. Auch hier spielen die anderen W\xF6rter im Satz f\xFCr die \xDCbersetzung von \u201Cthis\u201D keine Rolle. Bei komplexeren S\xE4tzen (und komplexeren Grammatikregeln) muss das Modell besonders auf W\xF6rter achten, die weiter entfernt im Satz vorkommen, um jedes Wort richtig zu \xFCbersetzen."),wu.forEach(n),mi=c(e),Fr=i(e,"P",{});var ku=s(Fr);yl=l(ku,"Das gleiche Konzept gilt f\xFCr jede Aufgabenstellung, die mit nat\xFCrlicher Sprache zu tun hat: Ein Wort an sich hat eine Bedeutung, aber diese Bedeutung h\xE4ngt stark vom Kontext ab, der sich durch ein anderes Wort (oder W\xF6rter) vor oder nach dem untersuchten Wort ergibt."),ku.forEach(n),pi=c(e),xr=i(e,"P",{});var zu=s(xr);Il=l(zu,"Nachdem du nun eine Vorstellung davon hast, worum es bei Attention-Layern geht, nehmen wir die Transformer-Architektur genauer unter die Lupe."),zu.forEach(n),vi=c(e),J=i(e,"H2",{class:!0});var es=s(J);ke=i(es,"A",{id:!0,class:!0,href:!0});var Eu=s(ke);Jn=i(Eu,"SPAN",{});var _u=s(Jn);p(ur.$$.fragment,_u),_u.forEach(n),Eu.forEach(n),Gl=c(es),Xn=i(es,"SPAN",{});var Mu=s(Xn);Ll=l(Mu,"Die urspr\xFCngliche Architektur"),Mu.forEach(n),es.forEach(n),bi=c(e),Nr=i(e,"P",{});var Au=s(Nr);Bl=l(Au,"Die Transformer-Architektur wurde urspr\xFCnglich f\xFCr die maschinelle \xDCbersetzung entwickelt. Beim Training erh\xE4lt der Encoder Inputs (S\xE4tze) in einer bestimmten Sprache, w\xE4hrend der Decoder die gleichen S\xE4tze in der gew\xFCnschten Zielsprache erh\xE4lt. Im Encoder k\xF6nnen die Attention-Layer alle W\xF6rter eines Satzes verwenden (denn wie wir gerade gesehen haben, kann die \xDCbersetzung eines bestimmten Wortes sowohl von dem abh\xE4ngen, was nach, als auch von dem, was vor dem Wort im Satz steht). Der Decoder arbeitet hingegen sequentiell und kann nur die W\xF6rter im Satz ber\xFCcksichtigen, die er bereits \xFCbersetzt hat (also nur die W\xF6rter vor dem Wort, das gerade generiert wird). Wenn wir zum Beispiel die ersten drei W\xF6rter der \xFCbersetzten Zielsequenz vorhergesagt haben, geben wir sie an den Decoder weiter, der dann alle Inputs des Encoders verwendet, um das vierte Wort vorherzusagen."),Au.forEach(n),wi=c(e),Vr=i(e,"P",{});var Tu=s(Vr);Rl=l(Tu,"Um das Training zu beschleunigen (insofern das Modell Zugriff auf die Ziels\xE4tze hat), wird der Decoder mit dem gesamten (vorherzusagenden) Zielsatz gef\xFCttert, aber er darf keine nachfolgenden W\xF6rter verwenden (wenn er Zugriff zum Wort an Position 2 h\xE4tte, w\xE4hrend er versucht, das Wort an Position 2 vorherzusagen, w\xE4re die Aufgabe nicht sonderlich schwer!). Wenn er zum Beispiel versucht, das vierte Wort vorherzusagen, hat der Attention-Layer nur Zugriff zu den W\xF6rtern an den Positionen 1 bis 3."),Tu.forEach(n),ki=c(e),jr=i(e,"P",{});var Du=s(jr);Fl=l(Du,"Die urspr\xFCngliche Transformer-Architektur sah wie folgt aus - mit dem Encoder auf der linken und dem Decoder auf der rechten Seite:"),Du.forEach(n),zi=c(e),X=i(e,"DIV",{class:!0});var rs=s(X);hr=i(rs,"IMG",{class:!0,src:!0,alt:!0}),xl=c(rs),cr=i(rs,"IMG",{class:!0,src:!0,alt:!0}),rs.forEach(n),Ei=c(e),Zr=i(e,"P",{});var Su=s(Zr);Nl=l(Su,"Beachte, dass die Attention des ersten Attention-Layers in einem Decoder-Block alle (vorangegangenen) Inputs, die der Decoder erhalten hat, ber\xFCcksichtigt, w\xE4hrend der zweite Attention-Layer den Output des Encoders verwendet. Im Rahmen der Vorhersage des aktuellen Wortes kann er also auf den gesamten Input-Satz zugreifen. Das ist vor allem deshalb n\xFCtzlich, da es in den verschiedenen Sprachen unterschiedliche grammatikalische Regeln geben kann, wodurch die W\xF6rter in einer anderen Reihenfolge aneinandergereiht werden. Ebenso k\xF6nnte ein erst sp\xE4ter im Satz enthaltener Zusammenhang dabei hilfreich sein, die beste \xDCbersetzung eines bestimmten Wortes zu bestimmen."),Su.forEach(n),_i=c(e),ze=i(e,"P",{});var ns=s(ze);Vl=l(ns,"Die "),Qn=i(ns,"EM",{});var $u=s(Qn);jl=l($u,"Attention-Mask"),$u.forEach(n),Zl=l(ns," kann auch im Encoder bzw. Decoder verwendet werden, um zu verhindern, dass das Modell bestimmte W\xF6rter beachtet - zum Beispiel das spezielle F\xFCllwort (engl. Padding Word), das verwendet wird, um alle Inputs auf die gleiche L\xE4nge zu bringen, wenn die S\xE4tze zu Batches zusammengef\xFChrt werden."),ns.forEach(n),Mi=c(e),Q=i(e,"H2",{class:!0});var ts=s(Q);Ee=i(ts,"A",{id:!0,class:!0,href:!0});var Pu=s(Ee);et=i(Pu,"SPAN",{});var Wu=s(et);p(fr.$$.fragment,Wu),Wu.forEach(n),Pu.forEach(n),Kl=c(ts),rt=i(ts,"SPAN",{});var yu=s(rt);Ol=l(yu,"Architekturen vs. Checkpoints"),yu.forEach(n),ts.forEach(n),Ai=c(e),z=i(e,"P",{});var _e=s(z);Cl=l(_e,"Wenn wir uns in diesem Kurs mit Transformer-Modellen besch\xE4ftigen, wirst du auf "),nt=i(_e,"EM",{});var Iu=s(nt);ql=l(Iu,"Architekturen"),Iu.forEach(n),Hl=l(_e,", "),tt=i(_e,"EM",{});var Gu=s(tt);Ul=l(Gu,"Checkpoints"),Gu.forEach(n),Yl=l(_e," und auch auf "),it=i(_e,"EM",{});var Lu=s(it);Jl=l(Lu,"Modelle"),Lu.forEach(n),Xl=l(_e," sto\xDFen. Diese Begriffe haben alle eine etwas unterschiedliche Bedeutung:"),_e.forEach(n),Ti=c(e),P=i(e,"UL",{});var Jr=s(P);Kr=i(Jr,"LI",{});var zo=s(Kr);st=i(zo,"STRONG",{});var Bu=s(st);Ql=l(Bu,"Architektur"),Bu.forEach(n),eo=l(zo,": Dies ist das Skelett des Modells - die Definition jedes Layers und jeder Operation, die innerhalb des Modells stattfindet."),zo.forEach(n),ro=c(Jr),Or=i(Jr,"LI",{});var Eo=s(Or);at=i(Eo,"STRONG",{});var Ru=s(at);no=l(Ru,"Checkpoints"),Ru.forEach(n),to=l(Eo,": Dies ist die Gewichtung, die f\xFCr eine bestimmte Architektur geladen wird."),Eo.forEach(n),io=c(Jr),W=i(Jr,"LI",{});var pr=s(W);lt=i(pr,"STRONG",{});var Fu=s(lt);so=l(Fu,"Modell"),Fu.forEach(n),ao=l(pr,": Dies ist ein Oberbegriff, der nicht so pr\xE4zise ist wie \u201CArchitektur\u201D oder \u201CCheckpoint\u201D: Er kann beides bedeuten. In diesem Kurs wird jeweils explizit spezifiziert, ob es sich um eine "),ot=i(pr,"EM",{});var xu=s(ot);lo=l(xu,"Architektur"),xu.forEach(n),oo=l(pr," oder um einen "),dt=i(pr,"EM",{});var Nu=s(dt);uo=l(Nu,"Checkpoint"),Nu.forEach(n),ho=l(pr," handelt, um Zweideutigkeiten zu vermeiden."),pr.forEach(n),Jr.forEach(n),Di=c(e),y=i(e,"P",{});var Xr=s(y);co=l(Xr,"BERT ist zum Beispiel eine Architektur, w\xE4hrend "),ut=i(Xr,"CODE",{});var Vu=s(ut);fo=l(Vu,"bert-base-cased"),Vu.forEach(n),go=l(Xr," - ein Satz von Gewichten, der vom Google-Team f\xFCr die erste Version von BERT trainiert wurde - ein Checkpoint ist. Man kann aber auch \u201Cdas BERT-Modell\u201D oder \u201Cdas "),ht=i(Xr,"CODE",{});var ju=s(ht);mo=l(ju,"bert-base-cased"),ju.forEach(n),po=l(Xr,"-Modell\u201D sagen."),Xr.forEach(n),this.h()},h(){u(G,"name","hf:doc:metadata"),u(G,"content",JSON.stringify(Ju)),u(ee,"id","wie-funktionieren-transformermodelle"),u(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ee,"href","#wie-funktionieren-transformermodelle"),u(L,"class","relative group"),u(re,"id","kurz-zur-entwicklungsgeschichte-der-transformermodelle"),u(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(re,"href","#kurz-zur-entwicklungsgeschichte-der-transformermodelle"),u(B,"class","relative group"),u(De,"class","block dark:hidden"),f(De.src,Ao="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono.svg")||u(De,"src",Ao),u(De,"alt","A brief chronology of Transformers models."),u(Se,"class","hidden dark:block"),f(Se.src,To="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg")||u(Se,"src",To),u(Se,"alt","A brief chronology of Transformers models."),u(R,"class","flex justify-center"),u($e,"href","https://arxiv.org/abs/1706.03762"),u($e,"rel","nofollow"),u(Pe,"href","https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"),u(Pe,"rel","nofollow"),u(We,"href","https://arxiv.org/abs/1810.04805"),u(We,"rel","nofollow"),u(ye,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),u(ye,"rel","nofollow"),u(Ie,"href","https://arxiv.org/abs/1910.01108"),u(Ie,"rel","nofollow"),u(Ge,"href","https://arxiv.org/abs/1910.13461"),u(Ge,"rel","nofollow"),u(Le,"href","https://arxiv.org/abs/1910.10683"),u(Le,"rel","nofollow"),u(Be,"href","https://arxiv.org/abs/2005.14165"),u(Be,"rel","nofollow"),u(le,"id","transformermodelle-sind-sprachmodelle"),u(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(le,"href","#transformermodelle-sind-sprachmodelle"),u(F,"class","relative group"),u(Ve,"class","block dark:hidden"),f(Ve.src,Do="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling.svg")||u(Ve,"src",Do),u(Ve,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),u(je,"class","hidden dark:block"),f(je.src,So="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg")||u(je,"src",So),u(je,"alt","Example of causal language modeling in which the next word from a sentence is predicted."),u(x,"class","flex justify-center"),u(Ze,"class","block dark:hidden"),f(Ze.src,$o="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling.svg")||u(Ze,"src",$o),u(Ze,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),u(Ke,"class","hidden dark:block"),f(Ke.src,Po="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/masked_modeling-dark.svg")||u(Ke,"src",Po),u(Ke,"alt","Example of masked language modeling in which a masked word from a sentence is predicted."),u(N,"class","flex justify-center"),u(he,"id","transformermodelle-sind-gro"),u(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(he,"href","#transformermodelle-sind-gro"),u(V,"class","relative group"),f(qe.src,Wo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png")||u(qe,"src",Wo),u(qe,"alt","Number of parameters of recent Transformers models"),u(qe,"width","90%"),u(Ce,"class","flex justify-center"),u(He,"class","block dark:hidden"),f(He.src,yo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint.svg")||u(He,"src",yo),u(He,"alt","The carbon footprint of a large language model."),u(Ue,"class","hidden dark:block"),f(Ue.src,Io="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/carbon_footprint-dark.svg")||u(Ue,"src",Io),u(Ue,"alt","The carbon footprint of a large language model."),u(j,"class","flex justify-center"),u(ce,"id","transfer-learning"),u(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ce,"href","#transfer-learning"),u(Z,"class","relative group"),u(Qe,"class","block dark:hidden"),f(Qe.src,Go="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining.svg")||u(Qe,"src",Go),u(Qe,"alt","The pretraining of a language model is costly in both time and money."),u(er,"class","hidden dark:block"),f(er.src,Lo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg")||u(er,"src",Lo),u(er,"alt","The pretraining of a language model is costly in both time and money."),u(K,"class","flex justify-center"),u(rr,"class","block dark:hidden"),f(rr.src,Bo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning.svg")||u(rr,"src",Bo),u(rr,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),u(nr,"class","hidden dark:block"),f(nr.src,Ro="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg")||u(nr,"src",Ro),u(nr,"alt","The fine-tuning of a language model is cheaper than pretraining in both time and money."),u(C,"class","flex justify-center"),u(me,"id","grundlegende-architektur"),u(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(me,"href","#grundlegende-architektur"),u(q,"class","relative group"),u(pe,"id","einfhrung"),u(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(pe,"href","#einfhrung"),u(H,"class","relative group"),u(ar,"class","block dark:hidden"),f(ar.src,Fo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks.svg")||u(ar,"src",Fo),u(ar,"alt","Architecture of a Transformers models"),u(lr,"class","hidden dark:block"),f(lr.src,xo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg")||u(lr,"src",xo),u(lr,"alt","Architecture of a Transformers models"),u(U,"class","flex justify-center"),u(we,"id","attentionlayer"),u(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(we,"href","#attentionlayer"),u(Y,"class","relative group"),u(dr,"href","https://arxiv.org/abs/1706.03762"),u(dr,"rel","nofollow"),u(ke,"id","die-ursprngliche-architektur"),u(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ke,"href","#die-ursprngliche-architektur"),u(J,"class","relative group"),u(hr,"class","block dark:hidden"),f(hr.src,No="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers.svg")||u(hr,"src",No),u(hr,"alt","Architecture of a Transformers models"),u(cr,"class","hidden dark:block"),f(cr.src,Vo="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg")||u(cr,"src",Vo),u(cr,"alt","Architecture of a Transformers models"),u(X,"class","flex justify-center"),u(Ee,"id","architekturen-vs-checkpoints"),u(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ee,"href","#architekturen-vs-checkpoints"),u(Q,"class","relative group")},m(e,o){r(document.head,G),d(e,bt,o),d(e,L,o),r(L,ee),r(ee,Qr),v(Me,Qr,null),r(L,is),r(L,en),r(en,ss),d(e,wt,o),v(Ae,e,o),d(e,kt,o),d(e,vr,o),r(vr,as),d(e,zt,o),d(e,B,o),r(B,re),r(re,rn),v(Te,rn,null),r(B,ls),r(B,nn),r(nn,os),d(e,Et,o),d(e,br,o),r(br,ds),d(e,_t,o),d(e,R,o),r(R,De),r(R,us),r(R,Se),d(e,Mt,o),d(e,ne,o),r(ne,hs),r(ne,$e),r($e,cs),r(ne,fs),d(e,At,o),d(e,g,o),r(g,tn),r(tn,te),r(te,sn),r(sn,gs),r(te,ms),r(te,Pe),r(Pe,ps),r(te,vs),r(g,bs),r(g,an),r(an,ie),r(ie,ln),r(ln,ws),r(ie,ks),r(ie,We),r(We,zs),r(ie,Es),r(g,_s),r(g,on),r(on,se),r(se,dn),r(dn,Ms),r(se,As),r(se,ye),r(ye,Ts),r(se,Ds),r(g,Ss),r(g,un),r(un,ae),r(ae,hn),r(hn,$s),r(ae,Ps),r(ae,Ie),r(Ie,Ws),r(ae,ys),r(g,Is),r(g,cn),r(cn,_),r(_,fn),r(fn,Gs),r(_,Ls),r(_,Ge),r(Ge,Bs),r(_,Rs),r(_,Le),r(Le,Fs),r(_,xs),r(g,Ns),r(g,gn),r(gn,M),r(M,mn),r(mn,Vs),r(M,js),r(M,Be),r(Be,Zs),r(M,Ks),r(M,pn),r(pn,Os),r(M,Cs),d(e,Tt,o),d(e,wr,o),r(wr,qs),d(e,Dt,o),d(e,A,o),r(A,Re),r(Re,Hs),r(Re,vn),r(vn,Us),r(Re,Ys),r(A,Js),r(A,Fe),r(Fe,Xs),r(Fe,bn),r(bn,Qs),r(Fe,ea),r(A,ra),r(A,xe),r(xe,na),r(xe,wn),r(wn,ta),r(xe,ia),d(e,St,o),d(e,kr,o),r(kr,sa),d(e,$t,o),d(e,F,o),r(F,le),r(le,kn),v(Ne,kn,null),r(F,aa),r(F,zn),r(zn,la),d(e,Pt,o),d(e,oe,o),r(oe,oa),r(oe,En),r(En,da),r(oe,ua),d(e,Wt,o),d(e,de,o),r(de,ha),r(de,_n),r(_n,ca),r(de,fa),d(e,yt,o),d(e,T,o),r(T,ga),r(T,Mn),r(Mn,ma),r(T,pa),r(T,An),r(An,va),r(T,ba),d(e,It,o),d(e,x,o),r(x,Ve),r(x,wa),r(x,je),d(e,Gt,o),d(e,ue,o),r(ue,ka),r(ue,Tn),r(Tn,za),r(ue,Ea),d(e,Lt,o),d(e,N,o),r(N,Ze),r(N,_a),r(N,Ke),d(e,Bt,o),d(e,V,o),r(V,he),r(he,Dn),v(Oe,Dn,null),r(V,Ma),r(V,Sn),r(Sn,Aa),d(e,Rt,o),d(e,zr,o),r(zr,Ta),d(e,Ft,o),d(e,Ce,o),r(Ce,qe),d(e,xt,o),d(e,Er,o),r(Er,Da),d(e,Nt,o),d(e,j,o),r(j,He),r(j,Sa),r(j,Ue),d(e,Vt,o),v(Ye,e,o),d(e,jt,o),d(e,_r,o),r(_r,$a),d(e,Zt,o),d(e,Mr,o),r(Mr,Pa),d(e,Kt,o),d(e,Ar,o),r(Ar,Wa),d(e,Ot,o),d(e,Z,o),r(Z,ce),r(ce,$n),v(Je,$n,null),r(Z,ya),r(Z,Pn),r(Pn,Ia),d(e,Ct,o),v(Xe,e,o),d(e,qt,o),d(e,fe,o),r(fe,Ga),r(fe,Wn),r(Wn,La),r(fe,Ba),d(e,Ht,o),d(e,K,o),r(K,Qe),r(K,Ra),r(K,er),d(e,Ut,o),d(e,Tr,o),r(Tr,Fa),d(e,Yt,o),d(e,O,o),r(O,yn),r(yn,xa),r(O,Na),r(O,In),r(In,Va),r(O,ja),d(e,Jt,o),d(e,D,o),r(D,Gn),r(Gn,Za),r(D,Ka),r(D,Ln),r(Ln,Oa),r(D,Ca),r(D,Bn),r(Bn,qa),d(e,Xt,o),d(e,ge,o),r(ge,Ha),r(ge,Rn),r(Rn,Ua),r(ge,Ya),d(e,Qt,o),d(e,C,o),r(C,rr),r(C,Ja),r(C,nr),d(e,ei,o),d(e,Dr,o),r(Dr,Xa),d(e,ri,o),d(e,Sr,o),r(Sr,Qa),d(e,ni,o),d(e,q,o),r(q,me),r(me,Fn),v(tr,Fn,null),r(q,el),r(q,xn),r(xn,rl),d(e,ti,o),d(e,$r,o),r($r,nl),d(e,ii,o),v(ir,e,o),d(e,si,o),d(e,H,o),r(H,pe),r(pe,Nn),v(sr,Nn,null),r(H,tl),r(H,Vn),r(Vn,il),d(e,ai,o),d(e,Pr,o),r(Pr,sl),d(e,li,o),d(e,ve,o),r(ve,Wr),r(Wr,jn),r(jn,al),r(Wr,ll),r(ve,ol),r(ve,yr),r(yr,Zn),r(Zn,dl),r(yr,ul),d(e,oi,o),d(e,U,o),r(U,ar),r(U,hl),r(U,lr),d(e,di,o),d(e,Ir,o),r(Ir,cl),d(e,ui,o),d(e,S,o),r(S,Gr),r(Gr,Kn),r(Kn,fl),r(Gr,gl),r(S,ml),r(S,Lr),r(Lr,On),r(On,pl),r(Lr,vl),r(S,bl),r(S,be),r(be,Cn),r(Cn,wl),r(be,kl),r(be,qn),r(qn,zl),r(be,El),d(e,hi,o),d(e,Br,o),r(Br,_l),d(e,ci,o),d(e,Y,o),r(Y,we),r(we,Hn),v(or,Hn,null),r(Y,Ml),r(Y,Un),r(Un,Al),d(e,fi,o),d(e,$,o),r($,Tl),r($,Yn),r(Yn,Dl),r($,Sl),r($,dr),r(dr,$l),r($,Pl),d(e,gi,o),d(e,Rr,o),r(Rr,Wl),d(e,mi,o),d(e,Fr,o),r(Fr,yl),d(e,pi,o),d(e,xr,o),r(xr,Il),d(e,vi,o),d(e,J,o),r(J,ke),r(ke,Jn),v(ur,Jn,null),r(J,Gl),r(J,Xn),r(Xn,Ll),d(e,bi,o),d(e,Nr,o),r(Nr,Bl),d(e,wi,o),d(e,Vr,o),r(Vr,Rl),d(e,ki,o),d(e,jr,o),r(jr,Fl),d(e,zi,o),d(e,X,o),r(X,hr),r(X,xl),r(X,cr),d(e,Ei,o),d(e,Zr,o),r(Zr,Nl),d(e,_i,o),d(e,ze,o),r(ze,Vl),r(ze,Qn),r(Qn,jl),r(ze,Zl),d(e,Mi,o),d(e,Q,o),r(Q,Ee),r(Ee,et),v(fr,et,null),r(Q,Kl),r(Q,rt),r(rt,Ol),d(e,Ai,o),d(e,z,o),r(z,Cl),r(z,nt),r(nt,ql),r(z,Hl),r(z,tt),r(tt,Ul),r(z,Yl),r(z,it),r(it,Jl),r(z,Xl),d(e,Ti,o),d(e,P,o),r(P,Kr),r(Kr,st),r(st,Ql),r(Kr,eo),r(P,ro),r(P,Or),r(Or,at),r(at,no),r(Or,to),r(P,io),r(P,W),r(W,lt),r(lt,so),r(W,ao),r(W,ot),r(ot,lo),r(W,oo),r(W,dt),r(dt,uo),r(W,ho),d(e,Di,o),d(e,y,o),r(y,co),r(y,ut),r(ut,fo),r(y,go),r(y,ht),r(ht,mo),r(y,po),Si=!0},p:qu,i(e){Si||(b(Me.$$.fragment,e),b(Ae.$$.fragment,e),b(Te.$$.fragment,e),b(Ne.$$.fragment,e),b(Oe.$$.fragment,e),b(Ye.$$.fragment,e),b(Je.$$.fragment,e),b(Xe.$$.fragment,e),b(tr.$$.fragment,e),b(ir.$$.fragment,e),b(sr.$$.fragment,e),b(or.$$.fragment,e),b(ur.$$.fragment,e),b(fr.$$.fragment,e),Si=!0)},o(e){w(Me.$$.fragment,e),w(Ae.$$.fragment,e),w(Te.$$.fragment,e),w(Ne.$$.fragment,e),w(Oe.$$.fragment,e),w(Ye.$$.fragment,e),w(Je.$$.fragment,e),w(Xe.$$.fragment,e),w(tr.$$.fragment,e),w(ir.$$.fragment,e),w(sr.$$.fragment,e),w(or.$$.fragment,e),w(ur.$$.fragment,e),w(fr.$$.fragment,e),Si=!1},d(e){n(G),e&&n(bt),e&&n(L),k(Me),e&&n(wt),k(Ae,e),e&&n(kt),e&&n(vr),e&&n(zt),e&&n(B),k(Te),e&&n(Et),e&&n(br),e&&n(_t),e&&n(R),e&&n(Mt),e&&n(ne),e&&n(At),e&&n(g),e&&n(Tt),e&&n(wr),e&&n(Dt),e&&n(A),e&&n(St),e&&n(kr),e&&n($t),e&&n(F),k(Ne),e&&n(Pt),e&&n(oe),e&&n(Wt),e&&n(de),e&&n(yt),e&&n(T),e&&n(It),e&&n(x),e&&n(Gt),e&&n(ue),e&&n(Lt),e&&n(N),e&&n(Bt),e&&n(V),k(Oe),e&&n(Rt),e&&n(zr),e&&n(Ft),e&&n(Ce),e&&n(xt),e&&n(Er),e&&n(Nt),e&&n(j),e&&n(Vt),k(Ye,e),e&&n(jt),e&&n(_r),e&&n(Zt),e&&n(Mr),e&&n(Kt),e&&n(Ar),e&&n(Ot),e&&n(Z),k(Je),e&&n(Ct),k(Xe,e),e&&n(qt),e&&n(fe),e&&n(Ht),e&&n(K),e&&n(Ut),e&&n(Tr),e&&n(Yt),e&&n(O),e&&n(Jt),e&&n(D),e&&n(Xt),e&&n(ge),e&&n(Qt),e&&n(C),e&&n(ei),e&&n(Dr),e&&n(ri),e&&n(Sr),e&&n(ni),e&&n(q),k(tr),e&&n(ti),e&&n($r),e&&n(ii),k(ir,e),e&&n(si),e&&n(H),k(sr),e&&n(ai),e&&n(Pr),e&&n(li),e&&n(ve),e&&n(oi),e&&n(U),e&&n(di),e&&n(Ir),e&&n(ui),e&&n(S),e&&n(hi),e&&n(Br),e&&n(ci),e&&n(Y),k(or),e&&n(fi),e&&n($),e&&n(gi),e&&n(Rr),e&&n(mi),e&&n(Fr),e&&n(pi),e&&n(xr),e&&n(vi),e&&n(J),k(ur),e&&n(bi),e&&n(Nr),e&&n(wi),e&&n(Vr),e&&n(ki),e&&n(jr),e&&n(zi),e&&n(X),e&&n(Ei),e&&n(Zr),e&&n(_i),e&&n(ze),e&&n(Mi),e&&n(Q),k(fr),e&&n(Ai),e&&n(z),e&&n(Ti),e&&n(P),e&&n(Di),e&&n(y)}}}const Ju={local:"wie-funktionieren-transformermodelle",sections:[{local:"kurz-zur-entwicklungsgeschichte-der-transformermodelle",title:"Kurz zur Entwicklungsgeschichte der Transformer-Modelle"},{local:"transformermodelle-sind-sprachmodelle",title:"Transformer-Modelle sind Sprachmodelle"},{local:"transformermodelle-sind-gro",title:"Transformer-Modelle sind gro\xDF"},{local:"transfer-learning",title:"Transfer Learning"},{local:"grundlegende-architektur",title:"Grundlegende Architektur"},{local:"einfhrung",title:"Einf\xFChrung"},{local:"attentionlayer",title:"Attention-Layer"},{local:"die-ursprngliche-architektur",title:"Die urspr\xFCngliche Architektur"},{local:"architekturen-vs-checkpoints",title:"Architekturen vs. Checkpoints"}],title:"Wie funktionieren Transformer-Modelle?"};function Xu(Mo){return Hu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ih extends Zu{constructor(G){super();Ku(this,G,Xu,Yu,Ou,{})}}export{ih as default,Ju as metadata};
