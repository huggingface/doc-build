import{S as fe,i as ge,s as we,e as t,k as m,w as F,t as l,M as be,c as i,d as s,m as f,a as o,x as V,h as d,b as h,G as a,g as r,y as R,L as ke,q as N,o as H,B as L,v as ve}from"../../chunks/vendor-hf-doc-builder.js";import{I as _e}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as me}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as $e}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function ze(te){let c,q,p,g,E,b,U,M,O,P,k,S,x,J,W,w,Q,y,X,Y,A,v,D,_,T,u,Z,$,ee,se,z,ne,ae,C,j,re,G;return b=new _e({}),k=new $e({props:{chapter:1,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/de/chapter1/section8.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/de/chapter1/section8.ipynb"}]}}),v=new me({props:{code:`from transformers import pipeline

unmasker = pipeline("fill-mask", model="bert-base-uncased")
result = unmasker("This man works as a [MASK].")
print([r["token_str"] for r in result])

result = unmasker("This woman works as a [MASK].")
print([r["token_str"] for r in result])`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

unmasker = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, model=<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
result = unmasker(<span class="hljs-string">&quot;This man works as a [MASK].&quot;</span>)
<span class="hljs-built_in">print</span>([r[<span class="hljs-string">&quot;token_str&quot;</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> result])

result = unmasker(<span class="hljs-string">&quot;This woman works as a [MASK].&quot;</span>)
<span class="hljs-built_in">print</span>([r[<span class="hljs-string">&quot;token_str&quot;</span>] <span class="hljs-keyword">for</span> r <span class="hljs-keyword">in</span> result])`}}),_=new me({props:{code:`['lawyer', 'carpenter', 'doctor', 'waiter', 'mechanic']
['nurse', 'waitress', 'teacher', 'maid', 'prostitute']`,highlighted:`[<span class="hljs-string">&#x27;lawyer&#x27;</span>, <span class="hljs-string">&#x27;carpenter&#x27;</span>, <span class="hljs-string">&#x27;doctor&#x27;</span>, <span class="hljs-string">&#x27;waiter&#x27;</span>, <span class="hljs-string">&#x27;mechanic&#x27;</span>]
[<span class="hljs-string">&#x27;nurse&#x27;</span>, <span class="hljs-string">&#x27;waitress&#x27;</span>, <span class="hljs-string">&#x27;teacher&#x27;</span>, <span class="hljs-string">&#x27;maid&#x27;</span>, <span class="hljs-string">&#x27;prostitute&#x27;</span>]`}}),{c(){c=t("meta"),q=m(),p=t("h1"),g=t("a"),E=t("span"),F(b.$$.fragment),U=m(),M=t("span"),O=l("Bias und Einschr\xE4nkungen"),P=m(),F(k.$$.fragment),S=m(),x=t("p"),J=l("Wenn du vorhast, ein vortrainiertes Modell oder eine feingetunte Modellversion in der Produktion zu verwenden, sei dir bitte dar\xFCber im Klaren, dass diese zwar leistungsstarke Werkzeuge sind, allerdings aber auch ihre Grenzen haben. Die gr\xF6\xDFte Einschr\xE4nkung ergibt sich daraus, dass Forscherinnen und Forscher f\xFCr das auf Basis gro\xDFer Datenmengen durchgef\xFChrte Pretraining oft alle Inhalte, die sie finden k\xF6nnen, zusammensuchen und dabei sowohl all das Gute als auch das Schlechte einbezogen wird, was das Internet zu bieten hat."),W=m(),w=t("p"),Q=l("Greifen wir zur Veranschaulichung noch einmal das Beispiel einer "),y=t("code"),X=l("fill-mask"),Y=l("-Pipeline mit dem BERT-Modell auf:"),A=m(),F(v.$$.fragment),D=m(),F(_.$$.fragment),T=m(),u=t("p"),Z=l("Wenn das Modell aufgefordert wird, das fehlende Wort in diesen beiden S\xE4tzen zu erg\xE4nzen, gibt es lediglich eine geschlechtsneutrale Antwort (Kellnerin/Kellner - waitress/waiter). Bei den anderen handelt es sich um Berufe, die normalerweise mit einem bestimmten Geschlecht assoziiert werden - und ja, \u201Cprostitute\u201D landete unter den Top 5, die das Modell mit \u201Cwoman\u201D und \u201Cwork\u201D assoziiert. Und das, obwohl BERT eines der wenigen Transformer-Modelle ist, das nicht auf Daten aus dem gesamten Internet beruht, sondern auf vermeintlich neutralen Daten (es wurde auf dem "),$=t("a"),ee=l("englischsprachigen Wikipedia-"),se=l(" und dem "),z=t("a"),ne=l("BookCorpus-Datensatz"),ae=l(" trainiert)."),C=m(),j=t("p"),re=l("Wenn du diese Werkzeuge verwendest, musst du daher im Hinterkopf behalten, dass das urspr\xFCngliche Modell, das du verwendest, sehr leicht sexistische, rassistische oder homophobe Inhalte hervorbringen k\xF6nnte. Beim Feintuning des Modells auf deinen Daten werden diese inh\xE4renten Voreingenommenheiten bzw. Vorurteile (engl. Bias) nicht verschwinden."),this.h()},l(e){const n=be('[data-svelte="svelte-1phssyn"]',document.head);c=i(n,"META",{name:!0,content:!0}),n.forEach(s),q=f(e),p=i(e,"H1",{class:!0});var I=o(p);g=i(I,"A",{id:!0,class:!0,href:!0});var ie=o(g);E=i(ie,"SPAN",{});var le=o(E);V(b.$$.fragment,le),le.forEach(s),ie.forEach(s),U=f(I),M=i(I,"SPAN",{});var oe=o(M);O=d(oe,"Bias und Einschr\xE4nkungen"),oe.forEach(s),I.forEach(s),P=f(e),V(k.$$.fragment,e),S=f(e),x=i(e,"P",{});var de=o(x);J=d(de,"Wenn du vorhast, ein vortrainiertes Modell oder eine feingetunte Modellversion in der Produktion zu verwenden, sei dir bitte dar\xFCber im Klaren, dass diese zwar leistungsstarke Werkzeuge sind, allerdings aber auch ihre Grenzen haben. Die gr\xF6\xDFte Einschr\xE4nkung ergibt sich daraus, dass Forscherinnen und Forscher f\xFCr das auf Basis gro\xDFer Datenmengen durchgef\xFChrte Pretraining oft alle Inhalte, die sie finden k\xF6nnen, zusammensuchen und dabei sowohl all das Gute als auch das Schlechte einbezogen wird, was das Internet zu bieten hat."),de.forEach(s),W=f(e),w=i(e,"P",{});var K=o(w);Q=d(K,"Greifen wir zur Veranschaulichung noch einmal das Beispiel einer "),y=i(K,"CODE",{});var ue=o(y);X=d(ue,"fill-mask"),ue.forEach(s),Y=d(K,"-Pipeline mit dem BERT-Modell auf:"),K.forEach(s),A=f(e),V(v.$$.fragment,e),D=f(e),V(_.$$.fragment,e),T=f(e),u=i(e,"P",{});var B=o(u);Z=d(B,"Wenn das Modell aufgefordert wird, das fehlende Wort in diesen beiden S\xE4tzen zu erg\xE4nzen, gibt es lediglich eine geschlechtsneutrale Antwort (Kellnerin/Kellner - waitress/waiter). Bei den anderen handelt es sich um Berufe, die normalerweise mit einem bestimmten Geschlecht assoziiert werden - und ja, \u201Cprostitute\u201D landete unter den Top 5, die das Modell mit \u201Cwoman\u201D und \u201Cwork\u201D assoziiert. Und das, obwohl BERT eines der wenigen Transformer-Modelle ist, das nicht auf Daten aus dem gesamten Internet beruht, sondern auf vermeintlich neutralen Daten (es wurde auf dem "),$=i(B,"A",{href:!0,rel:!0});var he=o($);ee=d(he,"englischsprachigen Wikipedia-"),he.forEach(s),se=d(B," und dem "),z=i(B,"A",{href:!0,rel:!0});var ce=o(z);ne=d(ce,"BookCorpus-Datensatz"),ce.forEach(s),ae=d(B," trainiert)."),B.forEach(s),C=f(e),j=i(e,"P",{});var pe=o(j);re=d(pe,"Wenn du diese Werkzeuge verwendest, musst du daher im Hinterkopf behalten, dass das urspr\xFCngliche Modell, das du verwendest, sehr leicht sexistische, rassistische oder homophobe Inhalte hervorbringen k\xF6nnte. Beim Feintuning des Modells auf deinen Daten werden diese inh\xE4renten Voreingenommenheiten bzw. Vorurteile (engl. Bias) nicht verschwinden."),pe.forEach(s),this.h()},h(){h(c,"name","hf:doc:metadata"),h(c,"content",JSON.stringify(xe)),h(g,"id","bias-und-einschrnkungen"),h(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(g,"href","#bias-und-einschrnkungen"),h(p,"class","relative group"),h($,"href","https://huggingface.co/datasets/wikipedia"),h($,"rel","nofollow"),h(z,"href","https://huggingface.co/datasets/bookcorpus"),h(z,"rel","nofollow")},m(e,n){a(document.head,c),r(e,q,n),r(e,p,n),a(p,g),a(g,E),R(b,E,null),a(p,U),a(p,M),a(M,O),r(e,P,n),R(k,e,n),r(e,S,n),r(e,x,n),a(x,J),r(e,W,n),r(e,w,n),a(w,Q),a(w,y),a(y,X),a(w,Y),r(e,A,n),R(v,e,n),r(e,D,n),R(_,e,n),r(e,T,n),r(e,u,n),a(u,Z),a(u,$),a($,ee),a(u,se),a(u,z),a(z,ne),a(u,ae),r(e,C,n),r(e,j,n),a(j,re),G=!0},p:ke,i(e){G||(N(b.$$.fragment,e),N(k.$$.fragment,e),N(v.$$.fragment,e),N(_.$$.fragment,e),G=!0)},o(e){H(b.$$.fragment,e),H(k.$$.fragment,e),H(v.$$.fragment,e),H(_.$$.fragment,e),G=!1},d(e){s(c),e&&s(q),e&&s(p),L(b),e&&s(P),L(k,e),e&&s(S),e&&s(x),e&&s(W),e&&s(w),e&&s(A),L(v,e),e&&s(D),L(_,e),e&&s(T),e&&s(u),e&&s(C),e&&s(j)}}}const xe={local:"bias-und-einschrnkungen",title:"Bias und Einschr\xE4nkungen"};function je(te){return ve(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Pe extends fe{constructor(c){super();ge(this,c,je,ze,we,{})}}export{Pe as default,xe as metadata};
