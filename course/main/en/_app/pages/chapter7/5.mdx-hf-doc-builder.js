import{S as mg,i as hg,s as cg,e as r,t as a,k as h,w as v,c as o,a as l,h as n,d as t,m as c,x as y,b as f,g as i,G as s,y as k,q as g,o as _,B as j,Y as ag,M as ug,Z as ng,N as Ml,p as Ao,v as dg,n as Oo}from"../../chunks/vendor-hf-doc-builder.js";import{T as ln}from"../../chunks/Tip-hf-doc-builder.js";import{Y as fu}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Bs}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as z}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as lg}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{F as fg}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function gg(K){let u,$;return u=new lg({props:{chapter:7,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_tf.ipynb"}]}}),{c(){v(u.$$.fragment)},l(d){y(u.$$.fragment,d)},m(d,E){k(u,d,E),$=!0},i(d){$||(g(u.$$.fragment,d),$=!0)},o(d){_(u.$$.fragment,d),$=!1},d(d){j(u,d)}}}function _g(K){let u,$;return u=new lg({props:{chapter:7,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter7/section5_pt.ipynb"}]}}),{c(){v(u.$$.fragment)},l(d){y(u.$$.fragment,d)},m(d,E){k(u,d,E),$=!0},i(d){$||(g(u.$$.fragment,d),$=!0)},o(d){_(u.$$.fragment,d),$=!1},d(d){j(u,d)}}}function wg(K){let u,$,d,E,C,x,A,P,T,S,I;return{c(){u=r("p"),$=a("\u270F\uFE0F "),d=r("strong"),E=a("Try it out!"),C=a(" Change the random seed in the "),x=r("code"),A=a("Dataset.shuffle()"),P=a(" command to explore other reviews in the corpus. If you\u2019re a Spanish speaker, take a look at some of the reviews in "),T=r("code"),S=a("spanish_dataset"),I=a(" to see if the titles also seem like reasonable summaries.")},l(D){u=o(D,"P",{});var O=l(u);$=n(O,"\u270F\uFE0F "),d=o(O,"STRONG",{});var F=l(d);E=n(F,"Try it out!"),F.forEach(t),C=n(O," Change the random seed in the "),x=o(O,"CODE",{});var V=l(x);A=n(V,"Dataset.shuffle()"),V.forEach(t),P=n(O," command to explore other reviews in the corpus. If you\u2019re a Spanish speaker, take a look at some of the reviews in "),T=o(O,"CODE",{});var J=l(T);S=n(J,"spanish_dataset"),J.forEach(t),I=n(O," to see if the titles also seem like reasonable summaries."),O.forEach(t)},m(D,O){i(D,u,O),s(u,$),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P),s(u,T),s(T,S),s(u,I)},d(D){D&&t(u)}}}function bg(K){let u,$,d,E,C,x,A,P;return{c(){u=r("p"),$=a("\u270F\uFE0F "),d=r("strong"),E=a("Try it out!"),C=a(" Once you\u2019ve worked through this section, see how well mT5 compares to mBART by fine-tuning the latter with the same techniques. For bonus points, you can also try fine-tuning T5 on just the English reviews. Since T5 has a special prefix prompt, you\u2019ll need to prepend "),x=r("code"),A=a("summarize:"),P=a(" to the input examples in the preprocessing steps below.")},l(T){u=o(T,"P",{});var S=l(u);$=n(S,"\u270F\uFE0F "),d=o(S,"STRONG",{});var I=l(d);E=n(I,"Try it out!"),I.forEach(t),C=n(S," Once you\u2019ve worked through this section, see how well mT5 compares to mBART by fine-tuning the latter with the same techniques. For bonus points, you can also try fine-tuning T5 on just the English reviews. Since T5 has a special prefix prompt, you\u2019ll need to prepend "),x=o(S,"CODE",{});var D=l(x);A=n(D,"summarize:"),D.forEach(t),P=n(S," to the input examples in the preprocessing steps below."),S.forEach(t)},m(T,S){i(T,u,S),s(u,$),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P)},d(T){T&&t(u)}}}function vg(K){let u,$;return{c(){u=r("p"),$=a("\u{1F4A1} In the early stages of your NLP projects, a good practice is to train a class of \u201Csmall\u201D models on a small sample of data. This allows you to debug and iterate faster toward an end-to-end workflow. Once you are confident in the results, you can always scale up the model by simply changing the model checkpoint!")},l(d){u=o(d,"P",{});var E=l(u);$=n(E,"\u{1F4A1} In the early stages of your NLP projects, a good practice is to train a class of \u201Csmall\u201D models on a small sample of data. This allows you to debug and iterate faster toward an end-to-end workflow. Once you are confident in the results, you can always scale up the model by simply changing the model checkpoint!"),E.forEach(t)},m(d,E){i(d,u,E),s(u,$)},d(d){d&&t(u)}}}function yg(K){let u,$,d,E,C,x,A,P,T,S,I;return{c(){u=r("p"),$=a("\u{1F4A1} You may have noticed that we used "),d=r("code"),E=a("batched=True"),C=a(" in our "),x=r("code"),A=a("Dataset.map()"),P=a(" function above. This encodes the examples in batches of 1,000 (the default) and allows you to make use of the multithreading capabilities of the fast tokenizers in \u{1F917} Transformers. Where possible, try using "),T=r("code"),S=a("batched=True"),I=a(" to get the most out of your preprocessing!")},l(D){u=o(D,"P",{});var O=l(u);$=n(O,"\u{1F4A1} You may have noticed that we used "),d=o(O,"CODE",{});var F=l(d);E=n(F,"batched=True"),F.forEach(t),C=n(O," in our "),x=o(O,"CODE",{});var V=l(x);A=n(V,"Dataset.map()"),V.forEach(t),P=n(O," function above. This encodes the examples in batches of 1,000 (the default) and allows you to make use of the multithreading capabilities of the fast tokenizers in \u{1F917} Transformers. Where possible, try using "),T=o(O,"CODE",{});var J=l(T);S=n(J,"batched=True"),J.forEach(t),I=n(O," to get the most out of your preprocessing!"),O.forEach(t)},m(D,O){i(D,u,O),s(u,$),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P),s(u,T),s(T,S),s(u,I)},d(D){D&&t(u)}}}function kg(K){let u,$,d,E,C,x,A,P;return{c(){u=r("p"),$=a("\u{1F64B} Don\u2019t worry if this is the first time you\u2019ve heard of precision and recall \u2014 we\u2019ll go through some explicit examples together to make it all clear. These metrics are usually encountered in classification tasks, so if you want to understand how precision and recall are defined in that context, we recommend checking out the "),d=r("code"),E=a("scikit-learn"),C=h(),x=r("a"),A=a("guides"),P=a("."),this.h()},l(T){u=o(T,"P",{});var S=l(u);$=n(S,"\u{1F64B} Don\u2019t worry if this is the first time you\u2019ve heard of precision and recall \u2014 we\u2019ll go through some explicit examples together to make it all clear. These metrics are usually encountered in classification tasks, so if you want to understand how precision and recall are defined in that context, we recommend checking out the "),d=o(S,"CODE",{});var I=l(d);E=n(I,"scikit-learn"),I.forEach(t),C=c(S),x=o(S,"A",{href:!0,rel:!0});var D=l(x);A=n(D,"guides"),D.forEach(t),P=n(S,"."),S.forEach(t),this.h()},h(){f(x,"href","https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html"),f(x,"rel","nofollow")},m(T,S){i(T,u,S),s(u,$),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P)},d(T){T&&t(u)}}}function jg(K){let u,$,d,E,C,x,A,P;return{c(){u=r("p"),$=a("\u270F\uFE0F "),d=r("strong"),E=a("Try it out!"),C=a(" Create your own example of a generated and reference summary and see if the resulting ROUGE scores agree with a manual calculation based on the formulas for precision and recall. For bonus points, split the text into bigrams and compare the precision and recall for the "),x=r("code"),A=a("rouge2"),P=a(" metric.")},l(T){u=o(T,"P",{});var S=l(u);$=n(S,"\u270F\uFE0F "),d=o(S,"STRONG",{});var I=l(d);E=n(I,"Try it out!"),I.forEach(t),C=n(S," Create your own example of a generated and reference summary and see if the resulting ROUGE scores agree with a manual calculation based on the formulas for precision and recall. For bonus points, split the text into bigrams and compare the precision and recall for the "),x=o(S,"CODE",{});var D=l(x);A=n(D,"rouge2"),D.forEach(t),P=n(S," metric."),S.forEach(t)},m(T,S){i(T,u,S),s(u,$),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P)},d(T){T&&t(u)}}}function $g(K){let u,$,d,E,C,x,A,P,T,S,I,D,O,F,V,J,Z,X,Q;return E=new Bs({}),X=new z({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`}}),{c(){u=r("h2"),$=r("a"),d=r("span"),v(E.$$.fragment),C=h(),x=r("span"),A=a("Fine-tuning mT5 with Keras"),P=h(),T=r("p"),S=a("Fine-tuning a model for summarization is very similar to the other tasks we\u2019ve covered in this chapter. The first thing we need to do is load the pretrained model from the "),I=r("code"),D=a("mt5-small"),O=a(" checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the "),F=r("code"),V=a("TFAutoModelForSeq2SeqLM"),J=a(" class, which will automatically download and cache the weights:"),Z=h(),v(X.$$.fragment),this.h()},l(L){u=o(L,"H2",{class:!0});var M=l(u);$=o(M,"A",{id:!0,class:!0,href:!0});var se=l($);d=o(se,"SPAN",{});var R=l(d);y(E.$$.fragment,R),R.forEach(t),se.forEach(t),C=c(M),x=o(M,"SPAN",{});var Y=l(x);A=n(Y,"Fine-tuning mT5 with Keras"),Y.forEach(t),M.forEach(t),P=c(L),T=o(L,"P",{});var ae=l(T);S=n(ae,"Fine-tuning a model for summarization is very similar to the other tasks we\u2019ve covered in this chapter. The first thing we need to do is load the pretrained model from the "),I=o(ae,"CODE",{});var W=l(I);D=n(W,"mt5-small"),W.forEach(t),O=n(ae," checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the "),F=o(ae,"CODE",{});var ie=l(F);V=n(ie,"TFAutoModelForSeq2SeqLM"),ie.forEach(t),J=n(ae," class, which will automatically download and cache the weights:"),ae.forEach(t),Z=c(L),y(X.$$.fragment,L),this.h()},h(){f($,"id","finetuning-mt5-with-keras"),f($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($,"href","#finetuning-mt5-with-keras"),f(u,"class","relative group")},m(L,M){i(L,u,M),s(u,$),s($,d),k(E,d,null),s(u,C),s(u,x),s(x,A),i(L,P,M),i(L,T,M),s(T,S),s(T,I),s(I,D),s(T,O),s(T,F),s(F,V),s(T,J),i(L,Z,M),k(X,L,M),Q=!0},i(L){Q||(g(E.$$.fragment,L),g(X.$$.fragment,L),Q=!0)},o(L){_(E.$$.fragment,L),_(X.$$.fragment,L),Q=!1},d(L){L&&t(u),j(E),L&&t(P),L&&t(T),L&&t(Z),j(X,L)}}}function xg(K){let u,$,d,E,C,x,A,P,T,S,I,D,O,F,V,J,Z,X,Q,L,M,se;return E=new Bs({}),M=new z({props:{code:`from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)`}}),{c(){u=r("h2"),$=r("a"),d=r("span"),v(E.$$.fragment),C=h(),x=r("span"),A=a("Fine-tuning mT5 with the "),P=r("code"),T=a("Trainer"),S=a(" API"),I=h(),D=r("p"),O=a("Fine-tuning a model for summarization is very similar to the other tasks we\u2019ve covered in this chapter. The first thing we need to do is load the pretrained model from the "),F=r("code"),V=a("mt5-small"),J=a(" checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the "),Z=r("code"),X=a("AutoModelForSeq2SeqLM"),Q=a(" class, which will automatically download and cache the weights:"),L=h(),v(M.$$.fragment),this.h()},l(R){u=o(R,"H2",{class:!0});var Y=l(u);$=o(Y,"A",{id:!0,class:!0,href:!0});var ae=l($);d=o(ae,"SPAN",{});var W=l(d);y(E.$$.fragment,W),W.forEach(t),ae.forEach(t),C=c(Y),x=o(Y,"SPAN",{});var ie=l(x);A=n(ie,"Fine-tuning mT5 with the "),P=o(ie,"CODE",{});var B=l(P);T=n(B,"Trainer"),B.forEach(t),S=n(ie," API"),ie.forEach(t),Y.forEach(t),I=c(R),D=o(R,"P",{});var ne=l(D);O=n(ne,"Fine-tuning a model for summarization is very similar to the other tasks we\u2019ve covered in this chapter. The first thing we need to do is load the pretrained model from the "),F=o(ne,"CODE",{});var fe=l(F);V=n(fe,"mt5-small"),fe.forEach(t),J=n(ne," checkpoint. Since summarization is a sequence-to-sequence task, we can load the model with the "),Z=o(ne,"CODE",{});var be=l(Z);X=n(be,"AutoModelForSeq2SeqLM"),be.forEach(t),Q=n(ne," class, which will automatically download and cache the weights:"),ne.forEach(t),L=c(R),y(M.$$.fragment,R),this.h()},h(){f($,"id","finetuning-mt5-with-the-trainer-api"),f($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($,"href","#finetuning-mt5-with-the-trainer-api"),f(u,"class","relative group")},m(R,Y){i(R,u,Y),s(u,$),s($,d),k(E,d,null),s(u,C),s(u,x),s(x,A),s(x,P),s(P,T),s(x,S),i(R,I,Y),i(R,D,Y),s(D,O),s(D,F),s(F,V),s(D,J),s(D,Z),s(Z,X),s(D,Q),i(R,L,Y),k(M,R,Y),se=!0},i(R){se||(g(E.$$.fragment,R),g(M.$$.fragment,R),se=!0)},o(R){_(E.$$.fragment,R),_(M.$$.fragment,R),se=!1},d(R){R&&t(u),j(E),R&&t(I),R&&t(D),R&&t(L),j(M,R)}}}function Eg(K){let u,$,d,E,C;return{c(){u=r("p"),$=a("\u{1F4A1} If you\u2019re wondering why you don\u2019t see any warnings about fine-tuning the model on a downstream task, that\u2019s because for sequence-to-sequence tasks we keep all the weights of the network. Compare this to our text classification model in "),d=r("a"),E=a("Chapter 3"),C=a(", where the head of the pretrained model was replaced with a randomly initialized network."),this.h()},l(x){u=o(x,"P",{});var A=l(u);$=n(A,"\u{1F4A1} If you\u2019re wondering why you don\u2019t see any warnings about fine-tuning the model on a downstream task, that\u2019s because for sequence-to-sequence tasks we keep all the weights of the network. Compare this to our text classification model in "),d=o(A,"A",{href:!0});var P=l(d);E=n(P,"Chapter 3"),P.forEach(t),C=n(A,", where the head of the pretrained model was replaced with a randomly initialized network."),A.forEach(t),this.h()},h(){f(d,"href","/course/chapter3")},m(x,A){i(x,u,A),s(u,$),s(u,d),s(d,E),s(u,C)},d(x){x&&t(u)}}}function rg(K){let u,$,d,E,C,x,A,P,T,S,I,D,O,F,V,J,Z,X,Q,L,M,se,R,Y,ae,W,ie,B,ne,fe,be,Se,G,Le,ge,re,ve,Te,oe,ye,$e,_e,le,pe,me,b,N,we,he,ce,Ie,w,H,Ge,xe,ee,te,Ee,Be,ke,Xe,Ye,rs,Ce,We,os,cs,Ke,Xs,us,Pe,ks,zs,Ne,ds;return S=new z({props:{code:`from transformers import Seq2SeqTrainingArguments

batch_size = 8
num_train_epochs = 8
# Show the training loss with every epoch
logging_steps = len(tokenized_datasets["train"]) // batch_size
model_name = model_checkpoint.split("/")[-1]

args = Seq2SeqTrainingArguments(
    output_dir=f"{model_name}-finetuned-amazon-en-es",
    evaluation_strategy="epoch",
    learning_rate=5.6e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=num_train_epochs,
    predict_with_generate=True,
    logging_steps=logging_steps,
    push_to_hub=True,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainingArguments

batch_size = <span class="hljs-number">8</span>
num_train_epochs = <span class="hljs-number">8</span>
<span class="hljs-comment"># Show the training loss with every epoch</span>
logging_steps = <span class="hljs-built_in">len</span>(tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

args = Seq2SeqTrainingArguments(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-amazon-en-es&quot;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">5.6e-5</span>,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    weight_decay=<span class="hljs-number">0.01</span>,
    save_total_limit=<span class="hljs-number">3</span>,
    num_train_epochs=num_train_epochs,
    predict_with_generate=<span class="hljs-literal">True</span>,
    logging_steps=logging_steps,
    push_to_hub=<span class="hljs-literal">True</span>,
)`}}),Ne=new z({props:{code:`import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    # Decode generated summaries into text
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in the labels as we can't decode them
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    # Decode reference summaries into text
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # ROUGE expects a newline after each sentence
    decoded_preds = ["\\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    # Compute ROUGE scores
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=True
    )
    # Extract the median scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    return {k: round(v, 4) for k, v in result.items()}`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions, labels = eval_pred
    <span class="hljs-comment"># Decode generated summaries into text</span>
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># Replace -100 in the labels as we can&#x27;t decode them</span>
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    <span class="hljs-comment"># Decode reference summaries into text</span>
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    <span class="hljs-comment"># ROUGE expects a newline after each sentence</span>
    decoded_preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(pred.strip())) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(label.strip())) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    <span class="hljs-comment"># Compute ROUGE scores</span>
    result = rouge_score.compute(
        predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="hljs-literal">True</span>
    )
    <span class="hljs-comment"># Extract the median scores</span>
    result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
    <span class="hljs-keyword">return</span> {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}`}}),{c(){u=r("p"),$=a("We\u2019ll need to generate summaries in order to compute ROUGE scores during training. Fortunately, \u{1F917} Transformers provides dedicated "),d=r("code"),E=a("Seq2SeqTrainingArguments"),C=a(" and "),x=r("code"),A=a("Seq2SeqTrainer"),P=a(" classes that can do this for us automatically! To see how this works, let\u2019s first define the hyperparameters and other arguments for our experiments:"),T=h(),v(S.$$.fragment),I=h(),D=r("p"),O=a("Here, the "),F=r("code"),V=a("predict_with_generate"),J=a(" argument has been set to indicate that we should generate summaries during evaluation so that we can compute ROUGE scores for each epoch. As discussed in "),Z=r("a"),X=a("Chapter 1"),Q=a(", the decoder performs inference by predicting tokens one by one, and this is implemented by the model\u2019s "),L=r("code"),M=a("generate()"),se=a(" method. Setting "),R=r("code"),Y=a("predict_with_generate=True"),ae=a(" tells the "),W=r("code"),ie=a("Seq2SeqTrainer"),B=a(" to use that method for evaluation. We\u2019ve also adjusted some of the default hyperparameters, like the learning rate, number of epochs, and weight decay, and we\u2019ve set the "),ne=r("code"),fe=a("save_total_limit"),be=a(" option to only save up to 3 checkpoints during training \u2014 this is because even the \u201Csmall\u201D version of mT5 uses around a GB of hard drive space, and we can save a bit of room by limiting the number of copies we save."),Se=h(),G=r("p"),Le=a("The "),ge=r("code"),re=a("push_to_hub=True"),ve=a(" argument will allow us to push the model to the Hub after training; you\u2019ll find the repository under your user profile in the location defined by "),Te=r("code"),oe=a("output_dir"),ye=a(". Note that you can specify the name of the repository you want to push to with the "),$e=r("code"),_e=a("hub_model_id"),le=a(" argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),pe=r("a"),me=r("code"),b=a("huggingface-course"),N=a(" organization"),we=a(", we added "),he=r("code"),ce=a('hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"'),Ie=a(" to "),w=r("code"),H=a("Seq2SeqTrainingArguments"),Ge=a("."),xe=h(),ee=r("p"),te=a("The next thing we need to do is provide the trainer with a "),Ee=r("code"),Be=a("compute_metrics()"),ke=a(" function so that we can evaluate our model during training. For summarization this is a bit more involved than simply calling "),Xe=r("code"),Ye=a("rouge_score.compute()"),rs=a(" on the model\u2019s predictions, since we need to "),Ce=r("em"),We=a("decode"),os=a(" the outputs and labels into text before we can compute the ROUGE scores. The following function does exactly that, and also makes use of the "),cs=r("code"),Ke=a("sent_tokenize()"),Xs=a(" function from "),us=r("code"),Pe=a("nltk"),ks=a(" to separate the summary sentences with newlines:"),zs=h(),v(Ne.$$.fragment),this.h()},l(U){u=o(U,"P",{});var ue=l(u);$=n(ue,"We\u2019ll need to generate summaries in order to compute ROUGE scores during training. Fortunately, \u{1F917} Transformers provides dedicated "),d=o(ue,"CODE",{});var Ve=l(d);E=n(Ve,"Seq2SeqTrainingArguments"),Ve.forEach(t),C=n(ue," and "),x=o(ue,"CODE",{});var Ys=l(x);A=n(Ys,"Seq2SeqTrainer"),Ys.forEach(t),P=n(ue," classes that can do this for us automatically! To see how this works, let\u2019s first define the hyperparameters and other arguments for our experiments:"),ue.forEach(t),T=c(U),y(S.$$.fragment,U),I=c(U),D=o(U,"P",{});var de=l(D);O=n(de,"Here, the "),F=o(de,"CODE",{});var fs=l(F);V=n(fs,"predict_with_generate"),fs.forEach(t),J=n(de," argument has been set to indicate that we should generate summaries during evaluation so that we can compute ROUGE scores for each epoch. As discussed in "),Z=o(de,"A",{href:!0});var ls=l(Z);X=n(ls,"Chapter 1"),ls.forEach(t),Q=n(de,", the decoder performs inference by predicting tokens one by one, and this is implemented by the model\u2019s "),L=o(de,"CODE",{});var Ds=l(L);M=n(Ds,"generate()"),Ds.forEach(t),se=n(de," method. Setting "),R=o(de,"CODE",{});var Ae=l(R);Y=n(Ae,"predict_with_generate=True"),Ae.forEach(t),ae=n(de," tells the "),W=o(de,"CODE",{});var Ks=l(W);ie=n(Ks,"Seq2SeqTrainer"),Ks.forEach(t),B=n(de," to use that method for evaluation. We\u2019ve also adjusted some of the default hyperparameters, like the learning rate, number of epochs, and weight decay, and we\u2019ve set the "),ne=o(de,"CODE",{});var is=l(ne);fe=n(is,"save_total_limit"),is.forEach(t),be=n(de," option to only save up to 3 checkpoints during training \u2014 this is because even the \u201Csmall\u201D version of mT5 uses around a GB of hard drive space, and we can save a bit of room by limiting the number of copies we save."),de.forEach(t),Se=c(U),G=o(U,"P",{});var qe=l(G);Le=n(qe,"The "),ge=o(qe,"CODE",{});var Vs=l(ge);re=n(Vs,"push_to_hub=True"),Vs.forEach(t),ve=n(qe," argument will allow us to push the model to the Hub after training; you\u2019ll find the repository under your user profile in the location defined by "),Te=o(qe,"CODE",{});var Je=l(Te);oe=n(Je,"output_dir"),Je.forEach(t),ye=n(qe,". Note that you can specify the name of the repository you want to push to with the "),$e=o(qe,"CODE",{});var Js=l($e);_e=n(Js,"hub_model_id"),Js.forEach(t),le=n(qe," argument (in particular, you will have to use this argument to push to an organization). For instance, when we pushed the model to the "),pe=o(qe,"A",{href:!0,rel:!0});var He=l(pe);me=o(He,"CODE",{});var Zs=l(me);b=n(Zs,"huggingface-course"),Zs.forEach(t),N=n(He," organization"),He.forEach(t),we=n(qe,", we added "),he=o(qe,"CODE",{});var Ze=l(he);ce=n(Ze,'hub_model_id="huggingface-course/mt5-finetuned-amazon-en-es"'),Ze.forEach(t),Ie=n(qe," to "),w=o(qe,"CODE",{});var Qs=l(w);H=n(Qs,"Seq2SeqTrainingArguments"),Qs.forEach(t),Ge=n(qe,"."),qe.forEach(t),xe=c(U),ee=o(U,"P",{});var je=l(ee);te=n(je,"The next thing we need to do is provide the trainer with a "),Ee=o(je,"CODE",{});var js=l(Ee);Be=n(js,"compute_metrics()"),js.forEach(t),ke=n(je," function so that we can evaluate our model during training. For summarization this is a bit more involved than simply calling "),Xe=o(je,"CODE",{});var Oe=l(Xe);Ye=n(Oe,"rouge_score.compute()"),Oe.forEach(t),rs=n(je," on the model\u2019s predictions, since we need to "),Ce=o(je,"EM",{});var bt=l(Ce);We=n(bt,"decode"),bt.forEach(t),os=n(je," the outputs and labels into text before we can compute the ROUGE scores. The following function does exactly that, and also makes use of the "),cs=o(je,"CODE",{});var $s=l(cs);Ke=n($s,"sent_tokenize()"),$s.forEach(t),Xs=n(je," function from "),us=o(je,"CODE",{});var xs=l(us);Pe=n(xs,"nltk"),xs.forEach(t),ks=n(je," to separate the summary sentences with newlines:"),je.forEach(t),zs=c(U),y(Ne.$$.fragment,U),this.h()},h(){f(Z,"href","/course/chapter1"),f(pe,"href","https://huggingface.co/huggingface-course"),f(pe,"rel","nofollow")},m(U,ue){i(U,u,ue),s(u,$),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P),i(U,T,ue),k(S,U,ue),i(U,I,ue),i(U,D,ue),s(D,O),s(D,F),s(F,V),s(D,J),s(D,Z),s(Z,X),s(D,Q),s(D,L),s(L,M),s(D,se),s(D,R),s(R,Y),s(D,ae),s(D,W),s(W,ie),s(D,B),s(D,ne),s(ne,fe),s(D,be),i(U,Se,ue),i(U,G,ue),s(G,Le),s(G,ge),s(ge,re),s(G,ve),s(G,Te),s(Te,oe),s(G,ye),s(G,$e),s($e,_e),s(G,le),s(G,pe),s(pe,me),s(me,b),s(pe,N),s(G,we),s(G,he),s(he,ce),s(G,Ie),s(G,w),s(w,H),s(G,Ge),i(U,xe,ue),i(U,ee,ue),s(ee,te),s(ee,Ee),s(Ee,Be),s(ee,ke),s(ee,Xe),s(Xe,Ye),s(ee,rs),s(ee,Ce),s(Ce,We),s(ee,os),s(ee,cs),s(cs,Ke),s(ee,Xs),s(ee,us),s(us,Pe),s(ee,ks),i(U,zs,ue),k(Ne,U,ue),ds=!0},i(U){ds||(g(S.$$.fragment,U),g(Ne.$$.fragment,U),ds=!0)},o(U){_(S.$$.fragment,U),_(Ne.$$.fragment,U),ds=!1},d(U){U&&t(u),U&&t(T),j(S,U),U&&t(I),U&&t(D),U&&t(Se),U&&t(G),U&&t(xe),U&&t(ee),U&&t(zs),j(Ne,U)}}}function Tg(K){let u,$;return u=new z({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){v(u.$$.fragment)},l(d){y(u.$$.fragment,d)},m(d,E){k(u,d,E),$=!0},i(d){$||(g(u.$$.fragment,d),$=!0)},o(d){_(u.$$.fragment,d),$=!1},d(d){j(u,d)}}}function qg(K){let u,$;return u=new z({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)`}}),{c(){v(u.$$.fragment)},l(d){y(u.$$.fragment,d)},m(d,E){k(u,d,E),$=!0},i(d){$||(g(u.$$.fragment,d),$=!0)},o(d){_(u.$$.fragment,d),$=!1},d(d){j(u,d)}}}function zg(K){let u,$,d,E,C,x,A,P,T,S,I,D,O,F,V,J,Z,X,Q,L,M,se,R,Y,ae,W,ie,B,ne,fe,be,Se,G,Le,ge,re,ve,Te,oe,ye,$e,_e,le,pe,me,b,N,we,he,ce,Ie;return O=new z({props:{code:`tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["train"],
    collate_fn=data_collator,
    shuffle=True,
    batch_size=8,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=data_collator,
    shuffle=False,
    batch_size=8,
)`,highlighted:`tf_train_dataset = model.prepare_tf_dataset(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">8</span>,
)
tf_eval_dataset = model.prepare_tf_dataset(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    collate_fn=data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">8</span>,
)`}}),X=new z({props:{code:`from transformers import create_optimizer
import tensorflow as tf

# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied
# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,
# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.
num_train_epochs = 8
num_train_steps = len(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split("/")[-1]

optimizer, schedule = create_optimizer(
    init_lr=5.6e-5,
    num_warmup_steps=0,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
)

model.compile(optimizer=optimizer)

# Train in mixed-precision float16
tf.keras.mixed_precision.set_global_policy("mixed_float16")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-comment"># The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied</span>
<span class="hljs-comment"># by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,</span>
<span class="hljs-comment"># not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.</span>
num_train_epochs = <span class="hljs-number">8</span>
num_train_steps = <span class="hljs-built_in">len</span>(tf_train_dataset) * num_train_epochs
model_name = model_checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]

optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">5.6e-5</span>,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
)

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)

<span class="hljs-comment"># Train in mixed-precision float16</span>
tf.keras.mixed_precision.set_global_policy(<span class="hljs-string">&quot;mixed_float16&quot;</span>)`}}),W=new z({props:{code:`from transformers.keras_callbacks import PushToHubCallback

callback = PushToHubCallback(
    output_dir=f"{model_name}-finetuned-amazon-en-es", tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=8
)`,highlighted:`<span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

callback = PushToHubCallback(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-amazon-en-es&quot;</span>, tokenizer=tokenizer
)

model.fit(
    tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback], epochs=<span class="hljs-number">8</span>
)`}}),le=new z({props:{code:`from tqdm import tqdm
import numpy as np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=320
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    collate_fn=generation_data_collator,
    shuffle=False,
    batch_size=8,
    drop_remainder=True,
)


@tf.function(jit_compile=True)
def generate_with_xla(batch):
    return model.generate(
        input_ids=batch["input_ids"],
        attention_mask=batch["attention_mask"],
        max_new_tokens=32,
    )


all_preds = []
all_labels = []
for batch, labels in tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    labels = labels.numpy()
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    decoded_preds = ["\\n".join(sent_tokenize(pred.strip())) for pred in decoded_preds]
    decoded_labels = ["\\n".join(sent_tokenize(label.strip())) for label in decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)`,highlighted:`<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

generation_data_collator = DataCollatorForSeq2Seq(
    tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, pad_to_multiple_of=<span class="hljs-number">320</span>
)

tf_generate_dataset = model.prepare_tf_dataset(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    collate_fn=generation_data_collator,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">8</span>,
    drop_remainder=<span class="hljs-literal">True</span>,
)


<span class="hljs-meta">@tf.function(<span class="hljs-params">jit_compile=<span class="hljs-literal">True</span></span>)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_with_xla</span>(<span class="hljs-params">batch</span>):
    <span class="hljs-keyword">return</span> model.generate(
        input_ids=batch[<span class="hljs-string">&quot;input_ids&quot;</span>],
        attention_mask=batch[<span class="hljs-string">&quot;attention_mask&quot;</span>],
        max_new_tokens=<span class="hljs-number">32</span>,
    )


all_preds = []
all_labels = []
<span class="hljs-keyword">for</span> batch, labels <span class="hljs-keyword">in</span> tqdm(tf_generate_dataset):
    predictions = generate_with_xla(batch)
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=<span class="hljs-literal">True</span>)
    labels = labels.numpy()
    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    decoded_preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(pred.strip())) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> decoded_preds]
    decoded_labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(label.strip())) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> decoded_labels]
    all_preds.extend(decoded_preds)
    all_labels.extend(decoded_labels)`}}),we=new z({props:{code:`result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=True
)
result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
{k: round(v, 4) for k, v in result.items()}`,highlighted:`result = rouge_score.compute(
    predictions=decoded_preds, references=decoded_labels, use_stemmer=<span class="hljs-literal">True</span>
)
result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
{k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}`}}),ce=new z({props:{code:"{'rouge1': 31.4815, 'rouge2': 25.4386, 'rougeL': 31.4815, 'rougeLsum': 31.4815}",highlighted:'{&#x27;rouge1&#x27;: <span class="hljs-number">31.4815</span>, &#x27;rouge2&#x27;: <span class="hljs-number">25.4386</span>, &#x27;rougeL&#x27;: <span class="hljs-number">31.4815</span>, &#x27;rougeLsum&#x27;: <span class="hljs-number">31.4815</span>}'}}),{c(){u=r("p"),$=a("We\u2019re almost ready to train! We just need to convert our datasets to "),d=r("code"),E=a("tf.data.Dataset"),C=a("s using the data collator we defined above, and then "),x=r("code"),A=a("compile()"),P=a(" and "),T=r("code"),S=a("fit()"),I=a(" the model. First, the datasets:"),D=h(),v(O.$$.fragment),F=h(),V=r("p"),J=a("Now, we define our training hyperparameters and compile:"),Z=h(),v(X.$$.fragment),Q=h(),L=r("p"),M=a("And finally, we fit the model. We use a "),se=r("code"),R=a("PushToHubCallback"),Y=a(" to save the model to the Hub after each epoch, which will allow us to use it for inference later:"),ae=h(),v(W.$$.fragment),ie=h(),B=r("p"),ne=a("We got some loss values during training, but really we\u2019d like to see the ROUGE metrics we computed earlier. To get those metrics, we\u2019ll need to generate outputs from the model and convert them to strings. Let\u2019s build some lists of labels and predictions for the ROUGE metric to compare (note that if you get import errors for this section, you may need to"),fe=r("code"),be=a("!pip install tqdm"),Se=a("). We\u2019re also going to use a trick that dramatically increases performance - compiling our generation code with "),G=r("a"),Le=a("XLA"),ge=a(", TensorFlow\u2019s accelerated linear algebra compiler. XLA applies various optimizations to the model\u2019s computation graph, and results in significant improvements to speed and memory usage. As described in the Hugging Face "),re=r("a"),ve=a("blog"),Te=a(", XLA works best when our input shapes don\u2019t vary too much. To handle this, we\u2019ll pad our inputs to multiples of 128, and make a new dataset with the padding collator, and then we\u2019ll apply the "),oe=r("code"),ye=a("@tf.function(jit_compile=True)"),$e=a(" decorator to our generation function, which marks the whole function for compilation with XLA."),_e=h(),v(le.$$.fragment),pe=h(),me=r("p"),b=a("Once we have our lists of label and prediction strings, computing the ROUGE score is easy:"),N=h(),v(we.$$.fragment),he=h(),v(ce.$$.fragment),this.h()},l(w){u=o(w,"P",{});var H=l(u);$=n(H,"We\u2019re almost ready to train! We just need to convert our datasets to "),d=o(H,"CODE",{});var Ge=l(d);E=n(Ge,"tf.data.Dataset"),Ge.forEach(t),C=n(H,"s using the data collator we defined above, and then "),x=o(H,"CODE",{});var xe=l(x);A=n(xe,"compile()"),xe.forEach(t),P=n(H," and "),T=o(H,"CODE",{});var ee=l(T);S=n(ee,"fit()"),ee.forEach(t),I=n(H," the model. First, the datasets:"),H.forEach(t),D=c(w),y(O.$$.fragment,w),F=c(w),V=o(w,"P",{});var te=l(V);J=n(te,"Now, we define our training hyperparameters and compile:"),te.forEach(t),Z=c(w),y(X.$$.fragment,w),Q=c(w),L=o(w,"P",{});var Ee=l(L);M=n(Ee,"And finally, we fit the model. We use a "),se=o(Ee,"CODE",{});var Be=l(se);R=n(Be,"PushToHubCallback"),Be.forEach(t),Y=n(Ee," to save the model to the Hub after each epoch, which will allow us to use it for inference later:"),Ee.forEach(t),ae=c(w),y(W.$$.fragment,w),ie=c(w),B=o(w,"P",{});var ke=l(B);ne=n(ke,"We got some loss values during training, but really we\u2019d like to see the ROUGE metrics we computed earlier. To get those metrics, we\u2019ll need to generate outputs from the model and convert them to strings. Let\u2019s build some lists of labels and predictions for the ROUGE metric to compare (note that if you get import errors for this section, you may need to"),fe=o(ke,"CODE",{});var Xe=l(fe);be=n(Xe,"!pip install tqdm"),Xe.forEach(t),Se=n(ke,"). We\u2019re also going to use a trick that dramatically increases performance - compiling our generation code with "),G=o(ke,"A",{href:!0,rel:!0});var Ye=l(G);Le=n(Ye,"XLA"),Ye.forEach(t),ge=n(ke,", TensorFlow\u2019s accelerated linear algebra compiler. XLA applies various optimizations to the model\u2019s computation graph, and results in significant improvements to speed and memory usage. As described in the Hugging Face "),re=o(ke,"A",{href:!0,rel:!0});var rs=l(re);ve=n(rs,"blog"),rs.forEach(t),Te=n(ke,", XLA works best when our input shapes don\u2019t vary too much. To handle this, we\u2019ll pad our inputs to multiples of 128, and make a new dataset with the padding collator, and then we\u2019ll apply the "),oe=o(ke,"CODE",{});var Ce=l(oe);ye=n(Ce,"@tf.function(jit_compile=True)"),Ce.forEach(t),$e=n(ke," decorator to our generation function, which marks the whole function for compilation with XLA."),ke.forEach(t),_e=c(w),y(le.$$.fragment,w),pe=c(w),me=o(w,"P",{});var We=l(me);b=n(We,"Once we have our lists of label and prediction strings, computing the ROUGE score is easy:"),We.forEach(t),N=c(w),y(we.$$.fragment,w),he=c(w),y(ce.$$.fragment,w),this.h()},h(){f(G,"href","https://www.tensorflow.org/xla"),f(G,"rel","nofollow"),f(re,"href","https://huggingface.co/blog/tf-xla-generate"),f(re,"rel","nofollow")},m(w,H){i(w,u,H),s(u,$),s(u,d),s(d,E),s(u,C),s(u,x),s(x,A),s(u,P),s(u,T),s(T,S),s(u,I),i(w,D,H),k(O,w,H),i(w,F,H),i(w,V,H),s(V,J),i(w,Z,H),k(X,w,H),i(w,Q,H),i(w,L,H),s(L,M),s(L,se),s(se,R),s(L,Y),i(w,ae,H),k(W,w,H),i(w,ie,H),i(w,B,H),s(B,ne),s(B,fe),s(fe,be),s(B,Se),s(B,G),s(G,Le),s(B,ge),s(B,re),s(re,ve),s(B,Te),s(B,oe),s(oe,ye),s(B,$e),i(w,_e,H),k(le,w,H),i(w,pe,H),i(w,me,H),s(me,b),i(w,N,H),k(we,w,H),i(w,he,H),k(ce,w,H),Ie=!0},i(w){Ie||(g(O.$$.fragment,w),g(X.$$.fragment,w),g(W.$$.fragment,w),g(le.$$.fragment,w),g(we.$$.fragment,w),g(ce.$$.fragment,w),Ie=!0)},o(w){_(O.$$.fragment,w),_(X.$$.fragment,w),_(W.$$.fragment,w),_(le.$$.fragment,w),_(we.$$.fragment,w),_(ce.$$.fragment,w),Ie=!1},d(w){w&&t(u),w&&t(D),j(O,w),w&&t(F),w&&t(V),w&&t(Z),j(X,w),w&&t(Q),w&&t(L),w&&t(ae),j(W,w),w&&t(ie),w&&t(B),w&&t(_e),j(le,w),w&&t(pe),w&&t(me),w&&t(N),j(we,w),w&&t(he),j(ce,w)}}}function Dg(K){let u,$,d,E,C,x,A,P,T,S,I,D,O,F,V,J,Z,X,Q,L,M,se,R,Y,ae,W,ie,B,ne,fe,be,Se,G,Le,ge,re,ve,Te,oe,ye,$e,_e,le,pe,me;return E=new z({props:{code:`from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)`}}),T=new z({props:{code:"trainer.train()",highlighted:"trainer.train()"}}),Z=new z({props:{code:"trainer.evaluate()",highlighted:"trainer.evaluate()"}}),Q=new z({props:{code:`{'eval_loss': 3.028524398803711,
 'eval_rouge1': 16.9728,
 'eval_rouge2': 8.2969,
 'eval_rougeL': 16.8366,
 'eval_rougeLsum': 16.851,
 'eval_gen_len': 10.1597,
 'eval_runtime': 6.1054,
 'eval_samples_per_second': 38.982,
 'eval_steps_per_second': 4.914}`,highlighted:`{<span class="hljs-string">&#x27;eval_loss&#x27;</span>: <span class="hljs-number">3.028524398803711</span>,
 <span class="hljs-string">&#x27;eval_rouge1&#x27;</span>: <span class="hljs-number">16.9728</span>,
 <span class="hljs-string">&#x27;eval_rouge2&#x27;</span>: <span class="hljs-number">8.2969</span>,
 <span class="hljs-string">&#x27;eval_rougeL&#x27;</span>: <span class="hljs-number">16.8366</span>,
 <span class="hljs-string">&#x27;eval_rougeLsum&#x27;</span>: <span class="hljs-number">16.851</span>,
 <span class="hljs-string">&#x27;eval_gen_len&#x27;</span>: <span class="hljs-number">10.1597</span>,
 <span class="hljs-string">&#x27;eval_runtime&#x27;</span>: <span class="hljs-number">6.1054</span>,
 <span class="hljs-string">&#x27;eval_samples_per_second&#x27;</span>: <span class="hljs-number">38.982</span>,
 <span class="hljs-string">&#x27;eval_steps_per_second&#x27;</span>: <span class="hljs-number">4.914</span>}`}}),Y=new z({props:{code:'trainer.push_to_hub(commit_message="Training complete", tags="summarization")',highlighted:'trainer.push_to_hub(<span class="hljs-attribute">commit_message</span>=<span class="hljs-string">&quot;Training complete&quot;</span>, <span class="hljs-attribute">tags</span>=<span class="hljs-string">&quot;summarization&quot;</span>)'}}),W=new z({props:{code:"'https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0'",highlighted:'<span class="hljs-string">&#x27;https://huggingface.co/huggingface-course/mt5-finetuned-amazon-en-es/commit/aa0536b829b28e73e1e4b94b8a5aacec420d40e0&#x27;</span>'}}),{c(){u=r("p"),$=a("We finally have all the ingredients we need to train with! We now simply need to instantiate the trainer with the standard arguments:"),d=h(),v(E.$$.fragment),C=h(),x=r("p"),A=a("and launch our training run:"),P=h(),v(T.$$.fragment),S=h(),I=r("p"),D=a("During training, you should see the training loss decrease and the ROUGE scores increase with each epoch. Once the training is complete, you can see the final ROUGE scores by running "),O=r("code"),F=a("Trainer.evaluate()"),V=a(":"),J=h(),v(Z.$$.fragment),X=h(),v(Q.$$.fragment),L=h(),M=r("p"),se=a("From the scores we can see that our model has handily outperformed our lead-3 baseline \u2014 nice! The final thing to do is push the model weights to the Hub, as follows:"),R=h(),v(Y.$$.fragment),ae=h(),v(W.$$.fragment),ie=h(),B=r("p"),ne=a("This will save the checkpoint and configuration files to "),fe=r("code"),be=a("output_dir"),Se=a(", before uploading all the files to the Hub. By specifying the "),G=r("code"),Le=a("tags"),ge=a(" argument, we also ensure that the widget on the Hub will be one for a summarization pipeline instead of the default text generation one associated with the mT5 architecture (for more information about model tags, see the "),re=r("a"),ve=a("\u{1F917} Hub documentation"),Te=a("). The output from "),oe=r("code"),ye=a("trainer.push_to_hub()"),$e=a(" is a URL to the Git commit hash, so you can easily see the changes that were made to the model repository!"),_e=h(),le=r("p"),pe=a("To wrap up this section, let\u2019s take a look at how we can also fine-tune mT5 using the low-level features provided by \u{1F917} Accelerate."),this.h()},l(b){u=o(b,"P",{});var N=l(u);$=n(N,"We finally have all the ingredients we need to train with! We now simply need to instantiate the trainer with the standard arguments:"),N.forEach(t),d=c(b),y(E.$$.fragment,b),C=c(b),x=o(b,"P",{});var we=l(x);A=n(we,"and launch our training run:"),we.forEach(t),P=c(b),y(T.$$.fragment,b),S=c(b),I=o(b,"P",{});var he=l(I);D=n(he,"During training, you should see the training loss decrease and the ROUGE scores increase with each epoch. Once the training is complete, you can see the final ROUGE scores by running "),O=o(he,"CODE",{});var ce=l(O);F=n(ce,"Trainer.evaluate()"),ce.forEach(t),V=n(he,":"),he.forEach(t),J=c(b),y(Z.$$.fragment,b),X=c(b),y(Q.$$.fragment,b),L=c(b),M=o(b,"P",{});var Ie=l(M);se=n(Ie,"From the scores we can see that our model has handily outperformed our lead-3 baseline \u2014 nice! The final thing to do is push the model weights to the Hub, as follows:"),Ie.forEach(t),R=c(b),y(Y.$$.fragment,b),ae=c(b),y(W.$$.fragment,b),ie=c(b),B=o(b,"P",{});var w=l(B);ne=n(w,"This will save the checkpoint and configuration files to "),fe=o(w,"CODE",{});var H=l(fe);be=n(H,"output_dir"),H.forEach(t),Se=n(w,", before uploading all the files to the Hub. By specifying the "),G=o(w,"CODE",{});var Ge=l(G);Le=n(Ge,"tags"),Ge.forEach(t),ge=n(w," argument, we also ensure that the widget on the Hub will be one for a summarization pipeline instead of the default text generation one associated with the mT5 architecture (for more information about model tags, see the "),re=o(w,"A",{href:!0,rel:!0});var xe=l(re);ve=n(xe,"\u{1F917} Hub documentation"),xe.forEach(t),Te=n(w,"). The output from "),oe=o(w,"CODE",{});var ee=l(oe);ye=n(ee,"trainer.push_to_hub()"),ee.forEach(t),$e=n(w," is a URL to the Git commit hash, so you can easily see the changes that were made to the model repository!"),w.forEach(t),_e=c(b),le=o(b,"P",{});var te=l(le);pe=n(te,"To wrap up this section, let\u2019s take a look at how we can also fine-tune mT5 using the low-level features provided by \u{1F917} Accelerate."),te.forEach(t),this.h()},h(){f(re,"href","https://huggingface.co/docs/hub/main#how-is-a-models-type-of-inference-api-and-widget-determined"),f(re,"rel","nofollow")},m(b,N){i(b,u,N),s(u,$),i(b,d,N),k(E,b,N),i(b,C,N),i(b,x,N),s(x,A),i(b,P,N),k(T,b,N),i(b,S,N),i(b,I,N),s(I,D),s(I,O),s(O,F),s(I,V),i(b,J,N),k(Z,b,N),i(b,X,N),k(Q,b,N),i(b,L,N),i(b,M,N),s(M,se),i(b,R,N),k(Y,b,N),i(b,ae,N),k(W,b,N),i(b,ie,N),i(b,B,N),s(B,ne),s(B,fe),s(fe,be),s(B,Se),s(B,G),s(G,Le),s(B,ge),s(B,re),s(re,ve),s(B,Te),s(B,oe),s(oe,ye),s(B,$e),i(b,_e,N),i(b,le,N),s(le,pe),me=!0},i(b){me||(g(E.$$.fragment,b),g(T.$$.fragment,b),g(Z.$$.fragment,b),g(Q.$$.fragment,b),g(Y.$$.fragment,b),g(W.$$.fragment,b),me=!0)},o(b){_(E.$$.fragment,b),_(T.$$.fragment,b),_(Z.$$.fragment,b),_(Q.$$.fragment,b),_(Y.$$.fragment,b),_(W.$$.fragment,b),me=!1},d(b){b&&t(u),b&&t(d),j(E,b),b&&t(C),b&&t(x),b&&t(P),j(T,b),b&&t(S),b&&t(I),b&&t(J),j(Z,b),b&&t(X),j(Q,b),b&&t(L),b&&t(M),b&&t(R),j(Y,b),b&&t(ae),j(W,b),b&&t(ie),b&&t(B),b&&t(_e),b&&t(le)}}}function og(K){let u,$,d,E,C,x,A,P,T,S,I,D,O,F,V,J,Z,X,Q,L,M,se,R,Y,ae,W,ie,B,ne,fe,be,Se,G,Le,ge,re,ve,Te,oe,ye,$e,_e,le,pe,me,b,N,we,he,ce,Ie,w,H,Ge,xe,ee,te,Ee,Be,ke,Xe,Ye,rs,Ce,We,os,cs,Ke,Xs,us,Pe,ks,zs,Ne,ds,U,ue,Ve,Ys,de,fs,ls,Ds,Ae,Ks,is,qe,Vs,Je,Js,He,Zs,Ze,Qs,je,js,Oe,bt,$s,xs,pn,vt,et,da,gs,fa,yt,Rt,ga,st,Ft,Qe,_a,ze,mn,Gt,wa,Es,_s,Nt,tt,Ht,Ut,hn,kt,jt,cn,$t,es,at,ba,Ts,va,xt,un,Et,nt,ya,qs,ka,ss,ps,Mt,Wt,dn,Bt,Xt,fn,Yt,Kt,gn,ja,Ss,rt,ot,Vt,ws,$a,As,bs,ts,vr,Jt,lt,yr,Zt;return E=new Bs({}),M=new Bs({}),ge=new z({props:{code:'tokenized_datasets.set_format("torch")',highlighted:'tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)'}}),le=new z({props:{code:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)",highlighted:"model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"}}),we=new z({props:{code:`from torch.utils.data import DataLoader

batch_size = 8
train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], collate_fn=data_collator, batch_size=batch_size
)`,highlighted:`<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

batch_size = <span class="hljs-number">8</span>
train_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    collate_fn=data_collator,
    batch_size=batch_size,
)
eval_dataloader = DataLoader(
    tokenized_datasets[<span class="hljs-string">&quot;validation&quot;</span>], collate_fn=data_collator, batch_size=batch_size
)`}}),ee=new z({props:{code:`from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)`,highlighted:`<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">2e-5</span>)`}}),Ce=new z({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader
)`}}),os=new ln({props:{$$slots:{default:[Sg]},$$scope:{ctx:K}}}),Ae=new z({props:{code:`from transformers import get_scheduler

num_train_epochs = 10
num_update_steps_per_epoch = len(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

num_train_epochs = <span class="hljs-number">10</span>
num_update_steps_per_epoch = <span class="hljs-built_in">len</span>(train_dataloader)
num_training_steps = num_train_epochs * num_update_steps_per_epoch

lr_scheduler = get_scheduler(
    <span class="hljs-string">&quot;linear&quot;</span>,
    optimizer=optimizer,
    num_warmup_steps=<span class="hljs-number">0</span>,
    num_training_steps=num_training_steps,
)`}}),Je=new z({props:{code:`def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]

    # ROUGE expects a newline after each sentence
    preds = ["\\n".join(nltk.sent_tokenize(pred)) for pred in preds]
    labels = ["\\n".join(nltk.sent_tokenize(label)) for label in labels]

    return preds, labels`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess_text</span>(<span class="hljs-params">preds, labels</span>):
    preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
    labels = [label.strip() <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

    <span class="hljs-comment"># ROUGE expects a newline after each sentence</span>
    preds = [<span class="hljs-string">&quot;\\n&quot;</span>.join(nltk.sent_tokenize(pred)) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
    labels = [<span class="hljs-string">&quot;\\n&quot;</span>.join(nltk.sent_tokenize(label)) <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

    <span class="hljs-keyword">return</span> preds, labels`}}),et=new z({props:{code:`from huggingface_hub import get_full_repo_name

model_name = "test-bert-finetuned-squad-accelerate"
repo_name = get_full_repo_name(model_name)
repo_name`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> get_full_repo_name

model_name = <span class="hljs-string">&quot;test-bert-finetuned-squad-accelerate&quot;</span>
repo_name = get_full_repo_name(model_name)
repo_name`}}),gs=new z({props:{code:"'lewtun/mt5-finetuned-amazon-en-es-accelerate'",highlighted:'<span class="hljs-string">&#x27;lewtun/mt5-finetuned-amazon-en-es-accelerate&#x27;</span>'}}),st=new z({props:{code:`from huggingface_hub import Repository

output_dir = "results-mt5-finetuned-squad-accelerate"
repo = Repository(output_dir, clone_from=repo_name)`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> Repository

output_dir = <span class="hljs-string">&quot;results-mt5-finetuned-squad-accelerate&quot;</span>
repo = Repository(output_dir, clone_from=repo_name)`}}),tt=new Bs({}),ws=new z({props:{code:`from tqdm.auto import tqdm
import torch
import numpy as np

progress_bar = tqdm(range(num_training_steps))

for epoch in range(num_train_epochs):
    # Training
    model.train()
    for step, batch in enumerate(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

    # Evaluation
    model.eval()
    for step, batch in enumerate(eval_dataloader):
        with torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id
            )
            labels = batch["labels"]

            # If we did not pad to max length, we need to pad the labels too
            labels = accelerator.pad_across_processes(
                batch["labels"], dim=1, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            # Replace -100 in the labels as we can't decode them
            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
            if isinstance(generated_tokens, tuple):
                generated_tokens = generated_tokens[0]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=True
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    # Compute metrics
    result = rouge_score.compute()
    # Extract the median ROUGE scores
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    result = {k: round(v, 4) for k, v in result.items()}
    print(f"Epoch {epoch}:", result)

    # Save and upload
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    if accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=f"Training in progress epoch {epoch}", blocking=False
        )`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_train_epochs):
    <span class="hljs-comment"># Training</span>
    model.train()
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_dataloader):
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(<span class="hljs-number">1</span>)

    <span class="hljs-comment"># Evaluation</span>
    model.<span class="hljs-built_in">eval</span>()
    <span class="hljs-keyword">for</span> step, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(eval_dataloader):
        <span class="hljs-keyword">with</span> torch.no_grad():
            generated_tokens = accelerator.unwrap_model(model).generate(
                batch[<span class="hljs-string">&quot;input_ids&quot;</span>],
                attention_mask=batch[<span class="hljs-string">&quot;attention_mask&quot;</span>],
            )

            generated_tokens = accelerator.pad_across_processes(
                generated_tokens, dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
            )
            labels = batch[<span class="hljs-string">&quot;labels&quot;</span>]

            <span class="hljs-comment"># If we did not pad to max length, we need to pad the labels too</span>
            labels = accelerator.pad_across_processes(
                batch[<span class="hljs-string">&quot;labels&quot;</span>], dim=<span class="hljs-number">1</span>, pad_index=tokenizer.pad_token_id
            )

            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()
            labels = accelerator.gather(labels).cpu().numpy()

            <span class="hljs-comment"># Replace -100 in the labels as we can&#x27;t decode them</span>
            labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(generated_tokens, <span class="hljs-built_in">tuple</span>):
                generated_tokens = generated_tokens[<span class="hljs-number">0</span>]
            decoded_preds = tokenizer.batch_decode(
                generated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>
            )
            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)

            decoded_preds, decoded_labels = postprocess_text(
                decoded_preds, decoded_labels
            )

            rouge_score.add_batch(predictions=decoded_preds, references=decoded_labels)

    <span class="hljs-comment"># Compute metrics</span>
    result = rouge_score.compute()
    <span class="hljs-comment"># Extract the median ROUGE scores</span>
    result = {key: value.mid.fmeasure * <span class="hljs-number">100</span> <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> result.items()}
    result = {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Epoch <span class="hljs-subst">{epoch}</span>:&quot;</span>, result)

    <span class="hljs-comment"># Save and upload</span>
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)
    <span class="hljs-keyword">if</span> accelerator.is_main_process:
        tokenizer.save_pretrained(output_dir)
        repo.push_to_hub(
            commit_message=<span class="hljs-string">f&quot;Training in progress epoch <span class="hljs-subst">{epoch}</span>&quot;</span>, blocking=<span class="hljs-literal">False</span>
        )`}}),As=new z({props:{code:`Epoch 0: {'rouge1': 5.6351, 'rouge2': 1.1625, 'rougeL': 5.4866, 'rougeLsum': 5.5005}
Epoch 1: {'rouge1': 9.8646, 'rouge2': 3.4106, 'rougeL': 9.9439, 'rougeLsum': 9.9306}
Epoch 2: {'rouge1': 11.0872, 'rouge2': 3.3273, 'rougeL': 11.0508, 'rougeLsum': 10.9468}
Epoch 3: {'rouge1': 11.8587, 'rouge2': 4.8167, 'rougeL': 11.7986, 'rougeLsum': 11.7518}
Epoch 4: {'rouge1': 12.9842, 'rouge2': 5.5887, 'rougeL': 12.7546, 'rougeLsum': 12.7029}
Epoch 5: {'rouge1': 13.4628, 'rouge2': 6.4598, 'rougeL': 13.312, 'rougeLsum': 13.2913}
Epoch 6: {'rouge1': 12.9131, 'rouge2': 5.8914, 'rougeL': 12.6896, 'rougeLsum': 12.5701}
Epoch 7: {'rouge1': 13.3079, 'rouge2': 6.2994, 'rougeL': 13.1536, 'rougeLsum': 13.1194}
Epoch 8: {'rouge1': 13.96, 'rouge2': 6.5998, 'rougeL': 13.9123, 'rougeLsum': 13.7744}
Epoch 9: {'rouge1': 14.1192, 'rouge2': 7.0059, 'rougeL': 14.1172, 'rougeLsum': 13.9509}`,highlighted:`Epoch <span class="hljs-number">0</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">5.6351</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">1.1625</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">5.4866</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">5.5005</span>}
Epoch <span class="hljs-number">1</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">9.8646</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">3.4106</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">9.9439</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">9.9306</span>}
Epoch <span class="hljs-number">2</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">11.0872</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">3.3273</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">11.0508</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">10.9468</span>}
Epoch <span class="hljs-number">3</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">11.8587</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">4.8167</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">11.7986</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">11.7518</span>}
Epoch <span class="hljs-number">4</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">12.9842</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">5.5887</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">12.7546</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">12.7029</span>}
Epoch <span class="hljs-number">5</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.4628</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.4598</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.312</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.2913</span>}
Epoch <span class="hljs-number">6</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">12.9131</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">5.8914</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">12.6896</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">12.5701</span>}
Epoch <span class="hljs-number">7</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.3079</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.2994</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.1536</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.1194</span>}
Epoch <span class="hljs-number">8</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">13.96</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">6.5998</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">13.9123</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.7744</span>}
Epoch <span class="hljs-number">9</span>: {<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">14.1192</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">7.0059</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">14.1172</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">13.9509</span>}`}}),{c(){u=r("h2"),$=r("a"),d=r("span"),v(E.$$.fragment),C=h(),x=r("span"),A=a("Fine-tuning mT5 with \u{1F917} Accelerate"),P=h(),T=r("p"),S=a("Fine-tuning our model with \u{1F917} Accelerate is very similar to the text classification example we encountered in "),I=r("a"),D=a("Chapter 3"),O=a(". The main differences will be the need to explicitly generate our summaries during training and define how we compute the ROUGE scores (recall that the "),F=r("code"),V=a("Seq2SeqTrainer"),J=a(" took care of the generation for us). Let\u2019s take a look how we can implement these two requirements within \u{1F917} Accelerate!"),Z=h(),X=r("h3"),Q=r("a"),L=r("span"),v(M.$$.fragment),se=h(),R=r("span"),Y=a("Preparing everything for training"),ae=h(),W=r("p"),ie=a("The first thing we need to do is create a "),B=r("code"),ne=a("DataLoader"),fe=a(" for each of our splits. Since the PyTorch dataloaders expect batches of tensors, we need to set the format to "),be=r("code"),Se=a('"torch"'),G=a(" in our datasets:"),Le=h(),v(ge.$$.fragment),re=h(),ve=r("p"),Te=a("Now that we\u2019ve got datasets consisting of just tensors, the next thing to do is instantiate the "),oe=r("code"),ye=a("DataCollatorForSeq2Seq"),$e=a(" again. For this we need to provide a fresh version of the model, so let\u2019s load it again from our cache:"),_e=h(),v(le.$$.fragment),pe=h(),me=r("p"),b=a("We can then instantiate the data collator and use this to define our dataloaders:"),N=h(),v(we.$$.fragment),he=h(),ce=r("p"),Ie=a("The next thing to do is define the optimizer we want to use. As in our other examples, we\u2019ll use "),w=r("code"),H=a("AdamW"),Ge=a(", which works well for most problems:"),xe=h(),v(ee.$$.fragment),te=h(),Ee=r("p"),Be=a("Finally, we feed our model, optimizer, and dataloaders to the "),ke=r("code"),Xe=a("accelerator.prepare()"),Ye=a(" method:"),rs=h(),v(Ce.$$.fragment),We=h(),v(os.$$.fragment),cs=h(),Ke=r("p"),Xs=a("Now that we\u2019ve prepared our objects, there are three remaining things to do:"),us=h(),Pe=r("ul"),ks=r("li"),zs=a("Define the learning rate schedule."),Ne=h(),ds=r("li"),U=a("Implement a function to post-process the summaries for evaluation."),ue=h(),Ve=r("li"),Ys=a("Create a repository on the Hub that we can push our model to."),de=h(),fs=r("p"),ls=a("For the learning rate schedule, we\u2019ll use the standard linear one from previous sections:"),Ds=h(),v(Ae.$$.fragment),Ks=h(),is=r("p"),qe=a("For post-processing, we need a function that splits the generated summaries into sentences that are separated by newlines. This is the format the ROUGE metric expects, and we can achieve this with the following snippet of code:"),Vs=h(),v(Je.$$.fragment),Js=h(),He=r("p"),Zs=a("This should look familiar to you if you recall how we defined the "),Ze=r("code"),Qs=a("compute_metrics()"),je=a(" function of the "),js=r("code"),Oe=a("Seq2SeqTrainer"),bt=a("."),$s=h(),xs=r("p"),pn=a("Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately titled \u{1F917} Hub library. We just need to define a name for our repository, and the library has a utility function to combine the repository ID with the user profile:"),vt=h(),v(et.$$.fragment),da=h(),v(gs.$$.fragment),fa=h(),yt=r("p"),Rt=a("Now we can use this repository name to clone a local version to our results directory that will store the training artifacts:"),ga=h(),v(st.$$.fragment),Ft=h(),Qe=r("p"),_a=a("This will allow us to push the artifacts back to the Hub by calling the "),ze=r("code"),mn=a("repo.push_to_hub()"),Gt=a(" method during training! Let\u2019s now wrap up our analysis by writing out the training loop."),wa=h(),Es=r("h3"),_s=r("a"),Nt=r("span"),v(tt.$$.fragment),Ht=h(),Ut=r("span"),hn=a("Training loop"),kt=h(),jt=r("p"),cn=a("The training loop for summarization is quite similar to the other \u{1F917} Accelerate examples that we\u2019ve encountered and is roughly split into four main steps:"),$t=h(),es=r("ol"),at=r("li"),ba=a("Train the model by iterating over all the examples in "),Ts=r("code"),va=a("train_dataloader"),xt=a(" for each epoch."),un=h(),Et=r("li"),nt=a("Generate model summaries at the end of each epoch, by first generating the tokens and then decoding them (and the reference summaries) into text."),ya=h(),qs=r("li"),ka=a("Compute the ROUGE scores using the same techniques we saw earlier."),ss=h(),ps=r("li"),Mt=a("Save the checkpoints and push everything to the Hub. Here we rely on the nifty "),Wt=r("code"),dn=a("blocking=False"),Bt=a(" argument of the "),Xt=r("code"),fn=a("Repository"),Yt=a(" object so that we can push the checkpoints per epoch "),Kt=r("em"),gn=a("asynchronously"),ja=a(". This allows us to continue training without having to wait for the somewhat slow upload associated with a GB-sized model!"),Ss=h(),rt=r("p"),ot=a("These steps can be seen in the following block of code:"),Vt=h(),v(ws.$$.fragment),$a=h(),v(As.$$.fragment),bs=h(),ts=r("p"),vr=a("And that\u2019s it! Once you run this, you\u2019ll have a model and results that are pretty similar to the ones we obtained with the "),Jt=r("code"),lt=a("Trainer"),yr=a("."),this.h()},l(m){u=o(m,"H2",{class:!0});var q=l(u);$=o(q,"A",{id:!0,class:!0,href:!0});var xa=l($);d=o(xa,"SPAN",{});var kr=l(d);y(E.$$.fragment,kr),kr.forEach(t),xa.forEach(t),C=c(q),x=o(q,"SPAN",{});var jr=l(x);A=n(jr,"Fine-tuning mT5 with \u{1F917} Accelerate"),jr.forEach(t),q.forEach(t),P=c(m),T=o(m,"P",{});var it=l(T);S=n(it,"Fine-tuning our model with \u{1F917} Accelerate is very similar to the text classification example we encountered in "),I=o(it,"A",{href:!0});var Tt=l(I);D=n(Tt,"Chapter 3"),Tt.forEach(t),O=n(it,". The main differences will be the need to explicitly generate our summaries during training and define how we compute the ROUGE scores (recall that the "),F=o(it,"CODE",{});var _n=l(F);V=n(_n,"Seq2SeqTrainer"),_n.forEach(t),J=n(it," took care of the generation for us). Let\u2019s take a look how we can implement these two requirements within \u{1F917} Accelerate!"),it.forEach(t),Z=c(m),X=o(m,"H3",{class:!0});var pt=l(X);Q=o(pt,"A",{id:!0,class:!0,href:!0});var $r=l(Q);L=o($r,"SPAN",{});var wn=l(L);y(M.$$.fragment,wn),wn.forEach(t),$r.forEach(t),se=c(pt),R=o(pt,"SPAN",{});var Os=l(R);Y=n(Os,"Preparing everything for training"),Os.forEach(t),pt.forEach(t),ae=c(m),W=o(m,"P",{});var ms=l(W);ie=n(ms,"The first thing we need to do is create a "),B=o(ms,"CODE",{});var Ea=l(B);ne=n(Ea,"DataLoader"),Ea.forEach(t),fe=n(ms," for each of our splits. Since the PyTorch dataloaders expect batches of tensors, we need to set the format to "),be=o(ms,"CODE",{});var qt=l(be);Se=n(qt,'"torch"'),qt.forEach(t),G=n(ms," in our datasets:"),ms.forEach(t),Le=c(m),y(ge.$$.fragment,m),re=c(m),ve=o(m,"P",{});var Ta=l(ve);Te=n(Ta,"Now that we\u2019ve got datasets consisting of just tensors, the next thing to do is instantiate the "),oe=o(Ta,"CODE",{});var qa=l(oe);ye=n(qa,"DataCollatorForSeq2Seq"),qa.forEach(t),$e=n(Ta," again. For this we need to provide a fresh version of the model, so let\u2019s load it again from our cache:"),Ta.forEach(t),_e=c(m),y(le.$$.fragment,m),pe=c(m),me=o(m,"P",{});var xr=l(me);b=n(xr,"We can then instantiate the data collator and use this to define our dataloaders:"),xr.forEach(t),N=c(m),y(we.$$.fragment,m),he=c(m),ce=o(m,"P",{});var Qt=l(ce);Ie=n(Qt,"The next thing to do is define the optimizer we want to use. As in our other examples, we\u2019ll use "),w=o(Qt,"CODE",{});var mt=l(w);H=n(mt,"AdamW"),mt.forEach(t),Ge=n(Qt,", which works well for most problems:"),Qt.forEach(t),xe=c(m),y(ee.$$.fragment,m),te=c(m),Ee=o(m,"P",{});var za=l(Ee);Be=n(za,"Finally, we feed our model, optimizer, and dataloaders to the "),ke=o(za,"CODE",{});var ea=l(ke);Xe=n(ea,"accelerator.prepare()"),ea.forEach(t),Ye=n(za," method:"),za.forEach(t),rs=c(m),y(Ce.$$.fragment,m),We=c(m),y(os.$$.fragment,m),cs=c(m),Ke=o(m,"P",{});var Er=l(Ke);Xs=n(Er,"Now that we\u2019ve prepared our objects, there are three remaining things to do:"),Er.forEach(t),us=c(m),Pe=o(m,"UL",{});var zt=l(Pe);ks=o(zt,"LI",{});var bn=l(ks);zs=n(bn,"Define the learning rate schedule."),bn.forEach(t),Ne=c(zt),ds=o(zt,"LI",{});var ht=l(ds);U=n(ht,"Implement a function to post-process the summaries for evaluation."),ht.forEach(t),ue=c(zt),Ve=o(zt,"LI",{});var Da=l(Ve);Ys=n(Da,"Create a repository on the Hub that we can push our model to."),Da.forEach(t),zt.forEach(t),de=c(m),fs=o(m,"P",{});var Cs=l(fs);ls=n(Cs,"For the learning rate schedule, we\u2019ll use the standard linear one from previous sections:"),Cs.forEach(t),Ds=c(m),y(Ae.$$.fragment,m),Ks=c(m),is=o(m,"P",{});var sa=l(is);qe=n(sa,"For post-processing, we need a function that splits the generated summaries into sentences that are separated by newlines. This is the format the ROUGE metric expects, and we can achieve this with the following snippet of code:"),sa.forEach(t),Vs=c(m),y(Je.$$.fragment,m),Js=c(m),He=o(m,"P",{});var Dt=l(He);Zs=n(Dt,"This should look familiar to you if you recall how we defined the "),Ze=o(Dt,"CODE",{});var Tr=l(Ze);Qs=n(Tr,"compute_metrics()"),Tr.forEach(t),je=n(Dt," function of the "),js=o(Dt,"CODE",{});var Sa=l(js);Oe=n(Sa,"Seq2SeqTrainer"),Sa.forEach(t),bt=n(Dt,"."),Dt.forEach(t),$s=c(m),xs=o(m,"P",{});var qr=l(xs);pn=n(qr,"Finally, we need to create a model repository on the Hugging Face Hub. For this, we can use the appropriately titled \u{1F917} Hub library. We just need to define a name for our repository, and the library has a utility function to combine the repository ID with the user profile:"),qr.forEach(t),vt=c(m),y(et.$$.fragment,m),da=c(m),y(gs.$$.fragment,m),fa=c(m),yt=o(m,"P",{});var zr=l(yt);Rt=n(zr,"Now we can use this repository name to clone a local version to our results directory that will store the training artifacts:"),zr.forEach(t),ga=c(m),y(st.$$.fragment,m),Ft=c(m),Qe=o(m,"P",{});var ct=l(Qe);_a=n(ct,"This will allow us to push the artifacts back to the Hub by calling the "),ze=o(ct,"CODE",{});var Dr=l(ze);mn=n(Dr,"repo.push_to_hub()"),Dr.forEach(t),Gt=n(ct," method during training! Let\u2019s now wrap up our analysis by writing out the training loop."),ct.forEach(t),wa=c(m),Es=o(m,"H3",{class:!0});var Aa=l(Es);_s=o(Aa,"A",{id:!0,class:!0,href:!0});var Ue=l(_s);Nt=o(Ue,"SPAN",{});var Ps=l(Nt);y(tt.$$.fragment,Ps),Ps.forEach(t),Ue.forEach(t),Ht=c(Aa),Ut=o(Aa,"SPAN",{});var ta=l(Ut);hn=n(ta,"Training loop"),ta.forEach(t),Aa.forEach(t),kt=c(m),jt=o(m,"P",{});var St=l(jt);cn=n(St,"The training loop for summarization is quite similar to the other \u{1F917} Accelerate examples that we\u2019ve encountered and is roughly split into four main steps:"),St.forEach(t),$t=c(m),es=o(m,"OL",{});var Ls=l(es);at=o(Ls,"LI",{});var Oa=l(at);ba=n(Oa,"Train the model by iterating over all the examples in "),Ts=o(Oa,"CODE",{});var Ca=l(Ts);va=n(Ca,"train_dataloader"),Ca.forEach(t),xt=n(Oa," for each epoch."),Oa.forEach(t),un=c(Ls),Et=o(Ls,"LI",{});var Sr=l(Et);nt=n(Sr,"Generate model summaries at the end of each epoch, by first generating the tokens and then decoding them (and the reference summaries) into text."),Sr.forEach(t),ya=c(Ls),qs=o(Ls,"LI",{});var Ar=l(qs);ka=n(Ar,"Compute the ROUGE scores using the same techniques we saw earlier."),Ar.forEach(t),ss=c(Ls),ps=o(Ls,"LI",{});var hs=l(ps);Mt=n(hs,"Save the checkpoints and push everything to the Hub. Here we rely on the nifty "),Wt=o(hs,"CODE",{});var Or=l(Wt);dn=n(Or,"blocking=False"),Or.forEach(t),Bt=n(hs," argument of the "),Xt=o(hs,"CODE",{});var Cr=l(Xt);fn=n(Cr,"Repository"),Cr.forEach(t),Yt=n(hs," object so that we can push the checkpoints per epoch "),Kt=o(hs,"EM",{});var Is=l(Kt);gn=n(Is,"asynchronously"),Is.forEach(t),ja=n(hs,". This allows us to continue training without having to wait for the somewhat slow upload associated with a GB-sized model!"),hs.forEach(t),Ls.forEach(t),Ss=c(m),rt=o(m,"P",{});var aa=l(rt);ot=n(aa,"These steps can be seen in the following block of code:"),aa.forEach(t),Vt=c(m),y(ws.$$.fragment,m),$a=c(m),y(As.$$.fragment,m),bs=c(m),ts=o(m,"P",{});var Rs=l(ts);vr=n(Rs,"And that\u2019s it! Once you run this, you\u2019ll have a model and results that are pretty similar to the ones we obtained with the "),Jt=o(Rs,"CODE",{});var Pr=l(Jt);lt=n(Pr,"Trainer"),Pr.forEach(t),yr=n(Rs,"."),Rs.forEach(t),this.h()},h(){f($,"id","finetuning-mt5-with-accelerate"),f($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($,"href","#finetuning-mt5-with-accelerate"),f(u,"class","relative group"),f(I,"href","/course/chapter3"),f(Q,"id","preparing-everything-for-training"),f(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Q,"href","#preparing-everything-for-training"),f(X,"class","relative group"),f(_s,"id","training-loop"),f(_s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(_s,"href","#training-loop"),f(Es,"class","relative group")},m(m,q){i(m,u,q),s(u,$),s($,d),k(E,d,null),s(u,C),s(u,x),s(x,A),i(m,P,q),i(m,T,q),s(T,S),s(T,I),s(I,D),s(T,O),s(T,F),s(F,V),s(T,J),i(m,Z,q),i(m,X,q),s(X,Q),s(Q,L),k(M,L,null),s(X,se),s(X,R),s(R,Y),i(m,ae,q),i(m,W,q),s(W,ie),s(W,B),s(B,ne),s(W,fe),s(W,be),s(be,Se),s(W,G),i(m,Le,q),k(ge,m,q),i(m,re,q),i(m,ve,q),s(ve,Te),s(ve,oe),s(oe,ye),s(ve,$e),i(m,_e,q),k(le,m,q),i(m,pe,q),i(m,me,q),s(me,b),i(m,N,q),k(we,m,q),i(m,he,q),i(m,ce,q),s(ce,Ie),s(ce,w),s(w,H),s(ce,Ge),i(m,xe,q),k(ee,m,q),i(m,te,q),i(m,Ee,q),s(Ee,Be),s(Ee,ke),s(ke,Xe),s(Ee,Ye),i(m,rs,q),k(Ce,m,q),i(m,We,q),k(os,m,q),i(m,cs,q),i(m,Ke,q),s(Ke,Xs),i(m,us,q),i(m,Pe,q),s(Pe,ks),s(ks,zs),s(Pe,Ne),s(Pe,ds),s(ds,U),s(Pe,ue),s(Pe,Ve),s(Ve,Ys),i(m,de,q),i(m,fs,q),s(fs,ls),i(m,Ds,q),k(Ae,m,q),i(m,Ks,q),i(m,is,q),s(is,qe),i(m,Vs,q),k(Je,m,q),i(m,Js,q),i(m,He,q),s(He,Zs),s(He,Ze),s(Ze,Qs),s(He,je),s(He,js),s(js,Oe),s(He,bt),i(m,$s,q),i(m,xs,q),s(xs,pn),i(m,vt,q),k(et,m,q),i(m,da,q),k(gs,m,q),i(m,fa,q),i(m,yt,q),s(yt,Rt),i(m,ga,q),k(st,m,q),i(m,Ft,q),i(m,Qe,q),s(Qe,_a),s(Qe,ze),s(ze,mn),s(Qe,Gt),i(m,wa,q),i(m,Es,q),s(Es,_s),s(_s,Nt),k(tt,Nt,null),s(Es,Ht),s(Es,Ut),s(Ut,hn),i(m,kt,q),i(m,jt,q),s(jt,cn),i(m,$t,q),i(m,es,q),s(es,at),s(at,ba),s(at,Ts),s(Ts,va),s(at,xt),s(es,un),s(es,Et),s(Et,nt),s(es,ya),s(es,qs),s(qs,ka),s(es,ss),s(es,ps),s(ps,Mt),s(ps,Wt),s(Wt,dn),s(ps,Bt),s(ps,Xt),s(Xt,fn),s(ps,Yt),s(ps,Kt),s(Kt,gn),s(ps,ja),i(m,Ss,q),i(m,rt,q),s(rt,ot),i(m,Vt,q),k(ws,m,q),i(m,$a,q),k(As,m,q),i(m,bs,q),i(m,ts,q),s(ts,vr),s(ts,Jt),s(Jt,lt),s(ts,yr),Zt=!0},i(m){Zt||(g(E.$$.fragment,m),g(M.$$.fragment,m),g(ge.$$.fragment,m),g(le.$$.fragment,m),g(we.$$.fragment,m),g(ee.$$.fragment,m),g(Ce.$$.fragment,m),g(os.$$.fragment,m),g(Ae.$$.fragment,m),g(Je.$$.fragment,m),g(et.$$.fragment,m),g(gs.$$.fragment,m),g(st.$$.fragment,m),g(tt.$$.fragment,m),g(ws.$$.fragment,m),g(As.$$.fragment,m),Zt=!0)},o(m){_(E.$$.fragment,m),_(M.$$.fragment,m),_(ge.$$.fragment,m),_(le.$$.fragment,m),_(we.$$.fragment,m),_(ee.$$.fragment,m),_(Ce.$$.fragment,m),_(os.$$.fragment,m),_(Ae.$$.fragment,m),_(Je.$$.fragment,m),_(et.$$.fragment,m),_(gs.$$.fragment,m),_(st.$$.fragment,m),_(tt.$$.fragment,m),_(ws.$$.fragment,m),_(As.$$.fragment,m),Zt=!1},d(m){m&&t(u),j(E),m&&t(P),m&&t(T),m&&t(Z),m&&t(X),j(M),m&&t(ae),m&&t(W),m&&t(Le),j(ge,m),m&&t(re),m&&t(ve),m&&t(_e),j(le,m),m&&t(pe),m&&t(me),m&&t(N),j(we,m),m&&t(he),m&&t(ce),m&&t(xe),j(ee,m),m&&t(te),m&&t(Ee),m&&t(rs),j(Ce,m),m&&t(We),j(os,m),m&&t(cs),m&&t(Ke),m&&t(us),m&&t(Pe),m&&t(de),m&&t(fs),m&&t(Ds),j(Ae,m),m&&t(Ks),m&&t(is),m&&t(Vs),j(Je,m),m&&t(Js),m&&t(He),m&&t($s),m&&t(xs),m&&t(vt),j(et,m),m&&t(da),j(gs,m),m&&t(fa),m&&t(yt),m&&t(ga),j(st,m),m&&t(Ft),m&&t(Qe),m&&t(wa),m&&t(Es),j(tt),m&&t(kt),m&&t(jt),m&&t($t),m&&t(es),m&&t(Ss),m&&t(rt),m&&t(Vt),j(ws,m),m&&t($a),j(As,m),m&&t(bs),m&&t(ts)}}}function Sg(K){let u,$,d,E,C;return{c(){u=r("p"),$=a("\u{1F6A8} If you\u2019re training on a TPU, you\u2019ll need to move all the code above into a dedicated training function. See "),d=r("a"),E=a("Chapter 3"),C=a(" for more details."),this.h()},l(x){u=o(x,"P",{});var A=l(u);$=n(A,"\u{1F6A8} If you\u2019re training on a TPU, you\u2019ll need to move all the code above into a dedicated training function. See "),d=o(A,"A",{href:!0});var P=l(d);E=n(P,"Chapter 3"),P.forEach(t),C=n(A," for more details."),A.forEach(t),this.h()},h(){f(d,"href","/course/chapter3")},m(x,A){i(x,u,A),s(u,$),s(u,d),s(d,E),s(u,C)},d(x){x&&t(u)}}}function Ag(K){let u,$,d,E,C,x,A,P,T,S,I,D,O,F,V,J,Z,X,Q,L,M,se,R,Y,ae,W,ie,B,ne,fe,be,Se,G,Le,ge,re,ve,Te,oe,ye,$e,_e,le,pe,me,b,N,we,he,ce,Ie,w,H,Ge,xe,ee,te,Ee,Be,ke,Xe,Ye,rs,Ce,We,os,cs,Ke,Xs,us,Pe,ks,zs,Ne,ds,U,ue,Ve,Ys,de,fs,ls,Ds,Ae,Ks,is,qe,Vs,Je,Js,He,Zs,Ze,Qs,je,js,Oe,bt,$s,xs,pn,vt,et,da,gs,fa,yt,Rt,ga,st,Ft,Qe,_a,ze,mn,Gt,wa,Es,_s,Nt,tt,Ht,Ut,hn,kt,jt,cn,$t,es,at,ba,Ts,va,xt,un,Et,nt,ya,qs,ka,ss,ps,Mt,Wt,dn,Bt,Xt,fn,Yt,Kt,gn,ja,Ss,rt,ot,Vt,ws,$a,As,bs,ts,vr,Jt,lt,yr,Zt,m,q,xa,kr,jr,it,Tt,_n,pt,$r,wn,Os,ms,Ea,qt,Ta,qa,xr,Qt,mt,za,ea,Er,zt,bn,ht,Da,Cs,sa,Dt,Tr,Sa,qr,zr,ct,Dr,Aa,Ue,Ps,ta,St,Ls,Oa,Ca,Sr,Ar,hs,Or,Cr,Is,aa,Rs,Pr,Kp,Co,Vp,Jp,Lr,Zp,Qp,na,Ir,vn,em,sm,yn,tm,Po,am,nm,rm,Rr,om,lm,ra,Fr,kn,im,pm,Lo,mm,hm,Gr,cm,um,oa,Nr,jn,dm,fm,Io,gm,_m,Hr,wm,bm,la,Ur,$n,vm,ym,Ro,km,jm,Mr,$m,Wl,Wr,xm,Bl,Pa,Em,Fo,Tm,qm,Xl,ia,xn,gu,zm,En,_u,Yl,Br,Dm,Kl,La,Vl,pa,Ia,Go,Tn,Sm,No,Am,Jl,qn,Zl,Ra,Om,Ho,Cm,Pm,Ql,zn,ei,Fa,si,Xr,Lm,ti,Dn,ai,Sn,ni,vs,Im,Uo,Rm,Fm,Mo,Gm,Nm,Yr,Hm,Um,Wo,Mm,Wm,ri,An,oi,On,li,Fs,Bm,Bo,Xm,Ym,Xo,Km,Vm,Kr,Jm,Zm,ii,Ga,Qm,Yo,eh,sh,pi,Cn,mi,At,th,Ko,ah,nh,Vo,rh,oh,hi,Ot,lh,Jo,ih,ph,Zo,mh,hh,ci,Pn,ui,Vr,ch,di,Na,fi,ma,Ha,Qo,Ln,uh,el,dh,gi,In,_i,Jr,fh,wi,Ua,gh,Rn,_h,wh,bi,Fn,vi,Ct,bh,sl,vh,yh,tl,kh,jh,yi,Ma,ki,Gn,$h,ji,ig='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">R</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">l</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mtext>\u2009</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi></mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mtext>\u2009</mtext><mi mathvariant="normal">n</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>\u2009</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mtext>\u2009</mtext><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">f</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">e</mi><mtext>\u2009</mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\mathrm{Recall} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, reference\\, summary}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord"><span class="mord mathrm">Recall</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2519em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Total</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">in</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">reference</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">summary</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">overlapping</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>',$i,Nn,xh,xi,pg='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">c</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">s</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">n</mi></mrow><mo>=</mo><mfrac><mrow><mi mathvariant="normal">N</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">v</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">g</mi><mtext>\u2009</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi></mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">l</mi><mtext>\u2009</mtext><mi mathvariant="normal">n</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mtext>\u2009</mtext><mi mathvariant="normal">o</mi><mi mathvariant="normal">f</mi><mtext>\u2009</mtext><mi mathvariant="normal">w</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">s</mi><mtext>\u2009</mtext><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mtext>\u2009</mtext><mi mathvariant="normal">g</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">d</mi><mtext>\u2009</mtext><mi mathvariant="normal">s</mi><mi mathvariant="normal">u</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">y</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \\mathrm{Precision} = \\frac{\\mathrm{Number\\,of\\,overlapping\\, words}}{\\mathrm{Total\\, number\\, of\\, words\\, in\\, generated\\, summary}} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord"><span class="mord mathrm">Precision</span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.2519em;vertical-align:-0.8804em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Total</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">in</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">generated</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">summary</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">Number</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.07778em;">of</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm" style="margin-right:0.01389em;">overlapping</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathrm">words</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>',Ei,Wa,Eh,al,Th,qh,Ti,Hn,qi,Zr,zh,zi,Un,Di,Ba,Dh,nl,Sh,Ah,Si,Mn,Ai,Wn,Oi,as,Oh,rl,Ch,Ph,ol,Lh,Ih,ll,Rh,Fh,il,Gh,Nh,pl,Hh,Uh,Ci,Bn,Pi,Xn,Li,ns,Mh,ml,Wh,Bh,hl,Xh,Yh,cl,Kh,Vh,ul,Jh,Zh,dl,Qh,ec,Ii,Xa,Ri,Qr,sc,Fi,ha,Ya,fl,Yn,tc,gl,ac,Gi,Gs,nc,_l,rc,oc,wl,lc,ic,bl,pc,mc,Ni,Kn,Hi,eo,hc,Ui,Vn,Mi,Ka,cc,vl,uc,dc,Wi,Jn,Bi,Zn,Xi,so,fc,Yi,Qn,Ki,to,gc,Vi,er,Ji,sr,Zi,Va,_c,yl,wc,bc,Qi,ut,dt,ao,Ja,ep,no,vc,sp,tr,tp,ro,yc,ap,ar,np,oo,Za,kc,lo,jc,$c,rp,Ns,xc,kl,Ec,Tc,jl,qc,zc,$l,Dc,Sc,op,ft,gt,io,po,Ac,lp,nr,ip,Pt,Oc,xl,Cc,Pc,El,Lc,Ic,pp,rr,mp,or,hp,De,Rc,Tl,Fc,Gc,ql,Nc,Hc,zl,Uc,Mc,Dl,Wc,Bc,Sl,Xc,Yc,Al,Kc,Vc,Ol,Jc,Zc,Cl,Qc,eu,cp,_t,wt,mo,ho,ca,Qa,Pl,lr,su,Ll,tu,up,en,au,Il,nu,ru,dp,ir,fp,co,ou,gp,pr,_p,uo,lu,wp,mr,bp,hr,vp,sn,iu,Rl,pu,mu,yp,cr,kp,ur,jp,fo,hu,$p,go,cu,xp;d=new fg({props:{fw:K[0]}}),P=new Bs({});const wu=[_g,gg],dr=[];function bu(e,p){return e[0]==="pt"?0:1}O=bu(K),F=dr[O]=wu[O](K),se=new fu({props:{id:"yHnr5Dk2zCI"}}),_e=new Bs({}),H=new z({props:{code:`from datasets import load_dataset

spanish_dataset = load_dataset("amazon_reviews_multi", "es")
english_dataset = load_dataset("amazon_reviews_multi", "en")
english_dataset`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

spanish_dataset = load_dataset(<span class="hljs-string">&quot;amazon_reviews_multi&quot;</span>, <span class="hljs-string">&quot;es&quot;</span>)
english_dataset = load_dataset(<span class="hljs-string">&quot;amazon_reviews_multi&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>)
english_dataset`}}),xe=new z({props:{code:`DatasetDict({
    train: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 200000
    })
    validation: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
    test: Dataset({
        features: ['review_id', 'product_id', 'reviewer_id', 'stars', 'review_body', 'review_title', 'language', 'product_category'],
        num_rows: 5000
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">200000</span>
    })
    validation: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">5000</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;review_id&#x27;</span>, <span class="hljs-string">&#x27;product_id&#x27;</span>, <span class="hljs-string">&#x27;reviewer_id&#x27;</span>, <span class="hljs-string">&#x27;stars&#x27;</span>, <span class="hljs-string">&#x27;review_body&#x27;</span>, <span class="hljs-string">&#x27;review_title&#x27;</span>, <span class="hljs-string">&#x27;language&#x27;</span>, <span class="hljs-string">&#x27;product_category&#x27;</span>],
        num_rows: <span class="hljs-number">5000</span>
    })
})`}}),Ve=new z({props:{code:`def show_samples(dataset, num_samples=3, seed=42):
    sample = dataset["train"].shuffle(seed=seed).select(range(num_samples))
    for example in sample:
        print(f"\\n'>> Title: {example['review_title']}'")
        print(f"'>> Review: {example['review_body']}'")


show_samples(english_dataset)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">show_samples</span>(<span class="hljs-params">dataset, num_samples=<span class="hljs-number">3</span>, seed=<span class="hljs-number">42</span></span>):
    sample = dataset[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=seed).select(<span class="hljs-built_in">range</span>(num_samples))
    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> sample:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt; Title: <span class="hljs-subst">{example[<span class="hljs-string">&#x27;review_title&#x27;</span>]}</span>&#x27;&quot;</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt; Review: <span class="hljs-subst">{example[<span class="hljs-string">&#x27;review_body&#x27;</span>]}</span>&#x27;&quot;</span>)


show_samples(english_dataset)`}}),de=new z({props:{code:`'>> Title: Worked in front position, not rear'
'>> Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.'

'>> Title: meh'
'>> Review: Does it\u2019s job and it\u2019s gorgeous but mine is falling apart, I had to basically put it together again with hot glue'

'>> Title: Can\\'t beat these for the money'
'>> Review: Bought this for handling miscellaneous aircraft parts and hanger "stuff" that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\'t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\'s heavy duty enough to hold metal parts, but being made of plastic it\\'s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\'t beat it. Best one of these I\\'ve bought to date-- and I\\'ve been using some version of these for over forty years.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: Worked in front position, not rear&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: 3 stars because these are not rear brakes as stated in the item description. At least the mount adapter only worked on the front fork of the bike that I got it for.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: meh&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Does it\u2019s job and it\u2019s gorgeous but mine is falling apart, I had to basically put it together again with hot glue&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Can\\&#x27;t beat these for the money&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Bought this for handling miscellaneous aircraft parts and hanger &quot;stuff&quot; that I needed to organize; it really fit the bill. The unit arrived quickly, was well packaged and arrived intact (always a good sign). There are five wall mounts-- three on the top and two on the bottom. I wanted to mount it on the wall, so all I had to do was to remove the top two layers of plastic drawers, as well as the bottom corner drawers, place it when I wanted and mark it; I then used some of the new plastic screw in wall anchors (the 50 pound variety) and it easily mounted to the wall. Some have remarked that they wanted dividers for the drawers, and that they made those. Good idea. My application was that I needed something that I can see the contents at about eye level, so I wanted the fuller-sized drawers. I also like that these are the new plastic that doesn\\&#x27;t get brittle and split like my older plastic drawers did. I like the all-plastic construction. It\\&#x27;s heavy duty enough to hold metal parts, but being made of plastic it\\&#x27;s not as heavy as a metal frame, so you can easily mount it to the wall and still load it up with heavy stuff, or light stuff. No problem there. For the money, you can\\&#x27;t beat it. Best one of these I\\&#x27;ve bought to date-- and I\\&#x27;ve been using some version of these for over forty years.&#x27;</span>`}}),ls=new ln({props:{$$slots:{default:[wg]},$$scope:{ctx:K}}}),Ze=new z({props:{code:`english_dataset.set_format("pandas")
english_df = english_dataset["train"][:]
# Show counts for top 20 products
english_df["product_category"].value_counts()[:20]`,highlighted:`english_dataset.set_format(<span class="hljs-string">&quot;pandas&quot;</span>)
english_df = english_dataset[<span class="hljs-string">&quot;train&quot;</span>][:]
<span class="hljs-comment"># Show counts for top 20 products</span>
english_df[<span class="hljs-string">&quot;product_category&quot;</span>].value_counts()[:<span class="hljs-number">20</span>]`}}),je=new z({props:{code:`home                      17679
apparel                   15951
wireless                  15717
other                     13418
beauty                    12091
drugstore                 11730
kitchen                   10382
toy                        8745
sports                     8277
automotive                 7506
lawn_and_garden            7327
home_improvement           7136
pet_products               7082
digital_ebook_purchase     6749
pc                         6401
electronics                6186
office_product             5521
shoes                      5197
grocery                    4730
book                       3756
Name: product_category, dtype: int64`,highlighted:`home                      <span class="hljs-number">17679</span>
apparel                   <span class="hljs-number">15951</span>
wireless                  <span class="hljs-number">15717</span>
other                     <span class="hljs-number">13418</span>
beauty                    <span class="hljs-number">12091</span>
drugstore                 <span class="hljs-number">11730</span>
kitchen                   <span class="hljs-number">10382</span>
toy                        <span class="hljs-number">8745</span>
sports                     <span class="hljs-number">8277</span>
automotive                 <span class="hljs-number">7506</span>
lawn_and_garden            <span class="hljs-number">7327</span>
home_improvement           <span class="hljs-number">7136</span>
pet_products               <span class="hljs-number">7082</span>
digital_ebook_purchase     <span class="hljs-number">6749</span>
pc                         <span class="hljs-number">6401</span>
electronics                <span class="hljs-number">6186</span>
office_product             <span class="hljs-number">5521</span>
shoes                      <span class="hljs-number">5197</span>
grocery                    <span class="hljs-number">4730</span>
book                       <span class="hljs-number">3756</span>
Name: product_category, dtype: int64`}}),Qe=new z({props:{code:`def filter_books(example):
    return (
        example["product_category"] == "book"
        or example["product_category"] == "digital_ebook_purchase"
    )`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">filter_books</span>(<span class="hljs-params">example</span>):
    <span class="hljs-keyword">return</span> (
        example[<span class="hljs-string">&quot;product_category&quot;</span>] == <span class="hljs-string">&quot;book&quot;</span>
        <span class="hljs-keyword">or</span> example[<span class="hljs-string">&quot;product_category&quot;</span>] == <span class="hljs-string">&quot;digital_ebook_purchase&quot;</span>
    )`}}),Ts=new z({props:{code:"english_dataset.reset_format()",highlighted:"english_dataset.reset_format()"}}),nt=new z({props:{code:`spanish_books = spanish_dataset.filter(filter_books)
english_books = english_dataset.filter(filter_books)
show_samples(english_books)`,highlighted:`spanish_books = spanish_dataset.<span class="hljs-built_in">filter</span>(filter_books)
english_books = english_dataset.<span class="hljs-built_in">filter</span>(filter_books)
show_samples(english_books)`}}),qs=new z({props:{code:`'>> Title: I\\'m dissapointed.'
'>> Review: I guess I had higher expectations for this book from the reviews. I really thought I\\'d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\'m dissapointed.'

'>> Title: Good art, good price, poor design'
'>> Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\'s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar'

'>> Title: Helpful'
'>> Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: I\\&#x27;m dissapointed.&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I guess I had higher expectations for this book from the reviews. I really thought I\\&#x27;d at least like it. The plot idea was great. I loved Ash but, it just didnt go anywhere. Most of the book was about their radio show and talking to callers. I wanted the author to dig deeper so we could really get to know the characters. All we know about Grace is that she is attractive looking, Latino and is kind of a brat. I\\&#x27;m dissapointed.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Good art, good price, poor design&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I had gotten the DC Vintage calendar the past two years, but it was on backorder forever this year and I saw they had shrunk the dimensions for no good reason. This one has good art choices but the design has the fold going through the picture, so it\\&#x27;s less aesthetically pleasing, especially if you want to keep a picture to hang. For the price, a good calendar&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: Helpful&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Nearly all the tips useful and. I consider myself an intermediate to advanced user of OneNote. I would highly recommend.&#x27;</span>`}}),Ss=new z({props:{code:`from datasets import concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

for split in english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=42)

# Peek at a few examples
show_samples(books_dataset)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> concatenate_datasets, DatasetDict

books_dataset = DatasetDict()

<span class="hljs-keyword">for</span> split <span class="hljs-keyword">in</span> english_books.keys():
    books_dataset[split] = concatenate_datasets(
        [english_books[split], spanish_books[split]]
    )
    books_dataset[split] = books_dataset[split].shuffle(seed=<span class="hljs-number">42</span>)

<span class="hljs-comment"># Peek at a few examples</span>
show_samples(books_dataset)`}}),ot=new z({props:{code:`'>> Title: Easy to follow!!!!'
'>> Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.'

'>> Title: PARCIALMENTE DA\xD1ADO'
'>> Review: Me lleg\xF3 el d\xEDa que tocaba, junto a otros libros que ped\xED, pero la caja lleg\xF3 en mal estado lo cual da\xF1\xF3 las esquinas de los libros porque ven\xEDan sin protecci\xF3n (forro).'

'>> Title: no lo he podido descargar'
'>> Review: igual que el anterior'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt; Title: Easy to follow!!!!&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: I loved The dash diet weight loss Solution. Never hungry. I would recommend this diet. Also the menus are well rounded. Try it. Has lots of the information need thanks.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: PARCIALMENTE DA\xD1ADO&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: Me lleg\xF3 el d\xEDa que tocaba, junto a otros libros que ped\xED, pero la caja lleg\xF3 en mal estado lo cual da\xF1\xF3 las esquinas de los libros porque ven\xEDan sin protecci\xF3n (forro).&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt; Title: no lo he podido descargar&#x27;</span>
<span class="hljs-string">&#x27;&gt;&gt; Review: igual que el anterior&#x27;</span>`}}),Tt=new z({props:{code:'books_dataset = books_dataset.filter(lambda x: len(x["review_title"].split()) > 2)',highlighted:'books_dataset = books_dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x[<span class="hljs-string">&quot;review_title&quot;</span>].split()) &gt; <span class="hljs-number">2</span>)'}}),qt=new Bs({}),La=new ln({props:{$$slots:{default:[bg]},$$scope:{ctx:K}}}),Tn=new Bs({}),qn=new fu({props:{id:"1m7BerpSq8A"}}),zn=new z({props:{code:`from transformers import AutoTokenizer

model_checkpoint = "google/mt5-small"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

model_checkpoint = <span class="hljs-string">&quot;google/mt5-small&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)`}}),Fa=new ln({props:{$$slots:{default:[vg]},$$scope:{ctx:K}}}),Dn=new z({props:{code:`inputs = tokenizer("I loved reading the Hunger Games!")
inputs`,highlighted:`inputs = tokenizer(<span class="hljs-string">&quot;I loved reading the Hunger Games!&quot;</span>)
inputs`}}),Sn=new z({props:{code:"{'input_ids': [336, 259, 28387, 11807, 287, 62893, 295, 12507, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}",highlighted:'{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">336</span>, <span class="hljs-number">259</span>, <span class="hljs-number">28387</span>, <span class="hljs-number">11807</span>, <span class="hljs-number">287</span>, <span class="hljs-number">62893</span>, <span class="hljs-number">295</span>, <span class="hljs-number">12507</span>, <span class="hljs-number">1</span>], <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}'}}),An=new z({props:{code:"tokenizer.convert_ids_to_tokens(inputs.input_ids)",highlighted:"tokenizer.convert_ids_to_tokens(inputs.input_ids)"}}),On=new z({props:{code:"['\u2581I', '\u2581', 'loved', '\u2581reading', '\u2581the', '\u2581Hung', 'er', '\u2581Games', '</s>']",highlighted:'[<span class="hljs-string">&#x27;\u2581I&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;loved&#x27;</span>, <span class="hljs-string">&#x27;\u2581reading&#x27;</span>, <span class="hljs-string">&#x27;\u2581the&#x27;</span>, <span class="hljs-string">&#x27;\u2581Hung&#x27;</span>, <span class="hljs-string">&#x27;er&#x27;</span>, <span class="hljs-string">&#x27;\u2581Games&#x27;</span>, <span class="hljs-string">&#x27;&lt;/s&gt;&#x27;</span>]'}}),Cn=new z({props:{code:`max_input_length = 512
max_target_length = 30


def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["review_body"],
        max_length=max_input_length,
        truncation=True,
    )
    labels = tokenizer(
        examples["review_title"], max_length=max_target_length, truncation=True
    )
    model_inputs["labels"] = labels["input_ids"]
    model_inputs["labels_mask"] = labels["attention_mask"]
    return model_inputs`,highlighted:`max_input_length = <span class="hljs-number">512</span>
max_target_length = <span class="hljs-number">30</span>


<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    model_inputs = tokenizer(
        examples[<span class="hljs-string">&quot;review_body&quot;</span>],
        max_length=max_input_length,
        truncation=<span class="hljs-literal">True</span>,
    )
    labels = tokenizer(
        examples[<span class="hljs-string">&quot;review_title&quot;</span>], max_length=max_target_length, truncation=<span class="hljs-literal">True</span>
    )
    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
    model_inputs[<span class="hljs-string">&quot;labels_mask&quot;</span>] = labels[<span class="hljs-string">&quot;attention_mask&quot;</span>]
    <span class="hljs-keyword">return</span> model_inputs`}}),Pn=new z({props:{code:"tokenized_datasets = books_dataset.map(preprocess_function, batched=True)",highlighted:'tokenized_datasets = books_dataset.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),Na=new ln({props:{$$slots:{default:[yg]},$$scope:{ctx:K}}}),Ln=new Bs({}),In=new fu({props:{id:"TMshhnrEXlg"}}),Fn=new z({props:{code:`generated_summary = "I absolutely loved reading the Hunger Games"
reference_summary = "I loved reading the Hunger Games"`,highlighted:`generated_summary = <span class="hljs-string">&quot;I absolutely loved reading the Hunger Games&quot;</span>
reference_summary = <span class="hljs-string">&quot;I loved reading the Hunger Games&quot;</span>`}}),Ma=new ln({props:{$$slots:{default:[kg]},$$scope:{ctx:K}}}),Hn=new z({props:{code:"!pip install rouge_score",highlighted:"!pip install rouge_score"}}),Un=new z({props:{code:`import evaluate

rouge_score = evaluate.load("rouge")`,highlighted:`<span class="hljs-keyword">import</span> evaluate

rouge_score = evaluate.load(<span class="hljs-string">&quot;rouge&quot;</span>)`}}),Mn=new z({props:{code:`scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores`,highlighted:`scores = rouge_score.compute(
    predictions=[generated_summary], references=[reference_summary]
)
scores`}}),Wn=new z({props:{code:`{'rouge1': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rouge2': AggregateScore(low=Score(precision=0.67, recall=0.8, fmeasure=0.73), mid=Score(precision=0.67, recall=0.8, fmeasure=0.73), high=Score(precision=0.67, recall=0.8, fmeasure=0.73)),
 'rougeL': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92)),
 'rougeLsum': AggregateScore(low=Score(precision=0.86, recall=1.0, fmeasure=0.92), mid=Score(precision=0.86, recall=1.0, fmeasure=0.92), high=Score(precision=0.86, recall=1.0, fmeasure=0.92))}`,highlighted:`{<span class="hljs-string">&#x27;rouge1&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)),
 <span class="hljs-string">&#x27;rouge2&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>), mid=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>), high=Score(precision=<span class="hljs-number">0.67</span>, recall=<span class="hljs-number">0.8</span>, fmeasure=<span class="hljs-number">0.73</span>)),
 <span class="hljs-string">&#x27;rougeL&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)),
 <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: AggregateScore(low=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), mid=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>), high=Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>))}`}}),Bn=new z({props:{code:'scores["rouge1"].mid',highlighted:'scores[<span class="hljs-string">&quot;rouge1&quot;</span>].mid'}}),Xn=new z({props:{code:"Score(precision=0.86, recall=1.0, fmeasure=0.92)",highlighted:'Score(precision=<span class="hljs-number">0.86</span>, recall=<span class="hljs-number">1.0</span>, fmeasure=<span class="hljs-number">0.92</span>)'}}),Xa=new ln({props:{$$slots:{default:[jg]},$$scope:{ctx:K}}}),Yn=new Bs({}),Kn=new z({props:{code:"!pip install nltk",highlighted:"!pip install nltk"}}),Vn=new z({props:{code:`import nltk

nltk.download("punkt")`,highlighted:`<span class="hljs-keyword">import</span> nltk

nltk.download(<span class="hljs-string">&quot;punkt&quot;</span>)`}}),Jn=new z({props:{code:`from nltk.tokenize import sent_tokenize


def three_sentence_summary(text):
    return "\\n".join(sent_tokenize(text)[:3])


print(three_sentence_summary(books_dataset["train"][1]["review_body"]))`,highlighted:`<span class="hljs-keyword">from</span> nltk.tokenize <span class="hljs-keyword">import</span> sent_tokenize


<span class="hljs-keyword">def</span> <span class="hljs-title function_">three_sentence_summary</span>(<span class="hljs-params">text</span>):
    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;\\n&quot;</span>.join(sent_tokenize(text)[:<span class="hljs-number">3</span>])


<span class="hljs-built_in">print</span>(three_sentence_summary(books_dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">1</span>][<span class="hljs-string">&quot;review_body&quot;</span>]))`}}),Zn=new z({props:{code:`'I grew up reading Koontz, and years ago, I stopped,convinced i had "outgrown" him.'
'Still,when a friend was looking for something suspenseful too read, I suggested Koontz.'
'She found Strangers.'`,highlighted:`<span class="hljs-string">&#x27;I grew up reading Koontz, and years ago, I stopped,convinced i had &quot;outgrown&quot; him.&#x27;</span>
<span class="hljs-string">&#x27;Still,when a friend was looking for something suspenseful too read, I suggested Koontz.&#x27;</span>
<span class="hljs-string">&#x27;She found Strangers.&#x27;</span>`}}),Qn=new z({props:{code:`def evaluate_baseline(dataset, metric):
    summaries = [three_sentence_summary(text) for text in dataset["review_body"]]
    return metric.compute(predictions=summaries, references=dataset["review_title"])`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_baseline</span>(<span class="hljs-params">dataset, metric</span>):
    summaries = [three_sentence_summary(text) <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> dataset[<span class="hljs-string">&quot;review_body&quot;</span>]]
    <span class="hljs-keyword">return</span> metric.compute(predictions=summaries, references=dataset[<span class="hljs-string">&quot;review_title&quot;</span>])`}}),er=new z({props:{code:`import pandas as pd

score = evaluate_baseline(books_dataset["validation"], rouge_score)
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, round(score[rn].mid.fmeasure * 100, 2)) for rn in rouge_names)
rouge_dict`,highlighted:`<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

score = evaluate_baseline(books_dataset[<span class="hljs-string">&quot;validation&quot;</span>], rouge_score)
rouge_names = [<span class="hljs-string">&quot;rouge1&quot;</span>, <span class="hljs-string">&quot;rouge2&quot;</span>, <span class="hljs-string">&quot;rougeL&quot;</span>, <span class="hljs-string">&quot;rougeLsum&quot;</span>]
rouge_dict = <span class="hljs-built_in">dict</span>((rn, <span class="hljs-built_in">round</span>(score[rn].mid.fmeasure * <span class="hljs-number">100</span>, <span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> rn <span class="hljs-keyword">in</span> rouge_names)
rouge_dict`}}),sr=new z({props:{code:"{'rouge1': 16.74, 'rouge2': 8.83, 'rougeL': 15.6, 'rougeLsum': 15.96}",highlighted:'{<span class="hljs-string">&#x27;rouge1&#x27;</span>: <span class="hljs-number">16.74</span>, <span class="hljs-string">&#x27;rouge2&#x27;</span>: <span class="hljs-number">8.83</span>, <span class="hljs-string">&#x27;rougeL&#x27;</span>: <span class="hljs-number">15.6</span>, <span class="hljs-string">&#x27;rougeLsum&#x27;</span>: <span class="hljs-number">15.96</span>}'}});const vu=[xg,$g],fr=[];function yu(e,p){return e[0]==="pt"?0:1}ut=yu(K),dt=fr[ut]=vu[ut](K),Ja=new ln({props:{$$slots:{default:[Eg]},$$scope:{ctx:K}}}),tr=new z({props:{code:`from huggingface_hub import notebook_login

notebook_login()`,highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`}}),ar=new z({props:{code:"huggingface-cli login",highlighted:'huggingface-<span class="hljs-keyword">cli</span> login'}});let Re=K[0]==="pt"&&rg();const ku=[qg,Tg],gr=[];function ju(e,p){return e[0]==="pt"?0:1}ft=ju(K),gt=gr[ft]=ku[ft](K),nr=new z({props:{code:`tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset["train"].column_names
)`,highlighted:`tokenized_datasets = tokenized_datasets.remove_columns(
    books_dataset[<span class="hljs-string">&quot;train&quot;</span>].column_names
)`}}),rr=new z({props:{code:`features = [tokenized_datasets["train"][i] for i in range(2)]
data_collator(features)`,highlighted:`features = [tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>][i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>)]
data_collator(features)`}}),or=new z({props:{code:`{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'input_ids': tensor([[  1494,    259,   8622,    390,    259,    262,   2316,   3435,    955,
            772,    281,    772,   1617,    263,    305,  14701,    260,   1385,
           3031,    259,  24146,    332,   1037,    259,  43906,    305,    336,
            260,      1,      0,      0,      0,      0,      0,      0],
        [   259,  27531,  13483,    259,   7505,    260, 112240,  15192,    305,
          53198,    276,    259,  74060,    263,    260,    459,  25640,    776,
           2119,    336,    259,   2220,    259,  18896,    288,   4906,    288,
           1037,   3931,    260,   7083, 101476,   1143,    260,      1]]), 'labels': tensor([[ 7483,   259,  2364, 15695,     1,  -100],
        [  259, 27531, 13483,   259,  7505,     1]]), 'decoder_input_ids': tensor([[    0,  7483,   259,  2364, 15695,     1],
        [    0,   259, 27531, 13483,   259,  7505]])}`,highlighted:`{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
         <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
         <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">1494</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">8622</span>,    <span class="hljs-number">390</span>,    <span class="hljs-number">259</span>,    <span class="hljs-number">262</span>,   <span class="hljs-number">2316</span>,   <span class="hljs-number">3435</span>,    <span class="hljs-number">955</span>,
            <span class="hljs-number">772</span>,    <span class="hljs-number">281</span>,    <span class="hljs-number">772</span>,   <span class="hljs-number">1617</span>,    <span class="hljs-number">263</span>,    <span class="hljs-number">305</span>,  <span class="hljs-number">14701</span>,    <span class="hljs-number">260</span>,   <span class="hljs-number">1385</span>,
           <span class="hljs-number">3031</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">24146</span>,    <span class="hljs-number">332</span>,   <span class="hljs-number">1037</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">43906</span>,    <span class="hljs-number">305</span>,    <span class="hljs-number">336</span>,
            <span class="hljs-number">260</span>,      <span class="hljs-number">1</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>,      <span class="hljs-number">0</span>],
        [   <span class="hljs-number">259</span>,  <span class="hljs-number">27531</span>,  <span class="hljs-number">13483</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">7505</span>,    <span class="hljs-number">260</span>, <span class="hljs-number">112240</span>,  <span class="hljs-number">15192</span>,    <span class="hljs-number">305</span>,
          <span class="hljs-number">53198</span>,    <span class="hljs-number">276</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">74060</span>,    <span class="hljs-number">263</span>,    <span class="hljs-number">260</span>,    <span class="hljs-number">459</span>,  <span class="hljs-number">25640</span>,    <span class="hljs-number">776</span>,
           <span class="hljs-number">2119</span>,    <span class="hljs-number">336</span>,    <span class="hljs-number">259</span>,   <span class="hljs-number">2220</span>,    <span class="hljs-number">259</span>,  <span class="hljs-number">18896</span>,    <span class="hljs-number">288</span>,   <span class="hljs-number">4906</span>,    <span class="hljs-number">288</span>,
           <span class="hljs-number">1037</span>,   <span class="hljs-number">3931</span>,    <span class="hljs-number">260</span>,   <span class="hljs-number">7083</span>, <span class="hljs-number">101476</span>,   <span class="hljs-number">1143</span>,    <span class="hljs-number">260</span>,      <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;labels&#x27;</span>: tensor([[ <span class="hljs-number">7483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">2364</span>, <span class="hljs-number">15695</span>,     <span class="hljs-number">1</span>,  -<span class="hljs-number">100</span>],
        [  <span class="hljs-number">259</span>, <span class="hljs-number">27531</span>, <span class="hljs-number">13483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">7505</span>,     <span class="hljs-number">1</span>]]), <span class="hljs-string">&#x27;decoder_input_ids&#x27;</span>: tensor([[    <span class="hljs-number">0</span>,  <span class="hljs-number">7483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">2364</span>, <span class="hljs-number">15695</span>,     <span class="hljs-number">1</span>],
        [    <span class="hljs-number">0</span>,   <span class="hljs-number">259</span>, <span class="hljs-number">27531</span>, <span class="hljs-number">13483</span>,   <span class="hljs-number">259</span>,  <span class="hljs-number">7505</span>]])}`}});const $u=[Dg,zg],_r=[];function xu(e,p){return e[0]==="pt"?0:1}_t=xu(K),wt=_r[_t]=$u[_t](K);let Fe=K[0]==="pt"&&og(K);return lr=new Bs({}),ir=new z({props:{code:`from transformers import pipeline

hub_model_id = "huggingface-course/mt5-small-finetuned-amazon-en-es"
summarizer = pipeline("summarization", model=hub_model_id)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

hub_model_id = <span class="hljs-string">&quot;huggingface-course/mt5-small-finetuned-amazon-en-es&quot;</span>
summarizer = pipeline(<span class="hljs-string">&quot;summarization&quot;</span>, model=hub_model_id)`}}),pr=new z({props:{code:`def print_summary(idx):
    review = books_dataset["test"][idx]["review_body"]
    title = books_dataset["test"][idx]["review_title"]
    summary = summarizer(books_dataset["test"][idx]["review_body"])[0]["summary_text"]
    print(f"'>>> Review: {review}'")
    print(f"\\n'>>> Title: {title}'")
    print(f"\\n'>>> Summary: {summary}'")`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">print_summary</span>(<span class="hljs-params">idx</span>):
    review = books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_body&quot;</span>]
    title = books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_title&quot;</span>]
    summary = summarizer(books_dataset[<span class="hljs-string">&quot;test&quot;</span>][idx][<span class="hljs-string">&quot;review_body&quot;</span>])[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;summary_text&quot;</span>]
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;&#x27;&gt;&gt;&gt; Review: <span class="hljs-subst">{review}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Title: <span class="hljs-subst">{title}</span>&#x27;&quot;</span>)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\\n&#x27;&gt;&gt;&gt; Summary: <span class="hljs-subst">{summary}</span>&#x27;&quot;</span>)`}}),mr=new z({props:{code:"print_summary(100)",highlighted:'print_summary(<span class="hljs-number">100</span>)'}}),hr=new z({props:{code:`'>>> Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn\u2019t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It\u2019s also really expensive for what it is.'

'>>> Title: Not impressed at all... buy something else'

'>>> Summary: Nothing special at all about this product'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Nothing special at all about this product... the book is too small and stiff and hard to write in. The huge sticker on the back doesn\u2019t come off and looks super tacky. I would not purchase this again. I could have just bought a journal from the dollar store and it would be basically the same thing. It\u2019s also really expensive for what it is.&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Title: Not impressed at all... buy something else&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Summary: Nothing special at all about this product&#x27;</span>`}}),cr=new z({props:{code:"print_summary(0)",highlighted:'print_summary(<span class="hljs-number">0</span>)'}}),ur=new z({props:{code:`'>>> Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada'

'>>> Title: Buena literatura para adolescentes'

'>>> Summary: Muy facil de leer'`,highlighted:`<span class="hljs-string">&#x27;&gt;&gt;&gt; Review: Es una trilogia que se hace muy facil de leer. Me ha gustado, no me esperaba el final para nada&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Title: Buena literatura para adolescentes&#x27;</span>

<span class="hljs-string">&#x27;&gt;&gt;&gt; Summary: Muy facil de leer&#x27;</span>`}}),{c(){u=r("meta"),$=h(),v(d.$$.fragment),E=h(),C=r("h1"),x=r("a"),A=r("span"),v(P.$$.fragment),T=h(),S=r("span"),I=a("Summarization"),D=h(),F.c(),V=h(),J=r("p"),Z=a("In this section we\u2019ll take a look at how Transformer models can be used to condense long documents into summaries, a task known as "),X=r("em"),Q=a("text summarization"),L=a(". This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail."),M=h(),v(se.$$.fragment),R=h(),Y=r("p"),ae=a("Although there already exist various fine-tuned models for summarization on the "),W=r("a"),ie=a("Hugging Face Hub"),B=a(", almost all of these are only suitable for English documents. So, to add a twist in this section, we\u2019ll train a bilingual model for English and Spanish. By the end of this section, you\u2019ll have a "),ne=r("a"),fe=a("model"),be=a(" that can summarize customer reviews like the one shown here:"),Se=h(),G=r("iframe"),ge=h(),re=r("p"),ve=a("As we\u2019ll see, these summaries are concise because they\u2019re learned from the titles that customers provide in their product reviews. Let\u2019s start by putting together a suitable bilingual corpus for this task."),Te=h(),oe=r("h2"),ye=r("a"),$e=r("span"),v(_e.$$.fragment),le=h(),pe=r("span"),me=a("Preparing a multilingual corpus"),b=h(),N=r("p"),we=a("We\u2019ll use the "),he=r("a"),ce=a("Multilingual Amazon Reviews Corpus"),Ie=a(" to create our bilingual summarizer. This corpus consists of Amazon product reviews in six languages and is typically used to benchmark multilingual classifiers. However, since each review is accompanied by a short title, we can use the titles as the target summaries for our model to learn from! To get started, let\u2019s download the English and Spanish subsets from the Hugging Face Hub:"),w=h(),v(H.$$.fragment),Ge=h(),v(xe.$$.fragment),ee=h(),te=r("p"),Ee=a("As you can see, for each language there are 200,000 reviews for the "),Be=r("code"),ke=a("train"),Xe=a(" split, and 5,000 reviews for each of the "),Ye=r("code"),rs=a("validation"),Ce=a(" and "),We=r("code"),os=a("test"),cs=a(" splits. The review information we are interested in is contained in the "),Ke=r("code"),Xs=a("review_body"),us=a(" and "),Pe=r("code"),ks=a("review_title"),zs=a(" columns. Let\u2019s take a look at a few examples by creating a simple function that takes a random sample from the training set with the techniques we learned in "),Ne=r("a"),ds=a("Chapter 5"),U=a(":"),ue=h(),v(Ve.$$.fragment),Ys=h(),v(de.$$.fragment),fs=h(),v(ls.$$.fragment),Ds=h(),Ae=r("p"),Ks=a("This sample shows the diversity of reviews one typically finds online, ranging from positive to negative (and everything in between!). Although the example with the \u201Cmeh\u201D title is not very informative, the other titles look like decent summaries of the reviews themselves. Training a summarization model on all 400,000 reviews would take far too long on a single GPU, so instead we\u2019ll focus on generating summaries for a single domain of products. To get a feel for what domains we can choose from, let\u2019s convert "),is=r("code"),qe=a("english_dataset"),Vs=a(" to a "),Je=r("code"),Js=a("pandas.DataFrame"),He=a(" and compute the number of reviews per product category:"),Zs=h(),v(Ze.$$.fragment),Qs=h(),v(je.$$.fragment),js=h(),Oe=r("p"),bt=a("The most popular products in the English dataset are about household items, clothing, and wireless electronics. To stick with the Amazon theme, though, let\u2019s focus on summarizing book reviews \u2014 after all, this is what the company was founded on! We can see two product categories that fit the bill ("),$s=r("code"),xs=a("book"),pn=a(" and "),vt=r("code"),et=a("digital_ebook_purchase"),da=a("), so let\u2019s filter the datasets in both languages for just these products. As we saw in "),gs=r("a"),fa=a("Chapter 5"),yt=a(", the "),Rt=r("code"),ga=a("Dataset.filter()"),st=a(" function allows us to slice a dataset very efficiently, so we can define a simple function to do this:"),Ft=h(),v(Qe.$$.fragment),_a=h(),ze=r("p"),mn=a("Now when we apply this function to "),Gt=r("code"),wa=a("english_dataset"),Es=a(" and "),_s=r("code"),Nt=a("spanish_dataset"),tt=a(", the result will contain just those rows involving the book categories. Before applying the filter, let\u2019s switch the format of "),Ht=r("code"),Ut=a("english_dataset"),hn=a(" from "),kt=r("code"),jt=a('"pandas"'),cn=a(" back to "),$t=r("code"),es=a('"arrow"'),at=a(":"),ba=h(),v(Ts.$$.fragment),va=h(),xt=r("p"),un=a("We can then apply the filter function, and as a sanity check let\u2019s inspect a sample of reviews to see if they are indeed about books:"),Et=h(),v(nt.$$.fragment),ya=h(),v(qs.$$.fragment),ka=h(),ss=r("p"),ps=a("Okay, we can see that the reviews are not strictly about books and might refer to things like calendars and electronic applications such as OneNote. Nevertheless, the domain seems about right to train a summarization model on. Before we look at various models that are suitable for this task, we have one last bit of data preparation to do: combining the English and Spanish reviews as a single "),Mt=r("code"),Wt=a("DatasetDict"),dn=a(" object. \u{1F917} Datasets provides a handy "),Bt=r("code"),Xt=a("concatenate_datasets()"),fn=a(" function that (as the name suggests) will stack two "),Yt=r("code"),Kt=a("Dataset"),gn=a(" objects on top of each other. So, to create our bilingual dataset, we\u2019ll loop over each split, concatenate the datasets for that split, and shuffle the result to ensure our model doesn\u2019t overfit to a single language:"),ja=h(),v(Ss.$$.fragment),rt=h(),v(ot.$$.fragment),Vt=h(),ws=r("p"),$a=a("This certainly looks like a mix of English and Spanish reviews! Now that we have a training corpus, one final thing to check is the distribution of words in the reviews and their titles. This is especially important for summarization tasks, where short reference summaries in the data can bias the model to only output one or two words in the generated summaries. The plots below show the word distributions, and we can see that the titles are heavily skewed toward just 1-2 words:"),As=h(),bs=r("div"),ts=r("img"),Jt=h(),lt=r("img"),Zt=h(),m=r("p"),q=a("To deal with this, we\u2019ll filter out the examples with very short titles so that our model can produce more interesting summaries. Since we\u2019re dealing with English and Spanish texts, we can use a rough heuristic to split the titles on whitespace and then use our trusty "),xa=r("code"),kr=a("Dataset.filter()"),jr=a(" method as follows:"),it=h(),v(Tt.$$.fragment),_n=h(),pt=r("p"),$r=a("Now that we\u2019ve prepared our corpus, let\u2019s take a look at a few possible Transformer models that one might fine-tune on it!"),wn=h(),Os=r("h2"),ms=r("a"),Ea=r("span"),v(qt.$$.fragment),Ta=h(),qa=r("span"),xr=a("Models for text summarization"),Qt=h(),mt=r("p"),za=a("If you think about it, text summarization is a similar sort of task to machine translation: we have a body of text like a review that we\u2019d like to \u201Ctranslate\u201D into a shorter version that captures the salient features of the input. Accordingly, most Transformer models for summarization adopt the encoder-decoder architecture that we first encountered in "),ea=r("a"),Er=a("Chapter 1"),zt=a(", although there are some exceptions like the GPT family of models which can also be used for summarization in few-shot settings. The following table lists some popular pretrained models that can be fine-tuned for summarization."),bn=h(),ht=r("table"),Da=r("thead"),Cs=r("tr"),sa=r("th"),Dt=a("Transformer model"),Tr=h(),Sa=r("th"),qr=a("Description"),zr=h(),ct=r("th"),Dr=a("Multilingual?"),Aa=h(),Ue=r("tbody"),Ps=r("tr"),ta=r("td"),St=r("a"),Ls=a("GPT-2"),Oa=h(),Ca=r("td"),Sr=a("Although trained as an auto-regressive language model, you can make GPT-2 generate summaries by appending \u201CTL;DR\u201D at the end of the input text."),Ar=h(),hs=r("td"),Or=a("\u274C"),Cr=h(),Is=r("tr"),aa=r("td"),Rs=r("a"),Pr=a("PEGASUS"),Kp=h(),Co=r("td"),Vp=a("Uses a pretraining objective to predict masked sentences in multi-sentence texts. This pretraining objective is closer to summarization than vanilla language modeling and scores highly on popular benchmarks."),Jp=h(),Lr=r("td"),Zp=a("\u274C"),Qp=h(),na=r("tr"),Ir=r("td"),vn=r("a"),em=a("T5"),sm=h(),yn=r("td"),tm=a("A universal Transformer architecture that formulates all tasks in a text-to-text framework; e.g., the input format for the model to summarize a document is "),Po=r("code"),am=a("summarize: ARTICLE"),nm=a("."),rm=h(),Rr=r("td"),om=a("\u274C"),lm=h(),ra=r("tr"),Fr=r("td"),kn=r("a"),im=a("mT5"),pm=h(),Lo=r("td"),mm=a("A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages."),hm=h(),Gr=r("td"),cm=a("\u2705"),um=h(),oa=r("tr"),Nr=r("td"),jn=r("a"),dm=a("BART"),fm=h(),Io=r("td"),gm=a("A novel Transformer architecture with both an encoder and a decoder stack trained to reconstruct corrupted input that combines the pretraining schemes of BERT and GPT-2."),_m=h(),Hr=r("td"),wm=a("\u274C"),bm=h(),la=r("tr"),Ur=r("td"),$n=r("a"),vm=a("mBART-50"),ym=h(),Ro=r("td"),km=a("A multilingual version of BART, pretrained on 50 languages."),jm=h(),Mr=r("td"),$m=a("\u2705"),Wl=h(),Wr=r("p"),xm=a("As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a \u201Chigh-resource\u201D language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!"),Bl=h(),Pa=r("p"),Em=a("We\u2019ll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like "),Fo=r("code"),Tm=a("summarize:"),qm=a(" which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model!"),Xl=h(),ia=r("div"),xn=r("img"),zm=h(),En=r("img"),Yl=h(),Br=r("p"),Dm=a("mT5 doesn\u2019t use prefixes, but shares much of the versatility of T5 and has the advantage of being multilingual. Now that we\u2019ve picked a model, let\u2019s take a look at preparing our data for training."),Kl=h(),v(La.$$.fragment),Vl=h(),pa=r("h2"),Ia=r("a"),Go=r("span"),v(Tn.$$.fragment),Sm=h(),No=r("span"),Am=a("Preprocessing the data"),Jl=h(),v(qn.$$.fragment),Zl=h(),Ra=r("p"),Om=a("Our next task is to tokenize and encode our reviews and their titles. As usual, we begin by loading the tokenizer associated with the pretrained model checkpoint. We\u2019ll use "),Ho=r("code"),Cm=a("mt5-small"),Pm=a(" as our checkpoint so we can fine-tune the model in a reasonable amount of time:"),Ql=h(),v(zn.$$.fragment),ei=h(),v(Fa.$$.fragment),si=h(),Xr=r("p"),Lm=a("Let\u2019s test out the mT5 tokenizer on a small example:"),ti=h(),v(Dn.$$.fragment),ai=h(),v(Sn.$$.fragment),ni=h(),vs=r("p"),Im=a("Here we can see the familiar "),Uo=r("code"),Rm=a("input_ids"),Fm=a(" and "),Mo=r("code"),Gm=a("attention_mask"),Nm=a(" that we encountered in our first fine-tuning experiments back in "),Yr=r("a"),Hm=a("Chapter 3"),Um=a(". Let\u2019s decode these input IDs with the tokenizer\u2019s "),Wo=r("code"),Mm=a("convert_ids_to_tokens()"),Wm=a(" function to see what kind of tokenizer we\u2019re dealing with:"),ri=h(),v(An.$$.fragment),oi=h(),v(On.$$.fragment),li=h(),Fs=r("p"),Bm=a("The special Unicode character "),Bo=r("code"),Xm=a("\u2581"),Ym=a(" and end-of-sequence token "),Xo=r("code"),Km=a("</s>"),Vm=a(" indicate that we\u2019re dealing with the SentencePiece tokenizer, which is based on the Unigram segmentation algorithm discussed in "),Kr=r("a"),Jm=a("Chapter 6"),Zm=a(". Unigram is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters."),ii=h(),Ga=r("p"),Qm=a("To tokenize our corpus, we have to deal with a subtlety associated with summarization: because our labels are also text, it is possible that they exceed the model\u2019s maximum context size. This means we need to apply truncation to both the reviews and their titles to ensure we don\u2019t pass excessively long inputs to our model. The tokenizers in \u{1F917} Transformers provide a nifty "),Yo=r("code"),eh=a("text_target"),sh=a(" argument that allows you to tokenize the labels in parallel to the inputs. Here is an example of how the inputs and targets are processed for mT5:"),pi=h(),v(Cn.$$.fragment),mi=h(),At=r("p"),th=a("Let\u2019s walk through this code to understand what\u2019s happening. The first thing we\u2019ve done is define values for "),Ko=r("code"),ah=a("max_input_length"),nh=a(" and "),Vo=r("code"),rh=a("max_target_length"),oh=a(", which set the upper limits for how long our reviews and titles can be. Since the review body is typically much larger than the title, we\u2019ve scaled these values accordingly."),hi=h(),Ot=r("p"),lh=a("With "),Jo=r("code"),ih=a("preprocess_function()"),ph=a(", it is then a simple matter to tokenize the whole corpus using the handy "),Zo=r("code"),mh=a("Dataset.map()"),hh=a(" function we\u2019ve used extensively throughout this course:"),ci=h(),v(Pn.$$.fragment),ui=h(),Vr=r("p"),ch=a("Now that the corpus has been preprocessed, let\u2019s take a look at some metrics that are commonly used for summarization. As we\u2019ll see, there is no silver bullet when it comes to measuring the quality of machine-generated text."),di=h(),v(Na.$$.fragment),fi=h(),ma=r("h2"),Ha=r("a"),Qo=r("span"),v(Ln.$$.fragment),uh=h(),el=r("span"),dh=a("Metrics for text summarization"),gi=h(),v(In.$$.fragment),_i=h(),Jr=r("p"),fh=a("In comparison to most of the other tasks we\u2019ve covered in this course, measuring the performance of text generation tasks like summarization or translation is not as straightforward. For example, given a review like \u201CI loved reading the Hunger Games\u201D, there are multiple valid summaries, like \u201CI loved the Hunger Games\u201D or \u201CHunger Games is a great read\u201D. Clearly, applying some sort of exact match between the generated summary and the label is not a good solution \u2014 even humans would fare poorly under such a metric, because we all have our own writing style."),wi=h(),Ua=r("p"),gh=a("For summarization, one of the most commonly used metrics is the "),Rn=r("a"),_h=a("ROUGE score"),wh=a(" (short for Recall-Oriented Understudy for Gisting Evaluation). The basic idea behind this metric is to compare a generated summary against a set of reference summaries that are typically created by humans. To make this more precise, suppose we want to compare the following two summaries:"),bi=h(),v(Fn.$$.fragment),vi=h(),Ct=r("p"),bh=a("One way to compare them could be to count the number of overlapping words, which in this case would be 6. However, this is a bit crude, so instead ROUGE is based on computing the "),sl=r("em"),vh=a("precision"),yh=a(" and "),tl=r("em"),kh=a("recall"),jh=a(" scores for the overlap."),yi=h(),v(Ma.$$.fragment),ki=h(),Gn=r("p"),$h=a(`For ROUGE, recall measures how much of the reference summary is captured by the generated one. If we are just comparing words, recall can be calculated according to the following formula:
`),ji=new ag,$i=h(),Nn=r("p"),xh=a(`For our simple example above, this formula gives a perfect recall of 6/6 = 1; i.e., all the words in the reference summary have been produced by the model. This may sound great, but imagine if our generated summary had been \u201CI really really loved reading the Hunger Games all night\u201D. This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we also compute the precision, which in the ROUGE context measures how much of the generated summary was relevant:
`),xi=new ag,Ei=h(),Wa=r("p"),Eh=a("Applying this to our verbose summary gives a precision of 6/10  = 0.6, which is considerably worse than the precision of 6/7 = 0.86 obtained by our shorter one. In practice, both precision and recall are usually computed, and then the F1-score (the harmonic mean of precision and recall) is reported. We can do this easily in \u{1F917} Datasets by first installing the "),al=r("code"),Th=a("rouge_score"),qh=a(" package:"),Ti=h(),v(Hn.$$.fragment),qi=h(),Zr=r("p"),zh=a("and then loading the ROUGE metric as follows:"),zi=h(),v(Un.$$.fragment),Di=h(),Ba=r("p"),Dh=a("Then we can use the "),nl=r("code"),Sh=a("rouge_score.compute()"),Ah=a(" function to calculate all the metrics at once:"),Si=h(),v(Mn.$$.fragment),Ai=h(),v(Wn.$$.fragment),Oi=h(),as=r("p"),Oh=a("Whoa, there\u2019s a lot of information in that output \u2014 what does it all mean? First, \u{1F917} Datasets actually computes confidence intervals for precision, recall, and F1-score; these are the "),rl=r("code"),Ch=a("low"),Ph=a(", "),ol=r("code"),Lh=a("mid"),Ih=a(", and "),ll=r("code"),Rh=a("high"),Fh=a(" attributes you can see here. Moreover, \u{1F917} Datasets computes a variety of ROUGE scores which are based on different types of text granularity when comparing the generated and reference summaries. The "),il=r("code"),Gh=a("rouge1"),Nh=a(" variant is the overlap of unigrams \u2014 this is just a fancy way of saying the overlap of words and is exactly the metric we\u2019ve discussed above. To verify this, let\u2019s pull out the "),pl=r("code"),Hh=a("mid"),Uh=a(" value of our scores:"),Ci=h(),v(Bn.$$.fragment),Pi=h(),v(Xn.$$.fragment),Li=h(),ns=r("p"),Mh=a("Great, the precision and recall numbers match up! Now what about those other ROUGE scores? "),ml=r("code"),Wh=a("rouge2"),Bh=a(" measures the overlap between bigrams (think the overlap of pairs of words), while "),hl=r("code"),Xh=a("rougeL"),Yh=a(" and "),cl=r("code"),Kh=a("rougeLsum"),Vh=a(" measure the longest matching sequences of words by looking for the longest common substrings in the generated and reference summaries. The \u201Csum\u201D in "),ul=r("code"),Jh=a("rougeLsum"),Zh=a(" refers to the fact that this metric is computed over a whole summary, while "),dl=r("code"),Qh=a("rougeL"),ec=a(" is computed as the average over individual sentences."),Ii=h(),v(Xa.$$.fragment),Ri=h(),Qr=r("p"),sc=a("We\u2019ll use these ROUGE scores to track the performance of our model, but before doing that let\u2019s do something every good NLP practitioner should do: create a strong, yet simple baseline!"),Fi=h(),ha=r("h3"),Ya=r("a"),fl=r("span"),v(Yn.$$.fragment),tc=h(),gl=r("span"),ac=a("Creating a strong baseline"),Gi=h(),Gs=r("p"),nc=a("A common baseline for text summarization is to simply take the first three sentences of an article, often called the "),_l=r("em"),rc=a("lead-3"),oc=a(" baseline. We could use full stops to track the sentence boundaries, but this will fail on acronyms like \u201CU.S.\u201D or \u201CU.N.\u201D \u2014 so instead we\u2019ll use the "),wl=r("code"),lc=a("nltk"),ic=a(" library, which includes a better algorithm to handle these cases. You can install the package using "),bl=r("code"),pc=a("pip"),mc=a(" as follows:"),Ni=h(),v(Kn.$$.fragment),Hi=h(),eo=r("p"),hc=a("and then download the punctuation rules:"),Ui=h(),v(Vn.$$.fragment),Mi=h(),Ka=r("p"),cc=a("Next, we import the sentence tokenizer from "),vl=r("code"),uc=a("nltk"),dc=a(" and create a simple function to extract the first three sentences in a review. The convention in text summarization is to separate each summary with a newline, so let\u2019s also include this and test it on a training example:"),Wi=h(),v(Jn.$$.fragment),Bi=h(),v(Zn.$$.fragment),Xi=h(),so=r("p"),fc=a("This seems to work, so let\u2019s now implement a function that extracts these \u201Csummaries\u201D from a dataset and computes the ROUGE scores for the baseline:"),Yi=h(),v(Qn.$$.fragment),Ki=h(),to=r("p"),gc=a("We can then use this function to compute the ROUGE scores over the validation set and prettify them a bit using Pandas:"),Vi=h(),v(er.$$.fragment),Ji=h(),v(sr.$$.fragment),Zi=h(),Va=r("p"),_c=a("We can see that the "),yl=r("code"),wc=a("rouge2"),bc=a(" score is significantly lower than the rest; this likely reflects the fact that review titles are typically concise and so the lead-3 baseline is too verbose. Now that we have a good baseline to work from, let\u2019s turn our attention toward fine-tuning mT5!"),Qi=h(),dt.c(),ao=h(),v(Ja.$$.fragment),ep=h(),no=r("p"),vc=a("The next thing we need to do is log in to the Hugging Face Hub. If you\u2019re running this code in a notebook, you can do so with the following utility function:"),sp=h(),v(tr.$$.fragment),tp=h(),ro=r("p"),yc=a("which will display a widget where you can enter your credentials. Alternatively, you can run this command in your terminal and log in there:"),ap=h(),v(ar.$$.fragment),np=h(),Re&&Re.c(),oo=h(),Za=r("p"),kc=a("Next, we need to define a data collator for our sequence-to-sequence task. Since mT5 is an encoder-decoder Transformer model, one subtlety with preparing our batches is that during decoding we need to shift the labels to the right by one. This is required to ensure that the decoder only sees the previous ground truth labels and not the current or future ones, which would be easy for the model to memorize. This is similar to how masked self-attention is applied to the inputs in a task like "),lo=r("a"),jc=a("causal language modeling"),$c=a("."),rp=h(),Ns=r("p"),xc=a("Luckily, \u{1F917} Transformers provides a "),kl=r("code"),Ec=a("DataCollatorForSeq2Seq"),Tc=a(" collator that will dynamically pad the inputs and the labels for us. To instantiate this collator, we simply need to provide the "),jl=r("code"),qc=a("tokenizer"),zc=a(" and "),$l=r("code"),Dc=a("model"),Sc=a(":"),op=h(),gt.c(),io=h(),po=r("p"),Ac=a("Let\u2019s see what this collator produces when fed a small batch of examples. First, we need to remove the columns with strings because the collator won\u2019t know how to pad these elements:"),lp=h(),v(nr.$$.fragment),ip=h(),Pt=r("p"),Oc=a("Since the collator expects a list of "),xl=r("code"),Cc=a("dict"),Pc=a("s, where each "),El=r("code"),Lc=a("dict"),Ic=a(" represents a single example in the dataset, we also need to wrangle the data into the expected format before passing it to the data collator:"),pp=h(),v(rr.$$.fragment),mp=h(),v(or.$$.fragment),hp=h(),De=r("p"),Rc=a("The main thing to notice here is that the first example is longer than the second one, so the "),Tl=r("code"),Fc=a("input_ids"),Gc=a(" and "),ql=r("code"),Nc=a("attention_mask"),Hc=a(" of the second example have been padded on the right with a "),zl=r("code"),Uc=a("[PAD]"),Mc=a(" token (whose ID is "),Dl=r("code"),Wc=a("0"),Bc=a("). Similarly, we can see that the "),Sl=r("code"),Xc=a("labels"),Yc=a(" have been padded with "),Al=r("code"),Kc=a("-100"),Vc=a("s, to make sure the padding tokens are ignored by the loss function. And finally, we can see a new "),Ol=r("code"),Jc=a("decoder_input_ids"),Zc=a(" which has shifted the labels to the right by inserting a "),Cl=r("code"),Qc=a("[PAD]"),eu=a(" token in the first entry."),cp=h(),wt.c(),mo=h(),Fe&&Fe.c(),ho=h(),ca=r("h2"),Qa=r("a"),Pl=r("span"),v(lr.$$.fragment),su=h(),Ll=r("span"),tu=a("Using your fine-tuned model"),up=h(),en=r("p"),au=a("Once you\u2019ve pushed the model to the Hub, you can play with it either via the inference widget or with a "),Il=r("code"),nu=a("pipeline"),ru=a(" object, as follows:"),dp=h(),v(ir.$$.fragment),fp=h(),co=r("p"),ou=a("We can feed some examples from the test set (which the model has not seen) to our pipeline to get a feel for the quality of the summaries. First let\u2019s implement a simple function to show the review, title, and generated summary together:"),gp=h(),v(pr.$$.fragment),_p=h(),uo=r("p"),lu=a("Let\u2019s take a look at one of the English examples we get:"),wp=h(),v(mr.$$.fragment),bp=h(),v(hr.$$.fragment),vp=h(),sn=r("p"),iu=a("This is not too bad! We can see that our model has actually been able to perform "),Rl=r("em"),pu=a("abstractive"),mu=a(" summarization by augmenting parts of the review with new words. And perhaps the coolest aspect of our model is that it is bilingual, so we can also generate summaries of Spanish reviews:"),yp=h(),v(cr.$$.fragment),kp=h(),v(ur.$$.fragment),jp=h(),fo=r("p"),hu=a("The summary translates into \u201CVery easy to read\u201D in English, which we can see in this case was extracted directly from the review. Nevertheless, this shows the versatility of the mT5 model and has given you a taste of what it\u2019s like to deal with a multilingual corpus!"),$p=h(),go=r("p"),cu=a("Next, we\u2019ll turn our attention to a slightly more complex task: training a language model from scratch."),this.h()},l(e){const p=ug('[data-svelte="svelte-1phssyn"]',document.head);u=o(p,"META",{name:!0,content:!0}),p.forEach(t),$=c(e),y(d.$$.fragment,e),E=c(e),C=o(e,"H1",{class:!0});var wr=l(C);x=o(wr,"A",{id:!0,class:!0,href:!0});var _o=l(x);A=o(_o,"SPAN",{});var Fl=l(A);y(P.$$.fragment,Fl),Fl.forEach(t),_o.forEach(t),T=c(wr),S=o(wr,"SPAN",{});var Gl=l(S);I=n(Gl,"Summarization"),Gl.forEach(t),wr.forEach(t),D=c(e),F.l(e),V=c(e),J=o(e,"P",{});var br=l(J);Z=n(br,"In this section we\u2019ll take a look at how Transformer models can be used to condense long documents into summaries, a task known as "),X=o(br,"EM",{});var Nl=l(X);Q=n(Nl,"text summarization"),Nl.forEach(t),L=n(br,". This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail."),br.forEach(t),M=c(e),y(se.$$.fragment,e),R=c(e),Y=o(e,"P",{});var ua=l(Y);ae=n(ua,"Although there already exist various fine-tuned models for summarization on the "),W=o(ua,"A",{href:!0,rel:!0});var Hl=l(W);ie=n(Hl,"Hugging Face Hub"),Hl.forEach(t),B=n(ua,", almost all of these are only suitable for English documents. So, to add a twist in this section, we\u2019ll train a bilingual model for English and Spanish. By the end of this section, you\u2019ll have a "),ne=o(ua,"A",{href:!0,rel:!0});var wo=l(ne);fe=n(wo,"model"),wo.forEach(t),be=n(ua," that can summarize customer reviews like the one shown here:"),ua.forEach(t),Se=c(e),G=o(e,"IFRAME",{src:!0,frameborder:!0,height:!0,title:!0,class:!0,allow:!0,sandbox:!0}),l(G).forEach(t),ge=c(e),re=o(e,"P",{});var Ul=l(re);ve=n(Ul,"As we\u2019ll see, these summaries are concise because they\u2019re learned from the titles that customers provide in their product reviews. Let\u2019s start by putting together a suitable bilingual corpus for this task."),Ul.forEach(t),Te=c(e),oe=o(e,"H2",{class:!0});var tn=l(oe);ye=o(tn,"A",{id:!0,class:!0,href:!0});var bo=l(ye);$e=o(bo,"SPAN",{});var Eu=l($e);y(_e.$$.fragment,Eu),Eu.forEach(t),bo.forEach(t),le=c(tn),pe=o(tn,"SPAN",{});var Tu=l(pe);me=n(Tu,"Preparing a multilingual corpus"),Tu.forEach(t),tn.forEach(t),b=c(e),N=o(e,"P",{});var Ep=l(N);we=n(Ep,"We\u2019ll use the "),he=o(Ep,"A",{href:!0,rel:!0});var qu=l(he);ce=n(qu,"Multilingual Amazon Reviews Corpus"),qu.forEach(t),Ie=n(Ep," to create our bilingual summarizer. This corpus consists of Amazon product reviews in six languages and is typically used to benchmark multilingual classifiers. However, since each review is accompanied by a short title, we can use the titles as the target summaries for our model to learn from! To get started, let\u2019s download the English and Spanish subsets from the Hugging Face Hub:"),Ep.forEach(t),w=c(e),y(H.$$.fragment,e),Ge=c(e),y(xe.$$.fragment,e),ee=c(e),te=o(e,"P",{});var ys=l(te);Ee=n(ys,"As you can see, for each language there are 200,000 reviews for the "),Be=o(ys,"CODE",{});var zu=l(Be);ke=n(zu,"train"),zu.forEach(t),Xe=n(ys," split, and 5,000 reviews for each of the "),Ye=o(ys,"CODE",{});var Du=l(Ye);rs=n(Du,"validation"),Du.forEach(t),Ce=n(ys," and "),We=o(ys,"CODE",{});var Su=l(We);os=n(Su,"test"),Su.forEach(t),cs=n(ys," splits. The review information we are interested in is contained in the "),Ke=o(ys,"CODE",{});var Au=l(Ke);Xs=n(Au,"review_body"),Au.forEach(t),us=n(ys," and "),Pe=o(ys,"CODE",{});var Ou=l(Pe);ks=n(Ou,"review_title"),Ou.forEach(t),zs=n(ys," columns. Let\u2019s take a look at a few examples by creating a simple function that takes a random sample from the training set with the techniques we learned in "),Ne=o(ys,"A",{href:!0});var Cu=l(Ne);ds=n(Cu,"Chapter 5"),Cu.forEach(t),U=n(ys,":"),ys.forEach(t),ue=c(e),y(Ve.$$.fragment,e),Ys=c(e),y(de.$$.fragment,e),fs=c(e),y(ls.$$.fragment,e),Ds=c(e),Ae=o(e,"P",{});var vo=l(Ae);Ks=n(vo,"This sample shows the diversity of reviews one typically finds online, ranging from positive to negative (and everything in between!). Although the example with the \u201Cmeh\u201D title is not very informative, the other titles look like decent summaries of the reviews themselves. Training a summarization model on all 400,000 reviews would take far too long on a single GPU, so instead we\u2019ll focus on generating summaries for a single domain of products. To get a feel for what domains we can choose from, let\u2019s convert "),is=o(vo,"CODE",{});var Pu=l(is);qe=n(Pu,"english_dataset"),Pu.forEach(t),Vs=n(vo," to a "),Je=o(vo,"CODE",{});var Lu=l(Je);Js=n(Lu,"pandas.DataFrame"),Lu.forEach(t),He=n(vo," and compute the number of reviews per product category:"),vo.forEach(t),Zs=c(e),y(Ze.$$.fragment,e),Qs=c(e),y(je.$$.fragment,e),js=c(e),Oe=o(e,"P",{});var Lt=l(Oe);bt=n(Lt,"The most popular products in the English dataset are about household items, clothing, and wireless electronics. To stick with the Amazon theme, though, let\u2019s focus on summarizing book reviews \u2014 after all, this is what the company was founded on! We can see two product categories that fit the bill ("),$s=o(Lt,"CODE",{});var Iu=l($s);xs=n(Iu,"book"),Iu.forEach(t),pn=n(Lt," and "),vt=o(Lt,"CODE",{});var Ru=l(vt);et=n(Ru,"digital_ebook_purchase"),Ru.forEach(t),da=n(Lt,"), so let\u2019s filter the datasets in both languages for just these products. As we saw in "),gs=o(Lt,"A",{href:!0});var Fu=l(gs);fa=n(Fu,"Chapter 5"),Fu.forEach(t),yt=n(Lt,", the "),Rt=o(Lt,"CODE",{});var Gu=l(Rt);ga=n(Gu,"Dataset.filter()"),Gu.forEach(t),st=n(Lt," function allows us to slice a dataset very efficiently, so we can define a simple function to do this:"),Lt.forEach(t),Ft=c(e),y(Qe.$$.fragment,e),_a=c(e),ze=o(e,"P",{});var Hs=l(ze);mn=n(Hs,"Now when we apply this function to "),Gt=o(Hs,"CODE",{});var Nu=l(Gt);wa=n(Nu,"english_dataset"),Nu.forEach(t),Es=n(Hs," and "),_s=o(Hs,"CODE",{});var Hu=l(_s);Nt=n(Hu,"spanish_dataset"),Hu.forEach(t),tt=n(Hs,", the result will contain just those rows involving the book categories. Before applying the filter, let\u2019s switch the format of "),Ht=o(Hs,"CODE",{});var Uu=l(Ht);Ut=n(Uu,"english_dataset"),Uu.forEach(t),hn=n(Hs," from "),kt=o(Hs,"CODE",{});var Mu=l(kt);jt=n(Mu,'"pandas"'),Mu.forEach(t),cn=n(Hs," back to "),$t=o(Hs,"CODE",{});var Wu=l($t);es=n(Wu,'"arrow"'),Wu.forEach(t),at=n(Hs,":"),Hs.forEach(t),ba=c(e),y(Ts.$$.fragment,e),va=c(e),xt=o(e,"P",{});var Bu=l(xt);un=n(Bu,"We can then apply the filter function, and as a sanity check let\u2019s inspect a sample of reviews to see if they are indeed about books:"),Bu.forEach(t),Et=c(e),y(nt.$$.fragment,e),ya=c(e),y(qs.$$.fragment,e),ka=c(e),ss=o(e,"P",{});var an=l(ss);ps=n(an,"Okay, we can see that the reviews are not strictly about books and might refer to things like calendars and electronic applications such as OneNote. Nevertheless, the domain seems about right to train a summarization model on. Before we look at various models that are suitable for this task, we have one last bit of data preparation to do: combining the English and Spanish reviews as a single "),Mt=o(an,"CODE",{});var Xu=l(Mt);Wt=n(Xu,"DatasetDict"),Xu.forEach(t),dn=n(an," object. \u{1F917} Datasets provides a handy "),Bt=o(an,"CODE",{});var Yu=l(Bt);Xt=n(Yu,"concatenate_datasets()"),Yu.forEach(t),fn=n(an," function that (as the name suggests) will stack two "),Yt=o(an,"CODE",{});var Ku=l(Yt);Kt=n(Ku,"Dataset"),Ku.forEach(t),gn=n(an," objects on top of each other. So, to create our bilingual dataset, we\u2019ll loop over each split, concatenate the datasets for that split, and shuffle the result to ensure our model doesn\u2019t overfit to a single language:"),an.forEach(t),ja=c(e),y(Ss.$$.fragment,e),rt=c(e),y(ot.$$.fragment,e),Vt=c(e),ws=o(e,"P",{});var Vu=l(ws);$a=n(Vu,"This certainly looks like a mix of English and Spanish reviews! Now that we have a training corpus, one final thing to check is the distribution of words in the reviews and their titles. This is especially important for summarization tasks, where short reference summaries in the data can bias the model to only output one or two words in the generated summaries. The plots below show the word distributions, and we can see that the titles are heavily skewed toward just 1-2 words:"),Vu.forEach(t),As=c(e),bs=o(e,"DIV",{class:!0});var Tp=l(bs);ts=o(Tp,"IMG",{class:!0,src:!0,alt:!0}),Jt=c(Tp),lt=o(Tp,"IMG",{class:!0,src:!0,alt:!0}),Tp.forEach(t),Zt=c(e),m=o(e,"P",{});var qp=l(m);q=n(qp,"To deal with this, we\u2019ll filter out the examples with very short titles so that our model can produce more interesting summaries. Since we\u2019re dealing with English and Spanish texts, we can use a rough heuristic to split the titles on whitespace and then use our trusty "),xa=o(qp,"CODE",{});var Ju=l(xa);kr=n(Ju,"Dataset.filter()"),Ju.forEach(t),jr=n(qp," method as follows:"),qp.forEach(t),it=c(e),y(Tt.$$.fragment,e),_n=c(e),pt=o(e,"P",{});var Zu=l(pt);$r=n(Zu,"Now that we\u2019ve prepared our corpus, let\u2019s take a look at a few possible Transformer models that one might fine-tune on it!"),Zu.forEach(t),wn=c(e),Os=o(e,"H2",{class:!0});var zp=l(Os);ms=o(zp,"A",{id:!0,class:!0,href:!0});var Qu=l(ms);Ea=o(Qu,"SPAN",{});var ed=l(Ea);y(qt.$$.fragment,ed),ed.forEach(t),Qu.forEach(t),Ta=c(zp),qa=o(zp,"SPAN",{});var sd=l(qa);xr=n(sd,"Models for text summarization"),sd.forEach(t),zp.forEach(t),Qt=c(e),mt=o(e,"P",{});var Dp=l(mt);za=n(Dp,"If you think about it, text summarization is a similar sort of task to machine translation: we have a body of text like a review that we\u2019d like to \u201Ctranslate\u201D into a shorter version that captures the salient features of the input. Accordingly, most Transformer models for summarization adopt the encoder-decoder architecture that we first encountered in "),ea=o(Dp,"A",{href:!0});var td=l(ea);Er=n(td,"Chapter 1"),td.forEach(t),zt=n(Dp,", although there are some exceptions like the GPT family of models which can also be used for summarization in few-shot settings. The following table lists some popular pretrained models that can be fine-tuned for summarization."),Dp.forEach(t),bn=c(e),ht=o(e,"TABLE",{});var Sp=l(ht);Da=o(Sp,"THEAD",{});var ad=l(Da);Cs=o(ad,"TR",{});var yo=l(Cs);sa=o(yo,"TH",{align:!0});var nd=l(sa);Dt=n(nd,"Transformer model"),nd.forEach(t),Tr=c(yo),Sa=o(yo,"TH",{});var rd=l(Sa);qr=n(rd,"Description"),rd.forEach(t),zr=c(yo),ct=o(yo,"TH",{align:!0});var od=l(ct);Dr=n(od,"Multilingual?"),od.forEach(t),yo.forEach(t),ad.forEach(t),Aa=c(Sp),Ue=o(Sp,"TBODY",{});var Us=l(Ue);Ps=o(Us,"TR",{});var ko=l(Ps);ta=o(ko,"TD",{align:!0});var ld=l(ta);St=o(ld,"A",{href:!0,rel:!0});var id=l(St);Ls=n(id,"GPT-2"),id.forEach(t),ld.forEach(t),Oa=c(ko),Ca=o(ko,"TD",{});var pd=l(Ca);Sr=n(pd,"Although trained as an auto-regressive language model, you can make GPT-2 generate summaries by appending \u201CTL;DR\u201D at the end of the input text."),pd.forEach(t),Ar=c(ko),hs=o(ko,"TD",{align:!0});var md=l(hs);Or=n(md,"\u274C"),md.forEach(t),ko.forEach(t),Cr=c(Us),Is=o(Us,"TR",{});var jo=l(Is);aa=o(jo,"TD",{align:!0});var hd=l(aa);Rs=o(hd,"A",{href:!0,rel:!0});var cd=l(Rs);Pr=n(cd,"PEGASUS"),cd.forEach(t),hd.forEach(t),Kp=c(jo),Co=o(jo,"TD",{});var ud=l(Co);Vp=n(ud,"Uses a pretraining objective to predict masked sentences in multi-sentence texts. This pretraining objective is closer to summarization than vanilla language modeling and scores highly on popular benchmarks."),ud.forEach(t),Jp=c(jo),Lr=o(jo,"TD",{align:!0});var dd=l(Lr);Zp=n(dd,"\u274C"),dd.forEach(t),jo.forEach(t),Qp=c(Us),na=o(Us,"TR",{});var $o=l(na);Ir=o($o,"TD",{align:!0});var fd=l(Ir);vn=o(fd,"A",{href:!0,rel:!0});var gd=l(vn);em=n(gd,"T5"),gd.forEach(t),fd.forEach(t),sm=c($o),yn=o($o,"TD",{});var Ap=l(yn);tm=n(Ap,"A universal Transformer architecture that formulates all tasks in a text-to-text framework; e.g., the input format for the model to summarize a document is "),Po=o(Ap,"CODE",{});var _d=l(Po);am=n(_d,"summarize: ARTICLE"),_d.forEach(t),nm=n(Ap,"."),Ap.forEach(t),rm=c($o),Rr=o($o,"TD",{align:!0});var wd=l(Rr);om=n(wd,"\u274C"),wd.forEach(t),$o.forEach(t),lm=c(Us),ra=o(Us,"TR",{});var xo=l(ra);Fr=o(xo,"TD",{align:!0});var bd=l(Fr);kn=o(bd,"A",{href:!0,rel:!0});var vd=l(kn);im=n(vd,"mT5"),vd.forEach(t),bd.forEach(t),pm=c(xo),Lo=o(xo,"TD",{});var yd=l(Lo);mm=n(yd,"A multilingual version of T5, pretrained on the multilingual Common Crawl corpus (mC4), covering 101 languages."),yd.forEach(t),hm=c(xo),Gr=o(xo,"TD",{align:!0});var kd=l(Gr);cm=n(kd,"\u2705"),kd.forEach(t),xo.forEach(t),um=c(Us),oa=o(Us,"TR",{});var Eo=l(oa);Nr=o(Eo,"TD",{align:!0});var jd=l(Nr);jn=o(jd,"A",{href:!0,rel:!0});var $d=l(jn);dm=n($d,"BART"),$d.forEach(t),jd.forEach(t),fm=c(Eo),Io=o(Eo,"TD",{});var xd=l(Io);gm=n(xd,"A novel Transformer architecture with both an encoder and a decoder stack trained to reconstruct corrupted input that combines the pretraining schemes of BERT and GPT-2."),xd.forEach(t),_m=c(Eo),Hr=o(Eo,"TD",{align:!0});var Ed=l(Hr);wm=n(Ed,"\u274C"),Ed.forEach(t),Eo.forEach(t),bm=c(Us),la=o(Us,"TR",{});var To=l(la);Ur=o(To,"TD",{align:!0});var Td=l(Ur);$n=o(Td,"A",{href:!0,rel:!0});var qd=l($n);vm=n(qd,"mBART-50"),qd.forEach(t),Td.forEach(t),ym=c(To),Ro=o(To,"TD",{});var zd=l(Ro);km=n(zd,"A multilingual version of BART, pretrained on 50 languages."),zd.forEach(t),jm=c(To),Mr=o(To,"TD",{align:!0});var Dd=l(Mr);$m=n(Dd,"\u2705"),Dd.forEach(t),To.forEach(t),Us.forEach(t),Sp.forEach(t),Wl=c(e),Wr=o(e,"P",{});var Sd=l(Wr);xm=n(Sd,"As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a \u201Chigh-resource\u201D language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!"),Sd.forEach(t),Bl=c(e),Pa=o(e,"P",{});var Op=l(Pa);Em=n(Op,"We\u2019ll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like "),Fo=o(Op,"CODE",{});var Ad=l(Fo);Tm=n(Ad,"summarize:"),Ad.forEach(t),qm=n(Op," which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model!"),Op.forEach(t),Xl=c(e),ia=o(e,"DIV",{class:!0});var Cp=l(ia);xn=o(Cp,"IMG",{class:!0,src:!0,alt:!0}),zm=c(Cp),En=o(Cp,"IMG",{class:!0,src:!0,alt:!0}),Cp.forEach(t),Yl=c(e),Br=o(e,"P",{});var Od=l(Br);Dm=n(Od,"mT5 doesn\u2019t use prefixes, but shares much of the versatility of T5 and has the advantage of being multilingual. Now that we\u2019ve picked a model, let\u2019s take a look at preparing our data for training."),Od.forEach(t),Kl=c(e),y(La.$$.fragment,e),Vl=c(e),pa=o(e,"H2",{class:!0});var Pp=l(pa);Ia=o(Pp,"A",{id:!0,class:!0,href:!0});var Cd=l(Ia);Go=o(Cd,"SPAN",{});var Pd=l(Go);y(Tn.$$.fragment,Pd),Pd.forEach(t),Cd.forEach(t),Sm=c(Pp),No=o(Pp,"SPAN",{});var Ld=l(No);Am=n(Ld,"Preprocessing the data"),Ld.forEach(t),Pp.forEach(t),Jl=c(e),y(qn.$$.fragment,e),Zl=c(e),Ra=o(e,"P",{});var Lp=l(Ra);Om=n(Lp,"Our next task is to tokenize and encode our reviews and their titles. As usual, we begin by loading the tokenizer associated with the pretrained model checkpoint. We\u2019ll use "),Ho=o(Lp,"CODE",{});var Id=l(Ho);Cm=n(Id,"mt5-small"),Id.forEach(t),Pm=n(Lp," as our checkpoint so we can fine-tune the model in a reasonable amount of time:"),Lp.forEach(t),Ql=c(e),y(zn.$$.fragment,e),ei=c(e),y(Fa.$$.fragment,e),si=c(e),Xr=o(e,"P",{});var Rd=l(Xr);Lm=n(Rd,"Let\u2019s test out the mT5 tokenizer on a small example:"),Rd.forEach(t),ti=c(e),y(Dn.$$.fragment,e),ai=c(e),y(Sn.$$.fragment,e),ni=c(e),vs=o(e,"P",{});var It=l(vs);Im=n(It,"Here we can see the familiar "),Uo=o(It,"CODE",{});var Fd=l(Uo);Rm=n(Fd,"input_ids"),Fd.forEach(t),Fm=n(It," and "),Mo=o(It,"CODE",{});var Gd=l(Mo);Gm=n(Gd,"attention_mask"),Gd.forEach(t),Nm=n(It," that we encountered in our first fine-tuning experiments back in "),Yr=o(It,"A",{href:!0});var Nd=l(Yr);Hm=n(Nd,"Chapter 3"),Nd.forEach(t),Um=n(It,". Let\u2019s decode these input IDs with the tokenizer\u2019s "),Wo=o(It,"CODE",{});var Hd=l(Wo);Mm=n(Hd,"convert_ids_to_tokens()"),Hd.forEach(t),Wm=n(It," function to see what kind of tokenizer we\u2019re dealing with:"),It.forEach(t),ri=c(e),y(An.$$.fragment,e),oi=c(e),y(On.$$.fragment,e),li=c(e),Fs=o(e,"P",{});var nn=l(Fs);Bm=n(nn,"The special Unicode character "),Bo=o(nn,"CODE",{});var Ud=l(Bo);Xm=n(Ud,"\u2581"),Ud.forEach(t),Ym=n(nn," and end-of-sequence token "),Xo=o(nn,"CODE",{});var Md=l(Xo);Km=n(Md,"</s>"),Md.forEach(t),Vm=n(nn," indicate that we\u2019re dealing with the SentencePiece tokenizer, which is based on the Unigram segmentation algorithm discussed in "),Kr=o(nn,"A",{href:!0});var Wd=l(Kr);Jm=n(Wd,"Chapter 6"),Wd.forEach(t),Zm=n(nn,". Unigram is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters."),nn.forEach(t),ii=c(e),Ga=o(e,"P",{});var Ip=l(Ga);Qm=n(Ip,"To tokenize our corpus, we have to deal with a subtlety associated with summarization: because our labels are also text, it is possible that they exceed the model\u2019s maximum context size. This means we need to apply truncation to both the reviews and their titles to ensure we don\u2019t pass excessively long inputs to our model. The tokenizers in \u{1F917} Transformers provide a nifty "),Yo=o(Ip,"CODE",{});var Bd=l(Yo);eh=n(Bd,"text_target"),Bd.forEach(t),sh=n(Ip," argument that allows you to tokenize the labels in parallel to the inputs. Here is an example of how the inputs and targets are processed for mT5:"),Ip.forEach(t),pi=c(e),y(Cn.$$.fragment,e),mi=c(e),At=o(e,"P",{});var qo=l(At);th=n(qo,"Let\u2019s walk through this code to understand what\u2019s happening. The first thing we\u2019ve done is define values for "),Ko=o(qo,"CODE",{});var Xd=l(Ko);ah=n(Xd,"max_input_length"),Xd.forEach(t),nh=n(qo," and "),Vo=o(qo,"CODE",{});var Yd=l(Vo);rh=n(Yd,"max_target_length"),Yd.forEach(t),oh=n(qo,", which set the upper limits for how long our reviews and titles can be. Since the review body is typically much larger than the title, we\u2019ve scaled these values accordingly."),qo.forEach(t),hi=c(e),Ot=o(e,"P",{});var zo=l(Ot);lh=n(zo,"With "),Jo=o(zo,"CODE",{});var Kd=l(Jo);ih=n(Kd,"preprocess_function()"),Kd.forEach(t),ph=n(zo,", it is then a simple matter to tokenize the whole corpus using the handy "),Zo=o(zo,"CODE",{});var Vd=l(Zo);mh=n(Vd,"Dataset.map()"),Vd.forEach(t),hh=n(zo," function we\u2019ve used extensively throughout this course:"),zo.forEach(t),ci=c(e),y(Pn.$$.fragment,e),ui=c(e),Vr=o(e,"P",{});var Jd=l(Vr);ch=n(Jd,"Now that the corpus has been preprocessed, let\u2019s take a look at some metrics that are commonly used for summarization. As we\u2019ll see, there is no silver bullet when it comes to measuring the quality of machine-generated text."),Jd.forEach(t),di=c(e),y(Na.$$.fragment,e),fi=c(e),ma=o(e,"H2",{class:!0});var Rp=l(ma);Ha=o(Rp,"A",{id:!0,class:!0,href:!0});var Zd=l(Ha);Qo=o(Zd,"SPAN",{});var Qd=l(Qo);y(Ln.$$.fragment,Qd),Qd.forEach(t),Zd.forEach(t),uh=c(Rp),el=o(Rp,"SPAN",{});var ef=l(el);dh=n(ef,"Metrics for text summarization"),ef.forEach(t),Rp.forEach(t),gi=c(e),y(In.$$.fragment,e),_i=c(e),Jr=o(e,"P",{});var sf=l(Jr);fh=n(sf,"In comparison to most of the other tasks we\u2019ve covered in this course, measuring the performance of text generation tasks like summarization or translation is not as straightforward. For example, given a review like \u201CI loved reading the Hunger Games\u201D, there are multiple valid summaries, like \u201CI loved the Hunger Games\u201D or \u201CHunger Games is a great read\u201D. Clearly, applying some sort of exact match between the generated summary and the label is not a good solution \u2014 even humans would fare poorly under such a metric, because we all have our own writing style."),sf.forEach(t),wi=c(e),Ua=o(e,"P",{});var Fp=l(Ua);gh=n(Fp,"For summarization, one of the most commonly used metrics is the "),Rn=o(Fp,"A",{href:!0,rel:!0});var tf=l(Rn);_h=n(tf,"ROUGE score"),tf.forEach(t),wh=n(Fp," (short for Recall-Oriented Understudy for Gisting Evaluation). The basic idea behind this metric is to compare a generated summary against a set of reference summaries that are typically created by humans. To make this more precise, suppose we want to compare the following two summaries:"),Fp.forEach(t),bi=c(e),y(Fn.$$.fragment,e),vi=c(e),Ct=o(e,"P",{});var Do=l(Ct);bh=n(Do,"One way to compare them could be to count the number of overlapping words, which in this case would be 6. However, this is a bit crude, so instead ROUGE is based on computing the "),sl=o(Do,"EM",{});var af=l(sl);vh=n(af,"precision"),af.forEach(t),yh=n(Do," and "),tl=o(Do,"EM",{});var nf=l(tl);kh=n(nf,"recall"),nf.forEach(t),jh=n(Do," scores for the overlap."),Do.forEach(t),yi=c(e),y(Ma.$$.fragment,e),ki=c(e),Gn=o(e,"P",{});var uu=l(Gn);$h=n(uu,`For ROUGE, recall measures how much of the reference summary is captured by the generated one. If we are just comparing words, recall can be calculated according to the following formula:
`),ji=ng(uu),uu.forEach(t),$i=c(e),Nn=o(e,"P",{});var du=l(Nn);xh=n(du,`For our simple example above, this formula gives a perfect recall of 6/6 = 1; i.e., all the words in the reference summary have been produced by the model. This may sound great, but imagine if our generated summary had been \u201CI really really loved reading the Hunger Games all night\u201D. This would also have perfect recall, but is arguably a worse summary since it is verbose. To deal with these scenarios we also compute the precision, which in the ROUGE context measures how much of the generated summary was relevant:
`),xi=ng(du),du.forEach(t),Ei=c(e),Wa=o(e,"P",{});var Gp=l(Wa);Eh=n(Gp,"Applying this to our verbose summary gives a precision of 6/10  = 0.6, which is considerably worse than the precision of 6/7 = 0.86 obtained by our shorter one. In practice, both precision and recall are usually computed, and then the F1-score (the harmonic mean of precision and recall) is reported. We can do this easily in \u{1F917} Datasets by first installing the "),al=o(Gp,"CODE",{});var rf=l(al);Th=n(rf,"rouge_score"),rf.forEach(t),qh=n(Gp," package:"),Gp.forEach(t),Ti=c(e),y(Hn.$$.fragment,e),qi=c(e),Zr=o(e,"P",{});var of=l(Zr);zh=n(of,"and then loading the ROUGE metric as follows:"),of.forEach(t),zi=c(e),y(Un.$$.fragment,e),Di=c(e),Ba=o(e,"P",{});var Np=l(Ba);Dh=n(Np,"Then we can use the "),nl=o(Np,"CODE",{});var lf=l(nl);Sh=n(lf,"rouge_score.compute()"),lf.forEach(t),Ah=n(Np," function to calculate all the metrics at once:"),Np.forEach(t),Si=c(e),y(Mn.$$.fragment,e),Ai=c(e),y(Wn.$$.fragment,e),Oi=c(e),as=o(e,"P",{});var Ms=l(as);Oh=n(Ms,"Whoa, there\u2019s a lot of information in that output \u2014 what does it all mean? First, \u{1F917} Datasets actually computes confidence intervals for precision, recall, and F1-score; these are the "),rl=o(Ms,"CODE",{});var pf=l(rl);Ch=n(pf,"low"),pf.forEach(t),Ph=n(Ms,", "),ol=o(Ms,"CODE",{});var mf=l(ol);Lh=n(mf,"mid"),mf.forEach(t),Ih=n(Ms,", and "),ll=o(Ms,"CODE",{});var hf=l(ll);Rh=n(hf,"high"),hf.forEach(t),Fh=n(Ms," attributes you can see here. Moreover, \u{1F917} Datasets computes a variety of ROUGE scores which are based on different types of text granularity when comparing the generated and reference summaries. The "),il=o(Ms,"CODE",{});var cf=l(il);Gh=n(cf,"rouge1"),cf.forEach(t),Nh=n(Ms," variant is the overlap of unigrams \u2014 this is just a fancy way of saying the overlap of words and is exactly the metric we\u2019ve discussed above. To verify this, let\u2019s pull out the "),pl=o(Ms,"CODE",{});var uf=l(pl);Hh=n(uf,"mid"),uf.forEach(t),Uh=n(Ms," value of our scores:"),Ms.forEach(t),Ci=c(e),y(Bn.$$.fragment,e),Pi=c(e),y(Xn.$$.fragment,e),Li=c(e),ns=o(e,"P",{});var Ws=l(ns);Mh=n(Ws,"Great, the precision and recall numbers match up! Now what about those other ROUGE scores? "),ml=o(Ws,"CODE",{});var df=l(ml);Wh=n(df,"rouge2"),df.forEach(t),Bh=n(Ws," measures the overlap between bigrams (think the overlap of pairs of words), while "),hl=o(Ws,"CODE",{});var ff=l(hl);Xh=n(ff,"rougeL"),ff.forEach(t),Yh=n(Ws," and "),cl=o(Ws,"CODE",{});var gf=l(cl);Kh=n(gf,"rougeLsum"),gf.forEach(t),Vh=n(Ws," measure the longest matching sequences of words by looking for the longest common substrings in the generated and reference summaries. The \u201Csum\u201D in "),ul=o(Ws,"CODE",{});var _f=l(ul);Jh=n(_f,"rougeLsum"),_f.forEach(t),Zh=n(Ws," refers to the fact that this metric is computed over a whole summary, while "),dl=o(Ws,"CODE",{});var wf=l(dl);Qh=n(wf,"rougeL"),wf.forEach(t),ec=n(Ws," is computed as the average over individual sentences."),Ws.forEach(t),Ii=c(e),y(Xa.$$.fragment,e),Ri=c(e),Qr=o(e,"P",{});var bf=l(Qr);sc=n(bf,"We\u2019ll use these ROUGE scores to track the performance of our model, but before doing that let\u2019s do something every good NLP practitioner should do: create a strong, yet simple baseline!"),bf.forEach(t),Fi=c(e),ha=o(e,"H3",{class:!0});var Hp=l(ha);Ya=o(Hp,"A",{id:!0,class:!0,href:!0});var vf=l(Ya);fl=o(vf,"SPAN",{});var yf=l(fl);y(Yn.$$.fragment,yf),yf.forEach(t),vf.forEach(t),tc=c(Hp),gl=o(Hp,"SPAN",{});var kf=l(gl);ac=n(kf,"Creating a strong baseline"),kf.forEach(t),Hp.forEach(t),Gi=c(e),Gs=o(e,"P",{});var rn=l(Gs);nc=n(rn,"A common baseline for text summarization is to simply take the first three sentences of an article, often called the "),_l=o(rn,"EM",{});var jf=l(_l);rc=n(jf,"lead-3"),jf.forEach(t),oc=n(rn," baseline. We could use full stops to track the sentence boundaries, but this will fail on acronyms like \u201CU.S.\u201D or \u201CU.N.\u201D \u2014 so instead we\u2019ll use the "),wl=o(rn,"CODE",{});var $f=l(wl);lc=n($f,"nltk"),$f.forEach(t),ic=n(rn," library, which includes a better algorithm to handle these cases. You can install the package using "),bl=o(rn,"CODE",{});var xf=l(bl);pc=n(xf,"pip"),xf.forEach(t),mc=n(rn," as follows:"),rn.forEach(t),Ni=c(e),y(Kn.$$.fragment,e),Hi=c(e),eo=o(e,"P",{});var Ef=l(eo);hc=n(Ef,"and then download the punctuation rules:"),Ef.forEach(t),Ui=c(e),y(Vn.$$.fragment,e),Mi=c(e),Ka=o(e,"P",{});var Up=l(Ka);cc=n(Up,"Next, we import the sentence tokenizer from "),vl=o(Up,"CODE",{});var Tf=l(vl);uc=n(Tf,"nltk"),Tf.forEach(t),dc=n(Up," and create a simple function to extract the first three sentences in a review. The convention in text summarization is to separate each summary with a newline, so let\u2019s also include this and test it on a training example:"),Up.forEach(t),Wi=c(e),y(Jn.$$.fragment,e),Bi=c(e),y(Zn.$$.fragment,e),Xi=c(e),so=o(e,"P",{});var qf=l(so);fc=n(qf,"This seems to work, so let\u2019s now implement a function that extracts these \u201Csummaries\u201D from a dataset and computes the ROUGE scores for the baseline:"),qf.forEach(t),Yi=c(e),y(Qn.$$.fragment,e),Ki=c(e),to=o(e,"P",{});var zf=l(to);gc=n(zf,"We can then use this function to compute the ROUGE scores over the validation set and prettify them a bit using Pandas:"),zf.forEach(t),Vi=c(e),y(er.$$.fragment,e),Ji=c(e),y(sr.$$.fragment,e),Zi=c(e),Va=o(e,"P",{});var Mp=l(Va);_c=n(Mp,"We can see that the "),yl=o(Mp,"CODE",{});var Df=l(yl);wc=n(Df,"rouge2"),Df.forEach(t),bc=n(Mp," score is significantly lower than the rest; this likely reflects the fact that review titles are typically concise and so the lead-3 baseline is too verbose. Now that we have a good baseline to work from, let\u2019s turn our attention toward fine-tuning mT5!"),Mp.forEach(t),Qi=c(e),dt.l(e),ao=c(e),y(Ja.$$.fragment,e),ep=c(e),no=o(e,"P",{});var Sf=l(no);vc=n(Sf,"The next thing we need to do is log in to the Hugging Face Hub. If you\u2019re running this code in a notebook, you can do so with the following utility function:"),Sf.forEach(t),sp=c(e),y(tr.$$.fragment,e),tp=c(e),ro=o(e,"P",{});var Af=l(ro);yc=n(Af,"which will display a widget where you can enter your credentials. Alternatively, you can run this command in your terminal and log in there:"),Af.forEach(t),ap=c(e),y(ar.$$.fragment,e),np=c(e),Re&&Re.l(e),oo=c(e),Za=o(e,"P",{});var Wp=l(Za);kc=n(Wp,"Next, we need to define a data collator for our sequence-to-sequence task. Since mT5 is an encoder-decoder Transformer model, one subtlety with preparing our batches is that during decoding we need to shift the labels to the right by one. This is required to ensure that the decoder only sees the previous ground truth labels and not the current or future ones, which would be easy for the model to memorize. This is similar to how masked self-attention is applied to the inputs in a task like "),lo=o(Wp,"A",{href:!0});var Of=l(lo);jc=n(Of,"causal language modeling"),Of.forEach(t),$c=n(Wp,"."),Wp.forEach(t),rp=c(e),Ns=o(e,"P",{});var on=l(Ns);xc=n(on,"Luckily, \u{1F917} Transformers provides a "),kl=o(on,"CODE",{});var Cf=l(kl);Ec=n(Cf,"DataCollatorForSeq2Seq"),Cf.forEach(t),Tc=n(on," collator that will dynamically pad the inputs and the labels for us. To instantiate this collator, we simply need to provide the "),jl=o(on,"CODE",{});var Pf=l(jl);qc=n(Pf,"tokenizer"),Pf.forEach(t),zc=n(on," and "),$l=o(on,"CODE",{});var Lf=l($l);Dc=n(Lf,"model"),Lf.forEach(t),Sc=n(on,":"),on.forEach(t),op=c(e),gt.l(e),io=c(e),po=o(e,"P",{});var If=l(po);Ac=n(If,"Let\u2019s see what this collator produces when fed a small batch of examples. First, we need to remove the columns with strings because the collator won\u2019t know how to pad these elements:"),If.forEach(t),lp=c(e),y(nr.$$.fragment,e),ip=c(e),Pt=o(e,"P",{});var So=l(Pt);Oc=n(So,"Since the collator expects a list of "),xl=o(So,"CODE",{});var Rf=l(xl);Cc=n(Rf,"dict"),Rf.forEach(t),Pc=n(So,"s, where each "),El=o(So,"CODE",{});var Ff=l(El);Lc=n(Ff,"dict"),Ff.forEach(t),Ic=n(So," represents a single example in the dataset, we also need to wrangle the data into the expected format before passing it to the data collator:"),So.forEach(t),pp=c(e),y(rr.$$.fragment,e),mp=c(e),y(or.$$.fragment,e),hp=c(e),De=o(e,"P",{});var Me=l(De);Rc=n(Me,"The main thing to notice here is that the first example is longer than the second one, so the "),Tl=o(Me,"CODE",{});var Gf=l(Tl);Fc=n(Gf,"input_ids"),Gf.forEach(t),Gc=n(Me," and "),ql=o(Me,"CODE",{});var Nf=l(ql);Nc=n(Nf,"attention_mask"),Nf.forEach(t),Hc=n(Me," of the second example have been padded on the right with a "),zl=o(Me,"CODE",{});var Hf=l(zl);Uc=n(Hf,"[PAD]"),Hf.forEach(t),Mc=n(Me," token (whose ID is "),Dl=o(Me,"CODE",{});var Uf=l(Dl);Wc=n(Uf,"0"),Uf.forEach(t),Bc=n(Me,"). Similarly, we can see that the "),Sl=o(Me,"CODE",{});var Mf=l(Sl);Xc=n(Mf,"labels"),Mf.forEach(t),Yc=n(Me," have been padded with "),Al=o(Me,"CODE",{});var Wf=l(Al);Kc=n(Wf,"-100"),Wf.forEach(t),Vc=n(Me,"s, to make sure the padding tokens are ignored by the loss function. And finally, we can see a new "),Ol=o(Me,"CODE",{});var Bf=l(Ol);Jc=n(Bf,"decoder_input_ids"),Bf.forEach(t),Zc=n(Me," which has shifted the labels to the right by inserting a "),Cl=o(Me,"CODE",{});var Xf=l(Cl);Qc=n(Xf,"[PAD]"),Xf.forEach(t),eu=n(Me," token in the first entry."),Me.forEach(t),cp=c(e),wt.l(e),mo=c(e),Fe&&Fe.l(e),ho=c(e),ca=o(e,"H2",{class:!0});var Bp=l(ca);Qa=o(Bp,"A",{id:!0,class:!0,href:!0});var Yf=l(Qa);Pl=o(Yf,"SPAN",{});var Kf=l(Pl);y(lr.$$.fragment,Kf),Kf.forEach(t),Yf.forEach(t),su=c(Bp),Ll=o(Bp,"SPAN",{});var Vf=l(Ll);tu=n(Vf,"Using your fine-tuned model"),Vf.forEach(t),Bp.forEach(t),up=c(e),en=o(e,"P",{});var Xp=l(en);au=n(Xp,"Once you\u2019ve pushed the model to the Hub, you can play with it either via the inference widget or with a "),Il=o(Xp,"CODE",{});var Jf=l(Il);nu=n(Jf,"pipeline"),Jf.forEach(t),ru=n(Xp," object, as follows:"),Xp.forEach(t),dp=c(e),y(ir.$$.fragment,e),fp=c(e),co=o(e,"P",{});var Zf=l(co);ou=n(Zf,"We can feed some examples from the test set (which the model has not seen) to our pipeline to get a feel for the quality of the summaries. First let\u2019s implement a simple function to show the review, title, and generated summary together:"),Zf.forEach(t),gp=c(e),y(pr.$$.fragment,e),_p=c(e),uo=o(e,"P",{});var Qf=l(uo);lu=n(Qf,"Let\u2019s take a look at one of the English examples we get:"),Qf.forEach(t),wp=c(e),y(mr.$$.fragment,e),bp=c(e),y(hr.$$.fragment,e),vp=c(e),sn=o(e,"P",{});var Yp=l(sn);iu=n(Yp,"This is not too bad! We can see that our model has actually been able to perform "),Rl=o(Yp,"EM",{});var eg=l(Rl);pu=n(eg,"abstractive"),eg.forEach(t),mu=n(Yp," summarization by augmenting parts of the review with new words. And perhaps the coolest aspect of our model is that it is bilingual, so we can also generate summaries of Spanish reviews:"),Yp.forEach(t),yp=c(e),y(cr.$$.fragment,e),kp=c(e),y(ur.$$.fragment,e),jp=c(e),fo=o(e,"P",{});var sg=l(fo);hu=n(sg,"The summary translates into \u201CVery easy to read\u201D in English, which we can see in this case was extracted directly from the review. Nevertheless, this shows the versatility of the mT5 model and has given you a taste of what it\u2019s like to deal with a multilingual corpus!"),sg.forEach(t),$p=c(e),go=o(e,"P",{});var tg=l(go);cu=n(tg,"Next, we\u2019ll turn our attention to a slightly more complex task: training a language model from scratch."),tg.forEach(t),this.h()},h(){f(u,"name","hf:doc:metadata"),f(u,"content",JSON.stringify(Og)),f(x,"id","summarization"),f(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(x,"href","#summarization"),f(C,"class","relative group"),f(W,"href","https://huggingface.co/models?pipeline_tag=summarization&sort=downloads"),f(W,"rel","nofollow"),f(ne,"href","https://huggingface.co/huggingface-course/mt5-small-finetuned-amazon-en-es"),f(ne,"rel","nofollow"),Ml(G.src,Le="https://hf.space/gradioiframe/course-demos/mt5-small-finetuned-amazon-en-es/+")||f(G,"src",Le),f(G,"frameborder","0"),f(G,"height","400"),f(G,"title","Gradio app"),f(G,"class","block dark:hidden container p-0 flex-grow space-iframe"),f(G,"allow","accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"),f(G,"sandbox","allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads"),f(ye,"id","preparing-a-multilingual-corpus"),f(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ye,"href","#preparing-a-multilingual-corpus"),f(oe,"class","relative group"),f(he,"href","https://huggingface.co/datasets/amazon_reviews_multi"),f(he,"rel","nofollow"),f(Ne,"href","/course/chapter5"),f(gs,"href","/course/chapter5"),f(ts,"class","block dark:hidden"),Ml(ts.src,vr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths.svg")||f(ts,"src",vr),f(ts,"alt","Word count distributions for the review titles and texts."),f(lt,"class","hidden dark:block"),Ml(lt.src,yr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/review-lengths-dark.svg")||f(lt,"src",yr),f(lt,"alt","Word count distributions for the review titles and texts."),f(bs,"class","flex justify-center"),f(ms,"id","models-for-text-summarization"),f(ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ms,"href","#models-for-text-summarization"),f(Os,"class","relative group"),f(ea,"href","/course/chapter1"),f(sa,"align","center"),f(ct,"align","center"),f(St,"href","https://huggingface.co/gpt2-xl"),f(St,"rel","nofollow"),f(ta,"align","center"),f(hs,"align","center"),f(Rs,"href","https://huggingface.co/google/pegasus-large"),f(Rs,"rel","nofollow"),f(aa,"align","center"),f(Lr,"align","center"),f(vn,"href","https://huggingface.co/t5-base"),f(vn,"rel","nofollow"),f(Ir,"align","center"),f(Rr,"align","center"),f(kn,"href","https://huggingface.co/google/mt5-base"),f(kn,"rel","nofollow"),f(Fr,"align","center"),f(Gr,"align","center"),f(jn,"href","https://huggingface.co/facebook/bart-base"),f(jn,"rel","nofollow"),f(Nr,"align","center"),f(Hr,"align","center"),f($n,"href","https://huggingface.co/facebook/mbart-large-50"),f($n,"rel","nofollow"),f(Ur,"align","center"),f(Mr,"align","center"),f(xn,"class","block dark:hidden"),Ml(xn.src,gu="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5.svg")||f(xn,"src",gu),f(xn,"alt","Different tasks performed by the T5 architecture."),f(En,"class","hidden dark:block"),Ml(En.src,_u="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/t5-dark.svg")||f(En,"src",_u),f(En,"alt","Different tasks performed by the T5 architecture."),f(ia,"class","flex justify-center"),f(Ia,"id","preprocessing-the-data"),f(Ia,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ia,"href","#preprocessing-the-data"),f(pa,"class","relative group"),f(Yr,"href","/course/chapter3"),f(Kr,"href","/course/chapter6"),f(Ha,"id","metrics-for-text-summarization"),f(Ha,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ha,"href","#metrics-for-text-summarization"),f(ma,"class","relative group"),f(Rn,"href","https://en.wikipedia.org/wiki/ROUGE_(metric)"),f(Rn,"rel","nofollow"),ji.a=null,xi.a=null,f(Ya,"id","creating-a-strong-baseline"),f(Ya,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ya,"href","#creating-a-strong-baseline"),f(ha,"class","relative group"),f(lo,"href","/course/chapter7/6"),f(Qa,"id","using-your-finetuned-model"),f(Qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Qa,"href","#using-your-finetuned-model"),f(ca,"class","relative group")},m(e,p){s(document.head,u),i(e,$,p),k(d,e,p),i(e,E,p),i(e,C,p),s(C,x),s(x,A),k(P,A,null),s(C,T),s(C,S),s(S,I),i(e,D,p),dr[O].m(e,p),i(e,V,p),i(e,J,p),s(J,Z),s(J,X),s(X,Q),s(J,L),i(e,M,p),k(se,e,p),i(e,R,p),i(e,Y,p),s(Y,ae),s(Y,W),s(W,ie),s(Y,B),s(Y,ne),s(ne,fe),s(Y,be),i(e,Se,p),i(e,G,p),i(e,ge,p),i(e,re,p),s(re,ve),i(e,Te,p),i(e,oe,p),s(oe,ye),s(ye,$e),k(_e,$e,null),s(oe,le),s(oe,pe),s(pe,me),i(e,b,p),i(e,N,p),s(N,we),s(N,he),s(he,ce),s(N,Ie),i(e,w,p),k(H,e,p),i(e,Ge,p),k(xe,e,p),i(e,ee,p),i(e,te,p),s(te,Ee),s(te,Be),s(Be,ke),s(te,Xe),s(te,Ye),s(Ye,rs),s(te,Ce),s(te,We),s(We,os),s(te,cs),s(te,Ke),s(Ke,Xs),s(te,us),s(te,Pe),s(Pe,ks),s(te,zs),s(te,Ne),s(Ne,ds),s(te,U),i(e,ue,p),k(Ve,e,p),i(e,Ys,p),k(de,e,p),i(e,fs,p),k(ls,e,p),i(e,Ds,p),i(e,Ae,p),s(Ae,Ks),s(Ae,is),s(is,qe),s(Ae,Vs),s(Ae,Je),s(Je,Js),s(Ae,He),i(e,Zs,p),k(Ze,e,p),i(e,Qs,p),k(je,e,p),i(e,js,p),i(e,Oe,p),s(Oe,bt),s(Oe,$s),s($s,xs),s(Oe,pn),s(Oe,vt),s(vt,et),s(Oe,da),s(Oe,gs),s(gs,fa),s(Oe,yt),s(Oe,Rt),s(Rt,ga),s(Oe,st),i(e,Ft,p),k(Qe,e,p),i(e,_a,p),i(e,ze,p),s(ze,mn),s(ze,Gt),s(Gt,wa),s(ze,Es),s(ze,_s),s(_s,Nt),s(ze,tt),s(ze,Ht),s(Ht,Ut),s(ze,hn),s(ze,kt),s(kt,jt),s(ze,cn),s(ze,$t),s($t,es),s(ze,at),i(e,ba,p),k(Ts,e,p),i(e,va,p),i(e,xt,p),s(xt,un),i(e,Et,p),k(nt,e,p),i(e,ya,p),k(qs,e,p),i(e,ka,p),i(e,ss,p),s(ss,ps),s(ss,Mt),s(Mt,Wt),s(ss,dn),s(ss,Bt),s(Bt,Xt),s(ss,fn),s(ss,Yt),s(Yt,Kt),s(ss,gn),i(e,ja,p),k(Ss,e,p),i(e,rt,p),k(ot,e,p),i(e,Vt,p),i(e,ws,p),s(ws,$a),i(e,As,p),i(e,bs,p),s(bs,ts),s(bs,Jt),s(bs,lt),i(e,Zt,p),i(e,m,p),s(m,q),s(m,xa),s(xa,kr),s(m,jr),i(e,it,p),k(Tt,e,p),i(e,_n,p),i(e,pt,p),s(pt,$r),i(e,wn,p),i(e,Os,p),s(Os,ms),s(ms,Ea),k(qt,Ea,null),s(Os,Ta),s(Os,qa),s(qa,xr),i(e,Qt,p),i(e,mt,p),s(mt,za),s(mt,ea),s(ea,Er),s(mt,zt),i(e,bn,p),i(e,ht,p),s(ht,Da),s(Da,Cs),s(Cs,sa),s(sa,Dt),s(Cs,Tr),s(Cs,Sa),s(Sa,qr),s(Cs,zr),s(Cs,ct),s(ct,Dr),s(ht,Aa),s(ht,Ue),s(Ue,Ps),s(Ps,ta),s(ta,St),s(St,Ls),s(Ps,Oa),s(Ps,Ca),s(Ca,Sr),s(Ps,Ar),s(Ps,hs),s(hs,Or),s(Ue,Cr),s(Ue,Is),s(Is,aa),s(aa,Rs),s(Rs,Pr),s(Is,Kp),s(Is,Co),s(Co,Vp),s(Is,Jp),s(Is,Lr),s(Lr,Zp),s(Ue,Qp),s(Ue,na),s(na,Ir),s(Ir,vn),s(vn,em),s(na,sm),s(na,yn),s(yn,tm),s(yn,Po),s(Po,am),s(yn,nm),s(na,rm),s(na,Rr),s(Rr,om),s(Ue,lm),s(Ue,ra),s(ra,Fr),s(Fr,kn),s(kn,im),s(ra,pm),s(ra,Lo),s(Lo,mm),s(ra,hm),s(ra,Gr),s(Gr,cm),s(Ue,um),s(Ue,oa),s(oa,Nr),s(Nr,jn),s(jn,dm),s(oa,fm),s(oa,Io),s(Io,gm),s(oa,_m),s(oa,Hr),s(Hr,wm),s(Ue,bm),s(Ue,la),s(la,Ur),s(Ur,$n),s($n,vm),s(la,ym),s(la,Ro),s(Ro,km),s(la,jm),s(la,Mr),s(Mr,$m),i(e,Wl,p),i(e,Wr,p),s(Wr,xm),i(e,Bl,p),i(e,Pa,p),s(Pa,Em),s(Pa,Fo),s(Fo,Tm),s(Pa,qm),i(e,Xl,p),i(e,ia,p),s(ia,xn),s(ia,zm),s(ia,En),i(e,Yl,p),i(e,Br,p),s(Br,Dm),i(e,Kl,p),k(La,e,p),i(e,Vl,p),i(e,pa,p),s(pa,Ia),s(Ia,Go),k(Tn,Go,null),s(pa,Sm),s(pa,No),s(No,Am),i(e,Jl,p),k(qn,e,p),i(e,Zl,p),i(e,Ra,p),s(Ra,Om),s(Ra,Ho),s(Ho,Cm),s(Ra,Pm),i(e,Ql,p),k(zn,e,p),i(e,ei,p),k(Fa,e,p),i(e,si,p),i(e,Xr,p),s(Xr,Lm),i(e,ti,p),k(Dn,e,p),i(e,ai,p),k(Sn,e,p),i(e,ni,p),i(e,vs,p),s(vs,Im),s(vs,Uo),s(Uo,Rm),s(vs,Fm),s(vs,Mo),s(Mo,Gm),s(vs,Nm),s(vs,Yr),s(Yr,Hm),s(vs,Um),s(vs,Wo),s(Wo,Mm),s(vs,Wm),i(e,ri,p),k(An,e,p),i(e,oi,p),k(On,e,p),i(e,li,p),i(e,Fs,p),s(Fs,Bm),s(Fs,Bo),s(Bo,Xm),s(Fs,Ym),s(Fs,Xo),s(Xo,Km),s(Fs,Vm),s(Fs,Kr),s(Kr,Jm),s(Fs,Zm),i(e,ii,p),i(e,Ga,p),s(Ga,Qm),s(Ga,Yo),s(Yo,eh),s(Ga,sh),i(e,pi,p),k(Cn,e,p),i(e,mi,p),i(e,At,p),s(At,th),s(At,Ko),s(Ko,ah),s(At,nh),s(At,Vo),s(Vo,rh),s(At,oh),i(e,hi,p),i(e,Ot,p),s(Ot,lh),s(Ot,Jo),s(Jo,ih),s(Ot,ph),s(Ot,Zo),s(Zo,mh),s(Ot,hh),i(e,ci,p),k(Pn,e,p),i(e,ui,p),i(e,Vr,p),s(Vr,ch),i(e,di,p),k(Na,e,p),i(e,fi,p),i(e,ma,p),s(ma,Ha),s(Ha,Qo),k(Ln,Qo,null),s(ma,uh),s(ma,el),s(el,dh),i(e,gi,p),k(In,e,p),i(e,_i,p),i(e,Jr,p),s(Jr,fh),i(e,wi,p),i(e,Ua,p),s(Ua,gh),s(Ua,Rn),s(Rn,_h),s(Ua,wh),i(e,bi,p),k(Fn,e,p),i(e,vi,p),i(e,Ct,p),s(Ct,bh),s(Ct,sl),s(sl,vh),s(Ct,yh),s(Ct,tl),s(tl,kh),s(Ct,jh),i(e,yi,p),k(Ma,e,p),i(e,ki,p),i(e,Gn,p),s(Gn,$h),ji.m(ig,Gn),i(e,$i,p),i(e,Nn,p),s(Nn,xh),xi.m(pg,Nn),i(e,Ei,p),i(e,Wa,p),s(Wa,Eh),s(Wa,al),s(al,Th),s(Wa,qh),i(e,Ti,p),k(Hn,e,p),i(e,qi,p),i(e,Zr,p),s(Zr,zh),i(e,zi,p),k(Un,e,p),i(e,Di,p),i(e,Ba,p),s(Ba,Dh),s(Ba,nl),s(nl,Sh),s(Ba,Ah),i(e,Si,p),k(Mn,e,p),i(e,Ai,p),k(Wn,e,p),i(e,Oi,p),i(e,as,p),s(as,Oh),s(as,rl),s(rl,Ch),s(as,Ph),s(as,ol),s(ol,Lh),s(as,Ih),s(as,ll),s(ll,Rh),s(as,Fh),s(as,il),s(il,Gh),s(as,Nh),s(as,pl),s(pl,Hh),s(as,Uh),i(e,Ci,p),k(Bn,e,p),i(e,Pi,p),k(Xn,e,p),i(e,Li,p),i(e,ns,p),s(ns,Mh),s(ns,ml),s(ml,Wh),s(ns,Bh),s(ns,hl),s(hl,Xh),s(ns,Yh),s(ns,cl),s(cl,Kh),s(ns,Vh),s(ns,ul),s(ul,Jh),s(ns,Zh),s(ns,dl),s(dl,Qh),s(ns,ec),i(e,Ii,p),k(Xa,e,p),i(e,Ri,p),i(e,Qr,p),s(Qr,sc),i(e,Fi,p),i(e,ha,p),s(ha,Ya),s(Ya,fl),k(Yn,fl,null),s(ha,tc),s(ha,gl),s(gl,ac),i(e,Gi,p),i(e,Gs,p),s(Gs,nc),s(Gs,_l),s(_l,rc),s(Gs,oc),s(Gs,wl),s(wl,lc),s(Gs,ic),s(Gs,bl),s(bl,pc),s(Gs,mc),i(e,Ni,p),k(Kn,e,p),i(e,Hi,p),i(e,eo,p),s(eo,hc),i(e,Ui,p),k(Vn,e,p),i(e,Mi,p),i(e,Ka,p),s(Ka,cc),s(Ka,vl),s(vl,uc),s(Ka,dc),i(e,Wi,p),k(Jn,e,p),i(e,Bi,p),k(Zn,e,p),i(e,Xi,p),i(e,so,p),s(so,fc),i(e,Yi,p),k(Qn,e,p),i(e,Ki,p),i(e,to,p),s(to,gc),i(e,Vi,p),k(er,e,p),i(e,Ji,p),k(sr,e,p),i(e,Zi,p),i(e,Va,p),s(Va,_c),s(Va,yl),s(yl,wc),s(Va,bc),i(e,Qi,p),fr[ut].m(e,p),i(e,ao,p),k(Ja,e,p),i(e,ep,p),i(e,no,p),s(no,vc),i(e,sp,p),k(tr,e,p),i(e,tp,p),i(e,ro,p),s(ro,yc),i(e,ap,p),k(ar,e,p),i(e,np,p),Re&&Re.m(e,p),i(e,oo,p),i(e,Za,p),s(Za,kc),s(Za,lo),s(lo,jc),s(Za,$c),i(e,rp,p),i(e,Ns,p),s(Ns,xc),s(Ns,kl),s(kl,Ec),s(Ns,Tc),s(Ns,jl),s(jl,qc),s(Ns,zc),s(Ns,$l),s($l,Dc),s(Ns,Sc),i(e,op,p),gr[ft].m(e,p),i(e,io,p),i(e,po,p),s(po,Ac),i(e,lp,p),k(nr,e,p),i(e,ip,p),i(e,Pt,p),s(Pt,Oc),s(Pt,xl),s(xl,Cc),s(Pt,Pc),s(Pt,El),s(El,Lc),s(Pt,Ic),i(e,pp,p),k(rr,e,p),i(e,mp,p),k(or,e,p),i(e,hp,p),i(e,De,p),s(De,Rc),s(De,Tl),s(Tl,Fc),s(De,Gc),s(De,ql),s(ql,Nc),s(De,Hc),s(De,zl),s(zl,Uc),s(De,Mc),s(De,Dl),s(Dl,Wc),s(De,Bc),s(De,Sl),s(Sl,Xc),s(De,Yc),s(De,Al),s(Al,Kc),s(De,Vc),s(De,Ol),s(Ol,Jc),s(De,Zc),s(De,Cl),s(Cl,Qc),s(De,eu),i(e,cp,p),_r[_t].m(e,p),i(e,mo,p),Fe&&Fe.m(e,p),i(e,ho,p),i(e,ca,p),s(ca,Qa),s(Qa,Pl),k(lr,Pl,null),s(ca,su),s(ca,Ll),s(Ll,tu),i(e,up,p),i(e,en,p),s(en,au),s(en,Il),s(Il,nu),s(en,ru),i(e,dp,p),k(ir,e,p),i(e,fp,p),i(e,co,p),s(co,ou),i(e,gp,p),k(pr,e,p),i(e,_p,p),i(e,uo,p),s(uo,lu),i(e,wp,p),k(mr,e,p),i(e,bp,p),k(hr,e,p),i(e,vp,p),i(e,sn,p),s(sn,iu),s(sn,Rl),s(Rl,pu),s(sn,mu),i(e,yp,p),k(cr,e,p),i(e,kp,p),k(ur,e,p),i(e,jp,p),i(e,fo,p),s(fo,hu),i(e,$p,p),i(e,go,p),s(go,cu),xp=!0},p(e,[p]){const wr={};p&1&&(wr.fw=e[0]),d.$set(wr);let _o=O;O=bu(e),O!==_o&&(Oo(),_(dr[_o],1,1,()=>{dr[_o]=null}),Ao(),F=dr[O],F||(F=dr[O]=wu[O](e),F.c()),g(F,1),F.m(V.parentNode,V));const Fl={};p&2&&(Fl.$$scope={dirty:p,ctx:e}),ls.$set(Fl);const Gl={};p&2&&(Gl.$$scope={dirty:p,ctx:e}),La.$set(Gl);const br={};p&2&&(br.$$scope={dirty:p,ctx:e}),Fa.$set(br);const Nl={};p&2&&(Nl.$$scope={dirty:p,ctx:e}),Na.$set(Nl);const ua={};p&2&&(ua.$$scope={dirty:p,ctx:e}),Ma.$set(ua);const Hl={};p&2&&(Hl.$$scope={dirty:p,ctx:e}),Xa.$set(Hl);let wo=ut;ut=yu(e),ut!==wo&&(Oo(),_(fr[wo],1,1,()=>{fr[wo]=null}),Ao(),dt=fr[ut],dt||(dt=fr[ut]=vu[ut](e),dt.c()),g(dt,1),dt.m(ao.parentNode,ao));const Ul={};p&2&&(Ul.$$scope={dirty:p,ctx:e}),Ja.$set(Ul),e[0]==="pt"?Re?p&1&&g(Re,1):(Re=rg(),Re.c(),g(Re,1),Re.m(oo.parentNode,oo)):Re&&(Oo(),_(Re,1,1,()=>{Re=null}),Ao());let tn=ft;ft=ju(e),ft!==tn&&(Oo(),_(gr[tn],1,1,()=>{gr[tn]=null}),Ao(),gt=gr[ft],gt||(gt=gr[ft]=ku[ft](e),gt.c()),g(gt,1),gt.m(io.parentNode,io));let bo=_t;_t=xu(e),_t!==bo&&(Oo(),_(_r[bo],1,1,()=>{_r[bo]=null}),Ao(),wt=_r[_t],wt||(wt=_r[_t]=$u[_t](e),wt.c()),g(wt,1),wt.m(mo.parentNode,mo)),e[0]==="pt"?Fe?p&1&&g(Fe,1):(Fe=og(e),Fe.c(),g(Fe,1),Fe.m(ho.parentNode,ho)):Fe&&(Oo(),_(Fe,1,1,()=>{Fe=null}),Ao())},i(e){xp||(g(d.$$.fragment,e),g(P.$$.fragment,e),g(F),g(se.$$.fragment,e),g(_e.$$.fragment,e),g(H.$$.fragment,e),g(xe.$$.fragment,e),g(Ve.$$.fragment,e),g(de.$$.fragment,e),g(ls.$$.fragment,e),g(Ze.$$.fragment,e),g(je.$$.fragment,e),g(Qe.$$.fragment,e),g(Ts.$$.fragment,e),g(nt.$$.fragment,e),g(qs.$$.fragment,e),g(Ss.$$.fragment,e),g(ot.$$.fragment,e),g(Tt.$$.fragment,e),g(qt.$$.fragment,e),g(La.$$.fragment,e),g(Tn.$$.fragment,e),g(qn.$$.fragment,e),g(zn.$$.fragment,e),g(Fa.$$.fragment,e),g(Dn.$$.fragment,e),g(Sn.$$.fragment,e),g(An.$$.fragment,e),g(On.$$.fragment,e),g(Cn.$$.fragment,e),g(Pn.$$.fragment,e),g(Na.$$.fragment,e),g(Ln.$$.fragment,e),g(In.$$.fragment,e),g(Fn.$$.fragment,e),g(Ma.$$.fragment,e),g(Hn.$$.fragment,e),g(Un.$$.fragment,e),g(Mn.$$.fragment,e),g(Wn.$$.fragment,e),g(Bn.$$.fragment,e),g(Xn.$$.fragment,e),g(Xa.$$.fragment,e),g(Yn.$$.fragment,e),g(Kn.$$.fragment,e),g(Vn.$$.fragment,e),g(Jn.$$.fragment,e),g(Zn.$$.fragment,e),g(Qn.$$.fragment,e),g(er.$$.fragment,e),g(sr.$$.fragment,e),g(dt),g(Ja.$$.fragment,e),g(tr.$$.fragment,e),g(ar.$$.fragment,e),g(Re),g(gt),g(nr.$$.fragment,e),g(rr.$$.fragment,e),g(or.$$.fragment,e),g(wt),g(Fe),g(lr.$$.fragment,e),g(ir.$$.fragment,e),g(pr.$$.fragment,e),g(mr.$$.fragment,e),g(hr.$$.fragment,e),g(cr.$$.fragment,e),g(ur.$$.fragment,e),xp=!0)},o(e){_(d.$$.fragment,e),_(P.$$.fragment,e),_(F),_(se.$$.fragment,e),_(_e.$$.fragment,e),_(H.$$.fragment,e),_(xe.$$.fragment,e),_(Ve.$$.fragment,e),_(de.$$.fragment,e),_(ls.$$.fragment,e),_(Ze.$$.fragment,e),_(je.$$.fragment,e),_(Qe.$$.fragment,e),_(Ts.$$.fragment,e),_(nt.$$.fragment,e),_(qs.$$.fragment,e),_(Ss.$$.fragment,e),_(ot.$$.fragment,e),_(Tt.$$.fragment,e),_(qt.$$.fragment,e),_(La.$$.fragment,e),_(Tn.$$.fragment,e),_(qn.$$.fragment,e),_(zn.$$.fragment,e),_(Fa.$$.fragment,e),_(Dn.$$.fragment,e),_(Sn.$$.fragment,e),_(An.$$.fragment,e),_(On.$$.fragment,e),_(Cn.$$.fragment,e),_(Pn.$$.fragment,e),_(Na.$$.fragment,e),_(Ln.$$.fragment,e),_(In.$$.fragment,e),_(Fn.$$.fragment,e),_(Ma.$$.fragment,e),_(Hn.$$.fragment,e),_(Un.$$.fragment,e),_(Mn.$$.fragment,e),_(Wn.$$.fragment,e),_(Bn.$$.fragment,e),_(Xn.$$.fragment,e),_(Xa.$$.fragment,e),_(Yn.$$.fragment,e),_(Kn.$$.fragment,e),_(Vn.$$.fragment,e),_(Jn.$$.fragment,e),_(Zn.$$.fragment,e),_(Qn.$$.fragment,e),_(er.$$.fragment,e),_(sr.$$.fragment,e),_(dt),_(Ja.$$.fragment,e),_(tr.$$.fragment,e),_(ar.$$.fragment,e),_(Re),_(gt),_(nr.$$.fragment,e),_(rr.$$.fragment,e),_(or.$$.fragment,e),_(wt),_(Fe),_(lr.$$.fragment,e),_(ir.$$.fragment,e),_(pr.$$.fragment,e),_(mr.$$.fragment,e),_(hr.$$.fragment,e),_(cr.$$.fragment,e),_(ur.$$.fragment,e),xp=!1},d(e){t(u),e&&t($),j(d,e),e&&t(E),e&&t(C),j(P),e&&t(D),dr[O].d(e),e&&t(V),e&&t(J),e&&t(M),j(se,e),e&&t(R),e&&t(Y),e&&t(Se),e&&t(G),e&&t(ge),e&&t(re),e&&t(Te),e&&t(oe),j(_e),e&&t(b),e&&t(N),e&&t(w),j(H,e),e&&t(Ge),j(xe,e),e&&t(ee),e&&t(te),e&&t(ue),j(Ve,e),e&&t(Ys),j(de,e),e&&t(fs),j(ls,e),e&&t(Ds),e&&t(Ae),e&&t(Zs),j(Ze,e),e&&t(Qs),j(je,e),e&&t(js),e&&t(Oe),e&&t(Ft),j(Qe,e),e&&t(_a),e&&t(ze),e&&t(ba),j(Ts,e),e&&t(va),e&&t(xt),e&&t(Et),j(nt,e),e&&t(ya),j(qs,e),e&&t(ka),e&&t(ss),e&&t(ja),j(Ss,e),e&&t(rt),j(ot,e),e&&t(Vt),e&&t(ws),e&&t(As),e&&t(bs),e&&t(Zt),e&&t(m),e&&t(it),j(Tt,e),e&&t(_n),e&&t(pt),e&&t(wn),e&&t(Os),j(qt),e&&t(Qt),e&&t(mt),e&&t(bn),e&&t(ht),e&&t(Wl),e&&t(Wr),e&&t(Bl),e&&t(Pa),e&&t(Xl),e&&t(ia),e&&t(Yl),e&&t(Br),e&&t(Kl),j(La,e),e&&t(Vl),e&&t(pa),j(Tn),e&&t(Jl),j(qn,e),e&&t(Zl),e&&t(Ra),e&&t(Ql),j(zn,e),e&&t(ei),j(Fa,e),e&&t(si),e&&t(Xr),e&&t(ti),j(Dn,e),e&&t(ai),j(Sn,e),e&&t(ni),e&&t(vs),e&&t(ri),j(An,e),e&&t(oi),j(On,e),e&&t(li),e&&t(Fs),e&&t(ii),e&&t(Ga),e&&t(pi),j(Cn,e),e&&t(mi),e&&t(At),e&&t(hi),e&&t(Ot),e&&t(ci),j(Pn,e),e&&t(ui),e&&t(Vr),e&&t(di),j(Na,e),e&&t(fi),e&&t(ma),j(Ln),e&&t(gi),j(In,e),e&&t(_i),e&&t(Jr),e&&t(wi),e&&t(Ua),e&&t(bi),j(Fn,e),e&&t(vi),e&&t(Ct),e&&t(yi),j(Ma,e),e&&t(ki),e&&t(Gn),e&&t($i),e&&t(Nn),e&&t(Ei),e&&t(Wa),e&&t(Ti),j(Hn,e),e&&t(qi),e&&t(Zr),e&&t(zi),j(Un,e),e&&t(Di),e&&t(Ba),e&&t(Si),j(Mn,e),e&&t(Ai),j(Wn,e),e&&t(Oi),e&&t(as),e&&t(Ci),j(Bn,e),e&&t(Pi),j(Xn,e),e&&t(Li),e&&t(ns),e&&t(Ii),j(Xa,e),e&&t(Ri),e&&t(Qr),e&&t(Fi),e&&t(ha),j(Yn),e&&t(Gi),e&&t(Gs),e&&t(Ni),j(Kn,e),e&&t(Hi),e&&t(eo),e&&t(Ui),j(Vn,e),e&&t(Mi),e&&t(Ka),e&&t(Wi),j(Jn,e),e&&t(Bi),j(Zn,e),e&&t(Xi),e&&t(so),e&&t(Yi),j(Qn,e),e&&t(Ki),e&&t(to),e&&t(Vi),j(er,e),e&&t(Ji),j(sr,e),e&&t(Zi),e&&t(Va),e&&t(Qi),fr[ut].d(e),e&&t(ao),j(Ja,e),e&&t(ep),e&&t(no),e&&t(sp),j(tr,e),e&&t(tp),e&&t(ro),e&&t(ap),j(ar,e),e&&t(np),Re&&Re.d(e),e&&t(oo),e&&t(Za),e&&t(rp),e&&t(Ns),e&&t(op),gr[ft].d(e),e&&t(io),e&&t(po),e&&t(lp),j(nr,e),e&&t(ip),e&&t(Pt),e&&t(pp),j(rr,e),e&&t(mp),j(or,e),e&&t(hp),e&&t(De),e&&t(cp),_r[_t].d(e),e&&t(mo),Fe&&Fe.d(e),e&&t(ho),e&&t(ca),j(lr),e&&t(up),e&&t(en),e&&t(dp),j(ir,e),e&&t(fp),e&&t(co),e&&t(gp),j(pr,e),e&&t(_p),e&&t(uo),e&&t(wp),j(mr,e),e&&t(bp),j(hr,e),e&&t(vp),e&&t(sn),e&&t(yp),j(cr,e),e&&t(kp),j(ur,e),e&&t(jp),e&&t(fo),e&&t($p),e&&t(go)}}}const Og={local:"summarization",sections:[{local:"preparing-a-multilingual-corpus",title:"Preparing a multilingual corpus"},{local:"models-for-text-summarization",title:"Models for text summarization"},{local:"preprocessing-the-data",title:"Preprocessing the data"},{local:"metrics-for-text-summarization",sections:[{local:"creating-a-strong-baseline",title:"Creating a strong baseline"}],title:"Metrics for text summarization"},{local:"finetuning-mt5-with-the-trainer-api",title:"Fine-tuning mT5 with the `Trainer` API"},{local:"finetuning-mt5-with-keras",title:"Fine-tuning mT5 with Keras"},{local:"finetuning-mt5-with-accelerate",sections:[{local:"preparing-everything-for-training",title:"Preparing everything for training"},{local:"training-loop",title:"Training loop"}],title:"Fine-tuning mT5 with \u{1F917} Accelerate"},{local:"using-your-finetuned-model",title:"Using your fine-tuned model"}],title:"Summarization"};function Cg(K,u,$){let d="pt";return dg(()=>{const E=new URLSearchParams(window.location.search);$(0,d=E.get("fw")||"pt")}),[d]}class Hg extends mg{constructor(u){super();hg(this,u,Cg,Ag,cg,{})}}export{Hg as default,Og as metadata};
