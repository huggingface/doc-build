import{S as ui,i as mi,s as _i,e as l,k as h,w as g,t as s,M as gi,c as i,d as a,m as f,a as n,x as w,h as o,b as p,G as e,g as d,y as v,q as $,o as E,B as y,v as wi}from"../../chunks/vendor-hf-doc-builder.js";import{T as Xo}from"../../chunks/Tip-hf-doc-builder.js";import{Y as vi}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ba}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as I}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as $i}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Ei(st){let c,A,u,b,S;return{c(){c=l("p"),A=s("\u270E If you\u2019re wondering why there\u2019s a "),u=l("code"),b=s("!"),S=s(" character in the above shell commands, that\u2019s because we\u2019re running them within a Jupyter notebook. Simply remove the prefix if you want to download and unzip the dataset within a terminal.")},l(m){c=i(m,"P",{});var j=n(c);A=o(j,"\u270E If you\u2019re wondering why there\u2019s a "),u=i(j,"CODE",{});var x=n(u);b=o(x,"!"),x.forEach(a),S=o(j," character in the above shell commands, that\u2019s because we\u2019re running them within a Jupyter notebook. Simply remove the prefix if you want to download and unzip the dataset within a terminal."),j.forEach(a)},m(m,j){d(m,c,j),e(c,A),e(c,u),e(u,b),e(c,S)},d(m){m&&a(c)}}}function yi(st){let c,A,u,b,S,m,j,x,k,z,C,q,_,F;return{c(){c=l("p"),A=s("The "),u=l("code"),b=s("data_files"),S=s(" argument of the "),m=l("code"),j=s("load_dataset()"),x=s(" function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting "),k=l("code"),z=s('data_files="*.json"'),C=s("). See the \u{1F917} Datasets "),q=l("a"),_=s("documentation"),F=s(" for more details."),this.h()},l(T){c=i(T,"P",{});var D=n(c);A=o(D,"The "),u=i(D,"CODE",{});var Ht=n(u);b=o(Ht,"data_files"),Ht.forEach(a),S=o(D," argument of the "),m=i(D,"CODE",{});var vt=n(m);j=o(vt,"load_dataset()"),vt.forEach(a),x=o(D," function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting "),k=i(D,"CODE",{});var R=n(k);z=o(R,'data_files="*.json"'),R.forEach(a),C=o(D,"). See the \u{1F917} Datasets "),q=i(D,"A",{href:!0,rel:!0});var $t=n(q);_=o($t,"documentation"),$t.forEach(a),F=o(D," for more details."),D.forEach(a),this.h()},h(){p(q,"href","https://huggingface.co/docs/datasets/loading.html#local-and-remote-files"),p(q,"rel","nofollow")},m(T,D){d(T,c,D),e(c,A),e(c,u),e(u,b),e(c,S),e(c,m),e(m,j),e(c,x),e(c,k),e(k,z),e(c,C),e(c,q),e(q,_),e(c,F)},d(T){T&&a(c)}}}function bi(st){let c,A,u,b,S,m,j,x,k,z,C;return{c(){c=l("p"),A=s("\u270F\uFE0F "),u=l("strong"),b=s("Try it out!"),S=s(" Pick another dataset hosted on GitHub or the "),m=l("a"),j=s("UCI Machine Learning Repository"),x=s(" and try loading it both locally and remotely using the techniques introduced above. For bonus points, try loading a dataset that\u2019s stored in a CSV or text format (see the "),k=l("a"),z=s("documentation"),C=s(" for more information on these formats)."),this.h()},l(q){c=i(q,"P",{});var _=n(c);A=o(_,"\u270F\uFE0F "),u=i(_,"STRONG",{});var F=n(u);b=o(F,"Try it out!"),F.forEach(a),S=o(_," Pick another dataset hosted on GitHub or the "),m=i(_,"A",{href:!0,rel:!0});var T=n(m);j=o(T,"UCI Machine Learning Repository"),T.forEach(a),x=o(_," and try loading it both locally and remotely using the techniques introduced above. For bonus points, try loading a dataset that\u2019s stored in a CSV or text format (see the "),k=i(_,"A",{href:!0,rel:!0});var D=n(k);z=o(D,"documentation"),D.forEach(a),C=o(_," for more information on these formats)."),_.forEach(a),this.h()},h(){p(m,"href","https://archive.ics.uci.edu/ml/index.php"),p(m,"rel","nofollow"),p(k,"href","https://huggingface.co/docs/datasets/loading.html#local-and-remote-files"),p(k,"rel","nofollow")},m(q,_){d(q,c,_),e(c,A),e(c,u),e(u,b),e(c,S),e(c,m),e(m,j),e(c,x),e(c,k),e(k,z),e(c,C)},d(q){q&&a(c)}}}function Di(st){let c,A,u,b,S,m,j,x,k,z,C,q,_,F,T,D,Ht,vt,R,$t,Y,ot,ce,Et,Ya,pe,Va,oa,Lt,Za,la,lt,ue,V,It,Ka,Xa,Jt,ts,es,Ft,as,ss,J,Z,Rt,os,ls,Gt,me,is,ns,Wt,_e,rs,ds,K,Mt,hs,fs,Ut,ge,cs,ps,Bt,we,us,ms,X,Yt,_s,gs,Vt,ve,ws,vs,Zt,$e,$s,Es,tt,Kt,ys,bs,Xt,Ee,Ds,js,te,ye,qs,ia,G,Ss,be,ks,As,De,xs,Ts,na,et,it,je,yt,Os,qe,Cs,ra,nt,Qs,bt,zs,Ps,da,rt,Ns,Se,Hs,Ls,ha,Dt,fa,P,Is,ke,Js,Fs,Ae,Rs,Gs,xe,Ws,Ms,ca,jt,pa,qt,ua,W,Us,Te,Bs,Ys,Oe,Vs,Zs,ma,dt,_a,N,Ks,Ce,Xs,to,Qe,eo,ao,ze,so,oo,ga,St,wa,H,lo,Pe,io,no,Ne,ro,ho,He,fo,co,va,kt,$a,At,Ea,ht,po,Le,uo,mo,ya,xt,ba,Tt,Da,O,_o,Ie,go,wo,Je,vo,$o,Fe,Eo,yo,Re,bo,Do,Ge,jo,qo,ja,Ot,qa,Ct,Sa,ee,So,ka,ft,Aa,M,ko,We,Ao,xo,Me,To,Oo,xa,Qt,Ta,ct,Co,Ue,Qo,zo,Oa,ae,Po,Ca,at,pt,Be,zt,No,Ye,Ho,Qa,Q,Lo,Ve,Io,Jo,Ze,Fo,Ro,Ke,Go,Wo,Xe,Mo,Uo,za,Pt,Pa,U,Bo,ta,Yo,Vo,ea,Zo,Ko,Na,ut,Ha;return m=new Ba({}),C=new $i({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter5/section2.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter5/section2.ipynb"}]}}),R=new vi({props:{id:"HyQgpJTkRdE"}}),Et=new Ba({}),yt=new Ba({}),Dt=new I({props:{code:`!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz`,highlighted:`!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-train.json.gz
!wget https://github.com/crux82/squad-it/raw/master/SQuAD_it-test.json.gz`}}),jt=new I({props:{code:"!gzip -dkv SQuAD_it-*.json.gz",highlighted:"!gzip -dkv SQuAD_it-*.json.gz"}}),qt=new I({props:{code:`SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json`,highlighted:`SQuAD_it-test.json.gz:	   87.4% -- replaced with SQuAD_it-test.json
SQuAD_it-train.json.gz:	   82.2% -- replaced with SQuAD_it-train.json`}}),dt=new Xo({props:{$$slots:{default:[Ei]},$$scope:{ctx:st}}}),St=new I({props:{code:"",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

squad_it_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;SQuAD_it-train.json&quot;</span>, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),kt=new I({props:{code:"squad_it_dataset",highlighted:"squad_it_dataset"}}),At=new I({props:{code:`DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;paragraphs&#x27;</span>],
        num_rows: <span class="hljs-number">442</span>
    })
})`}}),xt=new I({props:{code:'squad_it_dataset["train"][0]',highlighted:'squad_it_dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]'}}),Tt=new I({props:{code:`{
    "title": "Terremoto del Sichuan del 2008",
    "paragraphs": [
        {
            "context": "Il terremoto del Sichuan del 2008 o il terremoto...",
            "qas": [
                {
                    "answers": [{"answer_start": 29, "text": "2008"}],
                    "id": "56cdca7862d2951400fa6826",
                    "question": "In quale anno si \xE8 verificato il terremoto nel Sichuan?",
                },
                ...
            ],
        },
        ...
    ],
}`,highlighted:`{
    <span class="hljs-string">&quot;title&quot;</span>: <span class="hljs-string">&quot;Terremoto del Sichuan del 2008&quot;</span>,
    <span class="hljs-string">&quot;paragraphs&quot;</span>: [
        {
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;Il terremoto del Sichuan del 2008 o il terremoto...&quot;</span>,
            <span class="hljs-string">&quot;qas&quot;</span>: [
                {
                    <span class="hljs-string">&quot;answers&quot;</span>: [{<span class="hljs-string">&quot;answer_start&quot;</span>: <span class="hljs-number">29</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;2008&quot;</span>}],
                    <span class="hljs-string">&quot;id&quot;</span>: <span class="hljs-string">&quot;56cdca7862d2951400fa6826&quot;</span>,
                    <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;In quale anno si \xE8 verificato il terremoto nel Sichuan?&quot;</span>,
                },
                ...
            ],
        },
        ...
    ],
}`}}),Ot=new I({props:{code:`data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset`,highlighted:`data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;SQuAD_it-train.json&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;SQuAD_it-test.json&quot;</span>}
squad_it_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, field=<span class="hljs-string">&quot;data&quot;</span>)
squad_it_dataset`}}),Ct=new I({props:{code:`DatasetDict({
    train: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 442
    })
    test: Dataset({
        features: ['title', 'paragraphs'],
        num_rows: 48
    })
})`,highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;paragraphs&#x27;</span>],
        num_rows: <span class="hljs-number">442</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;title&#x27;</span>, <span class="hljs-string">&#x27;paragraphs&#x27;</span>],
        num_rows: <span class="hljs-number">48</span>
    })
})`}}),ft=new Xo({props:{$$slots:{default:[yi]},$$scope:{ctx:st}}}),Qt=new I({props:{code:`data_files = {"train": "SQuAD_it-train.json.gz", "test": "SQuAD_it-test.json.gz"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")`,highlighted:`data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;SQuAD_it-train.json.gz&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;SQuAD_it-test.json.gz&quot;</span>}
squad_it_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),zt=new Ba({}),Pt=new I({props:{code:`url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")`,highlighted:`url = <span class="hljs-string">&quot;https://github.com/crux82/squad-it/raw/master/&quot;</span>
data_files = {
    <span class="hljs-string">&quot;train&quot;</span>: url + <span class="hljs-string">&quot;SQuAD_it-train.json.gz&quot;</span>,
    <span class="hljs-string">&quot;test&quot;</span>: url + <span class="hljs-string">&quot;SQuAD_it-test.json.gz&quot;</span>,
}
squad_it_dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=data_files, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),ut=new Xo({props:{$$slots:{default:[bi]},$$scope:{ctx:st}}}),{c(){c=l("meta"),A=h(),u=l("h1"),b=l("a"),S=l("span"),g(m.$$.fragment),j=h(),x=l("span"),k=s("What if my dataset isn't on the Hub?"),z=h(),g(C.$$.fragment),q=h(),_=l("p"),F=s("You know how to use the "),T=l("a"),D=s("Hugging Face Hub"),Ht=s(" to download datasets, but you\u2019ll often find yourself working with data that is stored either on your laptop or on a remote server. In this section we\u2019ll show you how \u{1F917} Datasets can be used to load datasets that aren\u2019t available on the Hugging Face Hub."),vt=h(),g(R.$$.fragment),$t=h(),Y=l("h2"),ot=l("a"),ce=l("span"),g(Et.$$.fragment),Ya=h(),pe=l("span"),Va=s("Working with local and remote datasets"),oa=h(),Lt=l("p"),Za=s("\u{1F917} Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:"),la=h(),lt=l("table"),ue=l("thead"),V=l("tr"),It=l("th"),Ka=s("Data format"),Xa=h(),Jt=l("th"),ts=s("Loading script"),es=h(),Ft=l("th"),as=s("Example"),ss=h(),J=l("tbody"),Z=l("tr"),Rt=l("td"),os=s("CSV & TSV"),ls=h(),Gt=l("td"),me=l("code"),is=s("csv"),ns=h(),Wt=l("td"),_e=l("code"),rs=s('load_dataset("csv", data_files="my_file.csv")'),ds=h(),K=l("tr"),Mt=l("td"),hs=s("Text files"),fs=h(),Ut=l("td"),ge=l("code"),cs=s("text"),ps=h(),Bt=l("td"),we=l("code"),us=s('load_dataset("text", data_files="my_file.txt")'),ms=h(),X=l("tr"),Yt=l("td"),_s=s("JSON & JSON Lines"),gs=h(),Vt=l("td"),ve=l("code"),ws=s("json"),vs=h(),Zt=l("td"),$e=l("code"),$s=s('load_dataset("json", data_files="my_file.jsonl")'),Es=h(),tt=l("tr"),Kt=l("td"),ys=s("Pickled DataFrames"),bs=h(),Xt=l("td"),Ee=l("code"),Ds=s("pandas"),js=h(),te=l("td"),ye=l("code"),qs=s('load_dataset("pandas", data_files="my_dataframe.pkl")'),ia=h(),G=l("p"),Ss=s("As shown in the table, for each data format we just need to specify the type of loading script in the "),be=l("code"),ks=s("load_dataset()"),As=s(" function, along with a "),De=l("code"),xs=s("data_files"),Ts=s(" argument that specifies the path to one or more files. Let\u2019s start by loading a dataset from local files; later we\u2019ll see how to do the same with remote files."),na=h(),et=l("h2"),it=l("a"),je=l("span"),g(yt.$$.fragment),Os=h(),qe=l("span"),Cs=s("Loading a local dataset"),ra=h(),nt=l("p"),Qs=s("For this example we\u2019ll use the "),bt=l("a"),zs=s("SQuAD-it dataset"),Ps=s(", which is a large-scale dataset for question answering in Italian."),da=h(),rt=l("p"),Ns=s("The training and test splits are hosted on GitHub, so we can download them with a simple "),Se=l("code"),Hs=s("wget"),Ls=s(" command:"),ha=h(),g(Dt.$$.fragment),fa=h(),P=l("p"),Is=s("This will download two compressed files called "),ke=l("em"),Js=s("SQuAD_it-train.json.gz"),Fs=s(" and "),Ae=l("em"),Rs=s("SQuAD_it-test.json.gz"),Gs=s(", which we can decompress with the Linux "),xe=l("code"),Ws=s("gzip"),Ms=s(" command:"),ca=h(),g(jt.$$.fragment),pa=h(),g(qt.$$.fragment),ua=h(),W=l("p"),Us=s("We can see that the compressed files have been replaced with "),Te=l("em"),Bs=s("SQuAD_it-train.json"),Ys=s(" and "),Oe=l("em"),Vs=s("SQuAD_it-text.json"),Zs=s(", and that the data is stored in the JSON format."),ma=h(),g(dt.$$.fragment),_a=h(),N=l("p"),Ks=s("To load a JSON file with the "),Ce=l("code"),Xs=s("load_dataset()"),to=s(" function, we just need to know if we\u2019re dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a "),Qe=l("code"),eo=s("data"),ao=s(" field. This means we can load the dataset by specifying the "),ze=l("code"),so=s("field"),oo=s(" argument as follows:"),ga=h(),g(St.$$.fragment),wa=h(),H=l("p"),lo=s("By default, loading local files creates a "),Pe=l("code"),io=s("DatasetDict"),no=s(" object with a "),Ne=l("code"),ro=s("train"),ho=s(" split. We can see this by inspecting the "),He=l("code"),fo=s("squad_it_dataset"),co=s(" object:"),va=h(),g(kt.$$.fragment),$a=h(),g(At.$$.fragment),Ea=h(),ht=l("p"),po=s("This shows us the number of rows and the column names associated with the training set. We can view one of the examples by indexing into the "),Le=l("code"),uo=s("train"),mo=s(" split as follows:"),ya=h(),g(xt.$$.fragment),ba=h(),g(Tt.$$.fragment),Da=h(),O=l("p"),_o=s("Great, we\u2019ve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the "),Ie=l("code"),go=s("train"),wo=s(" and "),Je=l("code"),vo=s("test"),$o=s(" splits in a single "),Fe=l("code"),Eo=s("DatasetDict"),yo=s(" object so we can apply "),Re=l("code"),bo=s("Dataset.map()"),Do=s(" functions across both splits at once. To do this, we can provide a dictionary to the "),Ge=l("code"),jo=s("data_files"),qo=s(" argument that maps each split name to a file associated with that split:"),ja=h(),g(Ot.$$.fragment),qa=h(),g(Ct.$$.fragment),Sa=h(),ee=l("p"),So=s("This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on."),ka=h(),g(ft.$$.fragment),Aa=h(),M=l("p"),ko=s("The loading scripts in \u{1F917} Datasets actually support automatic decompression of the input files, so we could have skipped the use of "),We=l("code"),Ao=s("gzip"),xo=s(" by pointing the "),Me=l("code"),To=s("data_files"),Oo=s(" argument directly to the compressed files:"),xa=h(),g(Qt.$$.fragment),Ta=h(),ct=l("p"),Co=s("This can be useful if you don\u2019t want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point "),Ue=l("code"),Qo=s("data_files"),zo=s(" to the compressed files and you\u2019re good to go!"),Oa=h(),ae=l("p"),Po=s("Now that you know how to load local files on your laptop or desktop, let\u2019s take a look at loading remote files."),Ca=h(),at=l("h2"),pt=l("a"),Be=l("span"),g(zt.$$.fragment),No=h(),Ye=l("span"),Ho=s("Loading a remote dataset"),Qa=h(),Q=l("p"),Lo=s("If you\u2019re working as a data scientist or coder in a company, there\u2019s a good chance the datasets you want to analyze are stored on some remote server. Fortunately, loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the "),Ve=l("code"),Io=s("data_files"),Jo=s(" argument of "),Ze=l("code"),Fo=s("load_dataset()"),Ro=s(" to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point "),Ke=l("code"),Go=s("data_files"),Wo=s(" to the "),Xe=l("em"),Mo=s("SQuAD_it-*.json.gz"),Uo=s(" URLs as follows:"),za=h(),g(Pt.$$.fragment),Pa=h(),U=l("p"),Bo=s("This returns the same "),ta=l("code"),Yo=s("DatasetDict"),Vo=s(" object obtained above, but saves us the step of manually downloading and decompressing the "),ea=l("em"),Zo=s("SQuAD_it-*.json.gz"),Ko=s(" files. This wraps up our foray into the various ways to load datasets that aren\u2019t hosted on the Hugging Face Hub. Now that we\u2019ve got a dataset to play with, let\u2019s get our hands dirty with various data-wrangling techniques!"),Na=h(),g(ut.$$.fragment),this.h()},l(t){const r=gi('[data-svelte="svelte-1phssyn"]',document.head);c=i(r,"META",{name:!0,content:!0}),r.forEach(a),A=f(t),u=i(t,"H1",{class:!0});var Nt=n(u);b=i(Nt,"A",{id:!0,class:!0,href:!0});var aa=n(b);S=i(aa,"SPAN",{});var sa=n(S);w(m.$$.fragment,sa),sa.forEach(a),aa.forEach(a),j=f(Nt),x=i(Nt,"SPAN",{});var tl=n(x);k=o(tl,"What if my dataset isn't on the Hub?"),tl.forEach(a),Nt.forEach(a),z=f(t),w(C.$$.fragment,t),q=f(t),_=i(t,"P",{});var La=n(_);F=o(La,"You know how to use the "),T=i(La,"A",{href:!0,rel:!0});var el=n(T);D=o(el,"Hugging Face Hub"),el.forEach(a),Ht=o(La," to download datasets, but you\u2019ll often find yourself working with data that is stored either on your laptop or on a remote server. In this section we\u2019ll show you how \u{1F917} Datasets can be used to load datasets that aren\u2019t available on the Hugging Face Hub."),La.forEach(a),vt=f(t),w(R.$$.fragment,t),$t=f(t),Y=i(t,"H2",{class:!0});var Ia=n(Y);ot=i(Ia,"A",{id:!0,class:!0,href:!0});var al=n(ot);ce=i(al,"SPAN",{});var sl=n(ce);w(Et.$$.fragment,sl),sl.forEach(a),al.forEach(a),Ya=f(Ia),pe=i(Ia,"SPAN",{});var ol=n(pe);Va=o(ol,"Working with local and remote datasets"),ol.forEach(a),Ia.forEach(a),oa=f(t),Lt=i(t,"P",{});var ll=n(Lt);Za=o(ll,"\u{1F917} Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:"),ll.forEach(a),la=f(t),lt=i(t,"TABLE",{});var Ja=n(lt);ue=i(Ja,"THEAD",{});var il=n(ue);V=i(il,"TR",{});var se=n(V);It=i(se,"TH",{align:!0});var nl=n(It);Ka=o(nl,"Data format"),nl.forEach(a),Xa=f(se),Jt=i(se,"TH",{align:!0});var rl=n(Jt);ts=o(rl,"Loading script"),rl.forEach(a),es=f(se),Ft=i(se,"TH",{align:!0});var dl=n(Ft);as=o(dl,"Example"),dl.forEach(a),se.forEach(a),il.forEach(a),ss=f(Ja),J=i(Ja,"TBODY",{});var mt=n(J);Z=i(mt,"TR",{});var oe=n(Z);Rt=i(oe,"TD",{align:!0});var hl=n(Rt);os=o(hl,"CSV & TSV"),hl.forEach(a),ls=f(oe),Gt=i(oe,"TD",{align:!0});var fl=n(Gt);me=i(fl,"CODE",{});var cl=n(me);is=o(cl,"csv"),cl.forEach(a),fl.forEach(a),ns=f(oe),Wt=i(oe,"TD",{align:!0});var pl=n(Wt);_e=i(pl,"CODE",{});var ul=n(_e);rs=o(ul,'load_dataset("csv", data_files="my_file.csv")'),ul.forEach(a),pl.forEach(a),oe.forEach(a),ds=f(mt),K=i(mt,"TR",{});var le=n(K);Mt=i(le,"TD",{align:!0});var ml=n(Mt);hs=o(ml,"Text files"),ml.forEach(a),fs=f(le),Ut=i(le,"TD",{align:!0});var _l=n(Ut);ge=i(_l,"CODE",{});var gl=n(ge);cs=o(gl,"text"),gl.forEach(a),_l.forEach(a),ps=f(le),Bt=i(le,"TD",{align:!0});var wl=n(Bt);we=i(wl,"CODE",{});var vl=n(we);us=o(vl,'load_dataset("text", data_files="my_file.txt")'),vl.forEach(a),wl.forEach(a),le.forEach(a),ms=f(mt),X=i(mt,"TR",{});var ie=n(X);Yt=i(ie,"TD",{align:!0});var $l=n(Yt);_s=o($l,"JSON & JSON Lines"),$l.forEach(a),gs=f(ie),Vt=i(ie,"TD",{align:!0});var El=n(Vt);ve=i(El,"CODE",{});var yl=n(ve);ws=o(yl,"json"),yl.forEach(a),El.forEach(a),vs=f(ie),Zt=i(ie,"TD",{align:!0});var bl=n(Zt);$e=i(bl,"CODE",{});var Dl=n($e);$s=o(Dl,'load_dataset("json", data_files="my_file.jsonl")'),Dl.forEach(a),bl.forEach(a),ie.forEach(a),Es=f(mt),tt=i(mt,"TR",{});var ne=n(tt);Kt=i(ne,"TD",{align:!0});var jl=n(Kt);ys=o(jl,"Pickled DataFrames"),jl.forEach(a),bs=f(ne),Xt=i(ne,"TD",{align:!0});var ql=n(Xt);Ee=i(ql,"CODE",{});var Sl=n(Ee);Ds=o(Sl,"pandas"),Sl.forEach(a),ql.forEach(a),js=f(ne),te=i(ne,"TD",{align:!0});var kl=n(te);ye=i(kl,"CODE",{});var Al=n(ye);qs=o(Al,'load_dataset("pandas", data_files="my_dataframe.pkl")'),Al.forEach(a),kl.forEach(a),ne.forEach(a),mt.forEach(a),Ja.forEach(a),ia=f(t),G=i(t,"P",{});var re=n(G);Ss=o(re,"As shown in the table, for each data format we just need to specify the type of loading script in the "),be=i(re,"CODE",{});var xl=n(be);ks=o(xl,"load_dataset()"),xl.forEach(a),As=o(re," function, along with a "),De=i(re,"CODE",{});var Tl=n(De);xs=o(Tl,"data_files"),Tl.forEach(a),Ts=o(re," argument that specifies the path to one or more files. Let\u2019s start by loading a dataset from local files; later we\u2019ll see how to do the same with remote files."),re.forEach(a),na=f(t),et=i(t,"H2",{class:!0});var Fa=n(et);it=i(Fa,"A",{id:!0,class:!0,href:!0});var Ol=n(it);je=i(Ol,"SPAN",{});var Cl=n(je);w(yt.$$.fragment,Cl),Cl.forEach(a),Ol.forEach(a),Os=f(Fa),qe=i(Fa,"SPAN",{});var Ql=n(qe);Cs=o(Ql,"Loading a local dataset"),Ql.forEach(a),Fa.forEach(a),ra=f(t),nt=i(t,"P",{});var Ra=n(nt);Qs=o(Ra,"For this example we\u2019ll use the "),bt=i(Ra,"A",{href:!0,rel:!0});var zl=n(bt);zs=o(zl,"SQuAD-it dataset"),zl.forEach(a),Ps=o(Ra,", which is a large-scale dataset for question answering in Italian."),Ra.forEach(a),da=f(t),rt=i(t,"P",{});var Ga=n(rt);Ns=o(Ga,"The training and test splits are hosted on GitHub, so we can download them with a simple "),Se=i(Ga,"CODE",{});var Pl=n(Se);Hs=o(Pl,"wget"),Pl.forEach(a),Ls=o(Ga," command:"),Ga.forEach(a),ha=f(t),w(Dt.$$.fragment,t),fa=f(t),P=i(t,"P",{});var _t=n(P);Is=o(_t,"This will download two compressed files called "),ke=i(_t,"EM",{});var Nl=n(ke);Js=o(Nl,"SQuAD_it-train.json.gz"),Nl.forEach(a),Fs=o(_t," and "),Ae=i(_t,"EM",{});var Hl=n(Ae);Rs=o(Hl,"SQuAD_it-test.json.gz"),Hl.forEach(a),Gs=o(_t,", which we can decompress with the Linux "),xe=i(_t,"CODE",{});var Ll=n(xe);Ws=o(Ll,"gzip"),Ll.forEach(a),Ms=o(_t," command:"),_t.forEach(a),ca=f(t),w(jt.$$.fragment,t),pa=f(t),w(qt.$$.fragment,t),ua=f(t),W=i(t,"P",{});var de=n(W);Us=o(de,"We can see that the compressed files have been replaced with "),Te=i(de,"EM",{});var Il=n(Te);Bs=o(Il,"SQuAD_it-train.json"),Il.forEach(a),Ys=o(de," and "),Oe=i(de,"EM",{});var Jl=n(Oe);Vs=o(Jl,"SQuAD_it-text.json"),Jl.forEach(a),Zs=o(de,", and that the data is stored in the JSON format."),de.forEach(a),ma=f(t),w(dt.$$.fragment,t),_a=f(t),N=i(t,"P",{});var gt=n(N);Ks=o(gt,"To load a JSON file with the "),Ce=i(gt,"CODE",{});var Fl=n(Ce);Xs=o(Fl,"load_dataset()"),Fl.forEach(a),to=o(gt," function, we just need to know if we\u2019re dealing with ordinary JSON (similar to a nested dictionary) or JSON Lines (line-separated JSON). Like many question answering datasets, SQuAD-it uses the nested format, with all the text stored in a "),Qe=i(gt,"CODE",{});var Rl=n(Qe);eo=o(Rl,"data"),Rl.forEach(a),ao=o(gt," field. This means we can load the dataset by specifying the "),ze=i(gt,"CODE",{});var Gl=n(ze);so=o(Gl,"field"),Gl.forEach(a),oo=o(gt," argument as follows:"),gt.forEach(a),ga=f(t),w(St.$$.fragment,t),wa=f(t),H=i(t,"P",{});var wt=n(H);lo=o(wt,"By default, loading local files creates a "),Pe=i(wt,"CODE",{});var Wl=n(Pe);io=o(Wl,"DatasetDict"),Wl.forEach(a),no=o(wt," object with a "),Ne=i(wt,"CODE",{});var Ml=n(Ne);ro=o(Ml,"train"),Ml.forEach(a),ho=o(wt," split. We can see this by inspecting the "),He=i(wt,"CODE",{});var Ul=n(He);fo=o(Ul,"squad_it_dataset"),Ul.forEach(a),co=o(wt," object:"),wt.forEach(a),va=f(t),w(kt.$$.fragment,t),$a=f(t),w(At.$$.fragment,t),Ea=f(t),ht=i(t,"P",{});var Wa=n(ht);po=o(Wa,"This shows us the number of rows and the column names associated with the training set. We can view one of the examples by indexing into the "),Le=i(Wa,"CODE",{});var Bl=n(Le);uo=o(Bl,"train"),Bl.forEach(a),mo=o(Wa," split as follows:"),Wa.forEach(a),ya=f(t),w(xt.$$.fragment,t),ba=f(t),w(Tt.$$.fragment,t),Da=f(t),O=i(t,"P",{});var L=n(O);_o=o(L,"Great, we\u2019ve loaded our first local dataset! But while this worked for the training set, what we really want is to include both the "),Ie=i(L,"CODE",{});var Yl=n(Ie);go=o(Yl,"train"),Yl.forEach(a),wo=o(L," and "),Je=i(L,"CODE",{});var Vl=n(Je);vo=o(Vl,"test"),Vl.forEach(a),$o=o(L," splits in a single "),Fe=i(L,"CODE",{});var Zl=n(Fe);Eo=o(Zl,"DatasetDict"),Zl.forEach(a),yo=o(L," object so we can apply "),Re=i(L,"CODE",{});var Kl=n(Re);bo=o(Kl,"Dataset.map()"),Kl.forEach(a),Do=o(L," functions across both splits at once. To do this, we can provide a dictionary to the "),Ge=i(L,"CODE",{});var Xl=n(Ge);jo=o(Xl,"data_files"),Xl.forEach(a),qo=o(L," argument that maps each split name to a file associated with that split:"),L.forEach(a),ja=f(t),w(Ot.$$.fragment,t),qa=f(t),w(Ct.$$.fragment,t),Sa=f(t),ee=i(t,"P",{});var ti=n(ee);So=o(ti,"This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on."),ti.forEach(a),ka=f(t),w(ft.$$.fragment,t),Aa=f(t),M=i(t,"P",{});var he=n(M);ko=o(he,"The loading scripts in \u{1F917} Datasets actually support automatic decompression of the input files, so we could have skipped the use of "),We=i(he,"CODE",{});var ei=n(We);Ao=o(ei,"gzip"),ei.forEach(a),xo=o(he," by pointing the "),Me=i(he,"CODE",{});var ai=n(Me);To=o(ai,"data_files"),ai.forEach(a),Oo=o(he," argument directly to the compressed files:"),he.forEach(a),xa=f(t),w(Qt.$$.fragment,t),Ta=f(t),ct=i(t,"P",{});var Ma=n(ct);Co=o(Ma,"This can be useful if you don\u2019t want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point "),Ue=i(Ma,"CODE",{});var si=n(Ue);Qo=o(si,"data_files"),si.forEach(a),zo=o(Ma," to the compressed files and you\u2019re good to go!"),Ma.forEach(a),Oa=f(t),ae=i(t,"P",{});var oi=n(ae);Po=o(oi,"Now that you know how to load local files on your laptop or desktop, let\u2019s take a look at loading remote files."),oi.forEach(a),Ca=f(t),at=i(t,"H2",{class:!0});var Ua=n(at);pt=i(Ua,"A",{id:!0,class:!0,href:!0});var li=n(pt);Be=i(li,"SPAN",{});var ii=n(Be);w(zt.$$.fragment,ii),ii.forEach(a),li.forEach(a),No=f(Ua),Ye=i(Ua,"SPAN",{});var ni=n(Ye);Ho=o(ni,"Loading a remote dataset"),ni.forEach(a),Ua.forEach(a),Qa=f(t),Q=i(t,"P",{});var B=n(Q);Lo=o(B,"If you\u2019re working as a data scientist or coder in a company, there\u2019s a good chance the datasets you want to analyze are stored on some remote server. Fortunately, loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the "),Ve=i(B,"CODE",{});var ri=n(Ve);Io=o(ri,"data_files"),ri.forEach(a),Jo=o(B," argument of "),Ze=i(B,"CODE",{});var di=n(Ze);Fo=o(di,"load_dataset()"),di.forEach(a),Ro=o(B," to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point "),Ke=i(B,"CODE",{});var hi=n(Ke);Go=o(hi,"data_files"),hi.forEach(a),Wo=o(B," to the "),Xe=i(B,"EM",{});var fi=n(Xe);Mo=o(fi,"SQuAD_it-*.json.gz"),fi.forEach(a),Uo=o(B," URLs as follows:"),B.forEach(a),za=f(t),w(Pt.$$.fragment,t),Pa=f(t),U=i(t,"P",{});var fe=n(U);Bo=o(fe,"This returns the same "),ta=i(fe,"CODE",{});var ci=n(ta);Yo=o(ci,"DatasetDict"),ci.forEach(a),Vo=o(fe," object obtained above, but saves us the step of manually downloading and decompressing the "),ea=i(fe,"EM",{});var pi=n(ea);Zo=o(pi,"SQuAD_it-*.json.gz"),pi.forEach(a),Ko=o(fe," files. This wraps up our foray into the various ways to load datasets that aren\u2019t hosted on the Hugging Face Hub. Now that we\u2019ve got a dataset to play with, let\u2019s get our hands dirty with various data-wrangling techniques!"),fe.forEach(a),Na=f(t),w(ut.$$.fragment,t),this.h()},h(){p(c,"name","hf:doc:metadata"),p(c,"content",JSON.stringify(ji)),p(b,"id","what-if-my-dataset-isnt-on-the-hub"),p(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(b,"href","#what-if-my-dataset-isnt-on-the-hub"),p(u,"class","relative group"),p(T,"href","https://huggingface.co/datasets"),p(T,"rel","nofollow"),p(ot,"id","working-with-local-and-remote-datasets"),p(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ot,"href","#working-with-local-and-remote-datasets"),p(Y,"class","relative group"),p(It,"align","center"),p(Jt,"align","center"),p(Ft,"align","center"),p(Rt,"align","center"),p(Gt,"align","center"),p(Wt,"align","center"),p(Mt,"align","center"),p(Ut,"align","center"),p(Bt,"align","center"),p(Yt,"align","center"),p(Vt,"align","center"),p(Zt,"align","center"),p(Kt,"align","center"),p(Xt,"align","center"),p(te,"align","center"),p(it,"id","loading-a-local-dataset"),p(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(it,"href","#loading-a-local-dataset"),p(et,"class","relative group"),p(bt,"href","https://github.com/crux82/squad-it/"),p(bt,"rel","nofollow"),p(pt,"id","loading-a-remote-dataset"),p(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(pt,"href","#loading-a-remote-dataset"),p(at,"class","relative group")},m(t,r){e(document.head,c),d(t,A,r),d(t,u,r),e(u,b),e(b,S),v(m,S,null),e(u,j),e(u,x),e(x,k),d(t,z,r),v(C,t,r),d(t,q,r),d(t,_,r),e(_,F),e(_,T),e(T,D),e(_,Ht),d(t,vt,r),v(R,t,r),d(t,$t,r),d(t,Y,r),e(Y,ot),e(ot,ce),v(Et,ce,null),e(Y,Ya),e(Y,pe),e(pe,Va),d(t,oa,r),d(t,Lt,r),e(Lt,Za),d(t,la,r),d(t,lt,r),e(lt,ue),e(ue,V),e(V,It),e(It,Ka),e(V,Xa),e(V,Jt),e(Jt,ts),e(V,es),e(V,Ft),e(Ft,as),e(lt,ss),e(lt,J),e(J,Z),e(Z,Rt),e(Rt,os),e(Z,ls),e(Z,Gt),e(Gt,me),e(me,is),e(Z,ns),e(Z,Wt),e(Wt,_e),e(_e,rs),e(J,ds),e(J,K),e(K,Mt),e(Mt,hs),e(K,fs),e(K,Ut),e(Ut,ge),e(ge,cs),e(K,ps),e(K,Bt),e(Bt,we),e(we,us),e(J,ms),e(J,X),e(X,Yt),e(Yt,_s),e(X,gs),e(X,Vt),e(Vt,ve),e(ve,ws),e(X,vs),e(X,Zt),e(Zt,$e),e($e,$s),e(J,Es),e(J,tt),e(tt,Kt),e(Kt,ys),e(tt,bs),e(tt,Xt),e(Xt,Ee),e(Ee,Ds),e(tt,js),e(tt,te),e(te,ye),e(ye,qs),d(t,ia,r),d(t,G,r),e(G,Ss),e(G,be),e(be,ks),e(G,As),e(G,De),e(De,xs),e(G,Ts),d(t,na,r),d(t,et,r),e(et,it),e(it,je),v(yt,je,null),e(et,Os),e(et,qe),e(qe,Cs),d(t,ra,r),d(t,nt,r),e(nt,Qs),e(nt,bt),e(bt,zs),e(nt,Ps),d(t,da,r),d(t,rt,r),e(rt,Ns),e(rt,Se),e(Se,Hs),e(rt,Ls),d(t,ha,r),v(Dt,t,r),d(t,fa,r),d(t,P,r),e(P,Is),e(P,ke),e(ke,Js),e(P,Fs),e(P,Ae),e(Ae,Rs),e(P,Gs),e(P,xe),e(xe,Ws),e(P,Ms),d(t,ca,r),v(jt,t,r),d(t,pa,r),v(qt,t,r),d(t,ua,r),d(t,W,r),e(W,Us),e(W,Te),e(Te,Bs),e(W,Ys),e(W,Oe),e(Oe,Vs),e(W,Zs),d(t,ma,r),v(dt,t,r),d(t,_a,r),d(t,N,r),e(N,Ks),e(N,Ce),e(Ce,Xs),e(N,to),e(N,Qe),e(Qe,eo),e(N,ao),e(N,ze),e(ze,so),e(N,oo),d(t,ga,r),v(St,t,r),d(t,wa,r),d(t,H,r),e(H,lo),e(H,Pe),e(Pe,io),e(H,no),e(H,Ne),e(Ne,ro),e(H,ho),e(H,He),e(He,fo),e(H,co),d(t,va,r),v(kt,t,r),d(t,$a,r),v(At,t,r),d(t,Ea,r),d(t,ht,r),e(ht,po),e(ht,Le),e(Le,uo),e(ht,mo),d(t,ya,r),v(xt,t,r),d(t,ba,r),v(Tt,t,r),d(t,Da,r),d(t,O,r),e(O,_o),e(O,Ie),e(Ie,go),e(O,wo),e(O,Je),e(Je,vo),e(O,$o),e(O,Fe),e(Fe,Eo),e(O,yo),e(O,Re),e(Re,bo),e(O,Do),e(O,Ge),e(Ge,jo),e(O,qo),d(t,ja,r),v(Ot,t,r),d(t,qa,r),v(Ct,t,r),d(t,Sa,r),d(t,ee,r),e(ee,So),d(t,ka,r),v(ft,t,r),d(t,Aa,r),d(t,M,r),e(M,ko),e(M,We),e(We,Ao),e(M,xo),e(M,Me),e(Me,To),e(M,Oo),d(t,xa,r),v(Qt,t,r),d(t,Ta,r),d(t,ct,r),e(ct,Co),e(ct,Ue),e(Ue,Qo),e(ct,zo),d(t,Oa,r),d(t,ae,r),e(ae,Po),d(t,Ca,r),d(t,at,r),e(at,pt),e(pt,Be),v(zt,Be,null),e(at,No),e(at,Ye),e(Ye,Ho),d(t,Qa,r),d(t,Q,r),e(Q,Lo),e(Q,Ve),e(Ve,Io),e(Q,Jo),e(Q,Ze),e(Ze,Fo),e(Q,Ro),e(Q,Ke),e(Ke,Go),e(Q,Wo),e(Q,Xe),e(Xe,Mo),e(Q,Uo),d(t,za,r),v(Pt,t,r),d(t,Pa,r),d(t,U,r),e(U,Bo),e(U,ta),e(ta,Yo),e(U,Vo),e(U,ea),e(ea,Zo),e(U,Ko),d(t,Na,r),v(ut,t,r),Ha=!0},p(t,[r]){const Nt={};r&2&&(Nt.$$scope={dirty:r,ctx:t}),dt.$set(Nt);const aa={};r&2&&(aa.$$scope={dirty:r,ctx:t}),ft.$set(aa);const sa={};r&2&&(sa.$$scope={dirty:r,ctx:t}),ut.$set(sa)},i(t){Ha||($(m.$$.fragment,t),$(C.$$.fragment,t),$(R.$$.fragment,t),$(Et.$$.fragment,t),$(yt.$$.fragment,t),$(Dt.$$.fragment,t),$(jt.$$.fragment,t),$(qt.$$.fragment,t),$(dt.$$.fragment,t),$(St.$$.fragment,t),$(kt.$$.fragment,t),$(At.$$.fragment,t),$(xt.$$.fragment,t),$(Tt.$$.fragment,t),$(Ot.$$.fragment,t),$(Ct.$$.fragment,t),$(ft.$$.fragment,t),$(Qt.$$.fragment,t),$(zt.$$.fragment,t),$(Pt.$$.fragment,t),$(ut.$$.fragment,t),Ha=!0)},o(t){E(m.$$.fragment,t),E(C.$$.fragment,t),E(R.$$.fragment,t),E(Et.$$.fragment,t),E(yt.$$.fragment,t),E(Dt.$$.fragment,t),E(jt.$$.fragment,t),E(qt.$$.fragment,t),E(dt.$$.fragment,t),E(St.$$.fragment,t),E(kt.$$.fragment,t),E(At.$$.fragment,t),E(xt.$$.fragment,t),E(Tt.$$.fragment,t),E(Ot.$$.fragment,t),E(Ct.$$.fragment,t),E(ft.$$.fragment,t),E(Qt.$$.fragment,t),E(zt.$$.fragment,t),E(Pt.$$.fragment,t),E(ut.$$.fragment,t),Ha=!1},d(t){a(c),t&&a(A),t&&a(u),y(m),t&&a(z),y(C,t),t&&a(q),t&&a(_),t&&a(vt),y(R,t),t&&a($t),t&&a(Y),y(Et),t&&a(oa),t&&a(Lt),t&&a(la),t&&a(lt),t&&a(ia),t&&a(G),t&&a(na),t&&a(et),y(yt),t&&a(ra),t&&a(nt),t&&a(da),t&&a(rt),t&&a(ha),y(Dt,t),t&&a(fa),t&&a(P),t&&a(ca),y(jt,t),t&&a(pa),y(qt,t),t&&a(ua),t&&a(W),t&&a(ma),y(dt,t),t&&a(_a),t&&a(N),t&&a(ga),y(St,t),t&&a(wa),t&&a(H),t&&a(va),y(kt,t),t&&a($a),y(At,t),t&&a(Ea),t&&a(ht),t&&a(ya),y(xt,t),t&&a(ba),y(Tt,t),t&&a(Da),t&&a(O),t&&a(ja),y(Ot,t),t&&a(qa),y(Ct,t),t&&a(Sa),t&&a(ee),t&&a(ka),y(ft,t),t&&a(Aa),t&&a(M),t&&a(xa),y(Qt,t),t&&a(Ta),t&&a(ct),t&&a(Oa),t&&a(ae),t&&a(Ca),t&&a(at),y(zt),t&&a(Qa),t&&a(Q),t&&a(za),y(Pt,t),t&&a(Pa),t&&a(U),t&&a(Na),y(ut,t)}}}const ji={local:"what-if-my-dataset-isnt-on-the-hub",sections:[{local:"working-with-local-and-remote-datasets",title:"Working with local and remote datasets"},{local:"loading-a-local-dataset",title:"Loading a local dataset"},{local:"loading-a-remote-dataset",title:"Loading a remote dataset"}],title:"What if my dataset isn't on the Hub?"};function qi(st){return wi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ci extends ui{constructor(c){super();mi(this,c,qi,Di,_i,{})}}export{Ci as default,ji as metadata};
