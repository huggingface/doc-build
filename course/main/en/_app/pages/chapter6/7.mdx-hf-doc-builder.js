import{S as Si,i as Oi,s as Di,e as r,k as h,w as u,t as o,U as Tn,M as Ai,c as i,d as e,m,a as p,x as f,h as l,V as Cn,b as j,G as t,g as n,y as d,q as g,o as w,B as b,v as Ui}from"../../chunks/vendor-hf-doc-builder.js";import{T as Pt}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Wi}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Tt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as k}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as Hi}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Bi(C){let c,_;return{c(){c=r("p"),_=o("\u{1F4A1} This section covers Unigram in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm.")},l(y){c=i(y,"P",{});var v=p(c);_=l(v,"\u{1F4A1} This section covers Unigram in depth, going as far as showing a full implementation. You can skip to the end if you just want a general overview of the tokenization algorithm."),v.forEach(e)},m(y,v){n(y,c,v),t(c,_)},d(y){y&&e(c)}}}function Ii(C){let c,_,y,v,P;return{c(){c=r("p"),_=o("\u270F\uFE0F "),y=r("strong"),v=o("Now your turn!"),P=o(" Write the code to compute the the frequencies above and double-check that the results shown are correct, as well as the total sum.")},l($){c=i($,"P",{});var E=p(c);_=l(E,"\u270F\uFE0F "),y=i(E,"STRONG",{});var N=p(y);v=l(N,"Now your turn!"),N.forEach(e),P=l(E," Write the code to compute the the frequencies above and double-check that the results shown are correct, as well as the total sum."),E.forEach(e)},m($,E){n($,c,E),t(c,_),t(c,y),t(y,v),t(c,P)},d($){$&&e(c)}}}function Mi(C){let c,_,y,v,P,$,E,N;return{c(){c=r("p"),_=o("\u270F\uFE0F "),y=r("strong"),v=o("Now your turn!"),P=o(" Determine the tokenization of the word "),$=r("code"),E=o('"huggun"'),N=o(", and its score.")},l(F){c=i(F,"P",{});var T=p(c);_=l(T,"\u270F\uFE0F "),y=i(T,"STRONG",{});var H=p(y);v=l(H,"Now your turn!"),H.forEach(e),P=l(T," Determine the tokenization of the word "),$=i(T,"CODE",{});var gs=p($);E=l(gs,'"huggun"'),gs.forEach(e),N=l(T,", and its score."),T.forEach(e)},m(F,T){n(F,c,T),t(c,_),t(c,y),t(y,v),t(c,P),t(c,$),t($,E),t(c,N)},d(F){F&&e(c)}}}function Li(C){let c,_;return{c(){c=r("p"),_=o("\u{1F4A1} SentencePiece uses a more efficient algorithm called Enhanced Suffix Array (ESA) to create the initial vocabulary.")},l(y){c=i(y,"P",{});var v=p(c);_=l(v,"\u{1F4A1} SentencePiece uses a more efficient algorithm called Enhanced Suffix Array (ESA) to create the initial vocabulary."),v.forEach(e)},m(y,v){n(y,c,v),t(c,_)},d(y){y&&e(c)}}}function Fi(C){let c,_;return{c(){c=r("p"),_=o("\u{1F4A1} This approach is very inefficient, so SentencePiece uses an approximation of the loss of the model without token X: instead of starting from scratch, it just replaces token X by its segmentation in the vocabulary that is left. This way, all the scores can be computed at once at the same time as the model loss.")},l(y){c=i(y,"P",{});var v=p(c);_=l(v,"\u{1F4A1} This approach is very inefficient, so SentencePiece uses an approximation of the loss of the model without token X: instead of starting from scratch, it just replaces token X by its segmentation in the vocabulary that is left. This way, all the scores can be computed at once at the same time as the model loss."),v.forEach(e)},m(y,v){n(y,c,v),t(c,_)},d(y){y&&e(c)}}}function Xi(C){let c,_,y,v,P,$,E,N,F,T,H,gs,Qs,Nn,Ct,ws,Nt,Z,St,X,J,Ne,bs,Sn,Se,On,Ot,se,Dn,Dt,ee,An,At,B,Un,Ut,Pi='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span>',Wt,Ht,Ti='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">p</span></span></span></span>',Bt,It,te,Wn,Mt,ae,Hn,Lt,ne,Bn,Ft,ys,Xt,oe,In,Rt,vs,Vt,R,K,Oe,ks,Mn,De,Ln,Gt,le,Fn,Yt,z,Xn,Ae,Rn,Vn,Ue,Gn,Yn,We,Zn,Jn,He,Kn,Qn,Zt,re,so,Jt,_s,Kt,Q,eo,Be,to,ao,Qt,ss,sa,W,no,Ie,oo,lo,Me,ro,io,ea,Ci='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>p</mi><mi mathvariant="normal">&quot;</mi><mo separator="true">,</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>u</mi><mi mathvariant="normal">&quot;</mi><mo separator="true">,</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>g</mi><mi mathvariant="normal">&quot;</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>p</mi><mi mathvariant="normal">&quot;</mi><mo stretchy="false">)</mo><mo>\xD7</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>u</mi><mi mathvariant="normal">&quot;</mi><mo stretchy="false">)</mo><mo>\xD7</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>g</mi><mi mathvariant="normal">&quot;</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>5</mn><mn>210</mn></mfrac><mo>\xD7</mo><mfrac><mn>36</mn><mn>210</mn></mfrac><mo>\xD7</mo><mfrac><mn>20</mn><mn>210</mn></mfrac><mo>=</mo><mn>0.000389</mn></mrow><annotation encoding="application/x-tex">P([``p&quot;, ``u&quot;, ``g&quot;]) = P(``p&quot;) \\times P(``u&quot;) \\times P(``g&quot;) = \\frac{5}{210} \\times \\frac{36}{210} \\times \\frac{20}{210} = 0.000389</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">([</span><span class="mord">\u2018\u2018</span><span class="mord mathnormal">p</span><span class="mord">&quot;</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">\u2018\u2018</span><span class="mord mathnormal">u</span><span class="mord">&quot;</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">\u2018\u2018</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord">&quot;</span><span class="mclose">])</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord">\u2018\u2018</span><span class="mord mathnormal">p</span><span class="mord">&quot;</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord">\u2018\u2018</span><span class="mord mathnormal">u</span><span class="mord">&quot;</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord">\u2018\u2018</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord">&quot;</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">210</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">210</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">36</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">210</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">20</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.000389</span></span></span></span></span>',ta,V,po,Le,ho,mo,aa,Ni='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mo stretchy="false">[</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>p</mi><mi>u</mi><mi mathvariant="normal">&quot;</mi><mo separator="true">,</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>g</mi><mi mathvariant="normal">&quot;</mi><mo stretchy="false">]</mo><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>p</mi><mi>u</mi><mi mathvariant="normal">&quot;</mi><mo stretchy="false">)</mo><mo>\xD7</mo><mi>P</mi><mo stretchy="false">(</mo><mi mathvariant="normal">\u2018</mi><mi mathvariant="normal">\u2018</mi><mi>g</mi><mi mathvariant="normal">&quot;</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>5</mn><mn>210</mn></mfrac><mo>\xD7</mo><mfrac><mn>20</mn><mn>210</mn></mfrac><mo>=</mo><mn>0.0022676</mn></mrow><annotation encoding="application/x-tex">P([``pu&quot;, ``g&quot;]) = P(``pu&quot;) \\times P(``g&quot;) = \\frac{5}{210} \\times \\frac{20}{210} = 0.0022676</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">([</span><span class="mord">\u2018\u2018</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord">&quot;</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">\u2018\u2018</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord">&quot;</span><span class="mclose">])</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord">\u2018\u2018</span><span class="mord mathnormal">p</span><span class="mord mathnormal">u</span><span class="mord">&quot;</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord">\u2018\u2018</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord">&quot;</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">210</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">5</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:2.0074em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">210</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">20</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0.0022676</span></span></span></span></span>',na,ie,co,oa,es,uo,Fe,fo,go,la,js,ra,S,wo,Xe,bo,yo,Re,vo,ko,Ve,_o,jo,ia,q,$o,Ge,qo,xo,Ye,Eo,zo,Ze,Po,To,Je,Co,No,Ke,So,Oo,pa,pe,Do,ha,ts,Ao,Qe,Uo,Wo,ma,$s,ca,I,Ho,st,Bo,Io,et,Mo,Lo,ua,as,fa,G,ns,tt,qs,Fo,at,Xo,da,he,Ro,ga,os,Vo,nt,Go,Yo,wa,me,Zo,ba,xs,ya,ce,Jo,va,Es,ka,ue,Ko,_a,zs,ja,O,Qo,ot,sl,el,lt,tl,al,rt,nl,ol,$a,D,ll,it,rl,il,pt,pl,hl,ht,ml,cl,qa,Ps,xa,fe,ul,Ea,Ts,za,M,fl,mt,dl,gl,ct,wl,bl,Pa,Y,ls,ut,Cs,yl,ft,vl,Ta,de,kl,Ca,ge,_l,Na,Ns,Sa,rs,jl,dt,$l,ql,Oa,Ss,Da,we,xl,Aa,Os,Ua,be,El,Wa,Ds,Ha,As,Ba,ye,zl,Ia,Us,Ma,is,La,ve,Pl,Fa,Ws,Xa,ps,Tl,gt,Cl,Nl,Ra,hs,Sl,wt,Ol,Dl,Va,ke,Al,Ga,Hs,Ya,_e,Ul,Za,Bs,Ja,Is,Ka,je,Wl,Qa,Ms,sn,$e,Hl,en,Ls,tn,Fs,an,qe,Bl,nn,Xs,on,xe,Il,ln,Rs,rn,x,Ml,bt,Ll,Fl,yt,Xl,Rl,vt,Vl,Gl,kt,Yl,Zl,_t,Jl,Kl,pn,Vs,hn,ms,mn,Ee,Ql,cn,Gs,un,cs,sr,jt,er,tr,fn,Ys,dn,Zs,gn,ze,ar,wn;return $=new Tt({}),H=new Hi({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section7.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section7.ipynb"}]}}),ws=new Wi({props:{id:"TGZfZVuF9Yc"}}),Z=new Pt({props:{$$slots:{default:[Bi]},$$scope:{ctx:C}}}),bs=new Tt({}),ys=new k({props:{code:'("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)',highlighted:'(<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;bun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;hugs&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),vs=new k({props:{code:'["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]',highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;hu&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;pu&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;bu&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;hug&quot;</span>, <span class="hljs-string">&quot;gs&quot;</span>, <span class="hljs-string">&quot;ugs&quot;</span>]</span>'}}),ks=new Tt({}),_s=new k({props:{code:`("h", 15) ("u", 36) ("g", 20) ("hu", 15) ("ug", 20) ("p", 17) ("pu", 17) ("n", 16)
("un", 16) ("b", 4) ("bu", 4) ("s", 5) ("hug", 15) ("gs", 5) ("ugs", 5)`,highlighted:`(<span class="hljs-string">&quot;h&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">15</span>) (<span class="hljs-string">&quot;u&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">36</span>) (<span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">20</span>) (<span class="hljs-string">&quot;hu&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">15</span>) (<span class="hljs-string">&quot;ug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">20</span>) (<span class="hljs-string">&quot;p&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">17</span>) (<span class="hljs-string">&quot;pu&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">17</span>) (<span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">16</span>)
(<span class="hljs-string">&quot;un&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">16</span>) (<span class="hljs-string">&quot;b&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>) (<span class="hljs-string">&quot;bu&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>) (<span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>) (<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">15</span>) (<span class="hljs-string">&quot;gs&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>) (<span class="hljs-string">&quot;ugs&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)`}}),ss=new Pt({props:{$$slots:{default:[Ii]},$$scope:{ctx:C}}}),js=new k({props:{code:`["p", "u", "g"] : 0.000389
["p", "ug"] : 0.0022676
["pu", "g"] : 0.0022676`,highlighted:`[<span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>] : 0.000389
[<span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>] : 0.0022676
[<span class="hljs-string">&quot;pu&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>] : 0.0022676`}}),$s=new k({props:{code:`Character 0 (u): "u" (score 0.171429)
Character 1 (n): "un" (score 0.076191)
Character 2 (h): "un" "h" (score 0.005442)
Character 3 (u): "un" "hu" (score 0.005442)
Character 4 (g): "un" "hug" (score 0.005442)`,highlighted:`<span class="hljs-attribute">Character</span> <span class="hljs-number">0</span> (u): <span class="hljs-string">&quot;u&quot;</span> (score <span class="hljs-number">0</span>.<span class="hljs-number">171429</span>)
<span class="hljs-attribute">Character</span> <span class="hljs-number">1</span> (n): <span class="hljs-string">&quot;un&quot;</span> (score <span class="hljs-number">0</span>.<span class="hljs-number">076191</span>)
<span class="hljs-attribute">Character</span> <span class="hljs-number">2</span> (h): <span class="hljs-string">&quot;un&quot;</span> <span class="hljs-string">&quot;h&quot;</span> (score <span class="hljs-number">0</span>.<span class="hljs-number">005442</span>)
<span class="hljs-attribute">Character</span> <span class="hljs-number">3</span> (u): <span class="hljs-string">&quot;un&quot;</span> <span class="hljs-string">&quot;hu&quot;</span> (score <span class="hljs-number">0</span>.<span class="hljs-number">005442</span>)
<span class="hljs-attribute">Character</span> <span class="hljs-number">4</span> (g): <span class="hljs-string">&quot;un&quot;</span> <span class="hljs-string">&quot;hug&quot;</span> (score <span class="hljs-number">0</span>.<span class="hljs-number">005442</span>)`}}),as=new Pt({props:{$$slots:{default:[Mi]},$$scope:{ctx:C}}}),qs=new Tt({}),xs=new k({props:{code:'("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)',highlighted:'(<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;bun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;hugs&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),Es=new k({props:{code:`"hug": ["hug"] (score 0.071428)
"pug": ["pu", "g"] (score 0.007710)
"pun": ["pu", "n"] (score 0.006168)
"bun": ["bu", "n"] (score 0.001451)
"hugs": ["hug", "s"] (score 0.001701)`,highlighted:`<span class="hljs-string">&quot;hug&quot;</span>: [<span class="hljs-string">&quot;hug&quot;</span>] <span class="hljs-comment">(score 0.071428)</span>
<span class="hljs-string">&quot;pug&quot;</span>: [<span class="hljs-string">&quot;pu&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>] <span class="hljs-comment">(score 0.007710)</span>
<span class="hljs-string">&quot;pun&quot;</span>: [<span class="hljs-string">&quot;pu&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>] <span class="hljs-comment">(score 0.006168)</span>
<span class="hljs-string">&quot;bun&quot;</span>: [<span class="hljs-string">&quot;bu&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>] <span class="hljs-comment">(score 0.001451)</span>
<span class="hljs-string">&quot;hugs&quot;</span>: [<span class="hljs-string">&quot;hug&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>] <span class="hljs-comment">(score 0.001701)</span>`}}),zs=new k({props:{code:"10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8",highlighted:'<span class="hljs-attribute">10</span> * (-log(<span class="hljs-number">0</span>.<span class="hljs-number">071428</span>)) + <span class="hljs-number">5</span> * (-log(<span class="hljs-number">0</span>.<span class="hljs-number">007710</span>)) + <span class="hljs-number">12</span> * (-log(<span class="hljs-number">0</span>.<span class="hljs-number">006168</span>)) + <span class="hljs-number">4</span> * (-log(<span class="hljs-number">0</span>.<span class="hljs-number">001451</span>)) + <span class="hljs-number">5</span> * (-log(<span class="hljs-number">0</span>.<span class="hljs-number">001701</span>)) = <span class="hljs-number">169</span>.<span class="hljs-number">8</span>'}}),Ps=new k({props:{code:`"hug": ["hu", "g"] (score 0.006802)
"hugs": ["hu", "gs"] (score 0.001701)`,highlighted:`<span class="hljs-string">&quot;hug&quot;</span>: [<span class="hljs-string">&quot;hu&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>] <span class="hljs-comment">(score 0.006802)</span>
<span class="hljs-string">&quot;hugs&quot;</span>: [<span class="hljs-string">&quot;hu&quot;</span>, <span class="hljs-string">&quot;gs&quot;</span>] <span class="hljs-comment">(score 0.001701)</span>`}}),Ts=new k({props:{code:"- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5",highlighted:'- <span class="hljs-number">10</span> * (<span class="hljs-name">-log</span>(<span class="hljs-number">0.071428</span>)) + <span class="hljs-number">10</span> * (<span class="hljs-name">-log</span>(<span class="hljs-number">0.006802</span>)) = <span class="hljs-number">23.5</span>'}}),Cs=new Tt({}),Ns=new k({props:{code:`corpus = [
    "This is the Hugging Face Course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]`,highlighted:`corpus = [
    <span class="hljs-string">&quot;This is the Hugging Face Course.&quot;</span>,
    <span class="hljs-string">&quot;This chapter is about tokenization.&quot;</span>,
    <span class="hljs-string">&quot;This section shows several tokenizer algorithms.&quot;</span>,
    <span class="hljs-string">&quot;Hopefully, you will be able to understand how they are trained and generate tokens.&quot;</span>,
]`}}),Ss=new k({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;xlnet-base-cased&quot;</span>)`}}),Os=new k({props:{code:`from collections import defaultdict

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs`,highlighted:`<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict

word_freqs = defaultdict(<span class="hljs-built_in">int</span>)
<span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word <span class="hljs-keyword">for</span> word, offset <span class="hljs-keyword">in</span> words_with_offsets]
    <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> new_words:
        word_freqs[word] += <span class="hljs-number">1</span>

word_freqs`}}),Ds=new k({props:{code:`char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        # Loop through the subwords of length at least 2
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

# Sort subwords by frequency
sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]`,highlighted:`char_freqs = defaultdict(<span class="hljs-built_in">int</span>)
subwords_freqs = defaultdict(<span class="hljs-built_in">int</span>)
<span class="hljs-keyword">for</span> word, freq <span class="hljs-keyword">in</span> word_freqs.items():
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(word)):
        char_freqs[word[i]] += freq
        <span class="hljs-comment"># Loop through the subwords of length at least 2</span>
        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i + <span class="hljs-number">2</span>, <span class="hljs-built_in">len</span>(word) + <span class="hljs-number">1</span>):
            subwords_freqs[word[i:j]] += freq

<span class="hljs-comment"># Sort subwords by frequency</span>
sorted_subwords = <span class="hljs-built_in">sorted</span>(subwords_freqs.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>], reverse=<span class="hljs-literal">True</span>)
sorted_subwords[:<span class="hljs-number">10</span>]`}}),As=new k({props:{code:"[('\u2581t', 7), ('is', 5), ('er', 5), ('\u2581a', 5), ('\u2581to', 4), ('to', 4), ('en', 4), ('\u2581T', 3), ('\u2581Th', 3), ('\u2581Thi', 3)]",highlighted:'[(<span class="hljs-string">&#x27;\u2581t&#x27;</span>, <span class="hljs-number">7</span>), (<span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&#x27;er&#x27;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&#x27;\u2581a&#x27;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&#x27;\u2581to&#x27;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&#x27;to&#x27;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&#x27;en&#x27;</span>, <span class="hljs-number">4</span>), (<span class="hljs-string">&#x27;\u2581T&#x27;</span>, <span class="hljs-number">3</span>), (<span class="hljs-string">&#x27;\u2581Th&#x27;</span>, <span class="hljs-number">3</span>), (<span class="hljs-string">&#x27;\u2581Thi&#x27;</span>, <span class="hljs-number">3</span>)]'}}),Us=new k({props:{code:`token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]
token_freqs = {token: freq for token, freq in token_freqs}`,highlighted:`token_freqs = <span class="hljs-built_in">list</span>(char_freqs.items()) + sorted_subwords[: <span class="hljs-number">300</span> - <span class="hljs-built_in">len</span>(char_freqs)]
token_freqs = {token: freq <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> token_freqs}`}}),is=new Pt({props:{$$slots:{default:[Li]},$$scope:{ctx:C}}}),Ws=new k({props:{code:`from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}`,highlighted:`<span class="hljs-keyword">from</span> math <span class="hljs-keyword">import</span> log

total_sum = <span class="hljs-built_in">sum</span>([freq <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> token_freqs.items()])
model = {token: -log(freq / total_sum) <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> token_freqs.items()}`}}),Hs=new k({props:{code:`def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # This should be properly filled by the previous steps of the loop
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                # If we have found a better segmentation ending at end_idx, we update
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        # We did not find a tokenization of the word -> unknown
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">encode_word</span>(<span class="hljs-params">word, model</span>):
    best_segmentations = [{<span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">1</span>}] + [
        {<span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-literal">None</span>} <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(word))
    ]
    <span class="hljs-keyword">for</span> start_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(word)):
        <span class="hljs-comment"># This should be properly filled by the previous steps of the loop</span>
        best_score_at_start = best_segmentations[start_idx][<span class="hljs-string">&quot;score&quot;</span>]
        <span class="hljs-keyword">for</span> end_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(start_idx + <span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(word) + <span class="hljs-number">1</span>):
            token = word[start_idx:end_idx]
            <span class="hljs-keyword">if</span> token <span class="hljs-keyword">in</span> model <span class="hljs-keyword">and</span> best_score_at_start <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
                score = model[token] + best_score_at_start
                <span class="hljs-comment"># If we have found a better segmentation ending at end_idx, we update</span>
                <span class="hljs-keyword">if</span> (
                    best_segmentations[end_idx][<span class="hljs-string">&quot;score&quot;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>
                    <span class="hljs-keyword">or</span> best_segmentations[end_idx][<span class="hljs-string">&quot;score&quot;</span>] &gt; score
                ):
                    best_segmentations[end_idx] = {<span class="hljs-string">&quot;start&quot;</span>: start_idx, <span class="hljs-string">&quot;score&quot;</span>: score}

    segmentation = best_segmentations[-<span class="hljs-number">1</span>]
    <span class="hljs-keyword">if</span> segmentation[<span class="hljs-string">&quot;score&quot;</span>] <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
        <span class="hljs-comment"># We did not find a tokenization of the word -&gt; unknown</span>
        <span class="hljs-keyword">return</span> [<span class="hljs-string">&quot;&lt;unk&gt;&quot;</span>], <span class="hljs-literal">None</span>

    score = segmentation[<span class="hljs-string">&quot;score&quot;</span>]
    start = segmentation[<span class="hljs-string">&quot;start&quot;</span>]
    end = <span class="hljs-built_in">len</span>(word)
    tokens = []
    <span class="hljs-keyword">while</span> start != <span class="hljs-number">0</span>:
        tokens.insert(<span class="hljs-number">0</span>, word[start:end])
        next_start = best_segmentations[start][<span class="hljs-string">&quot;start&quot;</span>]
        end = start
        start = next_start
    tokens.insert(<span class="hljs-number">0</span>, word[start:end])
    <span class="hljs-keyword">return</span> tokens, score`}}),Bs=new k({props:{code:`print(encode_word("Hopefully", model))
print(encode_word("This", model))`,highlighted:`<span class="hljs-built_in">print</span>(encode_word(<span class="hljs-string">&quot;Hopefully&quot;</span>, model))
<span class="hljs-built_in">print</span>(encode_word(<span class="hljs-string">&quot;This&quot;</span>, model))`}}),Is=new k({props:{code:`(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)
(['This'], 6.288267030694535)`,highlighted:`([<span class="hljs-string">&#x27;H&#x27;</span>, <span class="hljs-string">&#x27;o&#x27;</span>, <span class="hljs-string">&#x27;p&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;f&#x27;</span>, <span class="hljs-string">&#x27;u&#x27;</span>, <span class="hljs-string">&#x27;ll&#x27;</span>, <span class="hljs-string">&#x27;y&#x27;</span>], <span class="hljs-number">41.5157494601402</span>)
([<span class="hljs-string">&#x27;This&#x27;</span>], <span class="hljs-number">6.288267030694535</span>)`}}),Ms=new k({props:{code:`def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">model</span>):
    loss = <span class="hljs-number">0</span>
    <span class="hljs-keyword">for</span> word, freq <span class="hljs-keyword">in</span> word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    <span class="hljs-keyword">return</span> loss`}}),Ls=new k({props:{code:"compute_loss(model)",highlighted:"compute_loss(model)"}}),Fs=new k({props:{code:"413.10377642940875",highlighted:'<span class="hljs-number">413.10377642940875</span>'}}),Xs=new k({props:{code:`import copy


def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        # We always keep tokens of length 1
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores`,highlighted:`<span class="hljs-keyword">import</span> copy


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_scores</span>(<span class="hljs-params">model</span>):
    scores = {}
    model_loss = compute_loss(model)
    <span class="hljs-keyword">for</span> token, score <span class="hljs-keyword">in</span> model.items():
        <span class="hljs-comment"># We always keep tokens of length 1</span>
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(token) == <span class="hljs-number">1</span>:
            <span class="hljs-keyword">continue</span>
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    <span class="hljs-keyword">return</span> scores`}}),Rs=new k({props:{code:`scores = compute_scores(model)
print(scores["ll"])
print(scores["his"])`,highlighted:`scores = compute_scores(model)
<span class="hljs-built_in">print</span>(scores[<span class="hljs-string">&quot;ll&quot;</span>])
<span class="hljs-built_in">print</span>(scores[<span class="hljs-string">&quot;his&quot;</span>])`}}),Vs=new k({props:{code:`6.376412403623874
0.0`,highlighted:`<span class="hljs-number">6.376412403623874</span>
<span class="hljs-number">0.0</span>`}}),ms=new Pt({props:{$$slots:{default:[Fi]},$$scope:{ctx:C}}}),Gs=new k({props:{code:`percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Remove percent_to_remove tokens with the lowest scores.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}`,highlighted:`percent_to_remove = <span class="hljs-number">0.1</span>
<span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(model) &gt; <span class="hljs-number">100</span>:
    scores = compute_scores(model)
    sorted_scores = <span class="hljs-built_in">sorted</span>(scores.items(), key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">1</span>])
    <span class="hljs-comment"># Remove percent_to_remove tokens with the lowest scores.</span>
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][<span class="hljs-number">0</span>])

    total_sum = <span class="hljs-built_in">sum</span>([freq <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> token_freqs.items()])
    model = {token: -log(freq / total_sum) <span class="hljs-keyword">for</span> token, freq <span class="hljs-keyword">in</span> token_freqs.items()}`}}),Ys=new k({props:{code:`def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])


tokenize("This is the Hugging Face course.", model)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">text, model</span>):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word <span class="hljs-keyword">for</span> word, offset <span class="hljs-keyword">in</span> words_with_offsets]
    encoded_words = [encode_word(word, model)[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> pre_tokenized_text]
    <span class="hljs-keyword">return</span> <span class="hljs-built_in">sum</span>(encoded_words, [])


tokenize(<span class="hljs-string">&quot;This is the Hugging Face course.&quot;</span>, model)`}}),Zs=new k({props:{code:"['\u2581This', '\u2581is', '\u2581the', '\u2581Hugging', '\u2581Face', '\u2581', 'c', 'ou', 'r', 's', 'e', '.']",highlighted:'[<span class="hljs-string">&#x27;\u2581This&#x27;</span>, <span class="hljs-string">&#x27;\u2581is&#x27;</span>, <span class="hljs-string">&#x27;\u2581the&#x27;</span>, <span class="hljs-string">&#x27;\u2581Hugging&#x27;</span>, <span class="hljs-string">&#x27;\u2581Face&#x27;</span>, <span class="hljs-string">&#x27;\u2581&#x27;</span>, <span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-string">&#x27;ou&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;e&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]'}}),{c(){c=r("meta"),_=h(),y=r("h1"),v=r("a"),P=r("span"),u($.$$.fragment),E=h(),N=r("span"),F=o("Unigram tokenization"),T=h(),u(H.$$.fragment),gs=h(),Qs=r("p"),Nn=o("The Unigram algorithm is often used in SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet."),Ct=h(),u(ws.$$.fragment),Nt=h(),u(Z.$$.fragment),St=h(),X=r("h2"),J=r("a"),Ne=r("span"),u(bs.$$.fragment),Sn=h(),Se=r("span"),On=o("Training algorithm"),Ot=h(),se=r("p"),Dn=o("Compared to BPE and WordPiece, Unigram works in the other direction: it starts from a big vocabulary and removes tokens from it until it reaches the desired vocabulary size. There are several options to use to build that base vocabulary: we can take the most common substrings in pre-tokenized words, for instance, or apply BPE on the initial corpus with a large vocabulary size."),Dt=h(),ee=r("p"),An=o("At each step of the training, the Unigram algorithm computes a loss over the corpus given the current vocabulary. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was removed, and looks for the symbols that would increase it the least. Those symbols have a lower effect on the overall loss over the corpus, so in a sense they are \u201Cless needed\u201D and are the best candidates for removal."),At=h(),B=r("p"),Un=o("This is all a very costly operation, so we don\u2019t just remove the single symbol associated with the lowest loss increase, but the "),Ut=new Tn,Wt=o(" ("),Ht=new Tn,Bt=o(" being a hyperparameter you can control, usually 10 or 20) percent of the symbols associated with the lowest loss increase. This process is then repeated until the vocabulary has reached the desired size."),It=h(),te=r("p"),Wn=o("Note that we never remove the base characters, to make sure any word can be tokenized."),Mt=h(),ae=r("p"),Hn=o("Now, this is still a bit vague: the main part of the algorithm is to compute a loss over the corpus and see how it changes when we remove some tokens from the vocabulary, but we haven\u2019t explained how to do this yet. This step relies on the tokenization algorithm of a Unigram model, so we\u2019ll dive into this next."),Lt=h(),ne=r("p"),Bn=o("We\u2019ll reuse the corpus from the previous examples:"),Ft=h(),u(ys.$$.fragment),Xt=h(),oe=r("p"),In=o("and for this example, we will take all strict substrings for the initial vocabulary :"),Rt=h(),u(vs.$$.fragment),Vt=h(),R=r("h2"),K=r("a"),Oe=r("span"),u(ks.$$.fragment),Mn=h(),De=r("span"),Ln=o("Tokenization algorithm"),Gt=h(),le=r("p"),Fn=o("A Unigram model is a type of language model that considers each token to be independent of the tokens before it. It\u2019s the simplest language model, in the sense that the probability of token X given the previous context is just the probability of token X. So, if we used a Unigram language model to generate text, we would always predict the most common token."),Yt=h(),z=r("p"),Xn=o("The probability of a given token is its frequency (the number of times we find it) in the original corpus, divided by the sum of all frequencies of all tokens in the vocabulary (to make sure the probabilities sum up to 1). For instance, "),Ae=r("code"),Rn=o('"ug"'),Vn=o(" is present in "),Ue=r("code"),Gn=o('"hug"'),Yn=o(", "),We=r("code"),Zn=o('"pug"'),Jn=o(", and "),He=r("code"),Kn=o('"hugs"'),Qn=o(", so it has a frequency of 20 in our corpus."),Zt=h(),re=r("p"),so=o("Here are the frequencies of all the possible subwords in the vocabulary:"),Jt=h(),u(_s.$$.fragment),Kt=h(),Q=r("p"),eo=o("So, the sum of all frequencies is 210, and the probability of the subword "),Be=r("code"),to=o('"ug"'),ao=o(" is thus 20/210."),Qt=h(),u(ss.$$.fragment),sa=h(),W=r("p"),no=o("Now, to tokenize a given word, we look at all the possible segmentations into tokens and compute the probability of each according to the Unigram model. Since all tokens are considered independent, this probability is just the product of the probability of each token. For instance, the tokenization "),Ie=r("code"),oo=o('["p", "u", "g"]'),lo=o(" of "),Me=r("code"),ro=o('"pug"'),io=o(` has the probability:
`),ea=new Tn,ta=h(),V=r("p"),po=o("Comparatively, the tokenization "),Le=r("code"),ho=o('["pu", "g"]'),mo=o(` has the probability:
`),aa=new Tn,na=h(),ie=r("p"),co=o("so that one is way more likely. In general, tokenizations with the least tokens possible will have the highest probability (because of that division by 210 repeated for each token), which corresponds to what we want intuitively: to split a word into the least number of tokens possible."),oa=h(),es=r("p"),uo=o("The tokenization of a word with the Unigram model is then the tokenization with the highest probability. In the example of "),Fe=r("code"),fo=o('"pug"'),go=o(", here are the probabilities we would get for each possible segmentation:"),la=h(),u(js.$$.fragment),ra=h(),S=r("p"),wo=o("So, "),Xe=r("code"),bo=o('"pug"'),yo=o(" would be tokenized as "),Re=r("code"),vo=o('["p", "ug"]'),ko=o(" or "),Ve=r("code"),_o=o('["pu", "g"]'),jo=o(", depending on which of those segmentations is encountered first (note that in a larger corpus, equality cases like this will be rare)."),ia=h(),q=r("p"),$o=o("In this case, it was easy to find all the possible segmentations and compute their probabilities, but in general it\u2019s going to be a bit harder. There is a classic algorithm used for this, called the "),Ge=r("em"),qo=o("Viterbi algorithm"),xo=o(". Essentially, we can build a graph to detect the possible segmentations of a given word by saying there is a branch from character "),Ye=r("em"),Eo=o("a"),zo=o(" to character "),Ze=r("em"),Po=o("b"),To=o(" if the subword from "),Je=r("em"),Co=o("a"),No=o(" to "),Ke=r("em"),So=o("b"),Oo=o(" is in the vocabulary, and attribute to that branch the probability of the subword."),pa=h(),pe=r("p"),Do=o("To find the path in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to unroll the path taken to arrive at the end."),ha=h(),ts=r("p"),Ao=o("Let\u2019s take a look at an example using our vocabulary and the word "),Qe=r("code"),Uo=o('"unhug"'),Wo=o(". For each position, the subwords with the best scores ending there are the following:"),ma=h(),u($s.$$.fragment),ca=h(),I=r("p"),Ho=o("Thus "),st=r("code"),Bo=o('"unhug"'),Io=o(" would be tokenized as "),et=r("code"),Mo=o('["un", "hug"]'),Lo=o("."),ua=h(),u(as.$$.fragment),fa=h(),G=r("h2"),ns=r("a"),tt=r("span"),u(qs.$$.fragment),Fo=h(),at=r("span"),Xo=o("Back to training"),da=h(),he=r("p"),Ro=o("Now that we have seen how the tokenization works, we can dive a little more deeply into the loss used during training. At any given stage, this loss is computed by tokenizing every word in the corpus, using the current vocabulary and the Unigram model determined by the frequencies of each token in the corpus (as seen before)."),ga=h(),os=r("p"),Vo=o("Each word in the corpus has a score, and the loss is the negative log likelihood of those scores \u2014 that is, the sum for all the words in the corpus of all the "),nt=r("code"),Go=o("-log(P(word))"),Yo=o("."),wa=h(),me=r("p"),Zo=o("Let\u2019s go back to our example with the following corpus:"),ba=h(),u(xs.$$.fragment),ya=h(),ce=r("p"),Jo=o("The tokenization of each word with their respective scores is:"),va=h(),u(Es.$$.fragment),ka=h(),ue=r("p"),Ko=o("So the loss is:"),_a=h(),u(zs.$$.fragment),ja=h(),O=r("p"),Qo=o("Now we need to compute how removing each token affects the loss. This is rather tedious, so we\u2019ll just do it for two tokens here and save the whole process for when we have code to help us. In this (very) particular case, we had two equivalent tokenizations of all the words: as we saw earlier, for example, "),ot=r("code"),sl=o('"pug"'),el=o(" could be tokenized "),lt=r("code"),tl=o('["p", "ug"]'),al=o(" with the same score. Thus, removing the "),rt=r("code"),nl=o('"pu"'),ol=o(" token from the vocabulary will give the exact same loss."),$a=h(),D=r("p"),ll=o("On the other hand, removing "),it=r("code"),rl=o('"hug"'),il=o(" will make the loss worse, because the tokenization of "),pt=r("code"),pl=o('"hug"'),hl=o(" and "),ht=r("code"),ml=o('"hugs"'),cl=o(" will become:"),qa=h(),u(Ps.$$.fragment),xa=h(),fe=r("p"),ul=o("These changes will cause the loss to rise by:"),Ea=h(),u(Ts.$$.fragment),za=h(),M=r("p"),fl=o("Therefore, the token "),mt=r("code"),dl=o('"pu"'),gl=o(" will probably be removed from the vocabulary, but not "),ct=r("code"),wl=o('"hug"'),bl=o("."),Pa=h(),Y=r("h2"),ls=r("a"),ut=r("span"),u(Cs.$$.fragment),yl=h(),ft=r("span"),vl=o("Implementing Unigram"),Ta=h(),de=r("p"),kl=o("Now let\u2019s implement everything we\u2019ve seen so far in code. Like with BPE and WordPiece, this is not an efficient implementation of the Unigram algorithm (quite the opposite), but it should help you understand it a bit better."),Ca=h(),ge=r("p"),_l=o("We will use the same corpus as before as an example:"),Na=h(),u(Ns.$$.fragment),Sa=h(),rs=r("p"),jl=o("This time, we will use "),dt=r("code"),$l=o("xlnet-base-cased"),ql=o(" as our model:"),Oa=h(),u(Ss.$$.fragment),Da=h(),we=r("p"),xl=o("Like for BPE and WordPiece, we begin by counting the number of occurrences of each word in the corpus:"),Aa=h(),u(Os.$$.fragment),Ua=h(),be=r("p"),El=o("Then, we need to initialize our vocabulary to something larger than the vocab size we will want at the end. We have to include all the basic characters (otherwise we won\u2019t be able to tokenize every word), but for the bigger substrings we\u2019ll only keep the most common ones, so we sort them by frequency:"),Wa=h(),u(Ds.$$.fragment),Ha=h(),u(As.$$.fragment),Ba=h(),ye=r("p"),zl=o("We group the characters with the best subwords to arrive at an initial vocabulary of size 300:"),Ia=h(),u(Us.$$.fragment),Ma=h(),u(is.$$.fragment),La=h(),ve=r("p"),Pl=o("Next, we compute the sum of all frequencies, to convert the frequencies into probabilities. For our model we will store the logarithms of the probabilities, because it\u2019s more numerically stable to add logarithms than to multiply small numbers, and this will simplify the computation of the loss of the model:"),Fa=h(),u(Ws.$$.fragment),Xa=h(),ps=r("p"),Tl=o("Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named "),gt=r("code"),Cl=o("best_segmentations"),Nl=o(". We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated."),Ra=h(),hs=r("p"),Sl=o("Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in "),wt=r("code"),Ol=o("best_segmentations"),Dl=o("."),Va=h(),ke=r("p"),Al=o("Once the main loop is finished, we just start from the end and hop from one start position to the next, recording the tokens as we go, until we reach the start of the word:"),Ga=h(),u(Hs.$$.fragment),Ya=h(),_e=r("p"),Ul=o("We can already try our initial model on some words:"),Za=h(),u(Bs.$$.fragment),Ja=h(),u(Is.$$.fragment),Ka=h(),je=r("p"),Wl=o("Now it\u2019s easy to compute the loss of the model on the corpus!"),Qa=h(),u(Ms.$$.fragment),sn=h(),$e=r("p"),Hl=o("We can check it works on the model we have:"),en=h(),u(Ls.$$.fragment),tn=h(),u(Fs.$$.fragment),an=h(),qe=r("p"),Bl=o("Computing the scores for each token is not very hard either; we just have to compute the loss for the models obtained by deleting each token:"),nn=h(),u(Xs.$$.fragment),on=h(),xe=r("p"),Il=o("We can try it on a given token:"),ln=h(),u(Rs.$$.fragment),rn=h(),x=r("p"),Ml=o("Since "),bt=r("code"),Ll=o('"ll"'),Fl=o(" is used in the tokenization of "),yt=r("code"),Xl=o('"Hopefully"'),Rl=o(", and removing it will probably make us use the token "),vt=r("code"),Vl=o('"l"'),Gl=o(" twice instead, we expect it will have a positive loss. "),kt=r("code"),Yl=o('"his"'),Zl=o(" is only used inside the word "),_t=r("code"),Jl=o('"This"'),Kl=o(", which is tokenized as itself, so we expect it to have a zero loss. Here are the results:"),pn=h(),u(Vs.$$.fragment),hn=h(),u(ms.$$.fragment),mn=h(),Ee=r("p"),Ql=o("With all of this in place, the last thing we need to do is add the special tokens used by the model to the vocabulary, then loop until we have pruned enough tokens from the vocabulary to reach our desired size:"),cn=h(),u(Gs.$$.fragment),un=h(),cs=r("p"),sr=o("Then, to tokenize some text, we just need to apply the pre-tokenization and then use our "),jt=r("code"),er=o("encode_word()"),tr=o(" function:"),fn=h(),u(Ys.$$.fragment),dn=h(),u(Zs.$$.fragment),gn=h(),ze=r("p"),ar=o("That\u2019s it for Unigram! Hopefully by now you\u2019re feeling like an expert in all things tokenizer. In the next section, we will delve into the building blocks of the \u{1F917} Tokenizers library, and show you how you can use them to build your own tokenizer."),this.h()},l(s){const a=Ai('[data-svelte="svelte-1phssyn"]',document.head);c=i(a,"META",{name:!0,content:!0}),a.forEach(e),_=m(s),y=i(s,"H1",{class:!0});var Js=p(y);v=i(Js,"A",{id:!0,class:!0,href:!0});var $t=p(v);P=i($t,"SPAN",{});var qt=p(P);f($.$$.fragment,qt),qt.forEach(e),$t.forEach(e),E=m(Js),N=i(Js,"SPAN",{});var xt=p(N);F=l(xt,"Unigram tokenization"),xt.forEach(e),Js.forEach(e),T=m(s),f(H.$$.fragment,s),gs=m(s),Qs=i(s,"P",{});var Et=p(Qs);Nn=l(Et,"The Unigram algorithm is often used in SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet."),Et.forEach(e),Ct=m(s),f(ws.$$.fragment,s),Nt=m(s),f(Z.$$.fragment,s),St=m(s),X=i(s,"H2",{class:!0});var bn=p(X);J=i(bn,"A",{id:!0,class:!0,href:!0});var nr=p(J);Ne=i(nr,"SPAN",{});var or=p(Ne);f(bs.$$.fragment,or),or.forEach(e),nr.forEach(e),Sn=m(bn),Se=i(bn,"SPAN",{});var lr=p(Se);On=l(lr,"Training algorithm"),lr.forEach(e),bn.forEach(e),Ot=m(s),se=i(s,"P",{});var rr=p(se);Dn=l(rr,"Compared to BPE and WordPiece, Unigram works in the other direction: it starts from a big vocabulary and removes tokens from it until it reaches the desired vocabulary size. There are several options to use to build that base vocabulary: we can take the most common substrings in pre-tokenized words, for instance, or apply BPE on the initial corpus with a large vocabulary size."),rr.forEach(e),Dt=m(s),ee=i(s,"P",{});var ir=p(ee);An=l(ir,"At each step of the training, the Unigram algorithm computes a loss over the corpus given the current vocabulary. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was removed, and looks for the symbols that would increase it the least. Those symbols have a lower effect on the overall loss over the corpus, so in a sense they are \u201Cless needed\u201D and are the best candidates for removal."),ir.forEach(e),At=m(s),B=i(s,"P",{});var Pe=p(B);Un=l(Pe,"This is all a very costly operation, so we don\u2019t just remove the single symbol associated with the lowest loss increase, but the "),Ut=Cn(Pe),Wt=l(Pe," ("),Ht=Cn(Pe),Bt=l(Pe," being a hyperparameter you can control, usually 10 or 20) percent of the symbols associated with the lowest loss increase. This process is then repeated until the vocabulary has reached the desired size."),Pe.forEach(e),It=m(s),te=i(s,"P",{});var pr=p(te);Wn=l(pr,"Note that we never remove the base characters, to make sure any word can be tokenized."),pr.forEach(e),Mt=m(s),ae=i(s,"P",{});var hr=p(ae);Hn=l(hr,"Now, this is still a bit vague: the main part of the algorithm is to compute a loss over the corpus and see how it changes when we remove some tokens from the vocabulary, but we haven\u2019t explained how to do this yet. This step relies on the tokenization algorithm of a Unigram model, so we\u2019ll dive into this next."),hr.forEach(e),Lt=m(s),ne=i(s,"P",{});var mr=p(ne);Bn=l(mr,"We\u2019ll reuse the corpus from the previous examples:"),mr.forEach(e),Ft=m(s),f(ys.$$.fragment,s),Xt=m(s),oe=i(s,"P",{});var cr=p(oe);In=l(cr,"and for this example, we will take all strict substrings for the initial vocabulary :"),cr.forEach(e),Rt=m(s),f(vs.$$.fragment,s),Vt=m(s),R=i(s,"H2",{class:!0});var yn=p(R);K=i(yn,"A",{id:!0,class:!0,href:!0});var ur=p(K);Oe=i(ur,"SPAN",{});var fr=p(Oe);f(ks.$$.fragment,fr),fr.forEach(e),ur.forEach(e),Mn=m(yn),De=i(yn,"SPAN",{});var dr=p(De);Ln=l(dr,"Tokenization algorithm"),dr.forEach(e),yn.forEach(e),Gt=m(s),le=i(s,"P",{});var gr=p(le);Fn=l(gr,"A Unigram model is a type of language model that considers each token to be independent of the tokens before it. It\u2019s the simplest language model, in the sense that the probability of token X given the previous context is just the probability of token X. So, if we used a Unigram language model to generate text, we would always predict the most common token."),gr.forEach(e),Yt=m(s),z=i(s,"P",{});var L=p(z);Xn=l(L,"The probability of a given token is its frequency (the number of times we find it) in the original corpus, divided by the sum of all frequencies of all tokens in the vocabulary (to make sure the probabilities sum up to 1). For instance, "),Ae=i(L,"CODE",{});var wr=p(Ae);Rn=l(wr,'"ug"'),wr.forEach(e),Vn=l(L," is present in "),Ue=i(L,"CODE",{});var br=p(Ue);Gn=l(br,'"hug"'),br.forEach(e),Yn=l(L,", "),We=i(L,"CODE",{});var yr=p(We);Zn=l(yr,'"pug"'),yr.forEach(e),Jn=l(L,", and "),He=i(L,"CODE",{});var vr=p(He);Kn=l(vr,'"hugs"'),vr.forEach(e),Qn=l(L,", so it has a frequency of 20 in our corpus."),L.forEach(e),Zt=m(s),re=i(s,"P",{});var kr=p(re);so=l(kr,"Here are the frequencies of all the possible subwords in the vocabulary:"),kr.forEach(e),Jt=m(s),f(_s.$$.fragment,s),Kt=m(s),Q=i(s,"P",{});var vn=p(Q);eo=l(vn,"So, the sum of all frequencies is 210, and the probability of the subword "),Be=i(vn,"CODE",{});var _r=p(Be);to=l(_r,'"ug"'),_r.forEach(e),ao=l(vn," is thus 20/210."),vn.forEach(e),Qt=m(s),f(ss.$$.fragment,s),sa=m(s),W=i(s,"P",{});var Ks=p(W);no=l(Ks,"Now, to tokenize a given word, we look at all the possible segmentations into tokens and compute the probability of each according to the Unigram model. Since all tokens are considered independent, this probability is just the product of the probability of each token. For instance, the tokenization "),Ie=i(Ks,"CODE",{});var jr=p(Ie);oo=l(jr,'["p", "u", "g"]'),jr.forEach(e),lo=l(Ks," of "),Me=i(Ks,"CODE",{});var $r=p(Me);ro=l($r,'"pug"'),$r.forEach(e),io=l(Ks,` has the probability:
`),ea=Cn(Ks),Ks.forEach(e),ta=m(s),V=i(s,"P",{});var zt=p(V);po=l(zt,"Comparatively, the tokenization "),Le=i(zt,"CODE",{});var qr=p(Le);ho=l(qr,'["pu", "g"]'),qr.forEach(e),mo=l(zt,` has the probability:
`),aa=Cn(zt),zt.forEach(e),na=m(s),ie=i(s,"P",{});var xr=p(ie);co=l(xr,"so that one is way more likely. In general, tokenizations with the least tokens possible will have the highest probability (because of that division by 210 repeated for each token), which corresponds to what we want intuitively: to split a word into the least number of tokens possible."),xr.forEach(e),oa=m(s),es=i(s,"P",{});var kn=p(es);uo=l(kn,"The tokenization of a word with the Unigram model is then the tokenization with the highest probability. In the example of "),Fe=i(kn,"CODE",{});var Er=p(Fe);fo=l(Er,'"pug"'),Er.forEach(e),go=l(kn,", here are the probabilities we would get for each possible segmentation:"),kn.forEach(e),la=m(s),f(js.$$.fragment,s),ra=m(s),S=i(s,"P",{});var us=p(S);wo=l(us,"So, "),Xe=i(us,"CODE",{});var zr=p(Xe);bo=l(zr,'"pug"'),zr.forEach(e),yo=l(us," would be tokenized as "),Re=i(us,"CODE",{});var Pr=p(Re);vo=l(Pr,'["p", "ug"]'),Pr.forEach(e),ko=l(us," or "),Ve=i(us,"CODE",{});var Tr=p(Ve);_o=l(Tr,'["pu", "g"]'),Tr.forEach(e),jo=l(us,", depending on which of those segmentations is encountered first (note that in a larger corpus, equality cases like this will be rare)."),us.forEach(e),ia=m(s),q=i(s,"P",{});var A=p(q);$o=l(A,"In this case, it was easy to find all the possible segmentations and compute their probabilities, but in general it\u2019s going to be a bit harder. There is a classic algorithm used for this, called the "),Ge=i(A,"EM",{});var Cr=p(Ge);qo=l(Cr,"Viterbi algorithm"),Cr.forEach(e),xo=l(A,". Essentially, we can build a graph to detect the possible segmentations of a given word by saying there is a branch from character "),Ye=i(A,"EM",{});var Nr=p(Ye);Eo=l(Nr,"a"),Nr.forEach(e),zo=l(A," to character "),Ze=i(A,"EM",{});var Sr=p(Ze);Po=l(Sr,"b"),Sr.forEach(e),To=l(A," if the subword from "),Je=i(A,"EM",{});var Or=p(Je);Co=l(Or,"a"),Or.forEach(e),No=l(A," to "),Ke=i(A,"EM",{});var Dr=p(Ke);So=l(Dr,"b"),Dr.forEach(e),Oo=l(A," is in the vocabulary, and attribute to that branch the probability of the subword."),A.forEach(e),pa=m(s),pe=i(s,"P",{});var Ar=p(pe);Do=l(Ar,"To find the path in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to unroll the path taken to arrive at the end."),Ar.forEach(e),ha=m(s),ts=i(s,"P",{});var _n=p(ts);Ao=l(_n,"Let\u2019s take a look at an example using our vocabulary and the word "),Qe=i(_n,"CODE",{});var Ur=p(Qe);Uo=l(Ur,'"unhug"'),Ur.forEach(e),Wo=l(_n,". For each position, the subwords with the best scores ending there are the following:"),_n.forEach(e),ma=m(s),f($s.$$.fragment,s),ca=m(s),I=i(s,"P",{});var Te=p(I);Ho=l(Te,"Thus "),st=i(Te,"CODE",{});var Wr=p(st);Bo=l(Wr,'"unhug"'),Wr.forEach(e),Io=l(Te," would be tokenized as "),et=i(Te,"CODE",{});var Hr=p(et);Mo=l(Hr,'["un", "hug"]'),Hr.forEach(e),Lo=l(Te,"."),Te.forEach(e),ua=m(s),f(as.$$.fragment,s),fa=m(s),G=i(s,"H2",{class:!0});var jn=p(G);ns=i(jn,"A",{id:!0,class:!0,href:!0});var Br=p(ns);tt=i(Br,"SPAN",{});var Ir=p(tt);f(qs.$$.fragment,Ir),Ir.forEach(e),Br.forEach(e),Fo=m(jn),at=i(jn,"SPAN",{});var Mr=p(at);Xo=l(Mr,"Back to training"),Mr.forEach(e),jn.forEach(e),da=m(s),he=i(s,"P",{});var Lr=p(he);Ro=l(Lr,"Now that we have seen how the tokenization works, we can dive a little more deeply into the loss used during training. At any given stage, this loss is computed by tokenizing every word in the corpus, using the current vocabulary and the Unigram model determined by the frequencies of each token in the corpus (as seen before)."),Lr.forEach(e),ga=m(s),os=i(s,"P",{});var $n=p(os);Vo=l($n,"Each word in the corpus has a score, and the loss is the negative log likelihood of those scores \u2014 that is, the sum for all the words in the corpus of all the "),nt=i($n,"CODE",{});var Fr=p(nt);Go=l(Fr,"-log(P(word))"),Fr.forEach(e),Yo=l($n,"."),$n.forEach(e),wa=m(s),me=i(s,"P",{});var Xr=p(me);Zo=l(Xr,"Let\u2019s go back to our example with the following corpus:"),Xr.forEach(e),ba=m(s),f(xs.$$.fragment,s),ya=m(s),ce=i(s,"P",{});var Rr=p(ce);Jo=l(Rr,"The tokenization of each word with their respective scores is:"),Rr.forEach(e),va=m(s),f(Es.$$.fragment,s),ka=m(s),ue=i(s,"P",{});var Vr=p(ue);Ko=l(Vr,"So the loss is:"),Vr.forEach(e),_a=m(s),f(zs.$$.fragment,s),ja=m(s),O=i(s,"P",{});var fs=p(O);Qo=l(fs,"Now we need to compute how removing each token affects the loss. This is rather tedious, so we\u2019ll just do it for two tokens here and save the whole process for when we have code to help us. In this (very) particular case, we had two equivalent tokenizations of all the words: as we saw earlier, for example, "),ot=i(fs,"CODE",{});var Gr=p(ot);sl=l(Gr,'"pug"'),Gr.forEach(e),el=l(fs," could be tokenized "),lt=i(fs,"CODE",{});var Yr=p(lt);tl=l(Yr,'["p", "ug"]'),Yr.forEach(e),al=l(fs," with the same score. Thus, removing the "),rt=i(fs,"CODE",{});var Zr=p(rt);nl=l(Zr,'"pu"'),Zr.forEach(e),ol=l(fs," token from the vocabulary will give the exact same loss."),fs.forEach(e),$a=m(s),D=i(s,"P",{});var ds=p(D);ll=l(ds,"On the other hand, removing "),it=i(ds,"CODE",{});var Jr=p(it);rl=l(Jr,'"hug"'),Jr.forEach(e),il=l(ds," will make the loss worse, because the tokenization of "),pt=i(ds,"CODE",{});var Kr=p(pt);pl=l(Kr,'"hug"'),Kr.forEach(e),hl=l(ds," and "),ht=i(ds,"CODE",{});var Qr=p(ht);ml=l(Qr,'"hugs"'),Qr.forEach(e),cl=l(ds," will become:"),ds.forEach(e),qa=m(s),f(Ps.$$.fragment,s),xa=m(s),fe=i(s,"P",{});var si=p(fe);ul=l(si,"These changes will cause the loss to rise by:"),si.forEach(e),Ea=m(s),f(Ts.$$.fragment,s),za=m(s),M=i(s,"P",{});var Ce=p(M);fl=l(Ce,"Therefore, the token "),mt=i(Ce,"CODE",{});var ei=p(mt);dl=l(ei,'"pu"'),ei.forEach(e),gl=l(Ce," will probably be removed from the vocabulary, but not "),ct=i(Ce,"CODE",{});var ti=p(ct);wl=l(ti,'"hug"'),ti.forEach(e),bl=l(Ce,"."),Ce.forEach(e),Pa=m(s),Y=i(s,"H2",{class:!0});var qn=p(Y);ls=i(qn,"A",{id:!0,class:!0,href:!0});var ai=p(ls);ut=i(ai,"SPAN",{});var ni=p(ut);f(Cs.$$.fragment,ni),ni.forEach(e),ai.forEach(e),yl=m(qn),ft=i(qn,"SPAN",{});var oi=p(ft);vl=l(oi,"Implementing Unigram"),oi.forEach(e),qn.forEach(e),Ta=m(s),de=i(s,"P",{});var li=p(de);kl=l(li,"Now let\u2019s implement everything we\u2019ve seen so far in code. Like with BPE and WordPiece, this is not an efficient implementation of the Unigram algorithm (quite the opposite), but it should help you understand it a bit better."),li.forEach(e),Ca=m(s),ge=i(s,"P",{});var ri=p(ge);_l=l(ri,"We will use the same corpus as before as an example:"),ri.forEach(e),Na=m(s),f(Ns.$$.fragment,s),Sa=m(s),rs=i(s,"P",{});var xn=p(rs);jl=l(xn,"This time, we will use "),dt=i(xn,"CODE",{});var ii=p(dt);$l=l(ii,"xlnet-base-cased"),ii.forEach(e),ql=l(xn," as our model:"),xn.forEach(e),Oa=m(s),f(Ss.$$.fragment,s),Da=m(s),we=i(s,"P",{});var pi=p(we);xl=l(pi,"Like for BPE and WordPiece, we begin by counting the number of occurrences of each word in the corpus:"),pi.forEach(e),Aa=m(s),f(Os.$$.fragment,s),Ua=m(s),be=i(s,"P",{});var hi=p(be);El=l(hi,"Then, we need to initialize our vocabulary to something larger than the vocab size we will want at the end. We have to include all the basic characters (otherwise we won\u2019t be able to tokenize every word), but for the bigger substrings we\u2019ll only keep the most common ones, so we sort them by frequency:"),hi.forEach(e),Wa=m(s),f(Ds.$$.fragment,s),Ha=m(s),f(As.$$.fragment,s),Ba=m(s),ye=i(s,"P",{});var mi=p(ye);zl=l(mi,"We group the characters with the best subwords to arrive at an initial vocabulary of size 300:"),mi.forEach(e),Ia=m(s),f(Us.$$.fragment,s),Ma=m(s),f(is.$$.fragment,s),La=m(s),ve=i(s,"P",{});var ci=p(ve);Pl=l(ci,"Next, we compute the sum of all frequencies, to convert the frequencies into probabilities. For our model we will store the logarithms of the probabilities, because it\u2019s more numerically stable to add logarithms than to multiply small numbers, and this will simplify the computation of the loss of the model:"),ci.forEach(e),Fa=m(s),f(Ws.$$.fragment,s),Xa=m(s),ps=i(s,"P",{});var En=p(ps);Tl=l(En,"Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named "),gt=i(En,"CODE",{});var ui=p(gt);Cl=l(ui,"best_segmentations"),ui.forEach(e),Nl=l(En,". We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated."),En.forEach(e),Ra=m(s),hs=i(s,"P",{});var zn=p(hs);Sl=l(zn,"Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in "),wt=i(zn,"CODE",{});var fi=p(wt);Ol=l(fi,"best_segmentations"),fi.forEach(e),Dl=l(zn,"."),zn.forEach(e),Va=m(s),ke=i(s,"P",{});var di=p(ke);Al=l(di,"Once the main loop is finished, we just start from the end and hop from one start position to the next, recording the tokens as we go, until we reach the start of the word:"),di.forEach(e),Ga=m(s),f(Hs.$$.fragment,s),Ya=m(s),_e=i(s,"P",{});var gi=p(_e);Ul=l(gi,"We can already try our initial model on some words:"),gi.forEach(e),Za=m(s),f(Bs.$$.fragment,s),Ja=m(s),f(Is.$$.fragment,s),Ka=m(s),je=i(s,"P",{});var wi=p(je);Wl=l(wi,"Now it\u2019s easy to compute the loss of the model on the corpus!"),wi.forEach(e),Qa=m(s),f(Ms.$$.fragment,s),sn=m(s),$e=i(s,"P",{});var bi=p($e);Hl=l(bi,"We can check it works on the model we have:"),bi.forEach(e),en=m(s),f(Ls.$$.fragment,s),tn=m(s),f(Fs.$$.fragment,s),an=m(s),qe=i(s,"P",{});var yi=p(qe);Bl=l(yi,"Computing the scores for each token is not very hard either; we just have to compute the loss for the models obtained by deleting each token:"),yi.forEach(e),nn=m(s),f(Xs.$$.fragment,s),on=m(s),xe=i(s,"P",{});var vi=p(xe);Il=l(vi,"We can try it on a given token:"),vi.forEach(e),ln=m(s),f(Rs.$$.fragment,s),rn=m(s),x=i(s,"P",{});var U=p(x);Ml=l(U,"Since "),bt=i(U,"CODE",{});var ki=p(bt);Ll=l(ki,'"ll"'),ki.forEach(e),Fl=l(U," is used in the tokenization of "),yt=i(U,"CODE",{});var _i=p(yt);Xl=l(_i,'"Hopefully"'),_i.forEach(e),Rl=l(U,", and removing it will probably make us use the token "),vt=i(U,"CODE",{});var ji=p(vt);Vl=l(ji,'"l"'),ji.forEach(e),Gl=l(U," twice instead, we expect it will have a positive loss. "),kt=i(U,"CODE",{});var $i=p(kt);Yl=l($i,'"his"'),$i.forEach(e),Zl=l(U," is only used inside the word "),_t=i(U,"CODE",{});var qi=p(_t);Jl=l(qi,'"This"'),qi.forEach(e),Kl=l(U,", which is tokenized as itself, so we expect it to have a zero loss. Here are the results:"),U.forEach(e),pn=m(s),f(Vs.$$.fragment,s),hn=m(s),f(ms.$$.fragment,s),mn=m(s),Ee=i(s,"P",{});var xi=p(Ee);Ql=l(xi,"With all of this in place, the last thing we need to do is add the special tokens used by the model to the vocabulary, then loop until we have pruned enough tokens from the vocabulary to reach our desired size:"),xi.forEach(e),cn=m(s),f(Gs.$$.fragment,s),un=m(s),cs=i(s,"P",{});var Pn=p(cs);sr=l(Pn,"Then, to tokenize some text, we just need to apply the pre-tokenization and then use our "),jt=i(Pn,"CODE",{});var Ei=p(jt);er=l(Ei,"encode_word()"),Ei.forEach(e),tr=l(Pn," function:"),Pn.forEach(e),fn=m(s),f(Ys.$$.fragment,s),dn=m(s),f(Zs.$$.fragment,s),gn=m(s),ze=i(s,"P",{});var zi=p(ze);ar=l(zi,"That\u2019s it for Unigram! Hopefully by now you\u2019re feeling like an expert in all things tokenizer. In the next section, we will delve into the building blocks of the \u{1F917} Tokenizers library, and show you how you can use them to build your own tokenizer."),zi.forEach(e),this.h()},h(){j(c,"name","hf:doc:metadata"),j(c,"content",JSON.stringify(Ri)),j(v,"id","unigram-tokenization"),j(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(v,"href","#unigram-tokenization"),j(y,"class","relative group"),j(J,"id","training-algorithm"),j(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(J,"href","#training-algorithm"),j(X,"class","relative group"),Ut.a=Wt,Ht.a=Bt,j(K,"id","tokenization-algorithm"),j(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(K,"href","#tokenization-algorithm"),j(R,"class","relative group"),ea.a=null,aa.a=null,j(ns,"id","back-to-training"),j(ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(ns,"href","#back-to-training"),j(G,"class","relative group"),j(ls,"id","implementing-unigram"),j(ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(ls,"href","#implementing-unigram"),j(Y,"class","relative group")},m(s,a){t(document.head,c),n(s,_,a),n(s,y,a),t(y,v),t(v,P),d($,P,null),t(y,E),t(y,N),t(N,F),n(s,T,a),d(H,s,a),n(s,gs,a),n(s,Qs,a),t(Qs,Nn),n(s,Ct,a),d(ws,s,a),n(s,Nt,a),d(Z,s,a),n(s,St,a),n(s,X,a),t(X,J),t(J,Ne),d(bs,Ne,null),t(X,Sn),t(X,Se),t(Se,On),n(s,Ot,a),n(s,se,a),t(se,Dn),n(s,Dt,a),n(s,ee,a),t(ee,An),n(s,At,a),n(s,B,a),t(B,Un),Ut.m(Pi,B),t(B,Wt),Ht.m(Ti,B),t(B,Bt),n(s,It,a),n(s,te,a),t(te,Wn),n(s,Mt,a),n(s,ae,a),t(ae,Hn),n(s,Lt,a),n(s,ne,a),t(ne,Bn),n(s,Ft,a),d(ys,s,a),n(s,Xt,a),n(s,oe,a),t(oe,In),n(s,Rt,a),d(vs,s,a),n(s,Vt,a),n(s,R,a),t(R,K),t(K,Oe),d(ks,Oe,null),t(R,Mn),t(R,De),t(De,Ln),n(s,Gt,a),n(s,le,a),t(le,Fn),n(s,Yt,a),n(s,z,a),t(z,Xn),t(z,Ae),t(Ae,Rn),t(z,Vn),t(z,Ue),t(Ue,Gn),t(z,Yn),t(z,We),t(We,Zn),t(z,Jn),t(z,He),t(He,Kn),t(z,Qn),n(s,Zt,a),n(s,re,a),t(re,so),n(s,Jt,a),d(_s,s,a),n(s,Kt,a),n(s,Q,a),t(Q,eo),t(Q,Be),t(Be,to),t(Q,ao),n(s,Qt,a),d(ss,s,a),n(s,sa,a),n(s,W,a),t(W,no),t(W,Ie),t(Ie,oo),t(W,lo),t(W,Me),t(Me,ro),t(W,io),ea.m(Ci,W),n(s,ta,a),n(s,V,a),t(V,po),t(V,Le),t(Le,ho),t(V,mo),aa.m(Ni,V),n(s,na,a),n(s,ie,a),t(ie,co),n(s,oa,a),n(s,es,a),t(es,uo),t(es,Fe),t(Fe,fo),t(es,go),n(s,la,a),d(js,s,a),n(s,ra,a),n(s,S,a),t(S,wo),t(S,Xe),t(Xe,bo),t(S,yo),t(S,Re),t(Re,vo),t(S,ko),t(S,Ve),t(Ve,_o),t(S,jo),n(s,ia,a),n(s,q,a),t(q,$o),t(q,Ge),t(Ge,qo),t(q,xo),t(q,Ye),t(Ye,Eo),t(q,zo),t(q,Ze),t(Ze,Po),t(q,To),t(q,Je),t(Je,Co),t(q,No),t(q,Ke),t(Ke,So),t(q,Oo),n(s,pa,a),n(s,pe,a),t(pe,Do),n(s,ha,a),n(s,ts,a),t(ts,Ao),t(ts,Qe),t(Qe,Uo),t(ts,Wo),n(s,ma,a),d($s,s,a),n(s,ca,a),n(s,I,a),t(I,Ho),t(I,st),t(st,Bo),t(I,Io),t(I,et),t(et,Mo),t(I,Lo),n(s,ua,a),d(as,s,a),n(s,fa,a),n(s,G,a),t(G,ns),t(ns,tt),d(qs,tt,null),t(G,Fo),t(G,at),t(at,Xo),n(s,da,a),n(s,he,a),t(he,Ro),n(s,ga,a),n(s,os,a),t(os,Vo),t(os,nt),t(nt,Go),t(os,Yo),n(s,wa,a),n(s,me,a),t(me,Zo),n(s,ba,a),d(xs,s,a),n(s,ya,a),n(s,ce,a),t(ce,Jo),n(s,va,a),d(Es,s,a),n(s,ka,a),n(s,ue,a),t(ue,Ko),n(s,_a,a),d(zs,s,a),n(s,ja,a),n(s,O,a),t(O,Qo),t(O,ot),t(ot,sl),t(O,el),t(O,lt),t(lt,tl),t(O,al),t(O,rt),t(rt,nl),t(O,ol),n(s,$a,a),n(s,D,a),t(D,ll),t(D,it),t(it,rl),t(D,il),t(D,pt),t(pt,pl),t(D,hl),t(D,ht),t(ht,ml),t(D,cl),n(s,qa,a),d(Ps,s,a),n(s,xa,a),n(s,fe,a),t(fe,ul),n(s,Ea,a),d(Ts,s,a),n(s,za,a),n(s,M,a),t(M,fl),t(M,mt),t(mt,dl),t(M,gl),t(M,ct),t(ct,wl),t(M,bl),n(s,Pa,a),n(s,Y,a),t(Y,ls),t(ls,ut),d(Cs,ut,null),t(Y,yl),t(Y,ft),t(ft,vl),n(s,Ta,a),n(s,de,a),t(de,kl),n(s,Ca,a),n(s,ge,a),t(ge,_l),n(s,Na,a),d(Ns,s,a),n(s,Sa,a),n(s,rs,a),t(rs,jl),t(rs,dt),t(dt,$l),t(rs,ql),n(s,Oa,a),d(Ss,s,a),n(s,Da,a),n(s,we,a),t(we,xl),n(s,Aa,a),d(Os,s,a),n(s,Ua,a),n(s,be,a),t(be,El),n(s,Wa,a),d(Ds,s,a),n(s,Ha,a),d(As,s,a),n(s,Ba,a),n(s,ye,a),t(ye,zl),n(s,Ia,a),d(Us,s,a),n(s,Ma,a),d(is,s,a),n(s,La,a),n(s,ve,a),t(ve,Pl),n(s,Fa,a),d(Ws,s,a),n(s,Xa,a),n(s,ps,a),t(ps,Tl),t(ps,gt),t(gt,Cl),t(ps,Nl),n(s,Ra,a),n(s,hs,a),t(hs,Sl),t(hs,wt),t(wt,Ol),t(hs,Dl),n(s,Va,a),n(s,ke,a),t(ke,Al),n(s,Ga,a),d(Hs,s,a),n(s,Ya,a),n(s,_e,a),t(_e,Ul),n(s,Za,a),d(Bs,s,a),n(s,Ja,a),d(Is,s,a),n(s,Ka,a),n(s,je,a),t(je,Wl),n(s,Qa,a),d(Ms,s,a),n(s,sn,a),n(s,$e,a),t($e,Hl),n(s,en,a),d(Ls,s,a),n(s,tn,a),d(Fs,s,a),n(s,an,a),n(s,qe,a),t(qe,Bl),n(s,nn,a),d(Xs,s,a),n(s,on,a),n(s,xe,a),t(xe,Il),n(s,ln,a),d(Rs,s,a),n(s,rn,a),n(s,x,a),t(x,Ml),t(x,bt),t(bt,Ll),t(x,Fl),t(x,yt),t(yt,Xl),t(x,Rl),t(x,vt),t(vt,Vl),t(x,Gl),t(x,kt),t(kt,Yl),t(x,Zl),t(x,_t),t(_t,Jl),t(x,Kl),n(s,pn,a),d(Vs,s,a),n(s,hn,a),d(ms,s,a),n(s,mn,a),n(s,Ee,a),t(Ee,Ql),n(s,cn,a),d(Gs,s,a),n(s,un,a),n(s,cs,a),t(cs,sr),t(cs,jt),t(jt,er),t(cs,tr),n(s,fn,a),d(Ys,s,a),n(s,dn,a),d(Zs,s,a),n(s,gn,a),n(s,ze,a),t(ze,ar),wn=!0},p(s,[a]){const Js={};a&2&&(Js.$$scope={dirty:a,ctx:s}),Z.$set(Js);const $t={};a&2&&($t.$$scope={dirty:a,ctx:s}),ss.$set($t);const qt={};a&2&&(qt.$$scope={dirty:a,ctx:s}),as.$set(qt);const xt={};a&2&&(xt.$$scope={dirty:a,ctx:s}),is.$set(xt);const Et={};a&2&&(Et.$$scope={dirty:a,ctx:s}),ms.$set(Et)},i(s){wn||(g($.$$.fragment,s),g(H.$$.fragment,s),g(ws.$$.fragment,s),g(Z.$$.fragment,s),g(bs.$$.fragment,s),g(ys.$$.fragment,s),g(vs.$$.fragment,s),g(ks.$$.fragment,s),g(_s.$$.fragment,s),g(ss.$$.fragment,s),g(js.$$.fragment,s),g($s.$$.fragment,s),g(as.$$.fragment,s),g(qs.$$.fragment,s),g(xs.$$.fragment,s),g(Es.$$.fragment,s),g(zs.$$.fragment,s),g(Ps.$$.fragment,s),g(Ts.$$.fragment,s),g(Cs.$$.fragment,s),g(Ns.$$.fragment,s),g(Ss.$$.fragment,s),g(Os.$$.fragment,s),g(Ds.$$.fragment,s),g(As.$$.fragment,s),g(Us.$$.fragment,s),g(is.$$.fragment,s),g(Ws.$$.fragment,s),g(Hs.$$.fragment,s),g(Bs.$$.fragment,s),g(Is.$$.fragment,s),g(Ms.$$.fragment,s),g(Ls.$$.fragment,s),g(Fs.$$.fragment,s),g(Xs.$$.fragment,s),g(Rs.$$.fragment,s),g(Vs.$$.fragment,s),g(ms.$$.fragment,s),g(Gs.$$.fragment,s),g(Ys.$$.fragment,s),g(Zs.$$.fragment,s),wn=!0)},o(s){w($.$$.fragment,s),w(H.$$.fragment,s),w(ws.$$.fragment,s),w(Z.$$.fragment,s),w(bs.$$.fragment,s),w(ys.$$.fragment,s),w(vs.$$.fragment,s),w(ks.$$.fragment,s),w(_s.$$.fragment,s),w(ss.$$.fragment,s),w(js.$$.fragment,s),w($s.$$.fragment,s),w(as.$$.fragment,s),w(qs.$$.fragment,s),w(xs.$$.fragment,s),w(Es.$$.fragment,s),w(zs.$$.fragment,s),w(Ps.$$.fragment,s),w(Ts.$$.fragment,s),w(Cs.$$.fragment,s),w(Ns.$$.fragment,s),w(Ss.$$.fragment,s),w(Os.$$.fragment,s),w(Ds.$$.fragment,s),w(As.$$.fragment,s),w(Us.$$.fragment,s),w(is.$$.fragment,s),w(Ws.$$.fragment,s),w(Hs.$$.fragment,s),w(Bs.$$.fragment,s),w(Is.$$.fragment,s),w(Ms.$$.fragment,s),w(Ls.$$.fragment,s),w(Fs.$$.fragment,s),w(Xs.$$.fragment,s),w(Rs.$$.fragment,s),w(Vs.$$.fragment,s),w(ms.$$.fragment,s),w(Gs.$$.fragment,s),w(Ys.$$.fragment,s),w(Zs.$$.fragment,s),wn=!1},d(s){e(c),s&&e(_),s&&e(y),b($),s&&e(T),b(H,s),s&&e(gs),s&&e(Qs),s&&e(Ct),b(ws,s),s&&e(Nt),b(Z,s),s&&e(St),s&&e(X),b(bs),s&&e(Ot),s&&e(se),s&&e(Dt),s&&e(ee),s&&e(At),s&&e(B),s&&e(It),s&&e(te),s&&e(Mt),s&&e(ae),s&&e(Lt),s&&e(ne),s&&e(Ft),b(ys,s),s&&e(Xt),s&&e(oe),s&&e(Rt),b(vs,s),s&&e(Vt),s&&e(R),b(ks),s&&e(Gt),s&&e(le),s&&e(Yt),s&&e(z),s&&e(Zt),s&&e(re),s&&e(Jt),b(_s,s),s&&e(Kt),s&&e(Q),s&&e(Qt),b(ss,s),s&&e(sa),s&&e(W),s&&e(ta),s&&e(V),s&&e(na),s&&e(ie),s&&e(oa),s&&e(es),s&&e(la),b(js,s),s&&e(ra),s&&e(S),s&&e(ia),s&&e(q),s&&e(pa),s&&e(pe),s&&e(ha),s&&e(ts),s&&e(ma),b($s,s),s&&e(ca),s&&e(I),s&&e(ua),b(as,s),s&&e(fa),s&&e(G),b(qs),s&&e(da),s&&e(he),s&&e(ga),s&&e(os),s&&e(wa),s&&e(me),s&&e(ba),b(xs,s),s&&e(ya),s&&e(ce),s&&e(va),b(Es,s),s&&e(ka),s&&e(ue),s&&e(_a),b(zs,s),s&&e(ja),s&&e(O),s&&e($a),s&&e(D),s&&e(qa),b(Ps,s),s&&e(xa),s&&e(fe),s&&e(Ea),b(Ts,s),s&&e(za),s&&e(M),s&&e(Pa),s&&e(Y),b(Cs),s&&e(Ta),s&&e(de),s&&e(Ca),s&&e(ge),s&&e(Na),b(Ns,s),s&&e(Sa),s&&e(rs),s&&e(Oa),b(Ss,s),s&&e(Da),s&&e(we),s&&e(Aa),b(Os,s),s&&e(Ua),s&&e(be),s&&e(Wa),b(Ds,s),s&&e(Ha),b(As,s),s&&e(Ba),s&&e(ye),s&&e(Ia),b(Us,s),s&&e(Ma),b(is,s),s&&e(La),s&&e(ve),s&&e(Fa),b(Ws,s),s&&e(Xa),s&&e(ps),s&&e(Ra),s&&e(hs),s&&e(Va),s&&e(ke),s&&e(Ga),b(Hs,s),s&&e(Ya),s&&e(_e),s&&e(Za),b(Bs,s),s&&e(Ja),b(Is,s),s&&e(Ka),s&&e(je),s&&e(Qa),b(Ms,s),s&&e(sn),s&&e($e),s&&e(en),b(Ls,s),s&&e(tn),b(Fs,s),s&&e(an),s&&e(qe),s&&e(nn),b(Xs,s),s&&e(on),s&&e(xe),s&&e(ln),b(Rs,s),s&&e(rn),s&&e(x),s&&e(pn),b(Vs,s),s&&e(hn),b(ms,s),s&&e(mn),s&&e(Ee),s&&e(cn),b(Gs,s),s&&e(un),s&&e(cs),s&&e(fn),b(Ys,s),s&&e(dn),b(Zs,s),s&&e(gn),s&&e(ze)}}}const Ri={local:"unigram-tokenization",sections:[{local:"training-algorithm",title:"Training algorithm"},{local:"tokenization-algorithm",title:"Tokenization algorithm"},{local:"back-to-training",title:"Back to training"},{local:"implementing-unigram",title:"Implementing Unigram"}],title:"Unigram tokenization"};function Vi(C){return Ui(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sp extends Si{constructor(c){super();Oi(this,c,Vi,Xi,Di,{})}}export{sp as default,Ri as metadata};
