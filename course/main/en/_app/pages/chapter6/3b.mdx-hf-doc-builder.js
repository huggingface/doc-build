import{S as hp,i as cp,s as up,e as m,k as u,w as b,t as r,U as ap,M as fp,c as h,d as s,m as f,x as k,a as c,h as i,V as op,b as C,N as rp,G as a,g as l,y as $,o as d,p as G,q as _,B as v,v as dp,n as J}from"../../chunks/vendor-hf-doc-builder.js";import{T as ro}from"../../chunks/Tip-hf-doc-builder.js";import{Y as ip}from"../../chunks/Youtube-hf-doc-builder.js";import{I as io}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as j}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as lp}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as _p}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";function wp(x){let n,p;return n=new lp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3b_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3b_tf.ipynb"}]}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function gp(x){let n,p;return n=new lp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter6/section3b_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter6/section3b_pt.ipynb"}]}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function bp(x){let n,p;return n=new ip({props:{id:"b3u8RzBCX9Y"}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function kp(x){let n,p;return n=new ip({props:{id:"_wxyB3j3mk4"}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function $p(x){let n,p;return n=new j({props:{code:`from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="tf")
outputs = model(**inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForQuestionAnswering

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
outputs = model(**inputs)`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function vp(x){let n,p;return n=new j({props:{code:`from transformers import AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = "distilbert-base-cased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors="pt")
outputs = model(**inputs)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForQuestionAnswering

model_checkpoint = <span class="hljs-string">&quot;distilbert-base-cased-distilled-squad&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)

inputs = tokenizer(question, context, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function xp(x){let n,p;return n=new j({props:{code:"(1, 66) (1, 66)",highlighted:'(<span class="hljs-number">1</span>, <span class="hljs-number">66</span>) (<span class="hljs-number">1</span>, <span class="hljs-number">66</span>)'}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function yp(x){let n,p;return n=new j({props:{code:"torch.Size([1, 66]) torch.Size([1, 66])",highlighted:'torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">66</span>]) torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">66</span>])'}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function jp(x){let n,p;return n=new j({props:{code:`import tensorflow as tf

sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
mask = tf.constant(mask)[None]

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
mask = tf.constant(mask)[<span class="hljs-literal">None</span>]

start_logits = tf.where(mask, -<span class="hljs-number">10000</span>, start_logits)
end_logits = tf.where(mask, -<span class="hljs-number">10000</span>, end_logits)`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function qp(x){let n,p;return n=new j({props:{code:`import torch

sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
mask = torch.tensor(mask)[None]

start_logits[mask] = -10000
end_logits[mask] = -10000`,highlighted:`<span class="hljs-keyword">import</span> torch

sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
mask = torch.tensor(mask)[<span class="hljs-literal">None</span>]

start_logits[mask] = -<span class="hljs-number">10000</span>
end_logits[mask] = -<span class="hljs-number">10000</span>`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Ep(x){let n,p;return n=new j({props:{code:`start_probabilities = tf.math.softmax(start_logits, axis=-1)[0].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1)[0].numpy()`,highlighted:`start_probabilities = tf.math.softmax(start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].numpy()`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Tp(x){let n,p;return n=new j({props:{code:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]`,highlighted:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Pp(x){let n,p,t,w,g,y,E,S,z,P,L,D,N,A;return N=new j({props:{code:`import numpy as np

scores = np.triu(scores)`,highlighted:`<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

scores = np.triu(scores)`}}),{c(){n=m("p"),p=r("Then we\u2019ll mask the values where "),t=m("code"),w=r("start_index > end_index"),g=r(" by setting them to "),y=m("code"),E=r("0"),S=r(" (the other probabilities are all positive numbers). The "),z=m("code"),P=r("np.triu()"),L=r(" function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"),D=u(),b(N.$$.fragment)},l(q){n=h(q,"P",{});var T=c(n);p=i(T,"Then we\u2019ll mask the values where "),t=h(T,"CODE",{});var xe=c(t);w=i(xe,"start_index > end_index"),xe.forEach(s),g=i(T," by setting them to "),y=h(T,"CODE",{});var Q=c(y);E=i(Q,"0"),Q.forEach(s),S=i(T," (the other probabilities are all positive numbers). The "),z=h(T,"CODE",{});var ye=c(z);P=i(ye,"np.triu()"),ye.forEach(s),L=i(T," function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"),T.forEach(s),D=f(q),k(N.$$.fragment,q)},m(q,T){l(q,n,T),a(n,p),a(n,t),a(t,w),a(n,g),a(n,y),a(y,E),a(n,S),a(n,z),a(z,P),a(n,L),l(q,D,T),$(N,q,T),A=!0},i(q){A||(_(N.$$.fragment,q),A=!0)},o(q){d(N.$$.fragment,q),A=!1},d(q){q&&s(n),q&&s(D),v(N,q)}}}function Sp(x){let n,p,t,w,g,y,E,S,z,P,L,D,N,A;return N=new j({props:{code:"scores = torch.triu(scores)",highlighted:"scores = torch.triu(scores)"}}),{c(){n=m("p"),p=r("Then we\u2019ll mask the values where "),t=m("code"),w=r("start_index > end_index"),g=r(" by setting them to "),y=m("code"),E=r("0"),S=r(" (the other probabilities are all positive numbers). The "),z=m("code"),P=r("torch.triu()"),L=r(" function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"),D=u(),b(N.$$.fragment)},l(q){n=h(q,"P",{});var T=c(n);p=i(T,"Then we\u2019ll mask the values where "),t=h(T,"CODE",{});var xe=c(t);w=i(xe,"start_index > end_index"),xe.forEach(s),g=i(T," by setting them to "),y=h(T,"CODE",{});var Q=c(y);E=i(Q,"0"),Q.forEach(s),S=i(T," (the other probabilities are all positive numbers). The "),z=h(T,"CODE",{});var ye=c(z);P=i(ye,"torch.triu()"),ye.forEach(s),L=i(T," function returns the upper triangular part of the 2D tensor passed as an argument, so it will do that masking for us:"),T.forEach(s),D=f(q),k(N.$$.fragment,q)},m(q,T){l(q,n,T),a(n,p),a(n,t),a(t,w),a(n,g),a(n,y),a(y,E),a(n,S),a(n,z),a(z,P),a(n,L),l(q,D,T),$(N,q,T),A=!0},i(q){A||(_(N.$$.fragment,q),A=!0)},o(q){d(N.$$.fragment,q),A=!1},d(q){q&&s(n),q&&s(D),v(N,q)}}}function Cp(x){let n,p,t,w,g;return{c(){n=m("p"),p=r("\u270F\uFE0F "),t=m("strong"),w=r("Try it out!"),g=r(" Compute the start and end indices for the five most likely answers.")},l(y){n=h(y,"P",{});var E=c(n);p=i(E,"\u270F\uFE0F "),t=h(E,"STRONG",{});var S=c(t);w=i(S,"Try it out!"),S.forEach(s),g=i(E," Compute the start and end indices for the five most likely answers."),E.forEach(s)},m(y,E){l(y,n,E),a(n,p),a(n,t),a(t,w),a(n,g)},d(y){y&&s(n)}}}function zp(x){let n,p,t,w,g,y,E,S;return{c(){n=m("p"),p=r("\u270F\uFE0F "),t=m("strong"),w=r("Try it out!"),g=r(" Use the best scores you computed earlier to show the five most likely answers. To check your results, go back to the first pipeline and pass in "),y=m("code"),E=r("top_k=5"),S=r(" when calling it.")},l(z){n=h(z,"P",{});var P=c(n);p=i(P,"\u270F\uFE0F "),t=h(P,"STRONG",{});var L=c(t);w=i(L,"Try it out!"),L.forEach(s),g=i(P," Use the best scores you computed earlier to show the five most likely answers. To check your results, go back to the first pipeline and pass in "),y=h(P,"CODE",{});var D=c(y);E=i(D,"top_k=5"),D.forEach(s),S=i(P," when calling it."),P.forEach(s)},m(z,P){l(z,n,P),a(n,p),a(n,t),a(t,w),a(n,g),a(n,y),a(y,E),a(n,S)},d(z){z&&s(n)}}}function Np(x){let n,p,t,w;return n=new j({props:{code:`_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("tf")
print(inputs["input_ids"].shape)`,highlighted:`_ = inputs.pop(<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>)
offsets = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)

inputs = inputs.convert_to_tensors(<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)`}}),t=new j({props:{code:"(2, 384)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">384</span>)'}}),{c(){b(n.$$.fragment),p=u(),b(t.$$.fragment)},l(g){k(n.$$.fragment,g),p=f(g),k(t.$$.fragment,g)},m(g,y){$(n,g,y),l(g,p,y),$(t,g,y),w=!0},i(g){w||(_(n.$$.fragment,g),_(t.$$.fragment,g),w=!0)},o(g){d(n.$$.fragment,g),d(t.$$.fragment,g),w=!1},d(g){v(n,g),g&&s(p),v(t,g)}}}function Ap(x){let n,p,t,w;return n=new j({props:{code:`_ = inputs.pop("overflow_to_sample_mapping")
offsets = inputs.pop("offset_mapping")

inputs = inputs.convert_to_tensors("pt")
print(inputs["input_ids"].shape)`,highlighted:`_ = inputs.pop(<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>)
offsets = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)

inputs = inputs.convert_to_tensors(<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].shape)`}}),t=new j({props:{code:"torch.Size([2, 384])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>])'}}),{c(){b(n.$$.fragment),p=u(),b(t.$$.fragment)},l(g){k(n.$$.fragment,g),p=f(g),k(t.$$.fragment,g)},m(g,y){$(n,g,y),l(g,p,y),$(t,g,y),w=!0},i(g){w||(_(n.$$.fragment,g),_(t.$$.fragment,g),w=!0)},o(g){d(n.$$.fragment,g),d(t.$$.fragment,g),w=!1},d(g){v(n,g),g&&s(p),v(t,g)}}}function Dp(x){let n,p;return n=new j({props:{code:"(2, 384) (2, 384)",highlighted:'(<span class="hljs-number">2</span>, <span class="hljs-number">384</span>) (<span class="hljs-number">2</span>, <span class="hljs-number">384</span>)'}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Lp(x){let n,p;return n=new j({props:{code:"torch.Size([2, 384]) torch.Size([2, 384])",highlighted:'torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>]) torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">384</span>])'}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Op(x){let n,p;return n=new j({props:{code:`sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = tf.math.logical_or(tf.constant(mask)[None], inputs["attention_mask"] == 0)

start_logits = tf.where(mask, -10000, start_logits)
end_logits = tf.where(mask, -10000, end_logits)`,highlighted:`sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
<span class="hljs-comment"># Mask all the [PAD] tokens</span>
mask = tf.math.logical_or(tf.constant(mask)[<span class="hljs-literal">None</span>], inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] == <span class="hljs-number">0</span>)

start_logits = tf.where(mask, -<span class="hljs-number">10000</span>, start_logits)
end_logits = tf.where(mask, -<span class="hljs-number">10000</span>, end_logits)`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Fp(x){let n,p;return n=new j({props:{code:`sequence_ids = inputs.sequence_ids()
# Mask everything apart from the tokens of the context
mask = [i != 1 for i in sequence_ids]
# Unmask the [CLS] token
mask[0] = False
# Mask all the [PAD] tokens
mask = torch.logical_or(torch.tensor(mask)[None], (inputs["attention_mask"] == 0))

start_logits[mask] = -10000
end_logits[mask] = -10000`,highlighted:`sequence_ids = inputs.sequence_ids()
<span class="hljs-comment"># Mask everything apart from the tokens of the context</span>
mask = [i != <span class="hljs-number">1</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sequence_ids]
<span class="hljs-comment"># Unmask the [CLS] token</span>
mask[<span class="hljs-number">0</span>] = <span class="hljs-literal">False</span>
<span class="hljs-comment"># Mask all the [PAD] tokens</span>
mask = torch.logical_or(torch.tensor(mask)[<span class="hljs-literal">None</span>], (inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>] == <span class="hljs-number">0</span>))

start_logits[mask] = -<span class="hljs-number">10000</span>
end_logits[mask] = -<span class="hljs-number">10000</span>`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Mp(x){let n,p;return n=new j({props:{code:`start_probabilities = tf.math.softmax(start_logits, axis=-1).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-1).numpy()`,highlighted:`start_probabilities = tf.math.softmax(start_logits, axis=-<span class="hljs-number">1</span>).numpy()
end_probabilities = tf.math.softmax(end_logits, axis=-<span class="hljs-number">1</span>).numpy()`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Up(x){let n,p;return n=new j({props:{code:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)`,highlighted:`start_probabilities = torch.nn.functional.softmax(start_logits, dim=-<span class="hljs-number">1</span>)
end_probabilities = torch.nn.functional.softmax(end_logits, dim=-<span class="hljs-number">1</span>)`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Wp(x){let n,p;return n=new j({props:{code:`candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[0]
    end_idx = idx % scores.shape[0]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)`,highlighted:`candidates = []
<span class="hljs-keyword">for</span> start_probs, end_probs <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(start_probabilities, end_probabilities):
    scores = start_probs[:, <span class="hljs-literal">None</span>] * end_probs[<span class="hljs-literal">None</span>, :]
    idx = np.triu(scores).argmax().item()

    start_idx = idx // scores.shape[<span class="hljs-number">0</span>]
    end_idx = idx % scores.shape[<span class="hljs-number">0</span>]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

<span class="hljs-built_in">print</span>(candidates)`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Ip(x){let n,p;return n=new j({props:{code:`candidates = []
for start_probs, end_probs in zip(start_probabilities, end_probabilities):
    scores = start_probs[:, None] * end_probs[None, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[0]
    end_idx = idx % scores.shape[0]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

print(candidates)`,highlighted:`candidates = []
<span class="hljs-keyword">for</span> start_probs, end_probs <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(start_probabilities, end_probabilities):
    scores = start_probs[:, <span class="hljs-literal">None</span>] * end_probs[<span class="hljs-literal">None</span>, :]
    idx = torch.triu(scores).argmax().item()

    start_idx = idx // scores.shape[<span class="hljs-number">0</span>]
    end_idx = idx % scores.shape[<span class="hljs-number">0</span>]
    score = scores[start_idx, end_idx].item()
    candidates.append((start_idx, end_idx, score))

<span class="hljs-built_in">print</span>(candidates)`}}),{c(){b(n.$$.fragment)},l(t){k(n.$$.fragment,t)},m(t,w){$(n,t,w),p=!0},i(t){p||(_(n.$$.fragment,t),p=!0)},o(t){d(n.$$.fragment,t),p=!1},d(t){v(n,t)}}}function Hp(x){let n,p,t,w,g;return{c(){n=m("p"),p=r("\u270F\uFE0F "),t=m("strong"),w=r("Try it out!"),g=r(" Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk).")},l(y){n=h(y,"P",{});var E=c(n);p=i(E,"\u270F\uFE0F "),t=h(E,"STRONG",{});var S=c(t);w=i(S,"Try it out!"),S.forEach(s),g=i(E," Adapt the code above to return the scores and spans for the five most likely answers (in total, not per chunk)."),E.forEach(s)},m(y,E){l(y,n,E),a(n,p),a(n,t),a(t,w),a(n,g)},d(y){y&&s(n)}}}function Gp(x){let n,p,t,w,g,y,E,S;return{c(){n=m("p"),p=r("\u270F\uFE0F "),t=m("strong"),w=r("Try it out!"),g=r(" Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass in "),y=m("code"),E=r("top_k=5"),S=r(" when calling it.")},l(z){n=h(z,"P",{});var P=c(n);p=i(P,"\u270F\uFE0F "),t=h(P,"STRONG",{});var L=c(t);w=i(L,"Try it out!"),L.forEach(s),g=i(P," Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass in "),y=h(P,"CODE",{});var D=c(y);E=i(D,"top_k=5"),D.forEach(s),S=i(P," when calling it."),P.forEach(s)},m(z,P){l(z,n,P),a(n,p),a(n,t),a(t,w),a(n,g),a(n,y),a(y,E),a(n,S)},d(z){z&&s(n)}}}function Jp(x){let n,p,t,w,g,y,E,S,z,P,L,D,N,A,q,T,xe,Q,ye,lo,En,R,K,Jt,je,Pe,Ls,Be,po,Ye,mo,Os,ho,co,Tn,_e,uo,Qt,fo,_o,Fs,wo,go,Pn,Ve,Sn,Xe,Cn,Rt,bo,zn,Ze,Nn,et,An,Kt,ko,Dn,qe,Se,Ms,tt,$o,Us,vo,Ln,U,xo,Ws,yo,jo,st,Is,qo,Eo,Bt,To,Po,On,B,Y,Yt,Vt,So,Fn,Ee,nt,ji,Co,at,qi,Mn,Xt,zo,Un,ot,Wn,V,X,Zt,W,No,Hs,Ao,Do,Gs,Lo,Oo,Js,Fo,Mo,In,Ce,Uo,Qs,Wo,Io,Hn,Z,ee,es,ts,Ho,Gn,te,se,ss,F,Go,Rs,Jo,Qo,Ks,Ro,Ko,Bs,Bo,Yo,Ys,Vo,Xo,Jn,O,Zo,Vs,er,tr,Xs,sr,nr,Zs,ar,or,en,rr,ir,Qn,pp='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo><mo>\xD7</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">start_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">start_index</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">end_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">end_index</span></span><span class="mclose">]</span></span></span></span></span>',Rn,we,lr,Kn,mp='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">s</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo><mo>\xD7</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">p</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">b</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">l</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">s</mi></mrow><mo stretchy="false">[</mo><mrow><mi mathvariant="normal">e</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">i</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">e</mi><mi mathvariant="normal">x</mi></mrow><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\\mathrm{start\\_probabilities}[\\mathrm{start\\_index}] \\times \\mathrm{end\\_probabilities}[\\mathrm{end\\_index}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">start_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">start_index</span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\xD7</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord mathrm">end_probabilities</span></span><span class="mopen">[</span><span class="mord"><span class="mord mathrm">end_index</span></span><span class="mclose">]</span></span></span></span>',Bn,tn,pr,mr,Yn,ns,hr,Vn,rt,Xn,ne,ae,as,M,cr,sn,ur,fr,nn,dr,_r,an,wr,gr,on,br,kr,Zn,it,ea,os,$r,ta,lt,sa,ze,na,ge,vr,rn,xr,yr,ln,jr,qr,aa,pt,oa,rs,Er,ra,mt,ia,ht,la,is,Tr,pa,Ne,ma,Te,Ae,pn,ct,Pr,mn,Sr,ha,De,Cr,hn,zr,Nr,ca,ut,ua,ft,fa,Le,Ar,cn,Dr,Lr,da,dt,_a,_t,wa,Oe,Or,un,Fr,Mr,ga,be,Ur,fn,Wr,Ir,dn,Hr,Gr,ba,wt,ka,gt,$a,Fe,Jr,_n,Qr,Rr,va,ls,Kr,xa,bt,ya,kt,ja,Me,Br,wn,Yr,Vr,qa,$t,Ea,vt,Ta,ps,Xr,Pa,xt,Sa,ms,Zr,Ca,yt,za,hs,ei,Na,I,ti,gn,si,ni,bn,ai,oi,kn,ri,ii,Aa,jt,Da,H,li,$n,pi,mi,vn,hi,ci,xn,ui,fi,La,oe,re,cs,us,di,Oa,qt,Fa,ie,le,fs,ds,_i,Ma,pe,me,_s,ws,wi,Ua,he,ce,gs,bs,gi,Wa,ue,fe,ks,Et,Ia,$s,bi,Ha,Ue,Ga,We,ki,yn,$i,vi,Ja,Tt,Qa,Pt,Ra,vs,xi,Ka,Ie,Ba,xs,yi,Ya;t=new _p({props:{fw:x[0]}}),S=new io({});const Ei=[gp,wp],St=[];function Ti(e,o){return e[0]==="pt"?0:1}N=Ti(x),A=St[N]=Ei[N](x);const Pi=[kp,bp],Ct=[];function Si(e,o){return e[0]==="pt"?0:1}R=Si(x),K=Ct[R]=Pi[R](x),Be=new io({}),Ve=new j({props:{code:`from transformers import pipeline

question_answerer = pipeline("question-answering")
context = """
\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch, and TensorFlow \u2014 with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question = "Which deep learning libraries back \u{1F917} Transformers?"
question_answerer(question=question, context=context)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

question_answerer = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>)
context = <span class="hljs-string">&quot;&quot;&quot;
\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch, and TensorFlow \u2014 with a seamless integration
between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.
&quot;&quot;&quot;</span>
question = <span class="hljs-string">&quot;Which deep learning libraries back \u{1F917} Transformers?&quot;</span>
question_answerer(question=question, context=context)`}}),Xe=new j({props:{code:`{'score': 0.97773,
 'start': 78,
 'end': 105,
 'answer': 'Jax, PyTorch and TensorFlow'}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97773</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">78</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">105</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>}`}}),Ze=new j({props:{code:`long_context = """
\u{1F917} Transformers: State of the Art NLP

\u{1F917} Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

\u{1F917} Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
"""
question_answerer(question=question, context=long_context)`,highlighted:`long_context = <span class="hljs-string">&quot;&quot;&quot;
\u{1F917} Transformers: State of the Art NLP

\u{1F917} Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

\u{1F917} Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model&#x27;s lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

\u{1F917} Transformers is backed by the three most popular deep learning libraries \u2014 Jax, PyTorch and TensorFlow \u2014 with a seamless integration
between them. It&#x27;s straightforward to train your models with one before loading them for inference with the other.
&quot;&quot;&quot;</span>
question_answerer(question=question, context=long_context)`}}),et=new j({props:{code:`{'score': 0.97149,
 'start': 1892,
 'end': 1919,
 'answer': 'Jax, PyTorch and TensorFlow'}`,highlighted:`{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97149</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">1892</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">1919</span>,
 <span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>}`}}),tt=new io({});const Ci=[vp,$p],zt=[];function zi(e,o){return e[0]==="pt"?0:1}B=zi(x),Y=zt[B]=Ci[B](x),ot=new j({props:{code:`start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)`,highlighted:`start_logits = outputs.start_logits
end_logits = outputs.end_logits
<span class="hljs-built_in">print</span>(start_logits.shape, end_logits.shape)`}});const Ni=[yp,xp],Nt=[];function Ai(e,o){return e[0]==="pt"?0:1}V=Ai(x),X=Nt[V]=Ni[V](x);const Di=[qp,jp],At=[];function Li(e,o){return e[0]==="pt"?0:1}Z=Li(x),ee=At[Z]=Di[Z](x);const Oi=[Tp,Ep],Dt=[];function Fi(e,o){return e[0]==="pt"?0:1}te=Fi(x),se=Dt[te]=Oi[te](x),rt=new j({props:{code:"scores = start_probabilities[:, None] * end_probabilities[None, :]",highlighted:'scores = start_probabilities[:, <span class="hljs-literal">None</span>] * end_probabilities[<span class="hljs-literal">None</span>, :]'}});const Mi=[Sp,Pp],Lt=[];function Ui(e,o){return e[0]==="pt"?0:1}ne=Ui(x),ae=Lt[ne]=Mi[ne](x),it=new j({props:{code:`max_index = scores.argmax().item()
start_index = max_index // scores.shape[1]
end_index = max_index % scores.shape[1]
print(scores[start_index, end_index])`,highlighted:`max_index = scores.argmax().item()
start_index = max_index // scores.shape[<span class="hljs-number">1</span>]
end_index = max_index % scores.shape[<span class="hljs-number">1</span>]
<span class="hljs-built_in">print</span>(scores[start_index, end_index])`}}),lt=new j({props:{code:"0.97773",highlighted:'<span class="hljs-number">0.97773</span>'}}),ze=new ro({props:{$$slots:{default:[Cp]},$$scope:{ctx:x}}}),pt=new j({props:{code:`inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)
offsets = inputs_with_offsets["offset_mapping"]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]`,highlighted:`inputs_with_offsets = tokenizer(question, context, return_offsets_mapping=<span class="hljs-literal">True</span>)
offsets = inputs_with_offsets[<span class="hljs-string">&quot;offset_mapping&quot;</span>]

start_char, _ = offsets[start_index]
_, end_char = offsets[end_index]
answer = context[start_char:end_char]`}}),mt=new j({props:{code:`result = {
    "answer": answer,
    "start": start_char,
    "end": end_char,
    "score": scores[start_index, end_index],
}
print(result)`,highlighted:`result = {
    <span class="hljs-string">&quot;answer&quot;</span>: answer,
    <span class="hljs-string">&quot;start&quot;</span>: start_char,
    <span class="hljs-string">&quot;end&quot;</span>: end_char,
    <span class="hljs-string">&quot;score&quot;</span>: scores[start_index, end_index],
}
<span class="hljs-built_in">print</span>(result)`}}),ht=new j({props:{code:`{'answer': 'Jax, PyTorch and TensorFlow',
 'start': 78,
 'end': 105,
 'score': 0.97773}`,highlighted:`{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>,
 <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">78</span>,
 <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">105</span>,
 <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97773</span>}`}}),Ne=new ro({props:{$$slots:{default:[zp]},$$scope:{ctx:x}}}),ct=new io({}),ut=new j({props:{code:`inputs = tokenizer(question, long_context)
print(len(inputs["input_ids"]))`,highlighted:`inputs = tokenizer(question, long_context)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))`}}),ft=new j({props:{code:"461",highlighted:'<span class="hljs-number">461</span>'}}),dt=new j({props:{code:`inputs = tokenizer(question, long_context, max_length=384, truncation="only_second")
print(tokenizer.decode(inputs["input_ids"]))`,highlighted:`inputs = tokenizer(question, long_context, max_length=<span class="hljs-number">384</span>, truncation=<span class="hljs-string">&quot;only_second&quot;</span>)
<span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))`}}),_t=new j({props:{code:`"""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
"""`,highlighted:`<span class="hljs-string">&quot;&quot;&quot;
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model&#x27;s lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.

4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internal [SEP]
&quot;&quot;&quot;</span>`}}),wt=new j({props:{code:`sentence = "This sentence is not too long but we are going to split it anyway."
inputs = tokenizer(
    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

for ids in inputs["input_ids"]:
    print(tokenizer.decode(ids))`,highlighted:`sentence = <span class="hljs-string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>
inputs = tokenizer(
    sentence, truncation=<span class="hljs-literal">True</span>, return_overflowing_tokens=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>
)

<span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]:
    <span class="hljs-built_in">print</span>(tokenizer.decode(ids))`}}),gt=new j({props:{code:`'[CLS] This sentence is not [SEP]'
'[CLS] is not too long [SEP]'
'[CLS] too long but we [SEP]'
'[CLS] but we are going [SEP]'
'[CLS] are going to split [SEP]'
'[CLS] to split it anyway [SEP]'
'[CLS] it anyway. [SEP]'`,highlighted:`<span class="hljs-string">&#x27;[CLS] This sentence is not [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] is not too long [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] too long but we [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] but we are going [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] are going to split [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] to split it anyway [SEP]&#x27;</span>
<span class="hljs-string">&#x27;[CLS] it anyway. [SEP]&#x27;</span>`}}),bt=new j({props:{code:"print(inputs.keys())",highlighted:'<span class="hljs-built_in">print</span>(inputs.keys())'}}),kt=new j({props:{code:"dict_keys(['input_ids', 'attention_mask', 'overflow_to_sample_mapping'])",highlighted:'dict_keys([<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;overflow_to_sample_mapping&#x27;</span>])'}}),$t=new j({props:{code:'print(inputs["overflow_to_sample_mapping"])',highlighted:'<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>])'}}),vt=new j({props:{code:"[0, 0, 0, 0, 0, 0, 0]",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]'}}),xt=new j({props:{code:`sentences = [
    "This sentence is not too long but we are going to split it anyway.",
    "This sentence is shorter but will still get split.",
]
inputs = tokenizer(
    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2
)

print(inputs["overflow_to_sample_mapping"])`,highlighted:`sentences = [
    <span class="hljs-string">&quot;This sentence is not too long but we are going to split it anyway.&quot;</span>,
    <span class="hljs-string">&quot;This sentence is shorter but will still get split.&quot;</span>,
]
inputs = tokenizer(
    sentences, truncation=<span class="hljs-literal">True</span>, return_overflowing_tokens=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">6</span>, stride=<span class="hljs-number">2</span>
)

<span class="hljs-built_in">print</span>(inputs[<span class="hljs-string">&quot;overflow_to_sample_mapping&quot;</span>])`}}),yt=new j({props:{code:"[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]",highlighted:'[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]'}}),jt=new j({props:{code:`inputs = tokenizer(
    question,
    long_context,
    stride=128,
    max_length=384,
    padding="longest",
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
)`,highlighted:`inputs = tokenizer(
    question,
    long_context,
    stride=<span class="hljs-number">128</span>,
    max_length=<span class="hljs-number">384</span>,
    padding=<span class="hljs-string">&quot;longest&quot;</span>,
    truncation=<span class="hljs-string">&quot;only_second&quot;</span>,
    return_overflowing_tokens=<span class="hljs-literal">True</span>,
    return_offsets_mapping=<span class="hljs-literal">True</span>,
)`}});const Wi=[Ap,Np],Ot=[];function Ii(e,o){return e[0]==="pt"?0:1}oe=Ii(x),re=Ot[oe]=Wi[oe](x),qt=new j({props:{code:`outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
print(start_logits.shape, end_logits.shape)`,highlighted:`outputs = model(**inputs)

start_logits = outputs.start_logits
end_logits = outputs.end_logits
<span class="hljs-built_in">print</span>(start_logits.shape, end_logits.shape)`}});const Hi=[Lp,Dp],Ft=[];function Gi(e,o){return e[0]==="pt"?0:1}ie=Gi(x),le=Ft[ie]=Hi[ie](x);const Ji=[Fp,Op],Mt=[];function Qi(e,o){return e[0]==="pt"?0:1}pe=Qi(x),me=Mt[pe]=Ji[pe](x);const Ri=[Up,Mp],Ut=[];function Ki(e,o){return e[0]==="pt"?0:1}he=Ki(x),ce=Ut[he]=Ri[he](x);const Bi=[Ip,Wp],Wt=[];function Yi(e,o){return e[0]==="pt"?0:1}return ue=Yi(x),fe=Wt[ue]=Bi[ue](x),Et=new j({props:{code:"[(0, 18, 0.33867), (173, 184, 0.97149)]",highlighted:'[(<span class="hljs-number">0</span>, <span class="hljs-number">18</span>, <span class="hljs-number">0.33867</span>), (<span class="hljs-number">173</span>, <span class="hljs-number">184</span>, <span class="hljs-number">0.97149</span>)]'}}),Ue=new ro({props:{$$slots:{default:[Hp]},$$scope:{ctx:x}}}),Tt=new j({props:{code:`for candidate, offset in zip(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {"answer": answer, "start": start_char, "end": end_char, "score": score}
    print(result)`,highlighted:`<span class="hljs-keyword">for</span> candidate, offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(candidates, offsets):
    start_token, end_token, score = candidate
    start_char, _ = offset[start_token]
    _, end_char = offset[end_token]
    answer = long_context[start_char:end_char]
    result = {<span class="hljs-string">&quot;answer&quot;</span>: answer, <span class="hljs-string">&quot;start&quot;</span>: start_char, <span class="hljs-string">&quot;end&quot;</span>: end_char, <span class="hljs-string">&quot;score&quot;</span>: score}
    <span class="hljs-built_in">print</span>(result)`}}),Pt=new j({props:{code:`{'answer': '\\n\u{1F917} Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}`,highlighted:`{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;\\n\u{1F917} Transformers: State of the Art NLP&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">37</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.33867</span>}
{<span class="hljs-string">&#x27;answer&#x27;</span>: <span class="hljs-string">&#x27;Jax, PyTorch and TensorFlow&#x27;</span>, <span class="hljs-string">&#x27;start&#x27;</span>: <span class="hljs-number">1892</span>, <span class="hljs-string">&#x27;end&#x27;</span>: <span class="hljs-number">1919</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.97149</span>}`}}),Ie=new ro({props:{$$slots:{default:[Gp]},$$scope:{ctx:x}}}),{c(){n=m("meta"),p=u(),b(t.$$.fragment),w=u(),g=m("h1"),y=m("a"),E=m("span"),b(S.$$.fragment),z=u(),P=m("span"),L=r("Fast tokenizers in the QA pipeline"),D=u(),A.c(),q=u(),T=m("p"),xe=r("We will now dive into the "),Q=m("code"),ye=r("question-answering"),lo=r(" pipeline and see how to leverage the offsets to grab the answer to the question at hand from the context, a bit like we did for the grouped entities in the previous section. Then we will see how we can deal with very long contexts that end up being truncated. You can skip this section if you\u2019re not interested in the question answering task."),En=u(),K.c(),Jt=u(),je=m("h2"),Pe=m("a"),Ls=m("span"),b(Be.$$.fragment),po=u(),Ye=m("span"),mo=r("Using the "),Os=m("code"),ho=r("question-answering"),co=r(" pipeline"),Tn=u(),_e=m("p"),uo=r("As we saw in "),Qt=m("a"),fo=r("Chapter 1"),_o=r(", we can use the "),Fs=m("code"),wo=r("question-answering"),go=r(" pipeline like this to get the answer to a question:"),Pn=u(),b(Ve.$$.fragment),Sn=u(),b(Xe.$$.fragment),Cn=u(),Rt=m("p"),bo=r("Unlike the other pipelines, which can\u2019t truncate and split texts that are longer than the maximum length accepted by the model (and thus may miss information at the end of a document), this pipeline can deal with very long contexts and will return the answer to the question even if it\u2019s at the end:"),zn=u(),b(Ze.$$.fragment),Nn=u(),b(et.$$.fragment),An=u(),Kt=m("p"),ko=r("Let\u2019s see how it does all of this!"),Dn=u(),qe=m("h2"),Se=m("a"),Ms=m("span"),b(tt.$$.fragment),$o=u(),Us=m("span"),vo=r("Using a model for question answering"),Ln=u(),U=m("p"),xo=r("Like with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the "),Ws=m("code"),yo=r("question-answering"),jo=r(" pipeline is "),st=m("a"),Is=m("code"),qo=r("distilbert-base-cased-distilled-squad"),Eo=r(" (the \u201Csquad\u201D in the name comes from the dataset on which the model was fine-tuned; we\u2019ll talk more about the SQuAD dataset in "),Bt=m("a"),To=r("Chapter 7"),Po=r("):"),On=u(),Y.c(),Yt=u(),Vt=m("p"),So=r("Note that we tokenize the question and the context as a pair, with the question first."),Fn=u(),Ee=m("div"),nt=m("img"),Co=u(),at=m("img"),Mn=u(),Xt=m("p"),zo=r("Models for question answering work a little differently from the models we\u2019ve seen up to now. Using the picture above as an example, the model has been trained to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24). This is why those models don\u2019t return one tensor of logits but two: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer. Since in this case we have only one input containing 66 tokens, we get:"),Un=u(),b(ot.$$.fragment),Wn=u(),X.c(),Zt=u(),W=m("p"),No=r("To convert those logits into probabilities, we will apply a softmax function \u2014 but before that, we need to make sure we mask the indices that are not part of the context. Our input is "),Hs=m("code"),Ao=r("[CLS] question [SEP] context [SEP]"),Do=r(", so we need to mask the tokens of the question as well as the "),Gs=m("code"),Lo=r("[SEP]"),Oo=r(" token. We\u2019ll keep the "),Js=m("code"),Fo=r("[CLS]"),Mo=r(" token, however, as some models use it to indicate that the answer is not in the context."),In=u(),Ce=m("p"),Uo=r("Since we will apply a softmax afterward, we just need to replace the logits we want to mask with a large negative number. Here, we use "),Qs=m("code"),Wo=r("-10000"),Io=r(":"),Hn=u(),ee.c(),es=u(),ts=m("p"),Ho=r("Now that we have properly masked the logits corresponding to positions we don\u2019t want to predict, we can apply the softmax:"),Gn=u(),se.c(),ss=u(),F=m("p"),Go=r("At this stage, we could take the argmax of the start and end probabilities \u2014 but we might end up with a start index that is greater than the end index, so we need to take a few more precautions. We will compute the probabilities of each possible "),Rs=m("code"),Jo=r("start_index"),Qo=r(" and "),Ks=m("code"),Ro=r("end_index"),Ko=r(" where "),Bs=m("code"),Bo=r("start_index <= end_index"),Yo=r(", then take the tuple "),Ys=m("code"),Vo=r("(start_index, end_index)"),Xo=r(" with the highest probability."),Jn=u(),O=m("p"),Zo=r("Assuming the events \u201CThe answer starts at "),Vs=m("code"),er=r("start_index"),tr=r("\u201D and \u201CThe answer ends at "),Xs=m("code"),sr=r("end_index"),nr=r("\u201D to be independent, the probability that the answer starts at "),Zs=m("code"),ar=r("start_index"),or=r(" and ends at "),en=m("code"),rr=r("end_index"),ir=r(` is:
`),Qn=new ap,Rn=u(),we=m("p"),lr=r("So, to compute all the scores, we just need to compute all the products "),Kn=new ap,Bn=r(" where "),tn=m("code"),pr=r("start_index <= end_index"),mr=r("."),Yn=u(),ns=m("p"),hr=r("First let\u2019s compute all the possible products:"),Vn=u(),b(rt.$$.fragment),Xn=u(),ae.c(),as=u(),M=m("p"),cr=r("Now we just have to get the index of the maximum. Since PyTorch will return the index in the flattened tensor, we need to use the floor division "),sn=m("code"),ur=r("//"),fr=r(" and modulus "),nn=m("code"),dr=r("%"),_r=r(" operations to get the "),an=m("code"),wr=r("start_index"),gr=r(" and "),on=m("code"),br=r("end_index"),kr=r(":"),Zn=u(),b(it.$$.fragment),ea=u(),os=m("p"),$r=r("We\u2019re not quite done yet, but at least we already have the correct score for the answer (you can check this by comparing it to the first result in the previous section):"),ta=u(),b(lt.$$.fragment),sa=u(),b(ze.$$.fragment),na=u(),ge=m("p"),vr=r("We have the "),rn=m("code"),xr=r("start_index"),yr=r(" and "),ln=m("code"),jr=r("end_index"),qr=r(" of the answer in terms of tokens, so now we just need to convert to the character indices in the context. This is where the offsets will be super useful. We can grab them and use them like we did in the token classification task:"),aa=u(),b(pt.$$.fragment),oa=u(),rs=m("p"),Er=r("Now we just have to format everything to get our result:"),ra=u(),b(mt.$$.fragment),ia=u(),b(ht.$$.fragment),la=u(),is=m("p"),Tr=r("Great! That\u2019s the same as in our first example!"),pa=u(),b(Ne.$$.fragment),ma=u(),Te=m("h2"),Ae=m("a"),pn=m("span"),b(ct.$$.fragment),Pr=u(),mn=m("span"),Sr=r("Handling long contexts"),ha=u(),De=m("p"),Cr=r("If we try to tokenize the question and long context we used as an example previously, we\u2019ll get a number of tokens higher than the maximum length used in the "),hn=m("code"),zr=r("question-answering"),Nr=r(" pipeline (which is 384):"),ca=u(),b(ut.$$.fragment),ua=u(),b(ft.$$.fragment),fa=u(),Le=m("p"),Ar=r("So, we\u2019ll need to truncate our inputs at that maximum length. There are several ways we can do this, but we don\u2019t want to truncate the question, only the context. Since the context is the second sentence, we\u2019ll use the "),cn=m("code"),Dr=r('"only_second"'),Lr=r(" truncation strategy. The problem that arises then is that the answer to the question may not be in the truncated context. Here, for instance, we picked a question where the answer is toward the end of the context, and when we truncate it that answer is not present:"),da=u(),b(dt.$$.fragment),_a=u(),b(_t.$$.fragment),wa=u(),Oe=m("p"),Or=r("This means the model will have a hard time picking the correct answer. To fix this, the "),un=m("code"),Fr=r("question-answering"),Mr=r(" pipeline allows us to split the context into smaller chunks, specifying the maximum length. To make sure we don\u2019t split the context at exactly the wrong place to make it possible to find the answer, it also includes some overlap between the chunks."),ga=u(),be=m("p"),Ur=r("We can have the tokenizer (fast or slow) do this for us by adding "),fn=m("code"),Wr=r("return_overflowing_tokens=True"),Ir=r(", and we can specify the overlap we want with the "),dn=m("code"),Hr=r("stride"),Gr=r(" argument. Here is an example, using a smaller sentence:"),ba=u(),b(wt.$$.fragment),ka=u(),b(gt.$$.fragment),$a=u(),Fe=m("p"),Jr=r("As we can see, the sentence has been split into chunks in such a way that each entry in "),_n=m("code"),Qr=r('inputs["input_ids"]'),Rr=r(" has at most 6 tokens (we would need to add padding to have the last entry be the same size as the others) and there is an overlap of 2 tokens between each of the entries."),va=u(),ls=m("p"),Kr=r("Let\u2019s take a closer look at the result of the tokenization:"),xa=u(),b(bt.$$.fragment),ya=u(),b(kt.$$.fragment),ja=u(),Me=m("p"),Br=r("As expected, we get input IDs and an attention mask. The last key, "),wn=m("code"),Yr=r("overflow_to_sample_mapping"),Vr=r(", is a map that tells us which sentence each of the results corresponds to \u2014 here we have 7 results that all come from the (only) sentence we passed the tokenizer:"),qa=u(),b($t.$$.fragment),Ea=u(),b(vt.$$.fragment),Ta=u(),ps=m("p"),Xr=r("This is more useful when we tokenize several sentences together. For instance, this:"),Pa=u(),b(xt.$$.fragment),Sa=u(),ms=m("p"),Zr=r("gets us:"),Ca=u(),b(yt.$$.fragment),za=u(),hs=m("p"),ei=r("which means the first sentence is split into 7 chunks as before, and the next 4 chunks come from the second sentence."),Na=u(),I=m("p"),ti=r("Now let\u2019s go back to our long context. By default the "),gn=m("code"),si=r("question-answering"),ni=r(" pipeline uses a maximum length of 384, as we mentioned earlier, and a stride of 128, which correspond to the way the model was fine-tuned (you can adjust those parameters by passing "),bn=m("code"),ai=r("max_seq_len"),oi=r(" and "),kn=m("code"),ri=r("stride"),ii=r(" arguments when calling the pipeline). We will thus use those parameters when tokenizing. We\u2019ll also add padding (to have samples of the same length, so we can build tensors) as well as ask for the offsets:"),Aa=u(),b(jt.$$.fragment),Da=u(),H=m("p"),li=r("Those "),$n=m("code"),pi=r("inputs"),mi=r(" will contain the input IDs and attention masks the model expects, as well as the offsets and the "),vn=m("code"),hi=r("overflow_to_sample_mapping"),ci=r(" we just talked about. Since those two are not parameters used by the model, we\u2019ll pop them out of the "),xn=m("code"),ui=r("inputs"),fi=r(" (and we won\u2019t store the map, since it\u2019s not useful here) before converting it to a tensor:"),La=u(),re.c(),cs=u(),us=m("p"),di=r("Our long context was split in two, which means that after it goes through our model, we will have two sets of start and end logits:"),Oa=u(),b(qt.$$.fragment),Fa=u(),le.c(),fs=u(),ds=m("p"),_i=r("Like before, we first mask the tokens that are not part of the context before taking the softmax. We also mask all the padding tokens (as flagged by the attention mask):"),Ma=u(),me.c(),_s=u(),ws=m("p"),wi=r("Then we can use the softmax to convert our logits to probabilities:"),Ua=u(),ce.c(),gs=u(),bs=m("p"),gi=r("The next step is similar to what we did for the small context, but we repeat it for each of our two chunks. We attribute a score to all possible spans of answer, then take the span with the best score:"),Wa=u(),fe.c(),ks=u(),b(Et.$$.fragment),Ia=u(),$s=m("p"),bi=r("Those two candidates correspond to the best answers the model was able to find in each chunk. The model is way more confident the right answer is in the second part (which is a good sign!). Now we just have to map those two token spans to spans of characters in the context (we only need to map the second one to have our answer, but it\u2019s interesting to see what the model has picked in the first chunk)."),Ha=u(),b(Ue.$$.fragment),Ga=u(),We=m("p"),ki=r("The "),yn=m("code"),$i=r("offsets"),vi=r(" we grabbed earlier is actually a list of offsets, with one list per chunk of text:"),Ja=u(),b(Tt.$$.fragment),Qa=u(),b(Pt.$$.fragment),Ra=u(),vs=m("p"),xi=r("If we ignore the first result, we get the same result as our pipeline for this long context \u2014 yay!"),Ka=u(),b(Ie.$$.fragment),Ba=u(),xs=m("p"),yi=r("This concludes our deep dive into the tokenizer\u2019s capabilities. We will put all of this in practice again in the next chapter, when we show you how to fine-tune a model on a range of common NLP tasks."),this.h()},l(e){const o=fp('[data-svelte="svelte-1phssyn"]',document.head);n=h(o,"META",{name:!0,content:!0}),o.forEach(s),p=f(e),k(t.$$.fragment,e),w=f(e),g=h(e,"H1",{class:!0});var It=c(g);y=h(It,"A",{id:!0,class:!0,href:!0});var ys=c(y);E=h(ys,"SPAN",{});var js=c(E);k(S.$$.fragment,js),js.forEach(s),ys.forEach(s),z=f(It),P=h(It,"SPAN",{});var qs=c(P);L=i(qs,"Fast tokenizers in the QA pipeline"),qs.forEach(s),It.forEach(s),D=f(e),A.l(e),q=f(e),T=h(e,"P",{});var He=c(T);xe=i(He,"We will now dive into the "),Q=h(He,"CODE",{});var Es=c(Q);ye=i(Es,"question-answering"),Es.forEach(s),lo=i(He," pipeline and see how to leverage the offsets to grab the answer to the question at hand from the context, a bit like we did for the grouped entities in the previous section. Then we will see how we can deal with very long contexts that end up being truncated. You can skip this section if you\u2019re not interested in the question answering task."),He.forEach(s),En=f(e),K.l(e),Jt=f(e),je=h(e,"H2",{class:!0});var Ge=c(je);Pe=h(Ge,"A",{id:!0,class:!0,href:!0});var Ts=c(Pe);Ls=h(Ts,"SPAN",{});var jn=c(Ls);k(Be.$$.fragment,jn),jn.forEach(s),Ts.forEach(s),po=f(Ge),Ye=h(Ge,"SPAN",{});var Ht=c(Ye);mo=i(Ht,"Using the "),Os=h(Ht,"CODE",{});var Ps=c(Os);ho=i(Ps,"question-answering"),Ps.forEach(s),co=i(Ht," pipeline"),Ht.forEach(s),Ge.forEach(s),Tn=f(e),_e=h(e,"P",{});var ke=c(_e);uo=i(ke,"As we saw in "),Qt=h(ke,"A",{href:!0});var Ss=c(Qt);fo=i(Ss,"Chapter 1"),Ss.forEach(s),_o=i(ke,", we can use the "),Fs=h(ke,"CODE",{});var Cs=c(Fs);wo=i(Cs,"question-answering"),Cs.forEach(s),go=i(ke," pipeline like this to get the answer to a question:"),ke.forEach(s),Pn=f(e),k(Ve.$$.fragment,e),Sn=f(e),k(Xe.$$.fragment,e),Cn=f(e),Rt=h(e,"P",{});var zs=c(Rt);bo=i(zs,"Unlike the other pipelines, which can\u2019t truncate and split texts that are longer than the maximum length accepted by the model (and thus may miss information at the end of a document), this pipeline can deal with very long contexts and will return the answer to the question even if it\u2019s at the end:"),zs.forEach(s),zn=f(e),k(Ze.$$.fragment,e),Nn=f(e),k(et.$$.fragment,e),An=f(e),Kt=h(e,"P",{});var qn=c(Kt);ko=i(qn,"Let\u2019s see how it does all of this!"),qn.forEach(s),Dn=f(e),qe=h(e,"H2",{class:!0});var Gt=c(qe);Se=h(Gt,"A",{id:!0,class:!0,href:!0});var Vi=c(Se);Ms=h(Vi,"SPAN",{});var Xi=c(Ms);k(tt.$$.fragment,Xi),Xi.forEach(s),Vi.forEach(s),$o=f(Gt),Us=h(Gt,"SPAN",{});var Zi=c(Us);vo=i(Zi,"Using a model for question answering"),Zi.forEach(s),Gt.forEach(s),Ln=f(e),U=h(e,"P",{});var Je=c(U);xo=i(Je,"Like with any other pipeline, we start by tokenizing our input and then send it through the model. The checkpoint used by default for the "),Ws=h(Je,"CODE",{});var el=c(Ws);yo=i(el,"question-answering"),el.forEach(s),jo=i(Je," pipeline is "),st=h(Je,"A",{href:!0,rel:!0});var tl=c(st);Is=h(tl,"CODE",{});var sl=c(Is);qo=i(sl,"distilbert-base-cased-distilled-squad"),sl.forEach(s),tl.forEach(s),Eo=i(Je," (the \u201Csquad\u201D in the name comes from the dataset on which the model was fine-tuned; we\u2019ll talk more about the SQuAD dataset in "),Bt=h(Je,"A",{href:!0});var nl=c(Bt);To=i(nl,"Chapter 7"),nl.forEach(s),Po=i(Je,"):"),Je.forEach(s),On=f(e),Y.l(e),Yt=f(e),Vt=h(e,"P",{});var al=c(Vt);So=i(al,"Note that we tokenize the question and the context as a pair, with the question first."),al.forEach(s),Fn=f(e),Ee=h(e,"DIV",{class:!0});var Va=c(Ee);nt=h(Va,"IMG",{class:!0,src:!0,alt:!0}),Co=f(Va),at=h(Va,"IMG",{class:!0,src:!0,alt:!0}),Va.forEach(s),Mn=f(e),Xt=h(e,"P",{});var ol=c(Xt);zo=i(ol,"Models for question answering work a little differently from the models we\u2019ve seen up to now. Using the picture above as an example, the model has been trained to predict the index of the token starting the answer (here 21) and the index of the token where the answer ends (here 24). This is why those models don\u2019t return one tensor of logits but two: one for the logits corresponding to the start token of the answer, and one for the logits corresponding to the end token of the answer. Since in this case we have only one input containing 66 tokens, we get:"),ol.forEach(s),Un=f(e),k(ot.$$.fragment,e),Wn=f(e),X.l(e),Zt=f(e),W=h(e,"P",{});var Qe=c(W);No=i(Qe,"To convert those logits into probabilities, we will apply a softmax function \u2014 but before that, we need to make sure we mask the indices that are not part of the context. Our input is "),Hs=h(Qe,"CODE",{});var rl=c(Hs);Ao=i(rl,"[CLS] question [SEP] context [SEP]"),rl.forEach(s),Do=i(Qe,", so we need to mask the tokens of the question as well as the "),Gs=h(Qe,"CODE",{});var il=c(Gs);Lo=i(il,"[SEP]"),il.forEach(s),Oo=i(Qe," token. We\u2019ll keep the "),Js=h(Qe,"CODE",{});var ll=c(Js);Fo=i(ll,"[CLS]"),ll.forEach(s),Mo=i(Qe," token, however, as some models use it to indicate that the answer is not in the context."),Qe.forEach(s),In=f(e),Ce=h(e,"P",{});var Xa=c(Ce);Uo=i(Xa,"Since we will apply a softmax afterward, we just need to replace the logits we want to mask with a large negative number. Here, we use "),Qs=h(Xa,"CODE",{});var pl=c(Qs);Wo=i(pl,"-10000"),pl.forEach(s),Io=i(Xa,":"),Xa.forEach(s),Hn=f(e),ee.l(e),es=f(e),ts=h(e,"P",{});var ml=c(ts);Ho=i(ml,"Now that we have properly masked the logits corresponding to positions we don\u2019t want to predict, we can apply the softmax:"),ml.forEach(s),Gn=f(e),se.l(e),ss=f(e),F=h(e,"P",{});var $e=c(F);Go=i($e,"At this stage, we could take the argmax of the start and end probabilities \u2014 but we might end up with a start index that is greater than the end index, so we need to take a few more precautions. We will compute the probabilities of each possible "),Rs=h($e,"CODE",{});var hl=c(Rs);Jo=i(hl,"start_index"),hl.forEach(s),Qo=i($e," and "),Ks=h($e,"CODE",{});var cl=c(Ks);Ro=i(cl,"end_index"),cl.forEach(s),Ko=i($e," where "),Bs=h($e,"CODE",{});var ul=c(Bs);Bo=i(ul,"start_index <= end_index"),ul.forEach(s),Yo=i($e,", then take the tuple "),Ys=h($e,"CODE",{});var fl=c(Ys);Vo=i(fl,"(start_index, end_index)"),fl.forEach(s),Xo=i($e," with the highest probability."),$e.forEach(s),Jn=f(e),O=h(e,"P",{});var de=c(O);Zo=i(de,"Assuming the events \u201CThe answer starts at "),Vs=h(de,"CODE",{});var dl=c(Vs);er=i(dl,"start_index"),dl.forEach(s),tr=i(de,"\u201D and \u201CThe answer ends at "),Xs=h(de,"CODE",{});var _l=c(Xs);sr=i(_l,"end_index"),_l.forEach(s),nr=i(de,"\u201D to be independent, the probability that the answer starts at "),Zs=h(de,"CODE",{});var wl=c(Zs);ar=i(wl,"start_index"),wl.forEach(s),or=i(de," and ends at "),en=h(de,"CODE",{});var gl=c(en);rr=i(gl,"end_index"),gl.forEach(s),ir=i(de,` is:
`),Qn=op(de),de.forEach(s),Rn=f(e),we=h(e,"P",{});var Ns=c(we);lr=i(Ns,"So, to compute all the scores, we just need to compute all the products "),Kn=op(Ns),Bn=i(Ns," where "),tn=h(Ns,"CODE",{});var bl=c(tn);pr=i(bl,"start_index <= end_index"),bl.forEach(s),mr=i(Ns,"."),Ns.forEach(s),Yn=f(e),ns=h(e,"P",{});var kl=c(ns);hr=i(kl,"First let\u2019s compute all the possible products:"),kl.forEach(s),Vn=f(e),k(rt.$$.fragment,e),Xn=f(e),ae.l(e),as=f(e),M=h(e,"P",{});var ve=c(M);cr=i(ve,"Now we just have to get the index of the maximum. Since PyTorch will return the index in the flattened tensor, we need to use the floor division "),sn=h(ve,"CODE",{});var $l=c(sn);ur=i($l,"//"),$l.forEach(s),fr=i(ve," and modulus "),nn=h(ve,"CODE",{});var vl=c(nn);dr=i(vl,"%"),vl.forEach(s),_r=i(ve," operations to get the "),an=h(ve,"CODE",{});var xl=c(an);wr=i(xl,"start_index"),xl.forEach(s),gr=i(ve," and "),on=h(ve,"CODE",{});var yl=c(on);br=i(yl,"end_index"),yl.forEach(s),kr=i(ve,":"),ve.forEach(s),Zn=f(e),k(it.$$.fragment,e),ea=f(e),os=h(e,"P",{});var jl=c(os);$r=i(jl,"We\u2019re not quite done yet, but at least we already have the correct score for the answer (you can check this by comparing it to the first result in the previous section):"),jl.forEach(s),ta=f(e),k(lt.$$.fragment,e),sa=f(e),k(ze.$$.fragment,e),na=f(e),ge=h(e,"P",{});var As=c(ge);vr=i(As,"We have the "),rn=h(As,"CODE",{});var ql=c(rn);xr=i(ql,"start_index"),ql.forEach(s),yr=i(As," and "),ln=h(As,"CODE",{});var El=c(ln);jr=i(El,"end_index"),El.forEach(s),qr=i(As," of the answer in terms of tokens, so now we just need to convert to the character indices in the context. This is where the offsets will be super useful. We can grab them and use them like we did in the token classification task:"),As.forEach(s),aa=f(e),k(pt.$$.fragment,e),oa=f(e),rs=h(e,"P",{});var Tl=c(rs);Er=i(Tl,"Now we just have to format everything to get our result:"),Tl.forEach(s),ra=f(e),k(mt.$$.fragment,e),ia=f(e),k(ht.$$.fragment,e),la=f(e),is=h(e,"P",{});var Pl=c(is);Tr=i(Pl,"Great! That\u2019s the same as in our first example!"),Pl.forEach(s),pa=f(e),k(Ne.$$.fragment,e),ma=f(e),Te=h(e,"H2",{class:!0});var Za=c(Te);Ae=h(Za,"A",{id:!0,class:!0,href:!0});var Sl=c(Ae);pn=h(Sl,"SPAN",{});var Cl=c(pn);k(ct.$$.fragment,Cl),Cl.forEach(s),Sl.forEach(s),Pr=f(Za),mn=h(Za,"SPAN",{});var zl=c(mn);Sr=i(zl,"Handling long contexts"),zl.forEach(s),Za.forEach(s),ha=f(e),De=h(e,"P",{});var eo=c(De);Cr=i(eo,"If we try to tokenize the question and long context we used as an example previously, we\u2019ll get a number of tokens higher than the maximum length used in the "),hn=h(eo,"CODE",{});var Nl=c(hn);zr=i(Nl,"question-answering"),Nl.forEach(s),Nr=i(eo," pipeline (which is 384):"),eo.forEach(s),ca=f(e),k(ut.$$.fragment,e),ua=f(e),k(ft.$$.fragment,e),fa=f(e),Le=h(e,"P",{});var to=c(Le);Ar=i(to,"So, we\u2019ll need to truncate our inputs at that maximum length. There are several ways we can do this, but we don\u2019t want to truncate the question, only the context. Since the context is the second sentence, we\u2019ll use the "),cn=h(to,"CODE",{});var Al=c(cn);Dr=i(Al,'"only_second"'),Al.forEach(s),Lr=i(to," truncation strategy. The problem that arises then is that the answer to the question may not be in the truncated context. Here, for instance, we picked a question where the answer is toward the end of the context, and when we truncate it that answer is not present:"),to.forEach(s),da=f(e),k(dt.$$.fragment,e),_a=f(e),k(_t.$$.fragment,e),wa=f(e),Oe=h(e,"P",{});var so=c(Oe);Or=i(so,"This means the model will have a hard time picking the correct answer. To fix this, the "),un=h(so,"CODE",{});var Dl=c(un);Fr=i(Dl,"question-answering"),Dl.forEach(s),Mr=i(so," pipeline allows us to split the context into smaller chunks, specifying the maximum length. To make sure we don\u2019t split the context at exactly the wrong place to make it possible to find the answer, it also includes some overlap between the chunks."),so.forEach(s),ga=f(e),be=h(e,"P",{});var Ds=c(be);Ur=i(Ds,"We can have the tokenizer (fast or slow) do this for us by adding "),fn=h(Ds,"CODE",{});var Ll=c(fn);Wr=i(Ll,"return_overflowing_tokens=True"),Ll.forEach(s),Ir=i(Ds,", and we can specify the overlap we want with the "),dn=h(Ds,"CODE",{});var Ol=c(dn);Hr=i(Ol,"stride"),Ol.forEach(s),Gr=i(Ds," argument. Here is an example, using a smaller sentence:"),Ds.forEach(s),ba=f(e),k(wt.$$.fragment,e),ka=f(e),k(gt.$$.fragment,e),$a=f(e),Fe=h(e,"P",{});var no=c(Fe);Jr=i(no,"As we can see, the sentence has been split into chunks in such a way that each entry in "),_n=h(no,"CODE",{});var Fl=c(_n);Qr=i(Fl,'inputs["input_ids"]'),Fl.forEach(s),Rr=i(no," has at most 6 tokens (we would need to add padding to have the last entry be the same size as the others) and there is an overlap of 2 tokens between each of the entries."),no.forEach(s),va=f(e),ls=h(e,"P",{});var Ml=c(ls);Kr=i(Ml,"Let\u2019s take a closer look at the result of the tokenization:"),Ml.forEach(s),xa=f(e),k(bt.$$.fragment,e),ya=f(e),k(kt.$$.fragment,e),ja=f(e),Me=h(e,"P",{});var ao=c(Me);Br=i(ao,"As expected, we get input IDs and an attention mask. The last key, "),wn=h(ao,"CODE",{});var Ul=c(wn);Yr=i(Ul,"overflow_to_sample_mapping"),Ul.forEach(s),Vr=i(ao,", is a map that tells us which sentence each of the results corresponds to \u2014 here we have 7 results that all come from the (only) sentence we passed the tokenizer:"),ao.forEach(s),qa=f(e),k($t.$$.fragment,e),Ea=f(e),k(vt.$$.fragment,e),Ta=f(e),ps=h(e,"P",{});var Wl=c(ps);Xr=i(Wl,"This is more useful when we tokenize several sentences together. For instance, this:"),Wl.forEach(s),Pa=f(e),k(xt.$$.fragment,e),Sa=f(e),ms=h(e,"P",{});var Il=c(ms);Zr=i(Il,"gets us:"),Il.forEach(s),Ca=f(e),k(yt.$$.fragment,e),za=f(e),hs=h(e,"P",{});var Hl=c(hs);ei=i(Hl,"which means the first sentence is split into 7 chunks as before, and the next 4 chunks come from the second sentence."),Hl.forEach(s),Na=f(e),I=h(e,"P",{});var Re=c(I);ti=i(Re,"Now let\u2019s go back to our long context. By default the "),gn=h(Re,"CODE",{});var Gl=c(gn);si=i(Gl,"question-answering"),Gl.forEach(s),ni=i(Re," pipeline uses a maximum length of 384, as we mentioned earlier, and a stride of 128, which correspond to the way the model was fine-tuned (you can adjust those parameters by passing "),bn=h(Re,"CODE",{});var Jl=c(bn);ai=i(Jl,"max_seq_len"),Jl.forEach(s),oi=i(Re," and "),kn=h(Re,"CODE",{});var Ql=c(kn);ri=i(Ql,"stride"),Ql.forEach(s),ii=i(Re," arguments when calling the pipeline). We will thus use those parameters when tokenizing. We\u2019ll also add padding (to have samples of the same length, so we can build tensors) as well as ask for the offsets:"),Re.forEach(s),Aa=f(e),k(jt.$$.fragment,e),Da=f(e),H=h(e,"P",{});var Ke=c(H);li=i(Ke,"Those "),$n=h(Ke,"CODE",{});var Rl=c($n);pi=i(Rl,"inputs"),Rl.forEach(s),mi=i(Ke," will contain the input IDs and attention masks the model expects, as well as the offsets and the "),vn=h(Ke,"CODE",{});var Kl=c(vn);hi=i(Kl,"overflow_to_sample_mapping"),Kl.forEach(s),ci=i(Ke," we just talked about. Since those two are not parameters used by the model, we\u2019ll pop them out of the "),xn=h(Ke,"CODE",{});var Bl=c(xn);ui=i(Bl,"inputs"),Bl.forEach(s),fi=i(Ke," (and we won\u2019t store the map, since it\u2019s not useful here) before converting it to a tensor:"),Ke.forEach(s),La=f(e),re.l(e),cs=f(e),us=h(e,"P",{});var Yl=c(us);di=i(Yl,"Our long context was split in two, which means that after it goes through our model, we will have two sets of start and end logits:"),Yl.forEach(s),Oa=f(e),k(qt.$$.fragment,e),Fa=f(e),le.l(e),fs=f(e),ds=h(e,"P",{});var Vl=c(ds);_i=i(Vl,"Like before, we first mask the tokens that are not part of the context before taking the softmax. We also mask all the padding tokens (as flagged by the attention mask):"),Vl.forEach(s),Ma=f(e),me.l(e),_s=f(e),ws=h(e,"P",{});var Xl=c(ws);wi=i(Xl,"Then we can use the softmax to convert our logits to probabilities:"),Xl.forEach(s),Ua=f(e),ce.l(e),gs=f(e),bs=h(e,"P",{});var Zl=c(bs);gi=i(Zl,"The next step is similar to what we did for the small context, but we repeat it for each of our two chunks. We attribute a score to all possible spans of answer, then take the span with the best score:"),Zl.forEach(s),Wa=f(e),fe.l(e),ks=f(e),k(Et.$$.fragment,e),Ia=f(e),$s=h(e,"P",{});var ep=c($s);bi=i(ep,"Those two candidates correspond to the best answers the model was able to find in each chunk. The model is way more confident the right answer is in the second part (which is a good sign!). Now we just have to map those two token spans to spans of characters in the context (we only need to map the second one to have our answer, but it\u2019s interesting to see what the model has picked in the first chunk)."),ep.forEach(s),Ha=f(e),k(Ue.$$.fragment,e),Ga=f(e),We=h(e,"P",{});var oo=c(We);ki=i(oo,"The "),yn=h(oo,"CODE",{});var tp=c(yn);$i=i(tp,"offsets"),tp.forEach(s),vi=i(oo," we grabbed earlier is actually a list of offsets, with one list per chunk of text:"),oo.forEach(s),Ja=f(e),k(Tt.$$.fragment,e),Qa=f(e),k(Pt.$$.fragment,e),Ra=f(e),vs=h(e,"P",{});var sp=c(vs);xi=i(sp,"If we ignore the first result, we get the same result as our pipeline for this long context \u2014 yay!"),sp.forEach(s),Ka=f(e),k(Ie.$$.fragment,e),Ba=f(e),xs=h(e,"P",{});var np=c(xs);yi=i(np,"This concludes our deep dive into the tokenizer\u2019s capabilities. We will put all of this in practice again in the next chapter, when we show you how to fine-tune a model on a range of common NLP tasks."),np.forEach(s),this.h()},h(){C(n,"name","hf:doc:metadata"),C(n,"content",JSON.stringify(Qp)),C(y,"id","fast-tokenizers-in-the-qa-pipeline"),C(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),C(y,"href","#fast-tokenizers-in-the-qa-pipeline"),C(g,"class","relative group"),C(Pe,"id","using-the-questionanswering-pipeline"),C(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),C(Pe,"href","#using-the-questionanswering-pipeline"),C(je,"class","relative group"),C(Qt,"href","/course/chapter1"),C(Se,"id","using-a-model-for-question-answering"),C(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),C(Se,"href","#using-a-model-for-question-answering"),C(qe,"class","relative group"),C(st,"href","https://huggingface.co/distilbert-base-cased-distilled-squad"),C(st,"rel","nofollow"),C(Bt,"href","/course/chapter7/7"),C(nt,"class","block dark:hidden"),rp(nt.src,ji="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens.svg")||C(nt,"src",ji),C(nt,"alt","An example of tokenization of question and context"),C(at,"class","hidden dark:block"),rp(at.src,qi="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter6/question_tokens-dark.svg")||C(at,"src",qi),C(at,"alt","An example of tokenization of question and context"),C(Ee,"class","flex justify-center"),Qn.a=null,Kn.a=Bn,C(Ae,"id","handling-long-contexts"),C(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),C(Ae,"href","#handling-long-contexts"),C(Te,"class","relative group")},m(e,o){a(document.head,n),l(e,p,o),$(t,e,o),l(e,w,o),l(e,g,o),a(g,y),a(y,E),$(S,E,null),a(g,z),a(g,P),a(P,L),l(e,D,o),St[N].m(e,o),l(e,q,o),l(e,T,o),a(T,xe),a(T,Q),a(Q,ye),a(T,lo),l(e,En,o),Ct[R].m(e,o),l(e,Jt,o),l(e,je,o),a(je,Pe),a(Pe,Ls),$(Be,Ls,null),a(je,po),a(je,Ye),a(Ye,mo),a(Ye,Os),a(Os,ho),a(Ye,co),l(e,Tn,o),l(e,_e,o),a(_e,uo),a(_e,Qt),a(Qt,fo),a(_e,_o),a(_e,Fs),a(Fs,wo),a(_e,go),l(e,Pn,o),$(Ve,e,o),l(e,Sn,o),$(Xe,e,o),l(e,Cn,o),l(e,Rt,o),a(Rt,bo),l(e,zn,o),$(Ze,e,o),l(e,Nn,o),$(et,e,o),l(e,An,o),l(e,Kt,o),a(Kt,ko),l(e,Dn,o),l(e,qe,o),a(qe,Se),a(Se,Ms),$(tt,Ms,null),a(qe,$o),a(qe,Us),a(Us,vo),l(e,Ln,o),l(e,U,o),a(U,xo),a(U,Ws),a(Ws,yo),a(U,jo),a(U,st),a(st,Is),a(Is,qo),a(U,Eo),a(U,Bt),a(Bt,To),a(U,Po),l(e,On,o),zt[B].m(e,o),l(e,Yt,o),l(e,Vt,o),a(Vt,So),l(e,Fn,o),l(e,Ee,o),a(Ee,nt),a(Ee,Co),a(Ee,at),l(e,Mn,o),l(e,Xt,o),a(Xt,zo),l(e,Un,o),$(ot,e,o),l(e,Wn,o),Nt[V].m(e,o),l(e,Zt,o),l(e,W,o),a(W,No),a(W,Hs),a(Hs,Ao),a(W,Do),a(W,Gs),a(Gs,Lo),a(W,Oo),a(W,Js),a(Js,Fo),a(W,Mo),l(e,In,o),l(e,Ce,o),a(Ce,Uo),a(Ce,Qs),a(Qs,Wo),a(Ce,Io),l(e,Hn,o),At[Z].m(e,o),l(e,es,o),l(e,ts,o),a(ts,Ho),l(e,Gn,o),Dt[te].m(e,o),l(e,ss,o),l(e,F,o),a(F,Go),a(F,Rs),a(Rs,Jo),a(F,Qo),a(F,Ks),a(Ks,Ro),a(F,Ko),a(F,Bs),a(Bs,Bo),a(F,Yo),a(F,Ys),a(Ys,Vo),a(F,Xo),l(e,Jn,o),l(e,O,o),a(O,Zo),a(O,Vs),a(Vs,er),a(O,tr),a(O,Xs),a(Xs,sr),a(O,nr),a(O,Zs),a(Zs,ar),a(O,or),a(O,en),a(en,rr),a(O,ir),Qn.m(pp,O),l(e,Rn,o),l(e,we,o),a(we,lr),Kn.m(mp,we),a(we,Bn),a(we,tn),a(tn,pr),a(we,mr),l(e,Yn,o),l(e,ns,o),a(ns,hr),l(e,Vn,o),$(rt,e,o),l(e,Xn,o),Lt[ne].m(e,o),l(e,as,o),l(e,M,o),a(M,cr),a(M,sn),a(sn,ur),a(M,fr),a(M,nn),a(nn,dr),a(M,_r),a(M,an),a(an,wr),a(M,gr),a(M,on),a(on,br),a(M,kr),l(e,Zn,o),$(it,e,o),l(e,ea,o),l(e,os,o),a(os,$r),l(e,ta,o),$(lt,e,o),l(e,sa,o),$(ze,e,o),l(e,na,o),l(e,ge,o),a(ge,vr),a(ge,rn),a(rn,xr),a(ge,yr),a(ge,ln),a(ln,jr),a(ge,qr),l(e,aa,o),$(pt,e,o),l(e,oa,o),l(e,rs,o),a(rs,Er),l(e,ra,o),$(mt,e,o),l(e,ia,o),$(ht,e,o),l(e,la,o),l(e,is,o),a(is,Tr),l(e,pa,o),$(Ne,e,o),l(e,ma,o),l(e,Te,o),a(Te,Ae),a(Ae,pn),$(ct,pn,null),a(Te,Pr),a(Te,mn),a(mn,Sr),l(e,ha,o),l(e,De,o),a(De,Cr),a(De,hn),a(hn,zr),a(De,Nr),l(e,ca,o),$(ut,e,o),l(e,ua,o),$(ft,e,o),l(e,fa,o),l(e,Le,o),a(Le,Ar),a(Le,cn),a(cn,Dr),a(Le,Lr),l(e,da,o),$(dt,e,o),l(e,_a,o),$(_t,e,o),l(e,wa,o),l(e,Oe,o),a(Oe,Or),a(Oe,un),a(un,Fr),a(Oe,Mr),l(e,ga,o),l(e,be,o),a(be,Ur),a(be,fn),a(fn,Wr),a(be,Ir),a(be,dn),a(dn,Hr),a(be,Gr),l(e,ba,o),$(wt,e,o),l(e,ka,o),$(gt,e,o),l(e,$a,o),l(e,Fe,o),a(Fe,Jr),a(Fe,_n),a(_n,Qr),a(Fe,Rr),l(e,va,o),l(e,ls,o),a(ls,Kr),l(e,xa,o),$(bt,e,o),l(e,ya,o),$(kt,e,o),l(e,ja,o),l(e,Me,o),a(Me,Br),a(Me,wn),a(wn,Yr),a(Me,Vr),l(e,qa,o),$($t,e,o),l(e,Ea,o),$(vt,e,o),l(e,Ta,o),l(e,ps,o),a(ps,Xr),l(e,Pa,o),$(xt,e,o),l(e,Sa,o),l(e,ms,o),a(ms,Zr),l(e,Ca,o),$(yt,e,o),l(e,za,o),l(e,hs,o),a(hs,ei),l(e,Na,o),l(e,I,o),a(I,ti),a(I,gn),a(gn,si),a(I,ni),a(I,bn),a(bn,ai),a(I,oi),a(I,kn),a(kn,ri),a(I,ii),l(e,Aa,o),$(jt,e,o),l(e,Da,o),l(e,H,o),a(H,li),a(H,$n),a($n,pi),a(H,mi),a(H,vn),a(vn,hi),a(H,ci),a(H,xn),a(xn,ui),a(H,fi),l(e,La,o),Ot[oe].m(e,o),l(e,cs,o),l(e,us,o),a(us,di),l(e,Oa,o),$(qt,e,o),l(e,Fa,o),Ft[ie].m(e,o),l(e,fs,o),l(e,ds,o),a(ds,_i),l(e,Ma,o),Mt[pe].m(e,o),l(e,_s,o),l(e,ws,o),a(ws,wi),l(e,Ua,o),Ut[he].m(e,o),l(e,gs,o),l(e,bs,o),a(bs,gi),l(e,Wa,o),Wt[ue].m(e,o),l(e,ks,o),$(Et,e,o),l(e,Ia,o),l(e,$s,o),a($s,bi),l(e,Ha,o),$(Ue,e,o),l(e,Ga,o),l(e,We,o),a(We,ki),a(We,yn),a(yn,$i),a(We,vi),l(e,Ja,o),$(Tt,e,o),l(e,Qa,o),$(Pt,e,o),l(e,Ra,o),l(e,vs,o),a(vs,xi),l(e,Ka,o),$(Ie,e,o),l(e,Ba,o),l(e,xs,o),a(xs,yi),Ya=!0},p(e,[o]){const It={};o&1&&(It.fw=e[0]),t.$set(It);let ys=N;N=Ti(e),N!==ys&&(J(),d(St[ys],1,1,()=>{St[ys]=null}),G(),A=St[N],A||(A=St[N]=Ei[N](e),A.c()),_(A,1),A.m(q.parentNode,q));let js=R;R=Si(e),R!==js&&(J(),d(Ct[js],1,1,()=>{Ct[js]=null}),G(),K=Ct[R],K||(K=Ct[R]=Pi[R](e),K.c()),_(K,1),K.m(Jt.parentNode,Jt));let qs=B;B=zi(e),B!==qs&&(J(),d(zt[qs],1,1,()=>{zt[qs]=null}),G(),Y=zt[B],Y||(Y=zt[B]=Ci[B](e),Y.c()),_(Y,1),Y.m(Yt.parentNode,Yt));let He=V;V=Ai(e),V!==He&&(J(),d(Nt[He],1,1,()=>{Nt[He]=null}),G(),X=Nt[V],X||(X=Nt[V]=Ni[V](e),X.c()),_(X,1),X.m(Zt.parentNode,Zt));let Es=Z;Z=Li(e),Z!==Es&&(J(),d(At[Es],1,1,()=>{At[Es]=null}),G(),ee=At[Z],ee||(ee=At[Z]=Di[Z](e),ee.c()),_(ee,1),ee.m(es.parentNode,es));let Ge=te;te=Fi(e),te!==Ge&&(J(),d(Dt[Ge],1,1,()=>{Dt[Ge]=null}),G(),se=Dt[te],se||(se=Dt[te]=Oi[te](e),se.c()),_(se,1),se.m(ss.parentNode,ss));let Ts=ne;ne=Ui(e),ne!==Ts&&(J(),d(Lt[Ts],1,1,()=>{Lt[Ts]=null}),G(),ae=Lt[ne],ae||(ae=Lt[ne]=Mi[ne](e),ae.c()),_(ae,1),ae.m(as.parentNode,as));const jn={};o&2&&(jn.$$scope={dirty:o,ctx:e}),ze.$set(jn);const Ht={};o&2&&(Ht.$$scope={dirty:o,ctx:e}),Ne.$set(Ht);let Ps=oe;oe=Ii(e),oe!==Ps&&(J(),d(Ot[Ps],1,1,()=>{Ot[Ps]=null}),G(),re=Ot[oe],re||(re=Ot[oe]=Wi[oe](e),re.c()),_(re,1),re.m(cs.parentNode,cs));let ke=ie;ie=Gi(e),ie!==ke&&(J(),d(Ft[ke],1,1,()=>{Ft[ke]=null}),G(),le=Ft[ie],le||(le=Ft[ie]=Hi[ie](e),le.c()),_(le,1),le.m(fs.parentNode,fs));let Ss=pe;pe=Qi(e),pe!==Ss&&(J(),d(Mt[Ss],1,1,()=>{Mt[Ss]=null}),G(),me=Mt[pe],me||(me=Mt[pe]=Ji[pe](e),me.c()),_(me,1),me.m(_s.parentNode,_s));let Cs=he;he=Ki(e),he!==Cs&&(J(),d(Ut[Cs],1,1,()=>{Ut[Cs]=null}),G(),ce=Ut[he],ce||(ce=Ut[he]=Ri[he](e),ce.c()),_(ce,1),ce.m(gs.parentNode,gs));let zs=ue;ue=Yi(e),ue!==zs&&(J(),d(Wt[zs],1,1,()=>{Wt[zs]=null}),G(),fe=Wt[ue],fe||(fe=Wt[ue]=Bi[ue](e),fe.c()),_(fe,1),fe.m(ks.parentNode,ks));const qn={};o&2&&(qn.$$scope={dirty:o,ctx:e}),Ue.$set(qn);const Gt={};o&2&&(Gt.$$scope={dirty:o,ctx:e}),Ie.$set(Gt)},i(e){Ya||(_(t.$$.fragment,e),_(S.$$.fragment,e),_(A),_(K),_(Be.$$.fragment,e),_(Ve.$$.fragment,e),_(Xe.$$.fragment,e),_(Ze.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(Y),_(ot.$$.fragment,e),_(X),_(ee),_(se),_(rt.$$.fragment,e),_(ae),_(it.$$.fragment,e),_(lt.$$.fragment,e),_(ze.$$.fragment,e),_(pt.$$.fragment,e),_(mt.$$.fragment,e),_(ht.$$.fragment,e),_(Ne.$$.fragment,e),_(ct.$$.fragment,e),_(ut.$$.fragment,e),_(ft.$$.fragment,e),_(dt.$$.fragment,e),_(_t.$$.fragment,e),_(wt.$$.fragment,e),_(gt.$$.fragment,e),_(bt.$$.fragment,e),_(kt.$$.fragment,e),_($t.$$.fragment,e),_(vt.$$.fragment,e),_(xt.$$.fragment,e),_(yt.$$.fragment,e),_(jt.$$.fragment,e),_(re),_(qt.$$.fragment,e),_(le),_(me),_(ce),_(fe),_(Et.$$.fragment,e),_(Ue.$$.fragment,e),_(Tt.$$.fragment,e),_(Pt.$$.fragment,e),_(Ie.$$.fragment,e),Ya=!0)},o(e){d(t.$$.fragment,e),d(S.$$.fragment,e),d(A),d(K),d(Be.$$.fragment,e),d(Ve.$$.fragment,e),d(Xe.$$.fragment,e),d(Ze.$$.fragment,e),d(et.$$.fragment,e),d(tt.$$.fragment,e),d(Y),d(ot.$$.fragment,e),d(X),d(ee),d(se),d(rt.$$.fragment,e),d(ae),d(it.$$.fragment,e),d(lt.$$.fragment,e),d(ze.$$.fragment,e),d(pt.$$.fragment,e),d(mt.$$.fragment,e),d(ht.$$.fragment,e),d(Ne.$$.fragment,e),d(ct.$$.fragment,e),d(ut.$$.fragment,e),d(ft.$$.fragment,e),d(dt.$$.fragment,e),d(_t.$$.fragment,e),d(wt.$$.fragment,e),d(gt.$$.fragment,e),d(bt.$$.fragment,e),d(kt.$$.fragment,e),d($t.$$.fragment,e),d(vt.$$.fragment,e),d(xt.$$.fragment,e),d(yt.$$.fragment,e),d(jt.$$.fragment,e),d(re),d(qt.$$.fragment,e),d(le),d(me),d(ce),d(fe),d(Et.$$.fragment,e),d(Ue.$$.fragment,e),d(Tt.$$.fragment,e),d(Pt.$$.fragment,e),d(Ie.$$.fragment,e),Ya=!1},d(e){s(n),e&&s(p),v(t,e),e&&s(w),e&&s(g),v(S),e&&s(D),St[N].d(e),e&&s(q),e&&s(T),e&&s(En),Ct[R].d(e),e&&s(Jt),e&&s(je),v(Be),e&&s(Tn),e&&s(_e),e&&s(Pn),v(Ve,e),e&&s(Sn),v(Xe,e),e&&s(Cn),e&&s(Rt),e&&s(zn),v(Ze,e),e&&s(Nn),v(et,e),e&&s(An),e&&s(Kt),e&&s(Dn),e&&s(qe),v(tt),e&&s(Ln),e&&s(U),e&&s(On),zt[B].d(e),e&&s(Yt),e&&s(Vt),e&&s(Fn),e&&s(Ee),e&&s(Mn),e&&s(Xt),e&&s(Un),v(ot,e),e&&s(Wn),Nt[V].d(e),e&&s(Zt),e&&s(W),e&&s(In),e&&s(Ce),e&&s(Hn),At[Z].d(e),e&&s(es),e&&s(ts),e&&s(Gn),Dt[te].d(e),e&&s(ss),e&&s(F),e&&s(Jn),e&&s(O),e&&s(Rn),e&&s(we),e&&s(Yn),e&&s(ns),e&&s(Vn),v(rt,e),e&&s(Xn),Lt[ne].d(e),e&&s(as),e&&s(M),e&&s(Zn),v(it,e),e&&s(ea),e&&s(os),e&&s(ta),v(lt,e),e&&s(sa),v(ze,e),e&&s(na),e&&s(ge),e&&s(aa),v(pt,e),e&&s(oa),e&&s(rs),e&&s(ra),v(mt,e),e&&s(ia),v(ht,e),e&&s(la),e&&s(is),e&&s(pa),v(Ne,e),e&&s(ma),e&&s(Te),v(ct),e&&s(ha),e&&s(De),e&&s(ca),v(ut,e),e&&s(ua),v(ft,e),e&&s(fa),e&&s(Le),e&&s(da),v(dt,e),e&&s(_a),v(_t,e),e&&s(wa),e&&s(Oe),e&&s(ga),e&&s(be),e&&s(ba),v(wt,e),e&&s(ka),v(gt,e),e&&s($a),e&&s(Fe),e&&s(va),e&&s(ls),e&&s(xa),v(bt,e),e&&s(ya),v(kt,e),e&&s(ja),e&&s(Me),e&&s(qa),v($t,e),e&&s(Ea),v(vt,e),e&&s(Ta),e&&s(ps),e&&s(Pa),v(xt,e),e&&s(Sa),e&&s(ms),e&&s(Ca),v(yt,e),e&&s(za),e&&s(hs),e&&s(Na),e&&s(I),e&&s(Aa),v(jt,e),e&&s(Da),e&&s(H),e&&s(La),Ot[oe].d(e),e&&s(cs),e&&s(us),e&&s(Oa),v(qt,e),e&&s(Fa),Ft[ie].d(e),e&&s(fs),e&&s(ds),e&&s(Ma),Mt[pe].d(e),e&&s(_s),e&&s(ws),e&&s(Ua),Ut[he].d(e),e&&s(gs),e&&s(bs),e&&s(Wa),Wt[ue].d(e),e&&s(ks),v(Et,e),e&&s(Ia),e&&s($s),e&&s(Ha),v(Ue,e),e&&s(Ga),e&&s(We),e&&s(Ja),v(Tt,e),e&&s(Qa),v(Pt,e),e&&s(Ra),e&&s(vs),e&&s(Ka),v(Ie,e),e&&s(Ba),e&&s(xs)}}}const Qp={local:"fast-tokenizers-in-the-qa-pipeline",sections:[{local:"using-the-questionanswering-pipeline",title:"Using the `question-answering` pipeline"},{local:"using-a-model-for-question-answering",title:"Using a model for question answering"},{local:"handling-long-contexts",title:"Handling long contexts"}],title:"Fast tokenizers in the QA pipeline"};function Rp(x,n,p){let t="pt";return dp(()=>{const w=new URLSearchParams(window.location.search);p(0,t=w.get("fw")||"pt")}),[t]}class tm extends hp{constructor(n){super();cp(this,n,Rp,Jp,up,{})}}export{tm as default,Qp as metadata};
