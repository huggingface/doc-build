import{S as ti,i as oi,s as ri,e as r,k as u,w as Y,t as n,M as ai,c as a,d as o,m as f,a as s,x as J,h as i,b as l,N as Xo,G as t,g as c,y as K,L as si,q as Q,o as X,B as V,v as ni}from"../../chunks/vendor-hf-doc-builder.js";import{Y as ii}from"../../chunks/Youtube-hf-doc-builder.js";import{I as to}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as li}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as hi}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";function ci(ks){let A,oo,L,D,it,Z,Vo,lt,Zo,ro,ee,ao,P,x,ht,te,er,ct,tr,so,oe,no,g,or,re,rr,ar,ae,sr,nr,se,ir,lr,ne,hr,cr,ie,ur,fr,le,gr,pr,io,H,O,ut,he,dr,ft,mr,lo,ze,vr,ho,T,ce,As,wr,ue,Ls,co,d,fe,yr,ge,br,_r,Er,gt,kr,Ar,pt,Lr,uo,Ue,Pr,fo,m,dt,Hr,Tr,v,Ir,pe,Nr,$r,de,Sr,Fr,me,Gr,Cr,I,Dr,ve,xr,Or,we,qr,Rr,go,q,Mr,ye,jr,Wr,po,N,R,mt,be,Br,vt,zr,mo,Ye,Ur,vo,$,wt,Yr,Jr,_e,Kr,Qr,wo,Ee,yt,Xr,Vr,yo,ke,bt,Zr,ea,bo,S,_t,ta,oa,Et,Ae,ra,aa,_o,Le,kt,sa,na,Eo,Pe,At,ia,la,ko,He,Lt,ha,ca,Ao,F,Pt,ua,fa,Te,ga,pa,Lo,G,Ht,da,ma,Ie,va,wa,Po,C,M,Tt,Ne,ya,It,ba,Ho,Je,_a,To,w,Nt,Ke,$t,Ea,ka,Aa,St,Qe,Ft,La,Pa,Ha,Gt,y,Ct,Ta,Ia,Dt,Na,$a,$e,Sa,Fa,Io,j,Ps,No,W,Ga,Se,Ca,Da,$o,Xe,Ve,xt,xa,Oa,So,B,Hs,Fo,b,qa,Fe,Ot,Ra,Ma,Ge,qt,ja,Wa,Go,z,Rt,_,Mt,Ba,za,Ce,jt,Ua,Ya,De,Ja,Ka,Qa,Wt,E,Bt,Xa,Va,zt,Za,es,xe,ts,os,Co,Ze,U,Ut,rs,as,Oe,ss,ns,Do,qe,xo,et,is,Oo,k,Re,ls,Yt,hs,cs,us,Jt,fs,gs,Kt,ps,qo;return Z=new to({}),ee=new hi({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),te=new to({}),oe=new ii({props:{id:"00GKzGyWFEs"}}),he=new to({}),be=new to({}),Ne=new to({}),qe=new li({props:{code:`@misc{huggingfacecourse,
  author = {Hugging Face},
  title = {The Hugging Face Course, 2022},
  howpublished = "\\url{https://huggingface.co/course}",
  year = {2022},
  note = "[Online; accessed <today>]"
}`,highlighted:`@misc{huggingfacecourse,
  <span class="hljs-attr">author</span> = {Hugging Face},
  <span class="hljs-attr">title</span> = {The Hugging Face Course, <span class="hljs-number">2022</span>},
  <span class="hljs-attr">howpublished</span> = <span class="hljs-string">&quot;\\url{https://huggingface.co/course}&quot;</span>,
  <span class="hljs-attr">year</span> = {<span class="hljs-number">2022</span>},
  <span class="hljs-attr">note</span> = <span class="hljs-string">&quot;[Online; accessed &lt;today&gt;]&quot;</span>
}`}}),{c(){A=r("meta"),oo=u(),L=r("h1"),D=r("a"),it=r("span"),Y(Z.$$.fragment),Vo=u(),lt=r("span"),Zo=n("Introduction"),ro=u(),Y(ee.$$.fragment),ao=u(),P=r("h2"),x=r("a"),ht=r("span"),Y(te.$$.fragment),er=u(),ct=r("span"),tr=n("Welcome to the \u{1F917} Course!"),so=u(),Y(oe.$$.fragment),no=u(),g=r("p"),or=n("This course will teach you about natural language processing (NLP) using libraries from the "),re=r("a"),rr=n("Hugging Face"),ar=n(" ecosystem \u2014 "),ae=r("a"),sr=n("\u{1F917} Transformers"),nr=n(", "),se=r("a"),ir=n("\u{1F917} Datasets"),lr=n(", "),ne=r("a"),hr=n("\u{1F917} Tokenizers"),cr=n(", and "),ie=r("a"),ur=n("\u{1F917} Accelerate"),fr=n(" \u2014 as well as the "),le=r("a"),gr=n("Hugging Face Hub"),pr=n(". It\u2019s completely free and without ads."),io=u(),H=r("h2"),O=r("a"),ut=r("span"),Y(he.$$.fragment),dr=u(),ft=r("span"),mr=n("What to expect?"),lo=u(),ze=r("p"),vr=n("Here is a brief overview of the course:"),ho=u(),T=r("div"),ce=r("img"),wr=u(),ue=r("img"),co=u(),d=r("ul"),fe=r("li"),yr=n("Chapters 1 to 4 provide an introduction to the main concepts of the \u{1F917} Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from the "),ge=r("a"),br=n("Hugging Face Hub"),_r=n(", fine-tune it on a dataset, and share your results on the Hub!"),Er=u(),gt=r("li"),kr=n("Chapters 5 to 8 teach the basics of \u{1F917} Datasets and \u{1F917} Tokenizers before diving into classic NLP tasks. By the end of this part, you will be able to tackle the most common NLP problems by yourself."),Ar=u(),pt=r("li"),Lr=n("Chapters 9 to 12 go beyond NLP, and explore how Transformer models can be used tackle tasks in speech processing and computer vision. Along the way, you\u2019ll learn how to build and share demos of your models, and optimize them for production environments. By the end of this part, you will be ready to apply \u{1F917} Transformers to (almost) any machine learning problem!"),uo=u(),Ue=r("p"),Pr=n("This course:"),fo=u(),m=r("ul"),dt=r("li"),Hr=n("Requires a good knowledge of Python"),Tr=u(),v=r("li"),Ir=n("Is better taken after an introductory deep learning course, such as "),pe=r("a"),Nr=n("fast.ai\u2019s"),$r=u(),de=r("a"),Sr=n("Practical Deep Learning for Coders"),Fr=n(" or one of the programs developed by "),me=r("a"),Gr=n("DeepLearning.AI"),Cr=u(),I=r("li"),Dr=n("Does not expect prior "),ve=r("a"),xr=n("PyTorch"),Or=n(" or "),we=r("a"),qr=n("TensorFlow"),Rr=n(" knowledge, though some familiarity with either of those will help"),go=u(),q=r("p"),Mr=n("After you\u2019ve completed this course, we recommend checking out DeepLearning.AI\u2019s "),ye=r("a"),jr=n("Natural Language Processing Specialization"),Wr=n(", which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are well worth knowing about!"),po=u(),N=r("h2"),R=r("a"),mt=r("span"),Y(be.$$.fragment),Br=u(),vt=r("span"),zr=n("Who are we?"),mo=u(),Ye=r("p"),Ur=n("About the authors:"),vo=u(),$=r("p"),wt=r("strong"),Yr=n("Abubakar Abid"),Jr=n(" completed his PhD at Stanford in applied machine learning. During his PhD, he founded "),_e=r("a"),Kr=n("Gradio"),Qr=n(", an open-source Python library that has been used to build over 600,000 machine learning demos. Gradio was acquired by Hugging Face, which is where Abubakar now serves as a machine learning team lead."),wo=u(),Ee=r("p"),yt=r("strong"),Xr=n("Matthew Carrigan"),Vr=n(" is a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we\u2019re going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless."),yo=u(),ke=r("p"),bt=r("strong"),Zr=n("Lysandre Debut"),ea=n(" is a Machine Learning Engineer at Hugging Face and has been working on the \u{1F917} Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API."),bo=u(),S=r("p"),_t=r("strong"),ta=n("Sylvain Gugger"),oa=n(" is a Research Engineer at Hugging Face and one of the core maintainers of the \u{1F917} Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wrote "),Et=r("em"),Ae=r("a"),ra=n("Deep Learning for Coders with fastai and PyTorch"),aa=n(" with Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources."),_o=u(),Le=r("p"),kt=r("strong"),sa=n("Dawood Khan"),na=n(" is a Machine Learning Engineer at Hugging Face. He\u2019s from NYC and graduated from New York University studying Computer Science. After working as an iOS Engineer for a few years, Dawood quit to start Gradio with his fellow co-founders. Gradio was eventually acquired by Hugging Face."),Eo=u(),Pe=r("p"),At=r("strong"),ia=n("Merve Noyan"),la=n(" is a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone."),ko=u(),He=r("p"),Lt=r("strong"),ha=n("Lucile Saulnier"),ca=n(" is a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience."),Ao=u(),F=r("p"),Pt=r("strong"),ua=n("Lewis Tunstall"),fa=n("  is a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of the O\u2019Reilly book "),Te=r("a"),ga=n("Natural Language Processing with Transformers"),pa=n("."),Lo=u(),G=r("p"),Ht=r("strong"),da=n("Leandro von Werra"),ma=n("  is a machine learning engineer in the open-source team at Hugging Face and also a co-author of the O\u2019Reilly book "),Ie=r("a"),va=n("Natural Language Processing with Transformers"),wa=n(". He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack.."),Po=u(),C=r("h2"),M=r("a"),Tt=r("span"),Y(Ne.$$.fragment),ya=u(),It=r("span"),ba=n("FAQ"),Ho=u(),Je=r("p"),_a=n("Here are some answers to frequently asked questions:"),To=u(),w=r("ul"),Nt=r("li"),Ke=r("p"),$t=r("strong"),Ea=n("Does taking this course lead to a certification?"),ka=n(`
Currently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem \u2014 stay tuned!`),Aa=u(),St=r("li"),Qe=r("p"),Ft=r("strong"),La=n("How much time should I spend on this course?"),Pa=n(`
Each chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.`),Ha=u(),Gt=r("li"),y=r("p"),Ct=r("strong"),Ta=n("Where can I ask a question if I have one?"),Ia=n(`
If you have a question about any section of the course, just click on the \u201D`),Dt=r("em"),Na=n("Ask a question"),$a=n("\u201D banner at the top of the page to be automatically redirected to the right section of the "),$e=r("a"),Sa=n("Hugging Face forums"),Fa=n(":"),Io=u(),j=r("img"),No=u(),W=r("p"),Ga=n("Note that a list of "),Se=r("a"),Ca=n("project ideas"),Da=n(" is also available on the forums if you wish to practice more once you have completed the course."),$o=u(),Xe=r("ul"),Ve=r("li"),xt=r("strong"),xa=n("Where can I get the code for the course?"),Oa=n(`
For each section, click on the banner at the top of the page to run the code in either Google Colab or Amazon SageMaker Studio Lab:`),So=u(),B=r("img"),Fo=u(),b=r("p"),qa=n("The Jupyter notebooks containing all the code from the course are hosted on the "),Fe=r("a"),Ot=r("code"),Ra=n("huggingface/notebooks"),Ma=n(" repo. If you wish to generate them locally, check out the instructions in the "),Ge=r("a"),qt=r("code"),ja=n("course"),Wa=n(" repo on GitHub."),Go=u(),z=r("ul"),Rt=r("li"),_=r("p"),Mt=r("strong"),Ba=n("How can I contribute to the course?"),za=n(`
There are many ways to contribute to the course! If you find a typo or a bug, please open an issue on the `),Ce=r("a"),jt=r("code"),Ua=n("course"),Ya=n(" repo. If you would like to help translate the course into your native language, check out the instructions "),De=r("a"),Ja=n("here"),Ka=n("."),Qa=u(),Wt=r("li"),E=r("p"),Bt=r("strong"),Xa=n("What were the choices made for the each translation?"),Va=n(`
Each translation has a glossary and `),zt=r("code"),Za=n("TRANSLATING.txt"),es=n(" file that details the choices that were made for machine learning jargon etc. You can find an example for German "),xe=r("a"),ts=n("here"),os=n("."),Co=u(),Ze=r("ul"),U=r("li"),Ut=r("strong"),rs=n("Can I reuse this course?"),as=n(`
Of course! The course is released under the permissive `),Oe=r("a"),ss=n("Apache 2 license"),ns=n(". This means that you must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. If you would like to cite the course, please use the following BibTeX:"),Do=u(),Y(qe.$$.fragment),xo=u(),et=r("p"),is=n("Are you ready to roll? In this chapter, you will learn:"),Oo=u(),k=r("ul"),Re=r("li"),ls=n("How to use the "),Yt=r("code"),hs=n("pipeline()"),cs=n(" function to solve NLP tasks such as text generation and classification"),us=u(),Jt=r("li"),fs=n("About the Transformer architecture"),gs=u(),Kt=r("li"),ps=n("How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases"),this.h()},l(e){const h=ai('[data-svelte="svelte-1phssyn"]',document.head);A=a(h,"META",{name:!0,content:!0}),h.forEach(o),oo=f(e),L=a(e,"H1",{class:!0});var Ro=s(L);D=a(Ro,"A",{id:!0,class:!0,href:!0});var Ts=s(D);it=a(Ts,"SPAN",{});var Is=s(it);J(Z.$$.fragment,Is),Is.forEach(o),Ts.forEach(o),Vo=f(Ro),lt=a(Ro,"SPAN",{});var Ns=s(lt);Zo=i(Ns,"Introduction"),Ns.forEach(o),Ro.forEach(o),ro=f(e),J(ee.$$.fragment,e),ao=f(e),P=a(e,"H2",{class:!0});var Mo=s(P);x=a(Mo,"A",{id:!0,class:!0,href:!0});var $s=s(x);ht=a($s,"SPAN",{});var Ss=s(ht);J(te.$$.fragment,Ss),Ss.forEach(o),$s.forEach(o),er=f(Mo),ct=a(Mo,"SPAN",{});var Fs=s(ct);tr=i(Fs,"Welcome to the \u{1F917} Course!"),Fs.forEach(o),Mo.forEach(o),so=f(e),J(oe.$$.fragment,e),no=f(e),g=a(e,"P",{});var p=s(g);or=i(p,"This course will teach you about natural language processing (NLP) using libraries from the "),re=a(p,"A",{href:!0,rel:!0});var Gs=s(re);rr=i(Gs,"Hugging Face"),Gs.forEach(o),ar=i(p," ecosystem \u2014 "),ae=a(p,"A",{href:!0,rel:!0});var Cs=s(ae);sr=i(Cs,"\u{1F917} Transformers"),Cs.forEach(o),nr=i(p,", "),se=a(p,"A",{href:!0,rel:!0});var Ds=s(se);ir=i(Ds,"\u{1F917} Datasets"),Ds.forEach(o),lr=i(p,", "),ne=a(p,"A",{href:!0,rel:!0});var xs=s(ne);hr=i(xs,"\u{1F917} Tokenizers"),xs.forEach(o),cr=i(p,", and "),ie=a(p,"A",{href:!0,rel:!0});var Os=s(ie);ur=i(Os,"\u{1F917} Accelerate"),Os.forEach(o),fr=i(p," \u2014 as well as the "),le=a(p,"A",{href:!0,rel:!0});var qs=s(le);gr=i(qs,"Hugging Face Hub"),qs.forEach(o),pr=i(p,". It\u2019s completely free and without ads."),p.forEach(o),io=f(e),H=a(e,"H2",{class:!0});var jo=s(H);O=a(jo,"A",{id:!0,class:!0,href:!0});var Rs=s(O);ut=a(Rs,"SPAN",{});var Ms=s(ut);J(he.$$.fragment,Ms),Ms.forEach(o),Rs.forEach(o),dr=f(jo),ft=a(jo,"SPAN",{});var js=s(ft);mr=i(js,"What to expect?"),js.forEach(o),jo.forEach(o),lo=f(e),ze=a(e,"P",{});var Ws=s(ze);vr=i(Ws,"Here is a brief overview of the course:"),Ws.forEach(o),ho=f(e),T=a(e,"DIV",{class:!0});var Wo=s(T);ce=a(Wo,"IMG",{class:!0,src:!0,alt:!0}),wr=f(Wo),ue=a(Wo,"IMG",{class:!0,src:!0,alt:!0}),Wo.forEach(o),co=f(e),d=a(e,"UL",{});var tt=s(d);fe=a(tt,"LI",{});var Bo=s(fe);yr=i(Bo,"Chapters 1 to 4 provide an introduction to the main concepts of the \u{1F917} Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from the "),ge=a(Bo,"A",{href:!0,rel:!0});var Bs=s(ge);br=i(Bs,"Hugging Face Hub"),Bs.forEach(o),_r=i(Bo,", fine-tune it on a dataset, and share your results on the Hub!"),Bo.forEach(o),Er=f(tt),gt=a(tt,"LI",{});var zs=s(gt);kr=i(zs,"Chapters 5 to 8 teach the basics of \u{1F917} Datasets and \u{1F917} Tokenizers before diving into classic NLP tasks. By the end of this part, you will be able to tackle the most common NLP problems by yourself."),zs.forEach(o),Ar=f(tt),pt=a(tt,"LI",{});var Us=s(pt);Lr=i(Us,"Chapters 9 to 12 go beyond NLP, and explore how Transformer models can be used tackle tasks in speech processing and computer vision. Along the way, you\u2019ll learn how to build and share demos of your models, and optimize them for production environments. By the end of this part, you will be ready to apply \u{1F917} Transformers to (almost) any machine learning problem!"),Us.forEach(o),tt.forEach(o),uo=f(e),Ue=a(e,"P",{});var Ys=s(Ue);Pr=i(Ys,"This course:"),Ys.forEach(o),fo=f(e),m=a(e,"UL",{});var ot=s(m);dt=a(ot,"LI",{});var Js=s(dt);Hr=i(Js,"Requires a good knowledge of Python"),Js.forEach(o),Tr=f(ot),v=a(ot,"LI",{});var Me=s(v);Ir=i(Me,"Is better taken after an introductory deep learning course, such as "),pe=a(Me,"A",{href:!0,rel:!0});var Ks=s(pe);Nr=i(Ks,"fast.ai\u2019s"),Ks.forEach(o),$r=f(Me),de=a(Me,"A",{href:!0,rel:!0});var Qs=s(de);Sr=i(Qs,"Practical Deep Learning for Coders"),Qs.forEach(o),Fr=i(Me," or one of the programs developed by "),me=a(Me,"A",{href:!0,rel:!0});var Xs=s(me);Gr=i(Xs,"DeepLearning.AI"),Xs.forEach(o),Me.forEach(o),Cr=f(ot),I=a(ot,"LI",{});var rt=s(I);Dr=i(rt,"Does not expect prior "),ve=a(rt,"A",{href:!0,rel:!0});var Vs=s(ve);xr=i(Vs,"PyTorch"),Vs.forEach(o),Or=i(rt," or "),we=a(rt,"A",{href:!0,rel:!0});var Zs=s(we);qr=i(Zs,"TensorFlow"),Zs.forEach(o),Rr=i(rt," knowledge, though some familiarity with either of those will help"),rt.forEach(o),ot.forEach(o),go=f(e),q=a(e,"P",{});var zo=s(q);Mr=i(zo,"After you\u2019ve completed this course, we recommend checking out DeepLearning.AI\u2019s "),ye=a(zo,"A",{href:!0,rel:!0});var en=s(ye);jr=i(en,"Natural Language Processing Specialization"),en.forEach(o),Wr=i(zo,", which covers a wide range of traditional NLP models like naive Bayes and LSTMs that are well worth knowing about!"),zo.forEach(o),po=f(e),N=a(e,"H2",{class:!0});var Uo=s(N);R=a(Uo,"A",{id:!0,class:!0,href:!0});var tn=s(R);mt=a(tn,"SPAN",{});var on=s(mt);J(be.$$.fragment,on),on.forEach(o),tn.forEach(o),Br=f(Uo),vt=a(Uo,"SPAN",{});var rn=s(vt);zr=i(rn,"Who are we?"),rn.forEach(o),Uo.forEach(o),mo=f(e),Ye=a(e,"P",{});var an=s(Ye);Ur=i(an,"About the authors:"),an.forEach(o),vo=f(e),$=a(e,"P",{});var Qt=s($);wt=a(Qt,"STRONG",{});var sn=s(wt);Yr=i(sn,"Abubakar Abid"),sn.forEach(o),Jr=i(Qt," completed his PhD at Stanford in applied machine learning. During his PhD, he founded "),_e=a(Qt,"A",{href:!0,rel:!0});var nn=s(_e);Kr=i(nn,"Gradio"),nn.forEach(o),Qr=i(Qt,", an open-source Python library that has been used to build over 600,000 machine learning demos. Gradio was acquired by Hugging Face, which is where Abubakar now serves as a machine learning team lead."),Qt.forEach(o),wo=f(e),Ee=a(e,"P",{});var ds=s(Ee);yt=a(ds,"STRONG",{});var ln=s(yt);Xr=i(ln,"Matthew Carrigan"),ln.forEach(o),Vr=i(ds," is a Machine Learning Engineer at Hugging Face. He lives in Dublin, Ireland and previously worked as an ML engineer at Parse.ly and before that as a post-doctoral researcher at Trinity College Dublin. He does not believe we\u2019re going to get to AGI by scaling existing architectures, but has high hopes for robot immortality regardless."),ds.forEach(o),yo=f(e),ke=a(e,"P",{});var ms=s(ke);bt=a(ms,"STRONG",{});var hn=s(bt);Zr=i(hn,"Lysandre Debut"),hn.forEach(o),ea=i(ms," is a Machine Learning Engineer at Hugging Face and has been working on the \u{1F917} Transformers library since the very early development stages. His aim is to make NLP accessible for everyone by developing tools with a very simple API."),ms.forEach(o),bo=f(e),S=a(e,"P",{});var Xt=s(S);_t=a(Xt,"STRONG",{});var cn=s(_t);ta=i(cn,"Sylvain Gugger"),cn.forEach(o),oa=i(Xt," is a Research Engineer at Hugging Face and one of the core maintainers of the \u{1F917} Transformers library. Previously he was a Research Scientist at fast.ai, and he co-wrote "),Et=a(Xt,"EM",{});var un=s(Et);Ae=a(un,"A",{href:!0,rel:!0});var fn=s(Ae);ra=i(fn,"Deep Learning for Coders with fastai and PyTorch"),fn.forEach(o),un.forEach(o),aa=i(Xt," with Jeremy Howard. The main focus of his research is on making deep learning more accessible, by designing and improving techniques that allow models to train fast on limited resources."),Xt.forEach(o),_o=f(e),Le=a(e,"P",{});var vs=s(Le);kt=a(vs,"STRONG",{});var gn=s(kt);sa=i(gn,"Dawood Khan"),gn.forEach(o),na=i(vs," is a Machine Learning Engineer at Hugging Face. He\u2019s from NYC and graduated from New York University studying Computer Science. After working as an iOS Engineer for a few years, Dawood quit to start Gradio with his fellow co-founders. Gradio was eventually acquired by Hugging Face."),vs.forEach(o),Eo=f(e),Pe=a(e,"P",{});var ws=s(Pe);At=a(ws,"STRONG",{});var pn=s(At);ia=i(pn,"Merve Noyan"),pn.forEach(o),la=i(ws," is a developer advocate at Hugging Face, working on developing tools and building content around them to democratize machine learning for everyone."),ws.forEach(o),ko=f(e),He=a(e,"P",{});var ys=s(He);Lt=a(ys,"STRONG",{});var dn=s(Lt);ha=i(dn,"Lucile Saulnier"),dn.forEach(o),ca=i(ys," is a machine learning engineer at Hugging Face, developing and supporting the use of open source tools. She is also actively involved in many research projects in the field of Natural Language Processing such as collaborative training and BigScience."),ys.forEach(o),Ao=f(e),F=a(e,"P",{});var Vt=s(F);Pt=a(Vt,"STRONG",{});var mn=s(Pt);ua=i(mn,"Lewis Tunstall"),mn.forEach(o),fa=i(Vt,"  is a machine learning engineer at Hugging Face, focused on developing open-source tools and making them accessible to the wider community. He is also a co-author of the O\u2019Reilly book "),Te=a(Vt,"A",{href:!0,rel:!0});var vn=s(Te);ga=i(vn,"Natural Language Processing with Transformers"),vn.forEach(o),pa=i(Vt,"."),Vt.forEach(o),Lo=f(e),G=a(e,"P",{});var Zt=s(G);Ht=a(Zt,"STRONG",{});var wn=s(Ht);da=i(wn,"Leandro von Werra"),wn.forEach(o),ma=i(Zt,"  is a machine learning engineer in the open-source team at Hugging Face and also a co-author of the O\u2019Reilly book "),Ie=a(Zt,"A",{href:!0,rel:!0});var yn=s(Ie);va=i(yn,"Natural Language Processing with Transformers"),yn.forEach(o),wa=i(Zt,". He has several years of industry experience bringing NLP projects to production by working across the whole machine learning stack.."),Zt.forEach(o),Po=f(e),C=a(e,"H2",{class:!0});var Yo=s(C);M=a(Yo,"A",{id:!0,class:!0,href:!0});var bn=s(M);Tt=a(bn,"SPAN",{});var _n=s(Tt);J(Ne.$$.fragment,_n),_n.forEach(o),bn.forEach(o),ya=f(Yo),It=a(Yo,"SPAN",{});var En=s(It);ba=i(En,"FAQ"),En.forEach(o),Yo.forEach(o),Ho=f(e),Je=a(e,"P",{});var kn=s(Je);_a=i(kn,"Here are some answers to frequently asked questions:"),kn.forEach(o),To=f(e),w=a(e,"UL",{});var at=s(w);Nt=a(at,"LI",{});var An=s(Nt);Ke=a(An,"P",{});var bs=s(Ke);$t=a(bs,"STRONG",{});var Ln=s($t);Ea=i(Ln,"Does taking this course lead to a certification?"),Ln.forEach(o),ka=i(bs,`
Currently we do not have any certification for this course. However, we are working on a certification program for the Hugging Face ecosystem \u2014 stay tuned!`),bs.forEach(o),An.forEach(o),Aa=f(at),St=a(at,"LI",{});var Pn=s(St);Qe=a(Pn,"P",{});var _s=s(Qe);Ft=a(_s,"STRONG",{});var Hn=s(Ft);La=i(Hn,"How much time should I spend on this course?"),Hn.forEach(o),Pa=i(_s,`
Each chapter in this course is designed to be completed in 1 week, with approximately 6-8 hours of work per week. However, you can take as much time as you need to complete the course.`),_s.forEach(o),Pn.forEach(o),Ha=f(at),Gt=a(at,"LI",{});var Tn=s(Gt);y=a(Tn,"P",{});var je=s(y);Ct=a(je,"STRONG",{});var In=s(Ct);Ta=i(In,"Where can I ask a question if I have one?"),In.forEach(o),Ia=i(je,`
If you have a question about any section of the course, just click on the \u201D`),Dt=a(je,"EM",{});var Nn=s(Dt);Na=i(Nn,"Ask a question"),Nn.forEach(o),$a=i(je,"\u201D banner at the top of the page to be automatically redirected to the right section of the "),$e=a(je,"A",{href:!0,rel:!0});var $n=s($e);Sa=i($n,"Hugging Face forums"),$n.forEach(o),Fa=i(je,":"),je.forEach(o),Tn.forEach(o),at.forEach(o),Io=f(e),j=a(e,"IMG",{src:!0,alt:!0,width:!0}),No=f(e),W=a(e,"P",{});var Jo=s(W);Ga=i(Jo,"Note that a list of "),Se=a(Jo,"A",{href:!0,rel:!0});var Sn=s(Se);Ca=i(Sn,"project ideas"),Sn.forEach(o),Da=i(Jo," is also available on the forums if you wish to practice more once you have completed the course."),Jo.forEach(o),$o=f(e),Xe=a(e,"UL",{});var Fn=s(Xe);Ve=a(Fn,"LI",{});var Es=s(Ve);xt=a(Es,"STRONG",{});var Gn=s(xt);xa=i(Gn,"Where can I get the code for the course?"),Gn.forEach(o),Oa=i(Es,`
For each section, click on the banner at the top of the page to run the code in either Google Colab or Amazon SageMaker Studio Lab:`),Es.forEach(o),Fn.forEach(o),So=f(e),B=a(e,"IMG",{src:!0,alt:!0,width:!0}),Fo=f(e),b=a(e,"P",{});var st=s(b);qa=i(st,"The Jupyter notebooks containing all the code from the course are hosted on the "),Fe=a(st,"A",{href:!0,rel:!0});var Cn=s(Fe);Ot=a(Cn,"CODE",{});var Dn=s(Ot);Ra=i(Dn,"huggingface/notebooks"),Dn.forEach(o),Cn.forEach(o),Ma=i(st," repo. If you wish to generate them locally, check out the instructions in the "),Ge=a(st,"A",{href:!0,rel:!0});var xn=s(Ge);qt=a(xn,"CODE",{});var On=s(qt);ja=i(On,"course"),On.forEach(o),xn.forEach(o),Wa=i(st," repo on GitHub."),st.forEach(o),Go=f(e),z=a(e,"UL",{});var Ko=s(z);Rt=a(Ko,"LI",{});var qn=s(Rt);_=a(qn,"P",{});var We=s(_);Mt=a(We,"STRONG",{});var Rn=s(Mt);Ba=i(Rn,"How can I contribute to the course?"),Rn.forEach(o),za=i(We,`
There are many ways to contribute to the course! If you find a typo or a bug, please open an issue on the `),Ce=a(We,"A",{href:!0,rel:!0});var Mn=s(Ce);jt=a(Mn,"CODE",{});var jn=s(jt);Ua=i(jn,"course"),jn.forEach(o),Mn.forEach(o),Ya=i(We," repo. If you would like to help translate the course into your native language, check out the instructions "),De=a(We,"A",{href:!0,rel:!0});var Wn=s(De);Ja=i(Wn,"here"),Wn.forEach(o),Ka=i(We,"."),We.forEach(o),qn.forEach(o),Qa=f(Ko),Wt=a(Ko,"LI",{});var Bn=s(Wt);E=a(Bn,"P",{});var Be=s(E);Bt=a(Be,"STRONG",{});var zn=s(Bt);Xa=i(zn,"What were the choices made for the each translation?"),zn.forEach(o),Va=i(Be,`
Each translation has a glossary and `),zt=a(Be,"CODE",{});var Un=s(zt);Za=i(Un,"TRANSLATING.txt"),Un.forEach(o),es=i(Be," file that details the choices that were made for machine learning jargon etc. You can find an example for German "),xe=a(Be,"A",{href:!0,rel:!0});var Yn=s(xe);ts=i(Yn,"here"),Yn.forEach(o),os=i(Be,"."),Be.forEach(o),Bn.forEach(o),Ko.forEach(o),Co=f(e),Ze=a(e,"UL",{});var Jn=s(Ze);U=a(Jn,"LI",{});var eo=s(U);Ut=a(eo,"STRONG",{});var Kn=s(Ut);rs=i(Kn,"Can I reuse this course?"),Kn.forEach(o),as=i(eo,`
Of course! The course is released under the permissive `),Oe=a(eo,"A",{href:!0,rel:!0});var Qn=s(Oe);ss=i(Qn,"Apache 2 license"),Qn.forEach(o),ns=i(eo,". This means that you must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. If you would like to cite the course, please use the following BibTeX:"),eo.forEach(o),Jn.forEach(o),Do=f(e),J(qe.$$.fragment,e),xo=f(e),et=a(e,"P",{});var Xn=s(et);is=i(Xn,"Are you ready to roll? In this chapter, you will learn:"),Xn.forEach(o),Oo=f(e),k=a(e,"UL",{});var nt=s(k);Re=a(nt,"LI",{});var Qo=s(Re);ls=i(Qo,"How to use the "),Yt=a(Qo,"CODE",{});var Vn=s(Yt);hs=i(Vn,"pipeline()"),Vn.forEach(o),cs=i(Qo," function to solve NLP tasks such as text generation and classification"),Qo.forEach(o),us=f(nt),Jt=a(nt,"LI",{});var Zn=s(Jt);fs=i(Zn,"About the Transformer architecture"),Zn.forEach(o),gs=f(nt),Kt=a(nt,"LI",{});var ei=s(Kt);ps=i(ei,"How to distinguish between encoder, decoder, and encoder-decoder architectures and use cases"),ei.forEach(o),nt.forEach(o),this.h()},h(){l(A,"name","hf:doc:metadata"),l(A,"content",JSON.stringify(ui)),l(D,"id","introduction"),l(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(D,"href","#introduction"),l(L,"class","relative group"),l(x,"id","welcome-to-the-course"),l(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(x,"href","#welcome-to-the-course"),l(P,"class","relative group"),l(re,"href","https://huggingface.co/"),l(re,"rel","nofollow"),l(ae,"href","https://github.com/huggingface/transformers"),l(ae,"rel","nofollow"),l(se,"href","https://github.com/huggingface/datasets"),l(se,"rel","nofollow"),l(ne,"href","https://github.com/huggingface/tokenizers"),l(ne,"rel","nofollow"),l(ie,"href","https://github.com/huggingface/accelerate"),l(ie,"rel","nofollow"),l(le,"href","https://huggingface.co/models"),l(le,"rel","nofollow"),l(O,"id","what-to-expect"),l(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(O,"href","#what-to-expect"),l(H,"class","relative group"),l(ce,"class","block dark:hidden"),Xo(ce.src,As="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg")||l(ce,"src",As),l(ce,"alt","Brief overview of the chapters of the course."),l(ue,"class","hidden dark:block"),Xo(ue.src,Ls="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg")||l(ue,"src",Ls),l(ue,"alt","Brief overview of the chapters of the course."),l(T,"class","flex justify-center"),l(ge,"href","https://huggingface.co/models"),l(ge,"rel","nofollow"),l(pe,"href","https://www.fast.ai/"),l(pe,"rel","nofollow"),l(de,"href","https://course.fast.ai/"),l(de,"rel","nofollow"),l(me,"href","https://www.deeplearning.ai/"),l(me,"rel","nofollow"),l(ve,"href","https://pytorch.org/"),l(ve,"rel","nofollow"),l(we,"href","https://www.tensorflow.org/"),l(we,"rel","nofollow"),l(ye,"href","https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh"),l(ye,"rel","nofollow"),l(R,"id","who-are-we"),l(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(R,"href","#who-are-we"),l(N,"class","relative group"),l(_e,"href","https://github.com/gradio-app/gradio"),l(_e,"rel","nofollow"),l(Ae,"href","https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/"),l(Ae,"rel","nofollow"),l(Te,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098136789/"),l(Te,"rel","nofollow"),l(Ie,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098136789/"),l(Ie,"rel","nofollow"),l(M,"id","faq"),l(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(M,"href","#faq"),l(C,"class","relative group"),l($e,"href","https://discuss.huggingface.co/"),l($e,"rel","nofollow"),Xo(j.src,Ps="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/forum-button.png")||l(j,"src",Ps),l(j,"alt","Link to the Hugging Face forums"),l(j,"width","75%"),l(Se,"href","https://discuss.huggingface.co/c/course/course-event/25"),l(Se,"rel","nofollow"),Xo(B.src,Hs="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/notebook-buttons.png")||l(B,"src",Hs),l(B,"alt","Link to the Hugging Face course notebooks"),l(B,"width","75%"),l(Fe,"href","https://github.com/huggingface/notebooks"),l(Fe,"rel","nofollow"),l(Ge,"href","https://github.com/huggingface/course#-jupyter-notebooks"),l(Ge,"rel","nofollow"),l(Ce,"href","https://github.com/huggingface/course"),l(Ce,"rel","nofollow"),l(De,"href","https://github.com/huggingface/course#translating-the-course-into-your-language"),l(De,"rel","nofollow"),l(xe,"href","https://github.com/huggingface/course/blob/main/chapters/de/TRANSLATING.txt"),l(xe,"rel","nofollow"),l(Oe,"href","https://www.apache.org/licenses/LICENSE-2.0.html"),l(Oe,"rel","nofollow")},m(e,h){t(document.head,A),c(e,oo,h),c(e,L,h),t(L,D),t(D,it),K(Z,it,null),t(L,Vo),t(L,lt),t(lt,Zo),c(e,ro,h),K(ee,e,h),c(e,ao,h),c(e,P,h),t(P,x),t(x,ht),K(te,ht,null),t(P,er),t(P,ct),t(ct,tr),c(e,so,h),K(oe,e,h),c(e,no,h),c(e,g,h),t(g,or),t(g,re),t(re,rr),t(g,ar),t(g,ae),t(ae,sr),t(g,nr),t(g,se),t(se,ir),t(g,lr),t(g,ne),t(ne,hr),t(g,cr),t(g,ie),t(ie,ur),t(g,fr),t(g,le),t(le,gr),t(g,pr),c(e,io,h),c(e,H,h),t(H,O),t(O,ut),K(he,ut,null),t(H,dr),t(H,ft),t(ft,mr),c(e,lo,h),c(e,ze,h),t(ze,vr),c(e,ho,h),c(e,T,h),t(T,ce),t(T,wr),t(T,ue),c(e,co,h),c(e,d,h),t(d,fe),t(fe,yr),t(fe,ge),t(ge,br),t(fe,_r),t(d,Er),t(d,gt),t(gt,kr),t(d,Ar),t(d,pt),t(pt,Lr),c(e,uo,h),c(e,Ue,h),t(Ue,Pr),c(e,fo,h),c(e,m,h),t(m,dt),t(dt,Hr),t(m,Tr),t(m,v),t(v,Ir),t(v,pe),t(pe,Nr),t(v,$r),t(v,de),t(de,Sr),t(v,Fr),t(v,me),t(me,Gr),t(m,Cr),t(m,I),t(I,Dr),t(I,ve),t(ve,xr),t(I,Or),t(I,we),t(we,qr),t(I,Rr),c(e,go,h),c(e,q,h),t(q,Mr),t(q,ye),t(ye,jr),t(q,Wr),c(e,po,h),c(e,N,h),t(N,R),t(R,mt),K(be,mt,null),t(N,Br),t(N,vt),t(vt,zr),c(e,mo,h),c(e,Ye,h),t(Ye,Ur),c(e,vo,h),c(e,$,h),t($,wt),t(wt,Yr),t($,Jr),t($,_e),t(_e,Kr),t($,Qr),c(e,wo,h),c(e,Ee,h),t(Ee,yt),t(yt,Xr),t(Ee,Vr),c(e,yo,h),c(e,ke,h),t(ke,bt),t(bt,Zr),t(ke,ea),c(e,bo,h),c(e,S,h),t(S,_t),t(_t,ta),t(S,oa),t(S,Et),t(Et,Ae),t(Ae,ra),t(S,aa),c(e,_o,h),c(e,Le,h),t(Le,kt),t(kt,sa),t(Le,na),c(e,Eo,h),c(e,Pe,h),t(Pe,At),t(At,ia),t(Pe,la),c(e,ko,h),c(e,He,h),t(He,Lt),t(Lt,ha),t(He,ca),c(e,Ao,h),c(e,F,h),t(F,Pt),t(Pt,ua),t(F,fa),t(F,Te),t(Te,ga),t(F,pa),c(e,Lo,h),c(e,G,h),t(G,Ht),t(Ht,da),t(G,ma),t(G,Ie),t(Ie,va),t(G,wa),c(e,Po,h),c(e,C,h),t(C,M),t(M,Tt),K(Ne,Tt,null),t(C,ya),t(C,It),t(It,ba),c(e,Ho,h),c(e,Je,h),t(Je,_a),c(e,To,h),c(e,w,h),t(w,Nt),t(Nt,Ke),t(Ke,$t),t($t,Ea),t(Ke,ka),t(w,Aa),t(w,St),t(St,Qe),t(Qe,Ft),t(Ft,La),t(Qe,Pa),t(w,Ha),t(w,Gt),t(Gt,y),t(y,Ct),t(Ct,Ta),t(y,Ia),t(y,Dt),t(Dt,Na),t(y,$a),t(y,$e),t($e,Sa),t(y,Fa),c(e,Io,h),c(e,j,h),c(e,No,h),c(e,W,h),t(W,Ga),t(W,Se),t(Se,Ca),t(W,Da),c(e,$o,h),c(e,Xe,h),t(Xe,Ve),t(Ve,xt),t(xt,xa),t(Ve,Oa),c(e,So,h),c(e,B,h),c(e,Fo,h),c(e,b,h),t(b,qa),t(b,Fe),t(Fe,Ot),t(Ot,Ra),t(b,Ma),t(b,Ge),t(Ge,qt),t(qt,ja),t(b,Wa),c(e,Go,h),c(e,z,h),t(z,Rt),t(Rt,_),t(_,Mt),t(Mt,Ba),t(_,za),t(_,Ce),t(Ce,jt),t(jt,Ua),t(_,Ya),t(_,De),t(De,Ja),t(_,Ka),t(z,Qa),t(z,Wt),t(Wt,E),t(E,Bt),t(Bt,Xa),t(E,Va),t(E,zt),t(zt,Za),t(E,es),t(E,xe),t(xe,ts),t(E,os),c(e,Co,h),c(e,Ze,h),t(Ze,U),t(U,Ut),t(Ut,rs),t(U,as),t(U,Oe),t(Oe,ss),t(U,ns),c(e,Do,h),K(qe,e,h),c(e,xo,h),c(e,et,h),t(et,is),c(e,Oo,h),c(e,k,h),t(k,Re),t(Re,ls),t(Re,Yt),t(Yt,hs),t(Re,cs),t(k,us),t(k,Jt),t(Jt,fs),t(k,gs),t(k,Kt),t(Kt,ps),qo=!0},p:si,i(e){qo||(Q(Z.$$.fragment,e),Q(ee.$$.fragment,e),Q(te.$$.fragment,e),Q(oe.$$.fragment,e),Q(he.$$.fragment,e),Q(be.$$.fragment,e),Q(Ne.$$.fragment,e),Q(qe.$$.fragment,e),qo=!0)},o(e){X(Z.$$.fragment,e),X(ee.$$.fragment,e),X(te.$$.fragment,e),X(oe.$$.fragment,e),X(he.$$.fragment,e),X(be.$$.fragment,e),X(Ne.$$.fragment,e),X(qe.$$.fragment,e),qo=!1},d(e){o(A),e&&o(oo),e&&o(L),V(Z),e&&o(ro),V(ee,e),e&&o(ao),e&&o(P),V(te),e&&o(so),V(oe,e),e&&o(no),e&&o(g),e&&o(io),e&&o(H),V(he),e&&o(lo),e&&o(ze),e&&o(ho),e&&o(T),e&&o(co),e&&o(d),e&&o(uo),e&&o(Ue),e&&o(fo),e&&o(m),e&&o(go),e&&o(q),e&&o(po),e&&o(N),V(be),e&&o(mo),e&&o(Ye),e&&o(vo),e&&o($),e&&o(wo),e&&o(Ee),e&&o(yo),e&&o(ke),e&&o(bo),e&&o(S),e&&o(_o),e&&o(Le),e&&o(Eo),e&&o(Pe),e&&o(ko),e&&o(He),e&&o(Ao),e&&o(F),e&&o(Lo),e&&o(G),e&&o(Po),e&&o(C),V(Ne),e&&o(Ho),e&&o(Je),e&&o(To),e&&o(w),e&&o(Io),e&&o(j),e&&o(No),e&&o(W),e&&o($o),e&&o(Xe),e&&o(So),e&&o(B),e&&o(Fo),e&&o(b),e&&o(Go),e&&o(z),e&&o(Co),e&&o(Ze),e&&o(Do),V(qe,e),e&&o(xo),e&&o(et),e&&o(Oo),e&&o(k)}}}const ui={local:"introduction",sections:[{local:"welcome-to-the-course",title:"Welcome to the \u{1F917} Course!"},{local:"what-to-expect",title:"What to expect?"},{local:"who-are-we",title:"Who are we?"},{local:"faq",title:"FAQ"}],title:"Introduction"};function fi(ks){return ni(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wi extends ti{constructor(A){super();oi(this,A,fi,ci,ri,{})}}export{wi as default,ui as metadata};
