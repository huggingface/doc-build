<meta charset="utf-8" /><meta http-equiv="content-security-policy" content=""><meta name="hf:doc:metadata" content="{&quot;local&quot;:&quot;summary&quot;,&quot;title&quot;:&quot;Summary&quot;}" data-svelte="svelte-1phssyn">
	<link rel="modulepreload" href="/docs/course/main/en/_app/assets/pages/__layout.svelte-9b9c9c55.css">
	<link rel="modulepreload" href="/docs/course/main/en/_app/start-ef7cedf9.js">
	<link rel="modulepreload" href="/docs/course/main/en/_app/chunks/vendor-1e8b365d.js">
	<link rel="modulepreload" href="/docs/course/main/en/_app/chunks/paths-4b3c6e7e.js">
	<link rel="modulepreload" href="/docs/course/main/en/_app/pages/__layout.svelte-05c2fb0c.js">
	<link rel="modulepreload" href="/docs/course/main/en/_app/pages/chapter1/9.mdx-0fc4cfb4.js">
	<link rel="modulepreload" href="/docs/course/main/en/_app/chunks/IconCopyLink-483c28ba.js"> 





<h1 class="relative group"><a id="summary" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#summary"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Summary
	</span></h1>

<p>In this chapter, you saw how to approach different NLP tasks using the high-level <code>pipeline()</code> function from ðŸ¤— Transformers. You also saw how to search for and use models in the Hub, as well as how to use the Inference API to test the models directly in your browser.</p>
<p>We discussed how Transformer models work at a high level, and talked about the importance of transfer learning and fine-tuning. A key aspect is that you can use the full architecture or only the encoder or decoder, depending on what kind of task you aim to solve. The following table summarizes this:</p>
<table><thead><tr><th>Model</th>
<th>Examples</th>
<th>Tasks</th></tr></thead>
<tbody><tr><td>Encoder</td>
<td>ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa</td>
<td>Sentence classification, named entity recognition, extractive question answering</td></tr>
<tr><td>Decoder</td>
<td>CTRL, GPT, GPT-2, Transformer XL</td>
<td>Text generation</td></tr>
<tr><td>Encoder-decoder</td>
<td>BART, T5, Marian, mBART</td>
<td>Summarization, translation, generative question answering</td></tr></tbody></table>


		<script type="module" data-hydrate="1r6ewdh">
		import { start } from "/docs/course/main/en/_app/start-ef7cedf9.js";
		start({
			target: document.querySelector('[data-hydrate="1r6ewdh"]').parentNode,
			paths: {"base":"/docs/course/main/en","assets":"/docs/course/main/en"},
			session: {},
			route: false,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/docs/course/main/en/_app/pages/__layout.svelte-05c2fb0c.js"),
						import("/docs/course/main/en/_app/pages/chapter1/9.mdx-0fc4cfb4.js")
				],
				params: {}
			}
		});
	</script>
