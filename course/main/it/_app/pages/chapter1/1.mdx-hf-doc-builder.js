import{S as bo,i as wo,s as Lo,e as t,k as p,w as Le,t as r,M as Po,c as l,d as i,m as d,a as n,x as Pe,h as o,b as s,N as Eo,G as a,g as u,y as ye,L as yo,q as $e,o as Ae,B as Ne,v as $o}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ao}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ma}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as No}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function To(Er){let b,ra,w,H,Te,C,Fa,Ie,Ca,oa,q,ta,L,k,Se,D,qa,He,Da,la,R,na,f,Ra,ke,Ga,Oa,G,xa,Ba,O,Qa,Ua,x,Ja,Va,B,Wa,Ya,Q,ja,Ka,U,Xa,Za,sa,P,M,Me,J,ei,Fe,ai,ca,fe,ii,ua,y,V,br,ri,W,wr,pa,v,Y,oi,j,ti,li,ni,Ce,si,ci,$,ui,qe,pi,di,De,fi,mi,da,me,hi,fa,z,Re,gi,vi,h,zi,Ge,_i,Ei,K,bi,wi,X,Li,Pi,Z,yi,$i,A,Ai,ee,Ni,Ti,ae,Ii,Si,ma,_,Hi,ie,ki,Mi,Oe,Fi,Ci,ha,N,F,xe,re,qi,Be,Di,ga,he,Ri,va,oe,Qe,Gi,Oi,za,te,Ue,xi,Bi,_a,g,Je,Qi,Ui,le,Ji,Vi,Ve,Wi,Yi,Ea,ne,We,ji,Ki,ba,se,Ye,Xi,Zi,wa,T,je,er,ar,ce,ir,rr,La,I,Ke,or,tr,ue,lr,nr,Pa,ge,sr,ya,E,pe,cr,Xe,ur,pr,dr,Ze,fr,mr,ea,hr,$a;return C=new Ma({}),q=new No({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),D=new Ma({}),R=new Ao({props:{id:"00GKzGyWFEs"}}),J=new Ma({}),re=new Ma({}),{c(){b=t("meta"),ra=p(),w=t("h1"),H=t("a"),Te=t("span"),Le(C.$$.fragment),Fa=p(),Ie=t("span"),Ca=r("Introduzione"),oa=p(),Le(q.$$.fragment),ta=p(),L=t("h2"),k=t("a"),Se=t("span"),Le(D.$$.fragment),qa=p(),He=t("span"),Da=r("Benvenuto/a al corso di \u{1F917}!"),la=p(),Le(R.$$.fragment),na=p(),f=t("p"),Ra=r("Questo corso ti insegner\xE0 a eseguire compiti di Natural Language Processing (NLP, "),ke=t("em"),Ga=r("elaborazione del linguaggio naturale"),Oa=r(") utilizzando le librerie dell\u2019ecosistema di "),G=t("a"),xa=r("Hugging Face"),Ba=r(": "),O=t("a"),Qa=r("\u{1F917} Transformers"),Ua=r(", "),x=t("a"),Ja=r("\u{1F917} Datasets"),Va=r(", "),B=t("a"),Wa=r("\u{1F917} Tokenizers"),Ya=r(", e "),Q=t("a"),ja=r("\u{1F917} Accelerate"),Ka=r(". Ti insegneremo anche ad usare il nostro "),U=t("a"),Xa=r("Hugging Face Hub"),Za=r(", che \xE8 completamente gratuito e senza pubblicit\xE0."),sa=p(),P=t("h2"),M=t("a"),Me=t("span"),Le(J.$$.fragment),ei=p(),Fe=t("span"),ai=r("Contenuti"),ca=p(),fe=t("p"),ii=r("Eccoti un breve riassunto dei contenuti del corso:"),ua=p(),y=t("div"),V=t("img"),ri=p(),W=t("img"),pa=p(),v=t("ul"),Y=t("li"),oi=r("I capitoli da 1 a 4 forniscono un\u2019introduzione ai concetti principali della libreria \u{1F917} Transformers. Alla fine di questa parte del corso, conoscerai come funzionano i modelli Transformers e saprai come utilizzare un modello dell\u2019"),j=t("a"),ti=r("Hugging Face Hub"),li=r(", affinarlo in un dataset, e condividere i tuoi risultati nell\u2019Hub!"),ni=p(),Ce=t("li"),si=r("I capitoli da 5 a 8 insegnano le basi degli \u{1F917} Dataset e degli \u{1F917} Tokenizer, per poi esplorare alcuni compiti classici di NLP. Alla fine di questa parte, saprai far fronte ai problemi di NLP pi\xF9 comuni in maniera autonoma."),ci=p(),$=t("li"),ui=r("I capitoli da 9 a 12 vanno oltre il Natural Language Processing, ed esplorano come i modelli Transformer possano essere utilizzati per affrontare compiti di elaborazione vocale o visione artificiale. Strada facendo, imparerai a costruire e condividere demo ("),qe=t("em"),pi=r("dimostrazioni"),di=r(") dei tuoi modelli, e ad ottimizzarli per la produzione. Alla fine di questa parte, sarai pronto ad utilizzare gli \u{1F917} Transformer per qualsiasi problema di machine learning ("),De=t("em"),fi=r("apprendimento automatico"),mi=r("), o quasi!"),da=p(),me=t("p"),hi=r("Questo corso:"),fa=p(),z=t("ul"),Re=t("li"),gi=r("Richiede una buona conoscenza di Python"),vi=p(),h=t("li"),zi=r("Andrebbe seguito di preferenza a seguito di un corso introduttivo di deep learning ("),Ge=t("em"),_i=r("apprendimento profondo"),Ei=r("), come ad esempio il "),K=t("a"),bi=r("Practical Deep Learning for Coders"),wi=r(" di "),X=t("a"),Li=r("fast.ai"),Pi=r(", oppure uno dei programmi sviluppati da "),Z=t("a"),yi=r("DeepLearning.AI"),$i=p(),A=t("li"),Ai=r("Non richiede conoscenze pregresse di "),ee=t("a"),Ni=r("PyTorch"),Ti=r(" o "),ae=t("a"),Ii=r("TensorFlow"),Si=r(", nonostante sia gradita una conoscenza anche superficiale dell\u2019uno o dell\u2019altro"),ma=p(),_=t("p"),Hi=r("Quando avrai completato questo corso, ti raccomandiamo di passare al "),ie=t("a"),ki=r("Natural Language Processing Specialization"),Mi=r(" di DeepLearning.AI, un corso che copre un ampio spettro di modelli tradizionali di NLP che vale davvero la pena di conoscere, come Naive Bayes e LSTM ("),Oe=t("em"),Fi=r("Memoria a breve termine a lungo termine"),Ci=r(")!"),ha=p(),N=t("h2"),F=t("a"),xe=t("span"),Le(re.$$.fragment),qi=p(),Be=t("span"),Di=r("Chi siamo?"),ga=p(),he=t("p"),Ri=r("A proposito degli autori:"),va=p(),oe=t("p"),Qe=t("strong"),Gi=r("Matthew Carrigan"),Oi=r(" \xE8 Machine Learning Engineer da Hugging Face. Vive a Dublino, in Irlanda, ed in passato \xE8 stato ML engineer da Parse.ly, e prima ancora ricercatore postdottorale al Trinity College di Dublin. Nonostante non creda che otterremo l\u2019Intelligenza artificiale forte semplicemente ingrandendo le architetture a nostra disposizione, spera comunque nell\u2019immortalit\xE0 cibernetica."),za=p(),te=t("p"),Ue=t("strong"),xi=r("Lysandre Debut"),Bi=r(" \xE8 Machine Learning Engineer da Hugging Face e ha lavorato agli \u{1F917} Transformer fin dalle primissime tappe del loro sviluppo. Il suo obiettivo \xE8 di rendere il NLP accessibile a tutti sviluppando strumenti con un semplice API."),_a=p(),g=t("p"),Je=t("strong"),Qi=r("Sylvain Gugger"),Ui=r(" \xE8 Research Engineer da Hugging Face e uno dei principali manutentori della libreria \u{1F917} Transformers. In passato, \xE8 stato Research Scientist da fast.ai, e ha scritto "),le=t("a"),Ji=r("Deep Learning for Coders with fastai and PyTorch"),Vi=r(" con Jeremy Howard. Il centro principale della sua ricerca consiste nel rendere il deep learning ("),Ve=t("em"),Wi=r("apprendimento profondo"),Yi=r(") pi\xF9 accessibile, concependo e migliorando tecniche che permettano di allenare modelli velocemente con risorse limitate."),Ea=p(),ne=t("p"),We=t("strong"),ji=r("Merve Noyan"),Ki=r(" \xE8 developer advocate da Hugging Face, e lavora allo sviluppo di strumenti e alla creazione di contenuti ad essi legati per democratizzare l\u2019accesso al deep learning."),ba=p(),se=t("p"),Ye=t("strong"),Xi=r("Lucile Saulnier"),Zi=r(" \xE8 machine learning engineer da Hugging Face, e sviluppa e supporta l\u2019utilizzo di strumenti open source. \xC8 anche attivamente coinvolta in numerosi progetti di ricerca nell\u2019ambito del NLP, come ad esempio collaborative training e BigScience."),wa=p(),T=t("p"),je=t("strong"),er=r("Lewis Tunstall"),ar=r(" \xE8 machine learning engineer da Hugging Face che si specializza nello sviluppo di strumenti open-source e la loro distribuzione alla comunit\xE0 pi\xF9 ampia. \xC8 anche co-autore dell\u2019imminente "),ce=t("a"),ir=r("O\u2019Reilly book on Transformers"),rr=r("."),La=p(),I=t("p"),Ke=t("strong"),or=r("Leandro von Werra"),tr=r(" \xE8 machine learning engineer nel team open-source di Hugging Face, nonch\xE9 co-autore dell\u2019imminente "),ue=t("a"),lr=r("O\u2019Reilly book on Transformers"),nr=r(". Ha tanti anni di esperienza nel portare progetti di NLP in produzione, lavorando a tutti i livelli di esecuzione di compiti di machine learning."),Pa=p(),ge=t("p"),sr=r("Sei pronto/a a iniziare? In questo capitolo, imparerai:"),ya=p(),E=t("ul"),pe=t("li"),cr=r("Ad utilizzare la funzione "),Xe=t("code"),ur=r("pipeline()"),pr=r(" per eseguire compiti di NLP come la generazione e classificazione di testi"),dr=p(),Ze=t("li"),fr=r("L\u2019architettura dei Transformer"),mr=p(),ea=t("li"),hr=r("Come fare la distinzione tra architetture encoder, decoder, encoder-decoder, e casi d\u2019uso"),this.h()},l(e){const c=Po('[data-svelte="svelte-1phssyn"]',document.head);b=l(c,"META",{name:!0,content:!0}),c.forEach(i),ra=d(e),w=l(e,"H1",{class:!0});var Aa=n(w);H=l(Aa,"A",{id:!0,class:!0,href:!0});var Lr=n(H);Te=l(Lr,"SPAN",{});var Pr=n(Te);Pe(C.$$.fragment,Pr),Pr.forEach(i),Lr.forEach(i),Fa=d(Aa),Ie=l(Aa,"SPAN",{});var yr=n(Ie);Ca=o(yr,"Introduzione"),yr.forEach(i),Aa.forEach(i),oa=d(e),Pe(q.$$.fragment,e),ta=d(e),L=l(e,"H2",{class:!0});var Na=n(L);k=l(Na,"A",{id:!0,class:!0,href:!0});var $r=n(k);Se=l($r,"SPAN",{});var Ar=n(Se);Pe(D.$$.fragment,Ar),Ar.forEach(i),$r.forEach(i),qa=d(Na),He=l(Na,"SPAN",{});var Nr=n(He);Da=o(Nr,"Benvenuto/a al corso di \u{1F917}!"),Nr.forEach(i),Na.forEach(i),la=d(e),Pe(R.$$.fragment,e),na=d(e),f=l(e,"P",{});var m=n(f);Ra=o(m,"Questo corso ti insegner\xE0 a eseguire compiti di Natural Language Processing (NLP, "),ke=l(m,"EM",{});var Tr=n(ke);Ga=o(Tr,"elaborazione del linguaggio naturale"),Tr.forEach(i),Oa=o(m,") utilizzando le librerie dell\u2019ecosistema di "),G=l(m,"A",{href:!0,rel:!0});var Ir=n(G);xa=o(Ir,"Hugging Face"),Ir.forEach(i),Ba=o(m,": "),O=l(m,"A",{href:!0,rel:!0});var Sr=n(O);Qa=o(Sr,"\u{1F917} Transformers"),Sr.forEach(i),Ua=o(m,", "),x=l(m,"A",{href:!0,rel:!0});var Hr=n(x);Ja=o(Hr,"\u{1F917} Datasets"),Hr.forEach(i),Va=o(m,", "),B=l(m,"A",{href:!0,rel:!0});var kr=n(B);Wa=o(kr,"\u{1F917} Tokenizers"),kr.forEach(i),Ya=o(m,", e "),Q=l(m,"A",{href:!0,rel:!0});var Mr=n(Q);ja=o(Mr,"\u{1F917} Accelerate"),Mr.forEach(i),Ka=o(m,". Ti insegneremo anche ad usare il nostro "),U=l(m,"A",{href:!0,rel:!0});var Fr=n(U);Xa=o(Fr,"Hugging Face Hub"),Fr.forEach(i),Za=o(m,", che \xE8 completamente gratuito e senza pubblicit\xE0."),m.forEach(i),sa=d(e),P=l(e,"H2",{class:!0});var Ta=n(P);M=l(Ta,"A",{id:!0,class:!0,href:!0});var Cr=n(M);Me=l(Cr,"SPAN",{});var qr=n(Me);Pe(J.$$.fragment,qr),qr.forEach(i),Cr.forEach(i),ei=d(Ta),Fe=l(Ta,"SPAN",{});var Dr=n(Fe);ai=o(Dr,"Contenuti"),Dr.forEach(i),Ta.forEach(i),ca=d(e),fe=l(e,"P",{});var Rr=n(fe);ii=o(Rr,"Eccoti un breve riassunto dei contenuti del corso:"),Rr.forEach(i),ua=d(e),y=l(e,"DIV",{class:!0});var Ia=n(y);V=l(Ia,"IMG",{class:!0,src:!0,alt:!0}),ri=d(Ia),W=l(Ia,"IMG",{class:!0,src:!0,alt:!0}),Ia.forEach(i),pa=d(e),v=l(e,"UL",{});var ve=n(v);Y=l(ve,"LI",{});var Sa=n(Y);oi=o(Sa,"I capitoli da 1 a 4 forniscono un\u2019introduzione ai concetti principali della libreria \u{1F917} Transformers. Alla fine di questa parte del corso, conoscerai come funzionano i modelli Transformers e saprai come utilizzare un modello dell\u2019"),j=l(Sa,"A",{href:!0,rel:!0});var Gr=n(j);ti=o(Gr,"Hugging Face Hub"),Gr.forEach(i),li=o(Sa,", affinarlo in un dataset, e condividere i tuoi risultati nell\u2019Hub!"),Sa.forEach(i),ni=d(ve),Ce=l(ve,"LI",{});var Or=n(Ce);si=o(Or,"I capitoli da 5 a 8 insegnano le basi degli \u{1F917} Dataset e degli \u{1F917} Tokenizer, per poi esplorare alcuni compiti classici di NLP. Alla fine di questa parte, saprai far fronte ai problemi di NLP pi\xF9 comuni in maniera autonoma."),Or.forEach(i),ci=d(ve),$=l(ve,"LI",{});var ze=n($);ui=o(ze,"I capitoli da 9 a 12 vanno oltre il Natural Language Processing, ed esplorano come i modelli Transformer possano essere utilizzati per affrontare compiti di elaborazione vocale o visione artificiale. Strada facendo, imparerai a costruire e condividere demo ("),qe=l(ze,"EM",{});var xr=n(qe);pi=o(xr,"dimostrazioni"),xr.forEach(i),di=o(ze,") dei tuoi modelli, e ad ottimizzarli per la produzione. Alla fine di questa parte, sarai pronto ad utilizzare gli \u{1F917} Transformer per qualsiasi problema di machine learning ("),De=l(ze,"EM",{});var Br=n(De);fi=o(Br,"apprendimento automatico"),Br.forEach(i),mi=o(ze,"), o quasi!"),ze.forEach(i),ve.forEach(i),da=d(e),me=l(e,"P",{});var Qr=n(me);hi=o(Qr,"Questo corso:"),Qr.forEach(i),fa=d(e),z=l(e,"UL",{});var _e=n(z);Re=l(_e,"LI",{});var Ur=n(Re);gi=o(Ur,"Richiede una buona conoscenza di Python"),Ur.forEach(i),vi=d(_e),h=l(_e,"LI",{});var S=n(h);zi=o(S,"Andrebbe seguito di preferenza a seguito di un corso introduttivo di deep learning ("),Ge=l(S,"EM",{});var Jr=n(Ge);_i=o(Jr,"apprendimento profondo"),Jr.forEach(i),Ei=o(S,"), come ad esempio il "),K=l(S,"A",{href:!0,rel:!0});var Vr=n(K);bi=o(Vr,"Practical Deep Learning for Coders"),Vr.forEach(i),wi=o(S," di "),X=l(S,"A",{href:!0,rel:!0});var Wr=n(X);Li=o(Wr,"fast.ai"),Wr.forEach(i),Pi=o(S,", oppure uno dei programmi sviluppati da "),Z=l(S,"A",{href:!0,rel:!0});var Yr=n(Z);yi=o(Yr,"DeepLearning.AI"),Yr.forEach(i),S.forEach(i),$i=d(_e),A=l(_e,"LI",{});var Ee=n(A);Ai=o(Ee,"Non richiede conoscenze pregresse di "),ee=l(Ee,"A",{href:!0,rel:!0});var jr=n(ee);Ni=o(jr,"PyTorch"),jr.forEach(i),Ti=o(Ee," o "),ae=l(Ee,"A",{href:!0,rel:!0});var Kr=n(ae);Ii=o(Kr,"TensorFlow"),Kr.forEach(i),Si=o(Ee,", nonostante sia gradita una conoscenza anche superficiale dell\u2019uno o dell\u2019altro"),Ee.forEach(i),_e.forEach(i),ma=d(e),_=l(e,"P",{});var be=n(_);Hi=o(be,"Quando avrai completato questo corso, ti raccomandiamo di passare al "),ie=l(be,"A",{href:!0,rel:!0});var Xr=n(ie);ki=o(Xr,"Natural Language Processing Specialization"),Xr.forEach(i),Mi=o(be," di DeepLearning.AI, un corso che copre un ampio spettro di modelli tradizionali di NLP che vale davvero la pena di conoscere, come Naive Bayes e LSTM ("),Oe=l(be,"EM",{});var Zr=n(Oe);Fi=o(Zr,"Memoria a breve termine a lungo termine"),Zr.forEach(i),Ci=o(be,")!"),be.forEach(i),ha=d(e),N=l(e,"H2",{class:!0});var Ha=n(N);F=l(Ha,"A",{id:!0,class:!0,href:!0});var eo=n(F);xe=l(eo,"SPAN",{});var ao=n(xe);Pe(re.$$.fragment,ao),ao.forEach(i),eo.forEach(i),qi=d(Ha),Be=l(Ha,"SPAN",{});var io=n(Be);Di=o(io,"Chi siamo?"),io.forEach(i),Ha.forEach(i),ga=d(e),he=l(e,"P",{});var ro=n(he);Ri=o(ro,"A proposito degli autori:"),ro.forEach(i),va=d(e),oe=l(e,"P",{});var gr=n(oe);Qe=l(gr,"STRONG",{});var oo=n(Qe);Gi=o(oo,"Matthew Carrigan"),oo.forEach(i),Oi=o(gr," \xE8 Machine Learning Engineer da Hugging Face. Vive a Dublino, in Irlanda, ed in passato \xE8 stato ML engineer da Parse.ly, e prima ancora ricercatore postdottorale al Trinity College di Dublin. Nonostante non creda che otterremo l\u2019Intelligenza artificiale forte semplicemente ingrandendo le architetture a nostra disposizione, spera comunque nell\u2019immortalit\xE0 cibernetica."),gr.forEach(i),za=d(e),te=l(e,"P",{});var vr=n(te);Ue=l(vr,"STRONG",{});var to=n(Ue);xi=o(to,"Lysandre Debut"),to.forEach(i),Bi=o(vr," \xE8 Machine Learning Engineer da Hugging Face e ha lavorato agli \u{1F917} Transformer fin dalle primissime tappe del loro sviluppo. Il suo obiettivo \xE8 di rendere il NLP accessibile a tutti sviluppando strumenti con un semplice API."),vr.forEach(i),_a=d(e),g=l(e,"P",{});var de=n(g);Je=l(de,"STRONG",{});var lo=n(Je);Qi=o(lo,"Sylvain Gugger"),lo.forEach(i),Ui=o(de," \xE8 Research Engineer da Hugging Face e uno dei principali manutentori della libreria \u{1F917} Transformers. In passato, \xE8 stato Research Scientist da fast.ai, e ha scritto "),le=l(de,"A",{href:!0,rel:!0});var no=n(le);Ji=o(no,"Deep Learning for Coders with fastai and PyTorch"),no.forEach(i),Vi=o(de," con Jeremy Howard. Il centro principale della sua ricerca consiste nel rendere il deep learning ("),Ve=l(de,"EM",{});var so=n(Ve);Wi=o(so,"apprendimento profondo"),so.forEach(i),Yi=o(de,") pi\xF9 accessibile, concependo e migliorando tecniche che permettano di allenare modelli velocemente con risorse limitate."),de.forEach(i),Ea=d(e),ne=l(e,"P",{});var zr=n(ne);We=l(zr,"STRONG",{});var co=n(We);ji=o(co,"Merve Noyan"),co.forEach(i),Ki=o(zr," \xE8 developer advocate da Hugging Face, e lavora allo sviluppo di strumenti e alla creazione di contenuti ad essi legati per democratizzare l\u2019accesso al deep learning."),zr.forEach(i),ba=d(e),se=l(e,"P",{});var _r=n(se);Ye=l(_r,"STRONG",{});var uo=n(Ye);Xi=o(uo,"Lucile Saulnier"),uo.forEach(i),Zi=o(_r," \xE8 machine learning engineer da Hugging Face, e sviluppa e supporta l\u2019utilizzo di strumenti open source. \xC8 anche attivamente coinvolta in numerosi progetti di ricerca nell\u2019ambito del NLP, come ad esempio collaborative training e BigScience."),_r.forEach(i),wa=d(e),T=l(e,"P",{});var aa=n(T);je=l(aa,"STRONG",{});var po=n(je);er=o(po,"Lewis Tunstall"),po.forEach(i),ar=o(aa," \xE8 machine learning engineer da Hugging Face che si specializza nello sviluppo di strumenti open-source e la loro distribuzione alla comunit\xE0 pi\xF9 ampia. \xC8 anche co-autore dell\u2019imminente "),ce=l(aa,"A",{href:!0,rel:!0});var fo=n(ce);ir=o(fo,"O\u2019Reilly book on Transformers"),fo.forEach(i),rr=o(aa,"."),aa.forEach(i),La=d(e),I=l(e,"P",{});var ia=n(I);Ke=l(ia,"STRONG",{});var mo=n(Ke);or=o(mo,"Leandro von Werra"),mo.forEach(i),tr=o(ia," \xE8 machine learning engineer nel team open-source di Hugging Face, nonch\xE9 co-autore dell\u2019imminente "),ue=l(ia,"A",{href:!0,rel:!0});var ho=n(ue);lr=o(ho,"O\u2019Reilly book on Transformers"),ho.forEach(i),nr=o(ia,". Ha tanti anni di esperienza nel portare progetti di NLP in produzione, lavorando a tutti i livelli di esecuzione di compiti di machine learning."),ia.forEach(i),Pa=d(e),ge=l(e,"P",{});var go=n(ge);sr=o(go,"Sei pronto/a a iniziare? In questo capitolo, imparerai:"),go.forEach(i),ya=d(e),E=l(e,"UL",{});var we=n(E);pe=l(we,"LI",{});var ka=n(pe);cr=o(ka,"Ad utilizzare la funzione "),Xe=l(ka,"CODE",{});var vo=n(Xe);ur=o(vo,"pipeline()"),vo.forEach(i),pr=o(ka," per eseguire compiti di NLP come la generazione e classificazione di testi"),ka.forEach(i),dr=d(we),Ze=l(we,"LI",{});var zo=n(Ze);fr=o(zo,"L\u2019architettura dei Transformer"),zo.forEach(i),mr=d(we),ea=l(we,"LI",{});var _o=n(ea);hr=o(_o,"Come fare la distinzione tra architetture encoder, decoder, encoder-decoder, e casi d\u2019uso"),_o.forEach(i),we.forEach(i),this.h()},h(){s(b,"name","hf:doc:metadata"),s(b,"content",JSON.stringify(Io)),s(H,"id","introduzione"),s(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(H,"href","#introduzione"),s(w,"class","relative group"),s(k,"id","benvenutoa-al-corso-di"),s(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(k,"href","#benvenutoa-al-corso-di"),s(L,"class","relative group"),s(G,"href","https://huggingface.co/"),s(G,"rel","nofollow"),s(O,"href","https://github.com/huggingface/transformers"),s(O,"rel","nofollow"),s(x,"href","https://github.com/huggingface/datasets"),s(x,"rel","nofollow"),s(B,"href","https://github.com/huggingface/tokenizers"),s(B,"rel","nofollow"),s(Q,"href","https://github.com/huggingface/accelerate"),s(Q,"rel","nofollow"),s(U,"href","https://huggingface.co/models"),s(U,"rel","nofollow"),s(M,"id","contenuti"),s(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(M,"href","#contenuti"),s(P,"class","relative group"),s(V,"class","block dark:hidden"),Eo(V.src,br="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary.svg")||s(V,"src",br),s(V,"alt","Brief overview of the chapters of the course."),s(W,"class","hidden dark:block"),Eo(W.src,wr="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/summary-dark.svg")||s(W,"src",wr),s(W,"alt","Brief overview of the chapters of the course."),s(y,"class","flex justify-center"),s(j,"href","https://huggingface.co/models"),s(j,"rel","nofollow"),s(K,"href","https://course.fast.ai/"),s(K,"rel","nofollow"),s(X,"href","https://www.fast.ai/"),s(X,"rel","nofollow"),s(Z,"href","https://www.deeplearning.ai/"),s(Z,"rel","nofollow"),s(ee,"href","https://pytorch.org/"),s(ee,"rel","nofollow"),s(ae,"href","https://www.tensorflow.org/"),s(ae,"rel","nofollow"),s(ie,"href","https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearning-ai&utm_medium=institutions&utm_campaign=20211011-nlp-2-hugging_face-page-nlp-refresh"),s(ie,"rel","nofollow"),s(F,"id","chi-siamo"),s(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(F,"href","#chi-siamo"),s(N,"class","relative group"),s(le,"href","https://learning.oreilly.com/library/view/deep-learning-for/9781492045519/"),s(le,"rel","nofollow"),s(ce,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098136789/"),s(ce,"rel","nofollow"),s(ue,"href","https://www.oreilly.com/library/view/natural-language-processing/9781098136789/"),s(ue,"rel","nofollow")},m(e,c){a(document.head,b),u(e,ra,c),u(e,w,c),a(w,H),a(H,Te),ye(C,Te,null),a(w,Fa),a(w,Ie),a(Ie,Ca),u(e,oa,c),ye(q,e,c),u(e,ta,c),u(e,L,c),a(L,k),a(k,Se),ye(D,Se,null),a(L,qa),a(L,He),a(He,Da),u(e,la,c),ye(R,e,c),u(e,na,c),u(e,f,c),a(f,Ra),a(f,ke),a(ke,Ga),a(f,Oa),a(f,G),a(G,xa),a(f,Ba),a(f,O),a(O,Qa),a(f,Ua),a(f,x),a(x,Ja),a(f,Va),a(f,B),a(B,Wa),a(f,Ya),a(f,Q),a(Q,ja),a(f,Ka),a(f,U),a(U,Xa),a(f,Za),u(e,sa,c),u(e,P,c),a(P,M),a(M,Me),ye(J,Me,null),a(P,ei),a(P,Fe),a(Fe,ai),u(e,ca,c),u(e,fe,c),a(fe,ii),u(e,ua,c),u(e,y,c),a(y,V),a(y,ri),a(y,W),u(e,pa,c),u(e,v,c),a(v,Y),a(Y,oi),a(Y,j),a(j,ti),a(Y,li),a(v,ni),a(v,Ce),a(Ce,si),a(v,ci),a(v,$),a($,ui),a($,qe),a(qe,pi),a($,di),a($,De),a(De,fi),a($,mi),u(e,da,c),u(e,me,c),a(me,hi),u(e,fa,c),u(e,z,c),a(z,Re),a(Re,gi),a(z,vi),a(z,h),a(h,zi),a(h,Ge),a(Ge,_i),a(h,Ei),a(h,K),a(K,bi),a(h,wi),a(h,X),a(X,Li),a(h,Pi),a(h,Z),a(Z,yi),a(z,$i),a(z,A),a(A,Ai),a(A,ee),a(ee,Ni),a(A,Ti),a(A,ae),a(ae,Ii),a(A,Si),u(e,ma,c),u(e,_,c),a(_,Hi),a(_,ie),a(ie,ki),a(_,Mi),a(_,Oe),a(Oe,Fi),a(_,Ci),u(e,ha,c),u(e,N,c),a(N,F),a(F,xe),ye(re,xe,null),a(N,qi),a(N,Be),a(Be,Di),u(e,ga,c),u(e,he,c),a(he,Ri),u(e,va,c),u(e,oe,c),a(oe,Qe),a(Qe,Gi),a(oe,Oi),u(e,za,c),u(e,te,c),a(te,Ue),a(Ue,xi),a(te,Bi),u(e,_a,c),u(e,g,c),a(g,Je),a(Je,Qi),a(g,Ui),a(g,le),a(le,Ji),a(g,Vi),a(g,Ve),a(Ve,Wi),a(g,Yi),u(e,Ea,c),u(e,ne,c),a(ne,We),a(We,ji),a(ne,Ki),u(e,ba,c),u(e,se,c),a(se,Ye),a(Ye,Xi),a(se,Zi),u(e,wa,c),u(e,T,c),a(T,je),a(je,er),a(T,ar),a(T,ce),a(ce,ir),a(T,rr),u(e,La,c),u(e,I,c),a(I,Ke),a(Ke,or),a(I,tr),a(I,ue),a(ue,lr),a(I,nr),u(e,Pa,c),u(e,ge,c),a(ge,sr),u(e,ya,c),u(e,E,c),a(E,pe),a(pe,cr),a(pe,Xe),a(Xe,ur),a(pe,pr),a(E,dr),a(E,Ze),a(Ze,fr),a(E,mr),a(E,ea),a(ea,hr),$a=!0},p:yo,i(e){$a||($e(C.$$.fragment,e),$e(q.$$.fragment,e),$e(D.$$.fragment,e),$e(R.$$.fragment,e),$e(J.$$.fragment,e),$e(re.$$.fragment,e),$a=!0)},o(e){Ae(C.$$.fragment,e),Ae(q.$$.fragment,e),Ae(D.$$.fragment,e),Ae(R.$$.fragment,e),Ae(J.$$.fragment,e),Ae(re.$$.fragment,e),$a=!1},d(e){i(b),e&&i(ra),e&&i(w),Ne(C),e&&i(oa),Ne(q,e),e&&i(ta),e&&i(L),Ne(D),e&&i(la),Ne(R,e),e&&i(na),e&&i(f),e&&i(sa),e&&i(P),Ne(J),e&&i(ca),e&&i(fe),e&&i(ua),e&&i(y),e&&i(pa),e&&i(v),e&&i(da),e&&i(me),e&&i(fa),e&&i(z),e&&i(ma),e&&i(_),e&&i(ha),e&&i(N),Ne(re),e&&i(ga),e&&i(he),e&&i(va),e&&i(oe),e&&i(za),e&&i(te),e&&i(_a),e&&i(g),e&&i(Ea),e&&i(ne),e&&i(ba),e&&i(se),e&&i(wa),e&&i(T),e&&i(La),e&&i(I),e&&i(Pa),e&&i(ge),e&&i(ya),e&&i(E)}}}const Io={local:"introduzione",sections:[{local:"benvenutoa-al-corso-di",title:"Benvenuto/a al corso di \u{1F917}!"},{local:"contenuti",title:"Contenuti"},{local:"chi-siamo",title:"Chi siamo?"}],title:"Introduzione"};function So(Er){return $o(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qo extends bo{constructor(b){super();wo(this,b,So,To,Lo,{})}}export{qo as default,Io as metadata};
