import{S as ze,i as Ce,s as Me,e as l,k as d,w as ne,t as c,M as qe,c as r,d as t,m,a as s,x as fe,h as p,b as i,G as o,g as n,y as de,L as Se,q as me,o as ce,B as pe,v as ke}from"../../chunks/vendor-hf-doc-builder.js";import{Y as Ge}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Qe}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ne}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function Re(ue){let u,G,h,_,b,g,O,z,j,Q,E,N,w,R,v,D,C,K,V,B,T,W,U,I,Z,X,x,ee,Y,f,M,y,te,oe,q,P,ae,le,S,A,re,se,k,L,ie,F;return g=new Qe({}),E=new Ne({props:{chapter:1,classNames:"absolute z-10 right-0 top-0"}}),w=new Ge({props:{id:"d_ixlCubqQw"}}),{c(){u=l("meta"),G=d(),h=l("h1"),_=l("a"),b=l("span"),ne(g.$$.fragment),O=d(),z=l("span"),j=c("Modelli decoder"),Q=d(),ne(E.$$.fragment),N=d(),ne(w.$$.fragment),R=d(),v=l("p"),D=c("I modelli decoder utilizzano solo il decoder di un modello Transformer. Ad ogni passaggio e per una data parola, gli attention layer hanno accesso solo alle parole che la precedono nella frase. Questi modelli sono spesso detti "),C=l("em"),K=c("auto-regressive models"),V=c("."),B=d(),T=l("p"),W=c("Il pre-addestramento dei modelli decoder ha spesso a che fare con la previsione della parola successiva in un contesto frasale."),U=d(),I=l("p"),Z=c("Questi modelli sono particolarmente adatti a compiti di generazione testuale."),X=d(),x=l("p"),ee=c("Alcuni rappresentanti di questa famiglia includono:"),Y=d(),f=l("ul"),M=l("li"),y=l("a"),te=c("CTRL"),oe=d(),q=l("li"),P=l("a"),ae=c("GPT"),le=d(),S=l("li"),A=l("a"),re=c("GPT-2"),se=d(),k=l("li"),L=l("a"),ie=c("Transformer XL"),this.h()},l(e){const a=qe('[data-svelte="svelte-1phssyn"]',document.head);u=r(a,"META",{name:!0,content:!0}),a.forEach(t),G=m(e),h=r(e,"H1",{class:!0});var H=s(h);_=r(H,"A",{id:!0,class:!0,href:!0});var he=s(_);b=r(he,"SPAN",{});var _e=s(b);fe(g.$$.fragment,_e),_e.forEach(t),he.forEach(t),O=m(H),z=r(H,"SPAN",{});var ve=s(z);j=p(ve,"Modelli decoder"),ve.forEach(t),H.forEach(t),Q=m(e),fe(E.$$.fragment,e),N=m(e),fe(w.$$.fragment,e),R=m(e),v=r(e,"P",{});var J=s(v);D=p(J,"I modelli decoder utilizzano solo il decoder di un modello Transformer. Ad ogni passaggio e per una data parola, gli attention layer hanno accesso solo alle parole che la precedono nella frase. Questi modelli sono spesso detti "),C=r(J,"EM",{});var $e=s(C);K=p($e,"auto-regressive models"),$e.forEach(t),V=p(J,"."),J.forEach(t),B=m(e),T=r(e,"P",{});var ge=s(T);W=p(ge,"Il pre-addestramento dei modelli decoder ha spesso a che fare con la previsione della parola successiva in un contesto frasale."),ge.forEach(t),U=m(e),I=r(e,"P",{});var Ee=s(I);Z=p(Ee,"Questi modelli sono particolarmente adatti a compiti di generazione testuale."),Ee.forEach(t),X=m(e),x=r(e,"P",{});var we=s(x);ee=p(we,"Alcuni rappresentanti di questa famiglia includono:"),we.forEach(t),Y=m(e),f=r(e,"UL",{});var $=s(f);M=r($,"LI",{});var ye=s(M);y=r(ye,"A",{href:!0,rel:!0});var Pe=s(y);te=p(Pe,"CTRL"),Pe.forEach(t),ye.forEach(t),oe=m($),q=r($,"LI",{});var Ae=s(q);P=r(Ae,"A",{href:!0,rel:!0});var Le=s(P);ae=p(Le,"GPT"),Le.forEach(t),Ae.forEach(t),le=m($),S=r($,"LI",{});var Te=s(S);A=r(Te,"A",{href:!0,rel:!0});var Ie=s(A);re=p(Ie,"GPT-2"),Ie.forEach(t),Te.forEach(t),se=m($),k=r($,"LI",{});var xe=s(k);L=r(xe,"A",{href:!0,rel:!0});var be=s(L);ie=p(be,"Transformer XL"),be.forEach(t),xe.forEach(t),$.forEach(t),this.h()},h(){i(u,"name","hf:doc:metadata"),i(u,"content",JSON.stringify(Be)),i(_,"id","modelli-decoder"),i(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(_,"href","#modelli-decoder"),i(h,"class","relative group"),i(y,"href","https://huggingface.co/transformers/model_doc/ctrl.html"),i(y,"rel","nofollow"),i(P,"href","https://huggingface.co/transformers/model_doc/gpt.html"),i(P,"rel","nofollow"),i(A,"href","https://huggingface.co/transformers/model_doc/gpt2.html"),i(A,"rel","nofollow"),i(L,"href","https://huggingface.co/transformers/model_doc/transfo-xl.html"),i(L,"rel","nofollow")},m(e,a){o(document.head,u),n(e,G,a),n(e,h,a),o(h,_),o(_,b),de(g,b,null),o(h,O),o(h,z),o(z,j),n(e,Q,a),de(E,e,a),n(e,N,a),de(w,e,a),n(e,R,a),n(e,v,a),o(v,D),o(v,C),o(C,K),o(v,V),n(e,B,a),n(e,T,a),o(T,W),n(e,U,a),n(e,I,a),o(I,Z),n(e,X,a),n(e,x,a),o(x,ee),n(e,Y,a),n(e,f,a),o(f,M),o(M,y),o(y,te),o(f,oe),o(f,q),o(q,P),o(P,ae),o(f,le),o(f,S),o(S,A),o(A,re),o(f,se),o(f,k),o(k,L),o(L,ie),F=!0},p:Se,i(e){F||(me(g.$$.fragment,e),me(E.$$.fragment,e),me(w.$$.fragment,e),F=!0)},o(e){ce(g.$$.fragment,e),ce(E.$$.fragment,e),ce(w.$$.fragment,e),F=!1},d(e){t(u),e&&t(G),e&&t(h),pe(g),e&&t(Q),pe(E,e),e&&t(N),pe(w,e),e&&t(R),e&&t(v),e&&t(B),e&&t(T),e&&t(U),e&&t(I),e&&t(X),e&&t(x),e&&t(Y),e&&t(f)}}}const Be={local:"modelli-decoder",title:"Modelli decoder"};function Ue(ue){return ke(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Oe extends ze{constructor(u){super();Ce(this,u,Ue,Re,Me,{})}}export{Oe as default,Be as metadata};
