import{S as yn,i as En,s as Tn,e as l,k as m,w as k,t as r,l as vn,M as In,c as p,d as s,m as f,x as z,a as c,h as u,b as $,G as t,g as o,y as _,o as g,p as $n,q as h,B as b,v as Pn,n as wn}from"../../chunks/vendor-hf-doc-builder.js";import{I as Us}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../../chunks/CodeBlock-hf-doc-builder.js";import{C as jn}from"../../chunks/CourseFloatingBanner-hf-doc-builder.js";import{F as Sn}from"../../chunks/FrameworkSwitchCourse-hf-doc-builder.js";import"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function An(j){let a,d;return a=new jn({props:{chapter:2,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_tf.ipynb"}]}}),{c(){k(a.$$.fragment)},l(i){z(a.$$.fragment,i)},m(i,q){_(a,i,q),d=!0},i(i){d||(h(a.$$.fragment,i),d=!0)},o(i){g(a.$$.fragment,i),d=!1},d(i){b(a,i)}}}function Cn(j){let a,d;return a=new jn({props:{chapter:2,classNames:"absolute z-10 right-0 top-0",notebooks:[{label:"Google Colab",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"},{label:"Aws Studio",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter2/section6_pt.ipynb"}]}}),{c(){k(a.$$.fragment)},l(i){z(a.$$.fragment,i)},m(i,q){_(a,i,q),d=!0},i(i){d||(h(a.$$.fragment,i),d=!0)},o(i){g(a.$$.fragment,i),d=!1},d(i){b(a,i)}}}function Fn(j){let a,d;return a=new w({props:{code:`import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="tf")
output = model(**tokens)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

tokens = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
output = model(**tokens)`}}),{c(){k(a.$$.fragment)},l(i){z(a.$$.fragment,i)},m(i,q){_(a,i,q),d=!0},i(i){d||(h(a.$$.fragment,i),d=!0)},o(i){g(a.$$.fragment,i),d=!1},d(i){b(a,i)}}}function Dn(j){let a,d;return a=new w({props:{code:`import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

tokens = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
output = model(**tokens)`}}),{c(){k(a.$$.fragment)},l(i){z(a.$$.fragment,i)},m(i,q){_(a,i,q),d=!0},i(i){d||(h(a.$$.fragment,i),d=!0)},o(i){g(a.$$.fragment,i),d=!1},d(i){b(a,i)}}}function xn(j){let a,d,i,q,C,x,he,B,rs,ke,ps,Ae,y,E,te,ie,us,Ce,H,cs,ze,ms,fs,Fe,O,De,P,ds,_e,gs,hs,be,ks,zs,xe,oe,_s,He,L,Ne,ae,bs,Re,Q,Me,le,qs,Be,G,Oe,re,vs,Le,U,Qe,v,$s,qe,ws,js,ve,ys,Es,$e,Ts,Is,we,Ps,Ss,Ge,J,Ue,F,N,je,K,As,ye,Cs,Je,pe,Fs,Ke,V,Ve,W,We,ue,Ds,Xe,X,Ye,Y,Ze,S,xs,Ee,Hs,Ns,Te,Rs,Ms,es,D,R,Ie,Z,Bs,Pe,Os,ss,M,Ls,Se,Qs,Gs,ns,T,I,ce,ts;i=new Sn({props:{fw:j[0]}}),B=new Us({});const Js=[Cn,An],ee=[];function Ks(e,n){return e[0]==="pt"?0:1}y=Ks(j),E=ee[y]=Js[y](j),O=new w({props:{code:`from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

checkpoint = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)`}}),L=new w({props:{code:`sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)`,highlighted:`sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)`}}),Q=new w({props:{code:`sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)`,highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

model_inputs = tokenizer(sequences)`}}),G=new w({props:{code:`# Effettua il padding della sequenza fino allla massima lunghezza della sequenza
model_inputs = tokenizer(sequences, padding="longest")

# Effettua il padding fino alla lunghezza massima del modello
# (512 per BERT o DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Effettua il padding fino alla lunghezza massima specificata
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)`,highlighted:`<span class="hljs-comment"># Effettua il padding della sequenza fino allla massima lunghezza della sequenza</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;longest&quot;</span>)

<span class="hljs-comment"># Effettua il padding fino alla lunghezza massima del modello</span>
<span class="hljs-comment"># (512 per BERT o DistilBERT)</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;max_length&quot;</span>)

<span class="hljs-comment"># Effettua il padding fino alla lunghezza massima specificata</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-string">&quot;max_length&quot;</span>, max_length=<span class="hljs-number">8</span>)`}}),U=new w({props:{code:`sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Tronca le sequenze pi\xF9 lunghe della lunghezza massima del modello.
# (512 per BERT o DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Tronca le sequenze pi\xF9 lunghe della lunghezza massima specificata.
model_inputs = tokenizer(sequences, max_length=8, truncation=True)`,highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

<span class="hljs-comment"># Tronca le sequenze pi\xF9 lunghe della lunghezza massima del modello.</span>
<span class="hljs-comment"># (512 per BERT o DistilBERT)</span>
model_inputs = tokenizer(sequences, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Tronca le sequenze pi\xF9 lunghe della lunghezza massima specificata.</span>
model_inputs = tokenizer(sequences, max_length=<span class="hljs-number">8</span>, truncation=<span class="hljs-literal">True</span>)`}}),J=new w({props:{code:`sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Ritorna tensori PyTorch
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Ritorna tensori TensorFlow
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# Ritorna NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")`,highlighted:`sequences = [<span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="hljs-string">&quot;So have I!&quot;</span>]

<span class="hljs-comment"># Ritorna tensori PyTorch</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-comment"># Ritorna tensori TensorFlow</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-comment"># Ritorna NumPy arrays</span>
model_inputs = tokenizer(sequences, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)`}}),K=new Us({}),V=new w({props:{code:`sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)`,highlighted:`sequence = <span class="hljs-string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>

model_inputs = tokenizer(sequence)
<span class="hljs-built_in">print</span>(model_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
<span class="hljs-built_in">print</span>(ids)`}}),W=new w({props:{code:`[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]`,highlighted:`[<span class="hljs-number">101</span>, <span class="hljs-number">1045</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">2310</span>, <span class="hljs-number">2042</span>, <span class="hljs-number">3403</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>, <span class="hljs-number">2878</span>, <span class="hljs-number">2166</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>]
[<span class="hljs-number">1045</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">2310</span>, <span class="hljs-number">2042</span>, <span class="hljs-number">3403</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">17662</span>, <span class="hljs-number">12172</span>, <span class="hljs-number">2607</span>, <span class="hljs-number">2026</span>, <span class="hljs-number">2878</span>, <span class="hljs-number">2166</span>, <span class="hljs-number">1012</span>]`}}),X=new w({props:{code:`print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))`,highlighted:`<span class="hljs-built_in">print</span>(tokenizer.decode(model_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]))
<span class="hljs-built_in">print</span>(tokenizer.decode(ids))`}}),Y=new w({props:{code:`"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."`,highlighted:`<span class="hljs-string">&quot;[CLS] i&#x27;ve been waiting for a huggingface course my whole life. [SEP]&quot;</span>
<span class="hljs-string">&quot;i&#x27;ve been waiting for a huggingface course my whole life.&quot;</span>`}}),Z=new Us({});const Vs=[Dn,Fn],se=[];function Ws(e,n){return e[0]==="pt"?0:1}return T=Ws(j),I=se[T]=Vs[T](j),{c(){a=l("meta"),d=m(),k(i.$$.fragment),q=m(),C=l("h1"),x=l("a"),he=l("span"),k(B.$$.fragment),rs=m(),ke=l("span"),ps=r("Mettiamo insieme i pezzi"),Ae=m(),E.c(),te=m(),ie=l("p"),us=r("Nelle ultime sezioni abbiamo fatto del nostro meglio per fare la maggior parte del lavoro a mano. Abbiamo esplorato il funzionamento dei tokenizer e abbiamo esaminato la tokenizzazione, la conversione in ID di input, il padding, il troncamento e le maschere di attenzione."),Ce=m(),H=l("p"),cs=r("Tuttavia, come abbiamo visto nella sezione 2, l\u2019API \u{1F917} Transformers pu\xF2 gestire tutto questo con una funzione di alto livello che approfondiremo qui. Quando si chiama il "),ze=l("code"),ms=r("tokenizer"),fs=r(" direttamente sulla frase, si ottengono input pronti per passare attraverso il modello:"),Fe=m(),k(O.$$.fragment),De=m(),P=l("p"),ds=r("Qui, la variabile "),_e=l("code"),gs=r("model_inputs"),hs=r(" contiene tutto ci\xF2 che \xE8 necessario per il buon funzionamento del modello. Per DistilBERT, questo include gli ID degli ingressi e la maschera di attenzione. Altri modelli che accettano input aggiuntivi avranno anche questi output dall\u2019oggetto "),be=l("code"),ks=r("tokenizer"),zs=r("."),xe=m(),oe=l("p"),_s=r("Come vedremo in alcuni esempi, questo metodo \xE8 molto potente. Innanzitutto, pu\xF2 tokenizzare una singola sequenza:"),He=m(),k(L.$$.fragment),Ne=m(),ae=l("p"),bs=r("Gestisce anche pi\xF9 sequenze alla volta, senza alcuna modifica dell\u2019API:"),Re=m(),k(Q.$$.fragment),Me=m(),le=l("p"),qs=r("Possiamo implementare il padding in diversi modi"),Be=m(),k(G.$$.fragment),Oe=m(),re=l("p"),vs=r("Pu\xF2 anche troncare le sequenze:"),Le=m(),k(U.$$.fragment),Qe=m(),v=l("p"),$s=r("L\u2019oggetto "),qe=l("code"),ws=r("tokenizer"),js=r(" pu\xF2 gestire la conversione in tensori di framework specifici, che possono successivamente essere inviati direttamente al modello. Per esempio, nel seguente esempio di codice si chiede al tokenizer di restituire i tensori dei diversi framework: "),ve=l("code"),ys=r('"pt"'),Es=r(" restituisce i tensori di PyTorch, "),$e=l("code"),Ts=r('"tf"'),Is=r(" restituisce i tensori di TensorFlow e "),we=l("code"),Ps=r('"np"'),Ss=r(" restituisce gli array di NumPy:"),Ge=m(),k(J.$$.fragment),Ue=m(),F=l("h2"),N=l("a"),je=l("span"),k(K.$$.fragment),As=m(),ye=l("span"),Cs=r("Token speciali"),Je=m(),pe=l("p"),Fs=r("Se diamo un\u2019occhiata agli ID di input restituiti dal tokenizer, noteremo che sono leggermente diversi da quelli che avevamo prima:"),Ke=m(),k(V.$$.fragment),Ve=m(),k(W.$$.fragment),We=m(),ue=l("p"),Ds=r("Un ID token \xE8 stato aggiunto all\u2019inizio e uno alla fine. Decodifichiamo le due sequenze di ID qui sopra per capire di cosa si tratta:"),Xe=m(),k(X.$$.fragment),Ye=m(),k(Y.$$.fragment),Ze=m(),S=l("p"),xs=r("Il tokenizer ha aggiunto la parola speciale "),Ee=l("code"),Hs=r("[CLS]"),Ns=r(" all\u2019inizio e la parola speciale "),Te=l("code"),Rs=r("[SEP]"),Ms=r(" alla fine. Questo perch\xE9 il modello \xE8 stato preaddestrato con queste parole, quindi per ottenere gli stessi risultati per l\u2019inferenza dobbiamo aggiungerle anche noi. Si noti che alcuni modelli non aggiungono parole speciali, o ne aggiungono di diverse; i modelli possono anche aggiungere queste parole speciali solo all\u2019inizio o solo alla fine. In ogni caso, il tokenizer sa quali sono previste e se ne occuper\xE0 per voi."),es=m(),D=l("h2"),R=l("a"),Ie=l("span"),k(Z.$$.fragment),Bs=m(),Pe=l("span"),Os=r("Conclusione: Dal tokenizer al modello"),ss=m(),M=l("p"),Ls=r("Ora che abbiamo visto tutti i singoli passaggi che l\u2019oggetto "),Se=l("code"),Qs=r("tokenizer"),Gs=r(" utilizza quando viene applicato ai testi, vediamo un\u2019ultima volta come pu\xF2 gestire sequenze multiple (padding!), sequenze molto lunghe (troncamento!) e diversi tipi di tensori con la sua API principale:"),ns=m(),I.c(),ce=vn(),this.h()},l(e){const n=In('[data-svelte="svelte-1phssyn"]',document.head);a=p(n,"META",{name:!0,content:!0}),n.forEach(s),d=f(e),z(i.$$.fragment,e),q=f(e),C=p(e,"H1",{class:!0});var ne=c(C);x=p(ne,"A",{id:!0,class:!0,href:!0});var me=c(x);he=p(me,"SPAN",{});var fe=c(he);z(B.$$.fragment,fe),fe.forEach(s),me.forEach(s),rs=f(ne),ke=p(ne,"SPAN",{});var Xs=c(ke);ps=u(Xs,"Mettiamo insieme i pezzi"),Xs.forEach(s),ne.forEach(s),Ae=f(e),E.l(e),te=f(e),ie=p(e,"P",{});var Ys=c(ie);us=u(Ys,"Nelle ultime sezioni abbiamo fatto del nostro meglio per fare la maggior parte del lavoro a mano. Abbiamo esplorato il funzionamento dei tokenizer e abbiamo esaminato la tokenizzazione, la conversione in ID di input, il padding, il troncamento e le maschere di attenzione."),Ys.forEach(s),Ce=f(e),H=p(e,"P",{});var is=c(H);cs=u(is,"Tuttavia, come abbiamo visto nella sezione 2, l\u2019API \u{1F917} Transformers pu\xF2 gestire tutto questo con una funzione di alto livello che approfondiremo qui. Quando si chiama il "),ze=p(is,"CODE",{});var Zs=c(ze);ms=u(Zs,"tokenizer"),Zs.forEach(s),fs=u(is," direttamente sulla frase, si ottengono input pronti per passare attraverso il modello:"),is.forEach(s),Fe=f(e),z(O.$$.fragment,e),De=f(e),P=p(e,"P",{});var de=c(P);ds=u(de,"Qui, la variabile "),_e=p(de,"CODE",{});var en=c(_e);gs=u(en,"model_inputs"),en.forEach(s),hs=u(de," contiene tutto ci\xF2 che \xE8 necessario per il buon funzionamento del modello. Per DistilBERT, questo include gli ID degli ingressi e la maschera di attenzione. Altri modelli che accettano input aggiuntivi avranno anche questi output dall\u2019oggetto "),be=p(de,"CODE",{});var sn=c(be);ks=u(sn,"tokenizer"),sn.forEach(s),zs=u(de,"."),de.forEach(s),xe=f(e),oe=p(e,"P",{});var nn=c(oe);_s=u(nn,"Come vedremo in alcuni esempi, questo metodo \xE8 molto potente. Innanzitutto, pu\xF2 tokenizzare una singola sequenza:"),nn.forEach(s),He=f(e),z(L.$$.fragment,e),Ne=f(e),ae=p(e,"P",{});var tn=c(ae);bs=u(tn,"Gestisce anche pi\xF9 sequenze alla volta, senza alcuna modifica dell\u2019API:"),tn.forEach(s),Re=f(e),z(Q.$$.fragment,e),Me=f(e),le=p(e,"P",{});var on=c(le);qs=u(on,"Possiamo implementare il padding in diversi modi"),on.forEach(s),Be=f(e),z(G.$$.fragment,e),Oe=f(e),re=p(e,"P",{});var an=c(re);vs=u(an,"Pu\xF2 anche troncare le sequenze:"),an.forEach(s),Le=f(e),z(U.$$.fragment,e),Qe=f(e),v=p(e,"P",{});var A=c(v);$s=u(A,"L\u2019oggetto "),qe=p(A,"CODE",{});var ln=c(qe);ws=u(ln,"tokenizer"),ln.forEach(s),js=u(A," pu\xF2 gestire la conversione in tensori di framework specifici, che possono successivamente essere inviati direttamente al modello. Per esempio, nel seguente esempio di codice si chiede al tokenizer di restituire i tensori dei diversi framework: "),ve=p(A,"CODE",{});var rn=c(ve);ys=u(rn,'"pt"'),rn.forEach(s),Es=u(A," restituisce i tensori di PyTorch, "),$e=p(A,"CODE",{});var pn=c($e);Ts=u(pn,'"tf"'),pn.forEach(s),Is=u(A," restituisce i tensori di TensorFlow e "),we=p(A,"CODE",{});var un=c(we);Ps=u(un,'"np"'),un.forEach(s),Ss=u(A," restituisce gli array di NumPy:"),A.forEach(s),Ge=f(e),z(J.$$.fragment,e),Ue=f(e),F=p(e,"H2",{class:!0});var os=c(F);N=p(os,"A",{id:!0,class:!0,href:!0});var cn=c(N);je=p(cn,"SPAN",{});var mn=c(je);z(K.$$.fragment,mn),mn.forEach(s),cn.forEach(s),As=f(os),ye=p(os,"SPAN",{});var fn=c(ye);Cs=u(fn,"Token speciali"),fn.forEach(s),os.forEach(s),Je=f(e),pe=p(e,"P",{});var dn=c(pe);Fs=u(dn,"Se diamo un\u2019occhiata agli ID di input restituiti dal tokenizer, noteremo che sono leggermente diversi da quelli che avevamo prima:"),dn.forEach(s),Ke=f(e),z(V.$$.fragment,e),Ve=f(e),z(W.$$.fragment,e),We=f(e),ue=p(e,"P",{});var gn=c(ue);Ds=u(gn,"Un ID token \xE8 stato aggiunto all\u2019inizio e uno alla fine. Decodifichiamo le due sequenze di ID qui sopra per capire di cosa si tratta:"),gn.forEach(s),Xe=f(e),z(X.$$.fragment,e),Ye=f(e),z(Y.$$.fragment,e),Ze=f(e),S=p(e,"P",{});var ge=c(S);xs=u(ge,"Il tokenizer ha aggiunto la parola speciale "),Ee=p(ge,"CODE",{});var hn=c(Ee);Hs=u(hn,"[CLS]"),hn.forEach(s),Ns=u(ge," all\u2019inizio e la parola speciale "),Te=p(ge,"CODE",{});var kn=c(Te);Rs=u(kn,"[SEP]"),kn.forEach(s),Ms=u(ge," alla fine. Questo perch\xE9 il modello \xE8 stato preaddestrato con queste parole, quindi per ottenere gli stessi risultati per l\u2019inferenza dobbiamo aggiungerle anche noi. Si noti che alcuni modelli non aggiungono parole speciali, o ne aggiungono di diverse; i modelli possono anche aggiungere queste parole speciali solo all\u2019inizio o solo alla fine. In ogni caso, il tokenizer sa quali sono previste e se ne occuper\xE0 per voi."),ge.forEach(s),es=f(e),D=p(e,"H2",{class:!0});var as=c(D);R=p(as,"A",{id:!0,class:!0,href:!0});var zn=c(R);Ie=p(zn,"SPAN",{});var _n=c(Ie);z(Z.$$.fragment,_n),_n.forEach(s),zn.forEach(s),Bs=f(as),Pe=p(as,"SPAN",{});var bn=c(Pe);Os=u(bn,"Conclusione: Dal tokenizer al modello"),bn.forEach(s),as.forEach(s),ss=f(e),M=p(e,"P",{});var ls=c(M);Ls=u(ls,"Ora che abbiamo visto tutti i singoli passaggi che l\u2019oggetto "),Se=p(ls,"CODE",{});var qn=c(Se);Qs=u(qn,"tokenizer"),qn.forEach(s),Gs=u(ls," utilizza quando viene applicato ai testi, vediamo un\u2019ultima volta come pu\xF2 gestire sequenze multiple (padding!), sequenze molto lunghe (troncamento!) e diversi tipi di tensori con la sua API principale:"),ls.forEach(s),ns=f(e),I.l(e),ce=vn(),this.h()},h(){$(a,"name","hf:doc:metadata"),$(a,"content",JSON.stringify(Hn)),$(x,"id","mettiamo-insieme-i-pezzi"),$(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(x,"href","#mettiamo-insieme-i-pezzi"),$(C,"class","relative group"),$(N,"id","token-speciali"),$(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(N,"href","#token-speciali"),$(F,"class","relative group"),$(R,"id","conclusione-dal-tokenizer-al-modello"),$(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(R,"href","#conclusione-dal-tokenizer-al-modello"),$(D,"class","relative group")},m(e,n){t(document.head,a),o(e,d,n),_(i,e,n),o(e,q,n),o(e,C,n),t(C,x),t(x,he),_(B,he,null),t(C,rs),t(C,ke),t(ke,ps),o(e,Ae,n),ee[y].m(e,n),o(e,te,n),o(e,ie,n),t(ie,us),o(e,Ce,n),o(e,H,n),t(H,cs),t(H,ze),t(ze,ms),t(H,fs),o(e,Fe,n),_(O,e,n),o(e,De,n),o(e,P,n),t(P,ds),t(P,_e),t(_e,gs),t(P,hs),t(P,be),t(be,ks),t(P,zs),o(e,xe,n),o(e,oe,n),t(oe,_s),o(e,He,n),_(L,e,n),o(e,Ne,n),o(e,ae,n),t(ae,bs),o(e,Re,n),_(Q,e,n),o(e,Me,n),o(e,le,n),t(le,qs),o(e,Be,n),_(G,e,n),o(e,Oe,n),o(e,re,n),t(re,vs),o(e,Le,n),_(U,e,n),o(e,Qe,n),o(e,v,n),t(v,$s),t(v,qe),t(qe,ws),t(v,js),t(v,ve),t(ve,ys),t(v,Es),t(v,$e),t($e,Ts),t(v,Is),t(v,we),t(we,Ps),t(v,Ss),o(e,Ge,n),_(J,e,n),o(e,Ue,n),o(e,F,n),t(F,N),t(N,je),_(K,je,null),t(F,As),t(F,ye),t(ye,Cs),o(e,Je,n),o(e,pe,n),t(pe,Fs),o(e,Ke,n),_(V,e,n),o(e,Ve,n),_(W,e,n),o(e,We,n),o(e,ue,n),t(ue,Ds),o(e,Xe,n),_(X,e,n),o(e,Ye,n),_(Y,e,n),o(e,Ze,n),o(e,S,n),t(S,xs),t(S,Ee),t(Ee,Hs),t(S,Ns),t(S,Te),t(Te,Rs),t(S,Ms),o(e,es,n),o(e,D,n),t(D,R),t(R,Ie),_(Z,Ie,null),t(D,Bs),t(D,Pe),t(Pe,Os),o(e,ss,n),o(e,M,n),t(M,Ls),t(M,Se),t(Se,Qs),t(M,Gs),o(e,ns,n),se[T].m(e,n),o(e,ce,n),ts=!0},p(e,[n]){const ne={};n&1&&(ne.fw=e[0]),i.$set(ne);let me=y;y=Ks(e),y!==me&&(wn(),g(ee[me],1,1,()=>{ee[me]=null}),$n(),E=ee[y],E||(E=ee[y]=Js[y](e),E.c()),h(E,1),E.m(te.parentNode,te));let fe=T;T=Ws(e),T!==fe&&(wn(),g(se[fe],1,1,()=>{se[fe]=null}),$n(),I=se[T],I||(I=se[T]=Vs[T](e),I.c()),h(I,1),I.m(ce.parentNode,ce))},i(e){ts||(h(i.$$.fragment,e),h(B.$$.fragment,e),h(E),h(O.$$.fragment,e),h(L.$$.fragment,e),h(Q.$$.fragment,e),h(G.$$.fragment,e),h(U.$$.fragment,e),h(J.$$.fragment,e),h(K.$$.fragment,e),h(V.$$.fragment,e),h(W.$$.fragment,e),h(X.$$.fragment,e),h(Y.$$.fragment,e),h(Z.$$.fragment,e),h(I),ts=!0)},o(e){g(i.$$.fragment,e),g(B.$$.fragment,e),g(E),g(O.$$.fragment,e),g(L.$$.fragment,e),g(Q.$$.fragment,e),g(G.$$.fragment,e),g(U.$$.fragment,e),g(J.$$.fragment,e),g(K.$$.fragment,e),g(V.$$.fragment,e),g(W.$$.fragment,e),g(X.$$.fragment,e),g(Y.$$.fragment,e),g(Z.$$.fragment,e),g(I),ts=!1},d(e){s(a),e&&s(d),b(i,e),e&&s(q),e&&s(C),b(B),e&&s(Ae),ee[y].d(e),e&&s(te),e&&s(ie),e&&s(Ce),e&&s(H),e&&s(Fe),b(O,e),e&&s(De),e&&s(P),e&&s(xe),e&&s(oe),e&&s(He),b(L,e),e&&s(Ne),e&&s(ae),e&&s(Re),b(Q,e),e&&s(Me),e&&s(le),e&&s(Be),b(G,e),e&&s(Oe),e&&s(re),e&&s(Le),b(U,e),e&&s(Qe),e&&s(v),e&&s(Ge),b(J,e),e&&s(Ue),e&&s(F),b(K),e&&s(Je),e&&s(pe),e&&s(Ke),b(V,e),e&&s(Ve),b(W,e),e&&s(We),e&&s(ue),e&&s(Xe),b(X,e),e&&s(Ye),b(Y,e),e&&s(Ze),e&&s(S),e&&s(es),e&&s(D),b(Z),e&&s(ss),e&&s(M),e&&s(ns),se[T].d(e),e&&s(ce)}}}const Hn={local:"mettiamo-insieme-i-pezzi",sections:[{local:"token-speciali",title:"Token speciali"},{local:"conclusione-dal-tokenizer-al-modello",title:"Conclusione: Dal tokenizer al modello"}],title:"Mettiamo insieme i pezzi"};function Nn(j,a,d){let i="pt";return Pn(()=>{const q=new URLSearchParams(window.location.search);d(0,i=q.get("fw")||"pt")}),[i]}class Gn extends yn{constructor(a){super();En(this,a,Nn,xn,Tn,{})}}export{Gn as default,Hn as metadata};
