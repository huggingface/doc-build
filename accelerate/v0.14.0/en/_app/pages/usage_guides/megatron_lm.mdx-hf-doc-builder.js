import{S as Mu,i as ju,s as $u,e as o,k as d,w as m,t as s,M as Tu,c as n,d as a,m as h,a as l,x as u,h as r,b as c,G as t,g as p,y as _,L as Pu,q as g,o as f,B as v,v as Lu}from"../../chunks/vendor-hf-doc-builder.js";import{I as he}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as y}from"../../chunks/CodeBlock-hf-doc-builder.js";function qu(gd){let Z,jr,ee,me,ya,ze,hn,ka,mn,$r,T,Ce,un,_n,Se,gn,fn,Ie,vn,bn,Ge,wn,yn,Ne,kn,En,Tr,te,ue,Ea,Re,Mn,Ma,jn,Pr,aa,$n,Lr,M,Tn,ja,Pn,Ln,$a,qn,xn,Ta,On,Dn,Fe,An,zn,Be,Cn,Sn,qr,E,In,Pa,Gn,Nn,La,Rn,Fn,qa,Bn,Un,xa,Wn,Hn,Ue,Vn,Yn,We,Qn,Kn,xr,b,Xn,Oa,Jn,Zn,Da,el,tl,Aa,al,sl,za,rl,ol,Ca,nl,ll,Sa,il,pl,Ia,cl,dl,Ga,hl,ml,Na,ul,_l,He,gl,fl,Or,A,vl,Ra,bl,wl,Ve,yl,kl,Ye,El,Ml,Dr,H,jl,Fa,$l,Tl,Qe,Pl,Ll,Ar,_e,ql,Ba,xl,Ol,zr,L,Dl,Ua,Al,zl,Wa,Cl,Sl,Ha,Il,Gl,Va,Nl,Rl,Cr,ge,Fl,Ya,Bl,Ul,Sr,ae,fe,Qa,Ke,Wl,Ka,Hl,Ir,V,Vl,Xe,Yl,Ql,Je,Kl,Xl,Gr,sa,Jl,Nr,ra,Xa,Zl,Rr,Ze,Fr,et,Ja,ei,Br,tt,Ur,at,Za,ti,Wr,st,Hr,rt,es,ai,Vr,ot,Yr,se,ve,ts,nt,si,as,ri,Qr,be,oi,ss,ni,li,Kr,lt,Xr,oa,ii,Jr,it,Zr,we,pi,rs,ci,di,eo,na,pt,hi,os,mi,ui,to,ct,ao,dt,ns,_i,so,ht,ro,mt,ls,gi,oo,ut,no,_t,la,fi,is,vi,lo,gt,io,Y,bi,ps,wi,yi,cs,ki,Ei,po,ft,co,ia,Mi,ho,vt,mo,ye,ji,ds,$i,Ti,uo,re,ke,hs,bt,Pi,ms,Li,_o,pa,qi,go,ca,O,xi,us,Oi,Di,_s,Ai,zi,gs,Ci,Si,fs,Ii,Gi,fo,wt,vo,yt,vs,Ni,bo,q,Ri,bs,Fi,Bi,ws,Ui,Wi,ys,Hi,Vi,kt,Yi,Qi,wo,Et,yo,Q,Ki,ks,Xi,Ji,Es,Zi,ep,ko,Mt,Eo,j,tp,Ms,ap,sp,js,rp,op,$s,np,lp,Ts,ip,pp,Ps,cp,dp,Mo,jt,jo,oe,Ee,Ls,$t,hp,qs,mp,$o,Me,xs,da,up,Tt,_p,gp,Os,Ds,fp,To,Pt,Po,Lt,I,vp,As,bp,wp,zs,yp,kp,Cs,Ep,Mp,Lo,qt,qo,ne,je,Ss,xt,jp,Ot,$p,Is,Tp,Pp,xo,ha,Dt,Lp,Gs,qp,xp,Oo,At,Do,zt,$e,Ns,Op,Dp,Rs,Ap,zp,Ao,Ct,zo,St,P,Cp,Fs,Sp,Ip,It,Gp,Np,Gt,Rp,Fp,Nt,Bp,Up,Rt,Wp,Hp,Co,le,Te,Bs,Ft,Vp,Us,Yp,So,ma,G,Qp,Ws,Kp,Xp,Hs,Jp,Zp,Vs,ec,tc,Io,Bt,Go,Ut,N,ac,Ys,sc,rc,Qs,oc,nc,Ks,lc,ic,No,Wt,Ro,ie,Pe,Xs,Ht,pc,Js,cc,Fo,k,Zs,er,dc,hc,tr,R,mc,ar,uc,_c,sr,gc,fc,rr,vc,bc,wc,or,F,yc,nr,kc,Ec,lr,Mc,jc,ir,$c,Tc,Pc,pr,pe,Lc,cr,qc,xc,dr,Oc,Dc,Ac,hr,mr,zc,Cc,ur,ua,_r,Sc,Ic,Gc,gr,ce,Nc,fr,Rc,Fc,vr,Bc,Uc,Wc,br,wr,Hc,Bo,B,Vc,Vt,Yc,Qc,yr,Kc,Xc,Yt,Jc,Uo,U,Zc,Qt,ed,td,kr,ad,sd,Kt,rd,Wo,D,od,Xt,nd,ld,Er,id,pd,Jt,cd,dd,Zt,hd,Ho;return ze=new he({}),Re=new he({}),Ke=new he({}),Ze=new y({props:{code:"conda create --name ml",highlighted:'conda <span class="hljs-built_in">create</span> <span class="hljs-comment">--name ml</span>'}}),tt=new y({props:{code:"conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch",highlighted:'<span class="hljs-attribute">conda</span> install pytorch torchvision torchaudio cudatoolkit=<span class="hljs-number">11</span>.<span class="hljs-number">3</span> -c pytorch'}}),st=new y({props:{code:`git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
cd ..`,highlighted:`git clone https:<span class="hljs-string">//github.com/NVIDIA/apex</span>
<span class="hljs-keyword">cd</span> apex
pip install -v <span class="hljs-params">--disable-pip-version-check</span> <span class="hljs-params">--no-cache-dir</span> <span class="hljs-params">--global-option=</span><span class="hljs-string">&quot;--cpp_ext&quot;</span> <span class="hljs-params">--global-option=</span><span class="hljs-string">&quot;--cuda_ext&quot;</span> <span class="hljs-string">./</span>
<span class="hljs-keyword">cd</span> <span class="hljs-string">..</span>`}}),ot=new y({props:{code:"pip install git+https://github.com/huggingface/Megatron-LM.git",highlighted:'pip install git+https:<span class="hljs-regexp">//gi</span>thub.com<span class="hljs-regexp">/huggingface/</span>Megatron-LM.git'}}),nt=new he({}),lt=new y({props:{code:`:~$ accelerate config --config_file "megatron_gpt_config.yaml"
In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0
Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU [4] MPS): 2
How many different machines will you use (use more than 1 for multi-node training)? [1]: 
Do you want to use DeepSpeed? [yes/NO]: 
Do you want to use FullyShardedDataParallel? [yes/NO]: 
Do you want to use Megatron-LM ? [yes/NO]: yes
What is the Tensor Parallelism degree/size? [1]:2
Do you want to enable Sequence Parallelism? [YES/no]: 
What is the Pipeline Parallelism degree/size? [1]:2
What is the number of micro-batches? [1]:2
Do you want to enable selective activation recomputation? [YES/no]: 
Do you want to use distributed optimizer which shards optimizer state and gradients across data pralellel ranks? [YES/no]: 
What is the gradient clipping value based on global L2 Norm (0 to disable)? [1.0]: 
How many GPU(s) should be used for distributed training? [1]:4
Do you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: bf16`,highlighted:`:~$ accelerate config --config_file <span class="hljs-string">&quot;megatron_gpt_config.yaml&quot;</span>
In <span class="hljs-built_in">which</span> compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0
Which <span class="hljs-built_in">type</span> of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU [4] MPS): 2
How many different machines will you use (use more than 1 <span class="hljs-keyword">for</span> multi-node training)? [1]: 
Do you want to use DeepSpeed? [<span class="hljs-built_in">yes</span>/NO]: 
Do you want to use FullyShardedDataParallel? [<span class="hljs-built_in">yes</span>/NO]: 
Do you want to use Megatron-LM ? [<span class="hljs-built_in">yes</span>/NO]: <span class="hljs-built_in">yes</span>
What is the Tensor Parallelism degree/size? [1]:2
Do you want to <span class="hljs-built_in">enable</span> Sequence Parallelism? [YES/no]: 
What is the Pipeline Parallelism degree/size? [1]:2
What is the number of micro-batches? [1]:2
Do you want to <span class="hljs-built_in">enable</span> selective activation recomputation? [YES/no]: 
Do you want to use distributed optimizer <span class="hljs-built_in">which</span> shards optimizer state and gradients across data pralellel ranks? [YES/no]: 
What is the gradient clipping value based on global L2 Norm (0 to <span class="hljs-built_in">disable</span>)? [1.0]: 
How many GPU(s) should be used <span class="hljs-keyword">for</span> distributed training? [1]:4
Do you wish to use FP16 or BF16 (mixed precision)? [NO/fp16/bf16]: bf16`}}),it=new y({props:{code:`~$ cat megatron_gpt_config.yaml 
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: MEGATRON_LM
downcast_bf16: 'no'
fsdp_config: {}
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
megatron_lm_config:
  megatron_lm_gradient_clipping: 1.0
  megatron_lm_num_micro_batches: 2
  megatron_lm_pp_degree: 2
  megatron_lm_recompute_activations: true
  megatron_lm_sequence_parallelism: true
  megatron_lm_tp_degree: 2
  megatron_lm_use_distributed_optimizer: true
mixed_precision: bf16
num_machines: 1
num_processes: 4
rdzv_backend: static
same_network: true
use_cpu: false`,highlighted:`<span class="hljs-string">~$</span> <span class="hljs-string">cat</span> <span class="hljs-string">megatron_gpt_config.yaml</span> 
<span class="hljs-attr">compute_environment:</span> <span class="hljs-string">LOCAL_MACHINE</span>
<span class="hljs-attr">deepspeed_config:</span> {}
<span class="hljs-attr">distributed_type:</span> <span class="hljs-string">MEGATRON_LM</span>
<span class="hljs-attr">downcast_bf16:</span> <span class="hljs-string">&#x27;no&#x27;</span>
<span class="hljs-attr">fsdp_config:</span> {}
<span class="hljs-attr">machine_rank:</span> <span class="hljs-number">0</span>
<span class="hljs-attr">main_process_ip:</span> <span class="hljs-literal">null</span>
<span class="hljs-attr">main_process_port:</span> <span class="hljs-literal">null</span>
<span class="hljs-attr">main_training_function:</span> <span class="hljs-string">main</span>
<span class="hljs-attr">megatron_lm_config:</span>
  <span class="hljs-attr">megatron_lm_gradient_clipping:</span> <span class="hljs-number">1.0</span>
  <span class="hljs-attr">megatron_lm_num_micro_batches:</span> <span class="hljs-number">2</span>
  <span class="hljs-attr">megatron_lm_pp_degree:</span> <span class="hljs-number">2</span>
  <span class="hljs-attr">megatron_lm_recompute_activations:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">megatron_lm_sequence_parallelism:</span> <span class="hljs-literal">true</span>
  <span class="hljs-attr">megatron_lm_tp_degree:</span> <span class="hljs-number">2</span>
  <span class="hljs-attr">megatron_lm_use_distributed_optimizer:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">mixed_precision:</span> <span class="hljs-string">bf16</span>
<span class="hljs-attr">num_machines:</span> <span class="hljs-number">1</span>
<span class="hljs-attr">num_processes:</span> <span class="hljs-number">4</span>
<span class="hljs-attr">rdzv_backend:</span> <span class="hljs-string">static</span>
<span class="hljs-attr">same_network:</span> <span class="hljs-literal">true</span>
<span class="hljs-attr">use_cpu:</span> <span class="hljs-literal">false</span>`}}),ct=new y({props:{code:`from accelerate.utils import MegatronLMDummyScheduler

if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    lr_scheduler = MegatronLMDummyScheduler(
        optimizer=optimizer,
        total_num_steps=args.max_train_steps,
        warmup_num_steps=args.num_warmup_steps,
    )
else:
    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
    )`,highlighted:`<span class="hljs-keyword">from</span> accelerate.utils <span class="hljs-keyword">import</span> MegatronLMDummyScheduler

<span class="hljs-keyword">if</span> accelerator.distributed_type == DistributedType.MEGATRON_LM:
    lr_scheduler = MegatronLMDummyScheduler(
        optimizer=optimizer,
        total_num_steps=args.max_train_steps,
        warmup_num_steps=args.num_warmup_steps,
    )
<span class="hljs-keyword">else</span>:
    lr_scheduler = get_scheduler(
        name=args.lr_scheduler_type,
        optimizer=optimizer,
        num_warmup_steps=args.num_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
    )`}}),ht=new y({props:{code:`if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    total_batch_size = accelerator.state.megatron_lm_plugin.global_batch_size
else:
    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps`,highlighted:`<span class="hljs-keyword">if</span> accelerator.distributed_type == DistributedType.MEGATRON_LM:
    total_batch_size = accelerator.state.megatron_lm_plugin.global_batch_size
<span class="hljs-keyword">else</span>:
    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps`}}),ut=new y({props:{code:`if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    losses.append(loss)
else:
    losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))

if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    losses = torch.tensor(losses)
else:
    losses = torch.cat(losses)`,highlighted:`<span class="hljs-keyword">if</span> accelerator.distributed_type == DistributedType.MEGATRON_LM:
    losses.append(loss)
<span class="hljs-keyword">else</span>:
    losses.append(accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size)))

<span class="hljs-keyword">if</span> accelerator.distributed_type == DistributedType.MEGATRON_LM:
    losses = torch.tensor(losses)
<span class="hljs-keyword">else</span>:
    losses = torch.cat(losses)`}}),gt=new y({props:{code:`if accelerator.distributed_type == DistributedType.MEGATRON_LM:
    accelerator.save_state(args.output_dir)
else:
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(
        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
    )`,highlighted:`<span class="hljs-keyword">if</span> accelerator.distributed_type == DistributedType.MEGATRON_LM:
    accelerator.save_state(args.output_dir)
<span class="hljs-keyword">else</span>:
    unwrapped_model = accelerator.unwrap_model(model)
    unwrapped_model.save_pretrained(
        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save
    )`}}),ft=new y({props:{code:`accelerate launch --config_file megatron_gpt_config.yaml \\
examples/by_feature/megatron_lm_gpt_pretraining.py \\
--config_name "gpt2-large" \\
--tokenizer_name "gpt2-large" \\
--dataset_name wikitext \\
--dataset_config_name wikitext-2-raw-v1 \\
--block_size 1024 \\
--learning_rate 5e-5 \\
--per_device_train_batch_size 24 \\
--per_device_eval_batch_size 24 \\
--num_train_epochs 5 \\
--with_tracking \\
--report_to "wandb" \\
--output_dir "awesome_model"`,highlighted:`accelerate launch --config_file megatron_gpt_config.yaml \\
examples/by_feature/megatron_lm_gpt_pretraining.py \\
--config_name <span class="hljs-string">&quot;gpt2-large&quot;</span> \\
--tokenizer_name <span class="hljs-string">&quot;gpt2-large&quot;</span> \\
--dataset_name wikitext \\
--dataset_config_name wikitext-2-raw-v1 \\
--block_size 1024 \\
--learning_rate 5e-5 \\
--per_device_train_batch_size 24 \\
--per_device_eval_batch_size 24 \\
--num_train_epochs 5 \\
--with_tracking \\
--report_to <span class="hljs-string">&quot;wandb&quot;</span> \\
--output_dir <span class="hljs-string">&quot;awesome_model&quot;</span>`}}),vt=new y({props:{code:"done with compiling and loading fused kernels. Compilation time: 3.569 seconds",highlighted:`Loading extension module fused_dense_cuda...
&gt;&gt;&gt; <span class="hljs-keyword">done</span> with compiling and loading fused kernels. Compilation time: 3.569 seconds
 &gt; padded vocab (size: 50257) with 175 dummy tokens (new size: 50432)
Building gpt model <span class="hljs-keyword">in</span> the pre-training mode.
The Megatron LM model weights are initialized at random <span class="hljs-keyword">in</span> \`accelerator.prepare\`. Please use \`accelerator.load_checkpoint\` to load a pre-trained checkpoint matching the distributed setup.
Preparing dataloader
Preparing dataloader
Preparing model
 &gt; number of parameters on (tensor, pipeline) model parallel rank (1, 0): 210753280
 &gt; number of parameters on (tensor, pipeline) model parallel rank (1, 1): 209445120
 &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 0): 210753280
 &gt; number of parameters on (tensor, pipeline) model parallel rank (0, 1): 209445120
Preparing optimizer
Preparing scheduler
&gt; learning rate decay style: linear
10/10/2022 22:57:22 - INFO - __main__ - ***** Running training *****
10/10/2022 22:57:22 - INFO - __main__ -   Num examples = 2318
10/10/2022 22:57:22 - INFO - __main__ -   Num Epochs = 5
10/10/2022 22:57:22 - INFO - __main__ -   Instantaneous batch size per device = 24
10/10/2022 22:57:22 - INFO - __main__ -   Total train batch size (w. parallel, distributed &amp; accumulation) = 48
10/10/2022 22:57:22 - INFO - __main__ -   Gradient Accumulation steps = 1
10/10/2022 22:57:22 - INFO - __main__ -   Total optimization steps = 245
 20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258D                                                 | 49/245 [01:04&lt;04:09,  1.27s/it]
 10/10/2022 22:58:29 - INFO - __main__ - epoch 0: perplexity: 1222.1594275215962 eval_loss: 7.10837459564209
 40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A                                     | 98/245 [02:10&lt;03:07,  1.28s/it]
 10/10/2022 22:59:35 - INFO - __main__ - epoch 1: perplexity: 894.5236583794557 eval_loss: 6.796291351318359
 60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258C                        | 147/245 [03:16&lt;02:05,  1.28s/it]
 10/10/2022 23:00:40 - INFO - __main__ - epoch 2: perplexity: 702.8458788508042 eval_loss: 6.555137634277344
 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258A            | 196/245 [04:22&lt;01:02,  1.28s/it]
 10/10/2022 23:01:46 - INFO - __main__ - epoch 3: perplexity: 600.3220028695281 eval_loss: 6.39746618270874
100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 245/245 [05:27&lt;00:00,  1.28s/it]`}}),bt=new he({}),wt=new y({props:{code:`from accelerate.utils import MegatronLMDummyScheduler, GPTTrainStep, avg_losses_across_data_parallel_group

# Custom loss function for the Megatron model
class GPTTrainStepWithCustomLoss(GPTTrainStep):
    def __init__(self, megatron_args, **kwargs):
        super().__init__(megatron_args)
        self.kwargs = kwargs

    def get_loss_func(self):
        def loss_func(inputs, loss_mask, output_tensor):
            batch_size, seq_length = output_tensor.shape
            losses = output_tensor.float()
            loss_mask = loss_mask.view(-1).float()
            loss = losses.view(-1) * loss_mask

            # Resize and average loss per sample
            loss_per_sample = loss.view(batch_size, seq_length).sum(axis=1)
            loss_mask_per_sample = loss_mask.view(batch_size, seq_length).sum(axis=1)
            loss_per_sample = loss_per_sample / loss_mask_per_sample

            # Calculate and scale weighting
            weights = torch.stack([(inputs == kt).float() for kt in self.kwargs["keytoken_ids"]]).sum(axis=[0, 2])
            weights = 1.0 + self.kwargs["alpha"] * weights
            # Calculate weighted average
            weighted_loss = (loss_per_sample * weights).mean()

            # Reduce loss across data parallel groups
            averaged_loss = avg_losses_across_data_parallel_group([weighted_loss])

            return weighted_loss, {"lm loss": averaged_loss[0]}

        return loss_func

    def get_forward_step_func(self):
        def forward_step(data_iterator, model):
            """Forward step."""
            # Get the batch.
            tokens, labels, loss_mask, attention_mask, position_ids = self.get_batch(data_iterator)
            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)

            return output_tensor, partial(self.loss_func, tokens, loss_mask)

        return forward_step


def main():
    # Custom loss function for the Megatron model
    keytoken_ids = []
    keywords = ["plt", "pd", "sk", "fit", "predict", " plt", " pd", " sk", " fit", " predict"]
    for keyword in keywords:
        ids = tokenizer([keyword]).input_ids[0]
        if len(ids) == 1:
            keytoken_ids.append(ids[0])
    accelerator.print(f"Keytoken ids: {keytoken_ids}")
    accelerator.state.megatron_lm_plugin.custom_train_step_class = GPTTrainStepWithCustomLoss
    accelerator.state.megatron_lm_plugin.custom_train_step_kwargs = {
        "keytoken_ids": keytoken_ids,
        "alpha": 0.25,
    }`,highlighted:`<span class="hljs-keyword">from</span> accelerate.utils <span class="hljs-keyword">import</span> MegatronLMDummyScheduler, GPTTrainStep, avg_losses_across_data_parallel_group

<span class="hljs-comment"># Custom loss function for the Megatron model</span>
<span class="hljs-keyword">class</span> <span class="hljs-title class_">GPTTrainStepWithCustomLoss</span>(<span class="hljs-title class_ inherited__">GPTTrainStep</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, megatron_args, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(megatron_args)
        self.kwargs = kwargs

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_loss_func</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">def</span> <span class="hljs-title function_">loss_func</span>(<span class="hljs-params">inputs, loss_mask, output_tensor</span>):
            batch_size, seq_length = output_tensor.shape
            losses = output_tensor.<span class="hljs-built_in">float</span>()
            loss_mask = loss_mask.view(-<span class="hljs-number">1</span>).<span class="hljs-built_in">float</span>()
            loss = losses.view(-<span class="hljs-number">1</span>) * loss_mask

            <span class="hljs-comment"># Resize and average loss per sample</span>
            loss_per_sample = loss.view(batch_size, seq_length).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)
            loss_mask_per_sample = loss_mask.view(batch_size, seq_length).<span class="hljs-built_in">sum</span>(axis=<span class="hljs-number">1</span>)
            loss_per_sample = loss_per_sample / loss_mask_per_sample

            <span class="hljs-comment"># Calculate and scale weighting</span>
            weights = torch.stack([(inputs == kt).<span class="hljs-built_in">float</span>() <span class="hljs-keyword">for</span> kt <span class="hljs-keyword">in</span> self.kwargs[<span class="hljs-string">&quot;keytoken_ids&quot;</span>]]).<span class="hljs-built_in">sum</span>(axis=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>])
            weights = <span class="hljs-number">1.0</span> + self.kwargs[<span class="hljs-string">&quot;alpha&quot;</span>] * weights
            <span class="hljs-comment"># Calculate weighted average</span>
            weighted_loss = (loss_per_sample * weights).mean()

            <span class="hljs-comment"># Reduce loss across data parallel groups</span>
            averaged_loss = avg_losses_across_data_parallel_group([weighted_loss])

            <span class="hljs-keyword">return</span> weighted_loss, {<span class="hljs-string">&quot;lm loss&quot;</span>: averaged_loss[<span class="hljs-number">0</span>]}

        <span class="hljs-keyword">return</span> loss_func

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_forward_step_func</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_step</span>(<span class="hljs-params">data_iterator, model</span>):
            <span class="hljs-string">&quot;&quot;&quot;Forward step.&quot;&quot;&quot;</span>
            <span class="hljs-comment"># Get the batch.</span>
            tokens, labels, loss_mask, attention_mask, position_ids = self.get_batch(data_iterator)
            output_tensor = model(tokens, position_ids, attention_mask, labels=labels)

            <span class="hljs-keyword">return</span> output_tensor, partial(self.loss_func, tokens, loss_mask)

        <span class="hljs-keyword">return</span> forward_step


<span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():
    <span class="hljs-comment"># Custom loss function for the Megatron model</span>
    keytoken_ids = []
    keywords = [<span class="hljs-string">&quot;plt&quot;</span>, <span class="hljs-string">&quot;pd&quot;</span>, <span class="hljs-string">&quot;sk&quot;</span>, <span class="hljs-string">&quot;fit&quot;</span>, <span class="hljs-string">&quot;predict&quot;</span>, <span class="hljs-string">&quot; plt&quot;</span>, <span class="hljs-string">&quot; pd&quot;</span>, <span class="hljs-string">&quot; sk&quot;</span>, <span class="hljs-string">&quot; fit&quot;</span>, <span class="hljs-string">&quot; predict&quot;</span>]
    <span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> keywords:
        ids = tokenizer([keyword]).input_ids[<span class="hljs-number">0</span>]
        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(ids) == <span class="hljs-number">1</span>:
            keytoken_ids.append(ids[<span class="hljs-number">0</span>])
    accelerator.<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Keytoken ids: <span class="hljs-subst">{keytoken_ids}</span>&quot;</span>)
    accelerator.state.megatron_lm_plugin.custom_train_step_class = GPTTrainStepWithCustomLoss
    accelerator.state.megatron_lm_plugin.custom_train_step_kwargs = {
        <span class="hljs-string">&quot;keytoken_ids&quot;</span>: keytoken_ids,
        <span class="hljs-string">&quot;alpha&quot;</span>: <span class="hljs-number">0.25</span>,
    }`}}),Et=new y({props:{code:`from accelerate.utils import MegatronLMDummyDataLoader

megatron_dataloader_config = {
    "data_path": args.data_path,
    "splits_string": args.splits_string,
    "seq_length": args.block_size,
    "micro_batch_size": args.per_device_train_batch_size,
}
megatron_dataloader = MegatronLMDummyDataLoader(**megatron_dataloader_config)
accelerator.state.megatron_lm_plugin.megatron_dataset_flag = True`,highlighted:`<span class="hljs-keyword">from</span> accelerate.utils <span class="hljs-keyword">import</span> MegatronLMDummyDataLoader

megatron_dataloader_config = {
    <span class="hljs-string">&quot;data_path&quot;</span>: args.data_path,
    <span class="hljs-string">&quot;splits_string&quot;</span>: args.splits_string,
    <span class="hljs-string">&quot;seq_length&quot;</span>: args.block_size,
    <span class="hljs-string">&quot;micro_batch_size&quot;</span>: args.per_device_train_batch_size,
}
megatron_dataloader = MegatronLMDummyDataLoader(**megatron_dataloader_config)
accelerator.state.megatron_lm_plugin.megatron_dataset_flag = <span class="hljs-literal">True</span>`}}),Mt=new y({props:{code:`model, optimizer, lr_scheduler, train_dataloader, eval_dataloader, _ = accelerator.prepare(
    model, optimizer, lr_scheduler, megatron_dataloader, megatron_dataloader, megatron_dataloader
)`,highlighted:`model, optimizer, lr_scheduler, train_dataloader, eval_dataloader, _ = accelerator.prepare(
    model, optimizer, lr_scheduler, megatron_dataloader, megatron_dataloader, megatron_dataloader
)`}}),jt=new y({props:{code:`while completed_steps < args.max_train_steps:
    model.train()
    batch = next(train_dataloader) if train_dataloader is not None else {}
    outputs = model(**batch)
    loss = outputs.loss
    ...

    if completed_steps % eval_interval == 0:
        eval_completed_steps = 0
        losses = []
        while eval_completed_steps < eval_iters:
            model.eval()
            with torch.no_grad():
                batch = next(eval_dataloader) if eval_dataloader is not None else {}
                outputs = model(**batch)`,highlighted:`<span class="hljs-keyword">while</span> completed_steps &lt; args.max_train_steps:
    model.train()
    batch = <span class="hljs-built_in">next</span>(train_dataloader) <span class="hljs-keyword">if</span> train_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> {}
    outputs = model(**batch)
    loss = outputs.loss
    ...

    <span class="hljs-keyword">if</span> completed_steps % eval_interval == <span class="hljs-number">0</span>:
        eval_completed_steps = <span class="hljs-number">0</span>
        losses = []
        <span class="hljs-keyword">while</span> eval_completed_steps &lt; eval_iters:
            model.<span class="hljs-built_in">eval</span>()
            <span class="hljs-keyword">with</span> torch.no_grad():
                batch = <span class="hljs-built_in">next</span>(eval_dataloader) <span class="hljs-keyword">if</span> eval_dataloader <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> {}
                outputs = model(**batch)`}}),$t=new he({}),Pt=new y({props:{code:`python checkpoint_reshaping_and_interoperability.py \\
--convert_checkpoint_from_megatron_to_transformers \\
--load_path "gpt/iter_0005000" \\
--save_path "gpt/trfs_checkpoint" \\
--max_shard_size "200MB" \\
--tokenizer_name "gpt2" \\
--print-checkpoint-structure`,highlighted:`python checkpoint_reshaping_and_interoperability.py \\
--convert_checkpoint_from_megatron_to_transformers \\
--load_path <span class="hljs-string">&quot;gpt/iter_0005000&quot;</span> \\
--save_path <span class="hljs-string">&quot;gpt/trfs_checkpoint&quot;</span> \\
--max_shard_size <span class="hljs-string">&quot;200MB&quot;</span> \\
--tokenizer_name <span class="hljs-string">&quot;gpt2&quot;</span> \\
--print-checkpoint-structure`}}),qt=new y({props:{code:`python checkpoint_utils/megatgron_gpt2/checkpoint_reshaping_and_interoperability.py \\
--load_path "gpt/trfs_checkpoint" \\
--save_path "gpt/megatron_lm_checkpoint" \\
--target_tensor_model_parallel_size 2 \\
--target_pipeline_model_parallel_size 2 \\
--target_data_parallel_size 2 \\
--target_params_dtype "bf16" \\
--make_vocab_size_divisible_by 128 \\
--use_distributed_optimizer \\
--print-checkpoint-structure`,highlighted:`python checkpoint_utils/megatgron_gpt2/checkpoint_reshaping_and_interoperability.py \\
--load_path <span class="hljs-string">&quot;gpt/trfs_checkpoint&quot;</span> \\
--save_path <span class="hljs-string">&quot;gpt/megatron_lm_checkpoint&quot;</span> \\
--target_tensor_model_parallel_size 2 \\
--target_pipeline_model_parallel_size 2 \\
--target_data_parallel_size 2 \\
--target_params_dtype <span class="hljs-string">&quot;bf16&quot;</span> \\
--make_vocab_size_divisible_by 128 \\
--use_distributed_optimizer \\
--print-checkpoint-structure`}}),xt=new he({}),At=new y({props:{code:"megatron_lm_plugin = MegatronLMPlugin(return_logits=True)",highlighted:'megatron_lm_plugin = MegatronLMPlugin(return_logits=<span class="hljs-literal">True</span>)'}}),Ct=new y({props:{code:`# specifying tokenizer's vocab and merges file
vocab_file = os.path.join(args.resume_from_checkpoint, "vocab.json")
merge_file = os.path.join(args.resume_from_checkpoint, "merges.txt")
other_megatron_args = {"vocab_file": vocab_file, "merge_file": merge_file}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)

# inference using \`megatron_generate\` functionality
tokenizer.pad_token = tokenizer.eos_token
max_new_tokens = 64
batch_texts = [
    "Are you human?",
    "The purpose of life is",
    "The arsenal was constructed at the request of",
    "How are you doing these days?",
]
batch_encodings = tokenizer(batch_texts, return_tensors="pt", padding=True)

# top-p sampling
generated_tokens = model.megatron_generate(
    batch_encodings["input_ids"],
    batch_encodings["attention_mask"],
    max_new_tokens=max_new_tokens,
    top_p=0.8,
    top_p_decay=0.5,
    temperature=0.9,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# top-k sampling
generated_tokens = model.megatron_generate(
    batch_encodings["input_ids"],
    batch_encodings["attention_mask"],
    max_new_tokens=max_new_tokens,
    top_k=50,
    temperature=0.9,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# adding \`bos\` token at the start
generated_tokens = model.megatron_generate(
    batch_encodings["input_ids"], batch_encodings["attention_mask"], max_new_tokens=max_new_tokens, add_BOS=True
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)

# beam search => only takes single prompt
batch_texts = ["The purpose of life is"]
batch_encodings = tokenizer(batch_texts, return_tensors="pt", padding=True)
generated_tokens = model.megatron_generate(
    batch_encodings["input_ids"],
    batch_encodings["attention_mask"],
    max_new_tokens=max_new_tokens,
    num_beams=20,
    length_penalty=1.5,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.print(decoded_preds)`,highlighted:`<span class="hljs-comment"># specifying tokenizer&#x27;s vocab and merges file</span>
vocab_file = os.path.join(args.resume_from_checkpoint, <span class="hljs-string">&quot;vocab.json&quot;</span>)
merge_file = os.path.join(args.resume_from_checkpoint, <span class="hljs-string">&quot;merges.txt&quot;</span>)
other_megatron_args = {<span class="hljs-string">&quot;vocab_file&quot;</span>: vocab_file, <span class="hljs-string">&quot;merge_file&quot;</span>: merge_file}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)

<span class="hljs-comment"># inference using \`megatron_generate\` functionality</span>
tokenizer.pad_token = tokenizer.eos_token
max_new_tokens = <span class="hljs-number">64</span>
batch_texts = [
    <span class="hljs-string">&quot;Are you human?&quot;</span>,
    <span class="hljs-string">&quot;The purpose of life is&quot;</span>,
    <span class="hljs-string">&quot;The arsenal was constructed at the request of&quot;</span>,
    <span class="hljs-string">&quot;How are you doing these days?&quot;</span>,
]
batch_encodings = tokenizer(batch_texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># top-p sampling</span>
generated_tokens = model.megatron_generate(
    batch_encodings[<span class="hljs-string">&quot;input_ids&quot;</span>],
    batch_encodings[<span class="hljs-string">&quot;attention_mask&quot;</span>],
    max_new_tokens=max_new_tokens,
    top_p=<span class="hljs-number">0.8</span>,
    top_p_decay=<span class="hljs-number">0.5</span>,
    temperature=<span class="hljs-number">0.9</span>,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.<span class="hljs-built_in">print</span>(decoded_preds)

<span class="hljs-comment"># top-k sampling</span>
generated_tokens = model.megatron_generate(
    batch_encodings[<span class="hljs-string">&quot;input_ids&quot;</span>],
    batch_encodings[<span class="hljs-string">&quot;attention_mask&quot;</span>],
    max_new_tokens=max_new_tokens,
    top_k=<span class="hljs-number">50</span>,
    temperature=<span class="hljs-number">0.9</span>,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.<span class="hljs-built_in">print</span>(decoded_preds)

<span class="hljs-comment"># adding \`bos\` token at the start</span>
generated_tokens = model.megatron_generate(
    batch_encodings[<span class="hljs-string">&quot;input_ids&quot;</span>], batch_encodings[<span class="hljs-string">&quot;attention_mask&quot;</span>], max_new_tokens=max_new_tokens, add_BOS=<span class="hljs-literal">True</span>
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.<span class="hljs-built_in">print</span>(decoded_preds)

<span class="hljs-comment"># beam search =&gt; only takes single prompt</span>
batch_texts = [<span class="hljs-string">&quot;The purpose of life is&quot;</span>]
batch_encodings = tokenizer(batch_texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
generated_tokens = model.megatron_generate(
    batch_encodings[<span class="hljs-string">&quot;input_ids&quot;</span>],
    batch_encodings[<span class="hljs-string">&quot;attention_mask&quot;</span>],
    max_new_tokens=max_new_tokens,
    num_beams=<span class="hljs-number">20</span>,
    length_penalty=<span class="hljs-number">1.5</span>,
)
decoded_preds = tokenizer.batch_decode(generated_tokens.cpu().numpy())
accelerator.<span class="hljs-built_in">print</span>(decoded_preds)`}}),Ft=new he({}),Bt=new y({props:{code:`other_megatron_args = {"position_embedding_type": "alibi"}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)`,highlighted:`other_megatron_args = {<span class="hljs-string">&quot;position_embedding_type&quot;</span>: <span class="hljs-string">&quot;alibi&quot;</span>}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)`}}),Wt=new y({props:{code:`other_megatron_args = {"attention_head_type": "multiquery"}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)`,highlighted:`other_megatron_args = {<span class="hljs-string">&quot;attention_head_type&quot;</span>: <span class="hljs-string">&quot;multiquery&quot;</span>}
megatron_lm_plugin = MegatronLMPlugin(other_megatron_args=other_megatron_args)`}}),Ht=new he({}),{c(){Z=o("meta"),jr=d(),ee=o("h1"),me=o("a"),ya=o("span"),m(ze.$$.fragment),hn=d(),ka=o("span"),mn=s("Megatron-LM"),$r=d(),T=o("p"),Ce=o("a"),un=s("Megatron-LM"),_n=s(` enables training large transformer language models at scale.
It provides efficient tensor, pipeline and sequence based model parallelism for pre-training transformer based
Language Models such as `),Se=o("a"),gn=s("GPT"),fn=s(" (Decoder Only), "),Ie=o("a"),vn=s("BERT"),bn=s(" (Encoder Only) and "),Ge=o("a"),wn=s("T5"),yn=s(` (Encoder-Decoder).
For detailed information and how things work behind the scene please refer the github `),Ne=o("a"),kn=s("repo"),En=s("."),Tr=d(),te=o("h2"),ue=o("a"),Ea=o("span"),m(Re.$$.fragment),Mn=d(),Ma=o("span"),jn=s("What is integrated?"),Pr=d(),aa=o("p"),$n=s(`Accelerate integrates following feature of Megatron-LM to enable large scale pre-training/finetuning
of BERT (Encoder), GPT (Decoder) or T5 models (Encoder and Decoder):`),Lr=d(),M=o("p"),Tn=s("a. "),ja=o("strong"),Pn=s("Tensor Parallelism (TP)"),Ln=s(`: Reduces memory footprint without much additional communication on intra-node ranks.
Each tensor is split into multiple chunks with each shard residing on separate GPU. At each step, the same mini-batch of data is processed
independently and in parallel by each shard followed by syncing across all GPUs (`),$a=o("code"),qn=s("all-reduce"),xn=s(` operation).
In a simple transformer layer, this leads to 2 `),Ta=o("code"),On=s("all-reduces"),Dn=s(` in the forward path and 2 in the backward path.
For more details, please refer research paper `),Fe=o("a"),An=s(`Megatron-LM: Training Multi-Billion Parameter Language Models Using
Model Parallelism`),zn=s(` and
this section of \u{1F917} blogpost `),Be=o("a"),Cn=s("The Technology Behind BLOOM Training"),Sn=s("."),qr=d(),E=o("p"),In=s("b. "),Pa=o("strong"),Gn=s("Pipeline Parallelism (PP)"),Nn=s(`: Reduces memory footprint and enables large scale training via inter-node parallelization.
Reduces the bubble of naive PP via PipeDream-Flush schedule/1F1B schedule and Interleaved 1F1B schedule.
Layers are distributed uniformly across PP stages. For example, if a model has `),La=o("code"),Rn=s("24"),Fn=s(" layers and we have "),qa=o("code"),Bn=s("4"),Un=s(` GPUs for
pipeline parallelism, each GPU will have `),xa=o("code"),Wn=s("6"),Hn=s(` layers (24/4). For more details on schedules to reduce the idle time of PP,
please refer to the research paper `),Ue=o("a"),Vn=s(`Efficient Large-Scale Language Model Training on GPU Clusters
Using Megatron-LM`),Yn=s(` and
this section of \u{1F917} blogpost `),We=o("a"),Qn=s("The Technology Behind BLOOM Training"),Kn=s("."),xr=d(),b=o("p"),Xn=s("c. "),Oa=o("strong"),Jn=s("Sequence Parallelism (SP)"),Zn=s(`: Reduces memory footprint without any additional communication. Only applicable when using TP.
It reduces activation memory required as it prevents the same copies to be on the tensor parallel ranks
post `),Da=o("code"),el=s("all-reduce"),tl=s(" by replacing then with "),Aa=o("code"),al=s("reduce-scatter"),sl=s(" and "),za=o("code"),rl=s("no-op"),ol=s(" operation would be replaced by "),Ca=o("code"),nl=s("all-gather"),ll=s(`.
As `),Sa=o("code"),il=s("all-reduce = reduce-scatter + all-gather"),pl=s(`, this saves a ton of activation memory at no added communication cost.
To put it simply, it shards the outputs of each transformer layer along sequence dimension, e.g.,
if the sequence length is `),Ia=o("code"),cl=s("1024"),dl=s(" and the TP size is "),Ga=o("code"),hl=s("4"),ml=s(", each GPU will have "),Na=o("code"),ul=s("256"),_l=s(` tokens (1024/4) for each sample.
This increases the batch size that can be supported for training. For more details, please refer to the research paper
`),He=o("a"),gl=s("Reducing Activation Recomputation in Large Transformer Models"),fl=s("."),Or=d(),A=o("p"),vl=s("d. "),Ra=o("strong"),bl=s("Data Parallelism (DP)"),wl=s(` via Distributed Optimizer: Reduces the memory footprint by sharding optimizer states and gradients across DP ranks
(versus the traditional method of replicating the optimizer state across data parallel ranks).
For example, when using Adam optimizer with mixed-precision training, each parameter accounts for 12 bytes of memory.
This gets distributed equally across the GPUs, i.e., each parameter would account for 3 bytes (12/4) if we have 4 GPUs.
For more details, please refer the research paper `),Ve=o("a"),yl=s(`ZeRO: Memory Optimizations Toward Training Trillion
Parameter Models`),kl=s(` and following section of \u{1F917} blog
`),Ye=o("a"),El=s("The Technology Behind BLOOM Training"),Ml=s("."),Dr=d(),H=o("p"),jl=s("e. "),Fa=o("strong"),$l=s("Selective Activation Recomputation"),Tl=s(`: Reduces the memory footprint of activations significantly via smart activation checkpointing.
It doesn\u2019t store activations occupying large memory while being fast to recompute thereby achieving great tradeoff between memory and recomputation.
For example, for GPT-3, this leads to 70% reduction in required memory for activations at the expense of
only 2.7% FLOPs overhead for recomputation of activations. For more details, please refer to the research paper
`),Qe=o("a"),Pl=s("Reducing Activation Recomputation in Large Transformer Models"),Ll=s("."),Ar=d(),_e=o("p"),ql=s("f. "),Ba=o("strong"),xl=s("Fused Kernels"),Ol=s(`: Fused Softmax, Mixed Precision Fused Layer Norm and  Fused gradient accumulation to weight gradient computation of linear layer.
PyTorch JIT compiled Fused GeLU and Fused Bias+Dropout+Residual addition.`),zr=d(),L=o("p"),Dl=s("g. "),Ua=o("strong"),Al=s("Support for Indexed datasets"),zl=s(": Efficient binary format of datasets for large scale training. Support for the "),Wa=o("code"),Cl=s("mmap"),Sl=s(", "),Ha=o("code"),Il=s("cached"),Gl=s(" index file and the "),Va=o("code"),Nl=s("lazy"),Rl=s(" loader format."),Cr=d(),ge=o("p"),Fl=s("h. "),Ya=o("strong"),Bl=s("Checkpoint reshaping and interoperability"),Ul=s(`: Utility for reshaping Megatron-LM checkpoints of variable
tensor and pipeline parallel sizes to the beloved \u{1F917} Transformers sharded checkpoints as it has great support with plethora of tools
such as \u{1F917} Accelerate Big Model Inference, Megatron-DeepSpeed Inference etc.
Support is also available for converting \u{1F917} Transformers sharded checkpoints to Megatron-LM checkpoint of variable tensor and pipeline parallel sizes
for large scale training.`),Sr=d(),ae=o("h2"),fe=o("a"),Qa=o("span"),m(Ke.$$.fragment),Wl=d(),Ka=o("span"),Hl=s("Pre-Requisites"),Ir=d(),V=o("p"),Vl=s("You will need to install the latest pytorch, cuda, nccl, and NVIDIA "),Xe=o("a"),Yl=s("APEX"),Ql=s(` releases and the nltk library.
See `),Je=o("a"),Kl=s("documentation"),Xl=s(` for more details.
Another way to setup the environment is to pull an NVIDIA PyTorch Container that comes with all the required installations from NGC.`),Gr=d(),sa=o("p"),Jl=s("Below is a step-by-step method to set up the conda environment:"),Nr=d(),ra=o("ol"),Xa=o("li"),Zl=s("Create a virtual environment"),Rr=d(),m(Ze.$$.fragment),Fr=d(),et=o("ol"),Ja=o("li"),ei=s("Assuming that the machine has CUDA 11.3 installed, installing the corresponding PyTorch GPU Version"),Br=d(),m(tt.$$.fragment),Ur=d(),at=o("ol"),Za=o("li"),ti=s("Install Nvidia APEX"),Wr=d(),m(st.$$.fragment),Hr=d(),rt=o("ol"),es=o("li"),ai=s("Installing Megatron-LM"),Vr=d(),m(ot.$$.fragment),Yr=d(),se=o("h2"),ve=o("a"),ts=o("span"),m(nt.$$.fragment),si=d(),as=o("span"),ri=s("Accelerate Megatron-LM Plugin"),Qr=d(),be=o("p"),oi=s("Important features are directly supported via the "),ss=o("code"),ni=s("accelerate config"),li=s(` command.
An example of thr corresponding questions for using Megatron-LM features is shown below:`),Kr=d(),m(lt.$$.fragment),Xr=d(),oa=o("p"),ii=s("The resulting config is shown below:"),Jr=d(),m(it.$$.fragment),Zr=d(),we=o("p"),pi=s("We will take the example of GPT pre-training. The minimal changes required to the official "),rs=o("code"),ci=s("run_clm_no_trainer.py"),di=s(`
to use Megatron-LM are as follows:`),eo=d(),na=o("ol"),pt=o("li"),hi=s(`As Megatron-LM uses its own implementation of Optimizer, the corresponding scheduler compatible with it needs to be used.
As such, support for only the Megatron-LM\u2019s scheduler is present. User will need to create `),os=o("code"),mi=s("accelerate.utils.MegatronLMDummyScheduler"),ui=s(`.
Example is given below:`),to=d(),m(ct.$$.fragment),ao=d(),dt=o("ol"),ns=o("li"),_i=s(`Getting the details of the total batch size now needs to be cognization of tensor and pipeline parallel sizes.
Example of getting the effective total batch size is shown below:`),so=d(),m(ht.$$.fragment),ro=d(),mt=o("ol"),ls=o("li"),gi=s("When using Megatron-LM, the losses are already averaged across the data parallel group"),oo=d(),m(ut.$$.fragment),no=d(),_t=o("ol"),la=o("li"),fi=s("For Megatron-LM, we need to save the model using "),is=o("code"),vi=s("accelerator.save_state"),lo=d(),m(gt.$$.fragment),io=d(),Y=o("p"),bi=s("That\u2019s it! We are good to go \u{1F680}. Please find the example script in the examples folder at the path "),ps=o("code"),wi=s("accelerate/examples/by_feature/megatron_lm_gpt_pretraining.py"),yi=s(`.
Let\u2019s run it for `),cs=o("code"),ki=s("gpt-large"),Ei=s(" model architecture using 4 A100-80GB GPUs."),po=d(),m(ft.$$.fragment),co=d(),ia=o("p"),Mi=s("Below are some important excerpts from the output logs:"),ho=d(),m(vt.$$.fragment),mo=d(),ye=o("p"),ji=s("There are a large number of other options/features that one can set using "),ds=o("code"),$i=s("accelerate.utils.MegatronLMPlugin"),Ti=s("."),uo=d(),re=o("h2"),ke=o("a"),hs=o("span"),m(bt.$$.fragment),Pi=d(),ms=o("span"),Li=s("Advanced features to leverage writing custom train step and Megatron-LM Indexed Datasets"),_o=d(),pa=o("p"),qi=s("For leveraging more features, please go through below details."),go=d(),ca=o("ol"),O=o("li"),xi=s(`Below is an example of changes required to customize the Train Step while using Megatron-LM.
You will implement the `),us=o("code"),Oi=s("accelerate.utils.AbstractTrainStep"),Di=s(` or inherit from their corresponding children
`),_s=o("code"),Ai=s("accelerate.utils.GPTTrainStep"),zi=s(", "),gs=o("code"),Ci=s("accelerate.utils.BertTrainStep"),Si=s(" or "),fs=o("code"),Ii=s("accelerate.utils.T5TrainStep"),Gi=s("."),fo=d(),m(wt.$$.fragment),vo=d(),yt=o("ol"),vs=o("li"),Ni=s(`For using the Megatron-LM datasets, a few more changes are required. Dataloaders for these datasets
are available only on rank 0 of each tensor parallel group. As such, there are rank where dataloader won\u2019t be
avaiable and this requires tweaks to the training loop. Being able to do all this shows how
felixble and extensible \u{1F917} Accelerate is. The changes required are as follows.`),bo=d(),q=o("p"),Ri=s("a. For Megatron-LM indexed datasets, we need to use "),bs=o("code"),Fi=s("MegatronLMDummyDataLoader"),Bi=s(`
and pass the required dataset args to it such as `),ws=o("code"),Ui=s("data_path"),Wi=s(", "),ys=o("code"),Hi=s("seq_length"),Vi=s(` etc.
See `),kt=o("a"),Yi=s("here"),Qi=s(" for the list of available args."),wo=d(),m(Et.$$.fragment),yo=d(),Q=o("p"),Ki=s("b. "),ks=o("code"),Xi=s("megatron_dataloader"),Ji=s(` is repeated 3 times to get training, validation and test dataloaders
as per the `),Es=o("code"),Zi=s("args.splits_string"),ep=s(" proportions"),ko=d(),m(Mt.$$.fragment),Eo=d(),j=o("p"),tp=s(`c. Changes to training and evaluation loops as dataloader is only available on tensor parallel ranks 0
So, we need to iterate only if the dataloader isn\u2019t `),Ms=o("code"),ap=s("None"),sp=s(` else provide empty dict
As such, we loop using `),js=o("code"),rp=s("while"),op=s(" loop and break when "),$s=o("code"),np=s("completed_steps"),lp=s(" is equal to "),Ts=o("code"),ip=s("args.max_train_steps"),pp=s(`
This is similar to the Megatron-LM setup wherein user has to provide `),Ps=o("code"),cp=s("max_train_steps"),dp=s(` when using Megaton-LM indexed datasets.
This displays how flexible and extensible \u{1F917} Accelerate is.`),Mo=d(),m(jt.$$.fragment),jo=d(),oe=o("h2"),Ee=o("a"),Ls=o("span"),m($t.$$.fragment),hp=d(),qs=o("span"),mp=s("Utility for Checkpoint reshaping and interoperability"),$o=d(),Me=o("ol"),xs=o("li"),da=o("p"),up=s(`The scripts for these are present in \u{1F917} Transformers library under respective models.
Currently, it is available for GPT model `),Tt=o("a"),_p=s("checkpoint_reshaping_and_interoperability.py"),gp=d(),Os=o("li"),Ds=o("p"),fp=s("Below is an example of conversion of checkpoint from Megatron-LM to universal \u{1F917} Transformers sharded checkpoint."),To=d(),m(Pt.$$.fragment),Po=d(),Lt=o("ol"),I=o("li"),vp=s("Conversion of checkpoint from transformers to megatron with "),As=o("code"),bp=s("tp_size=2"),wp=s(", "),zs=o("code"),yp=s("pp_size=2"),kp=s(" and "),Cs=o("code"),Ep=s("dp_size=2"),Mp=s("."),Lo=d(),m(qt.$$.fragment),qo=d(),ne=o("h2"),je=o("a"),Ss=o("span"),m(xt.$$.fragment),jp=d(),Ot=o("span"),$p=s("Megatron-LM GPT models support returning logits and "),Is=o("code"),Tp=s("megatron_generate"),Pp=s(" function for text generation"),xo=d(),ha=o("ol"),Dt=o("li"),Lp=s("Returning logits require setting "),Gs=o("code"),qp=s("require_logits=True"),xp=s(` in MegatronLMPlugin as shown below.
These would be available on the in the last stage of pipeline.`),Oo=d(),m(At.$$.fragment),Do=d(),zt=o("ol"),$e=o("li"),Ns=o("code"),Op=s("megatron_generate"),Dp=s(` method for Megatron-LM GPT model: This will use Tensor and Pipeline Parallelism to complete
generations for a batch of inputs when using greedy with/without top_k/top_p sampling and for individual prompt inputs when using beam search decoding.
Only a subset of features of transformers generate is supported. This will help in using large models via tensor and pipeline parallelism
for generation (already does key-value caching and uses fused kernels by default).
This requires data parallel size to be 1, sequence parallelism and activation checkpointing to be disabled.
It also requires specifying path to tokenizer\u2019s vocab file and merges file.
Below example shows how to configure and use `),Rs=o("code"),Ap=s("megatron_generate"),zp=s(" method for Megatron-LM GPT model."),Ao=d(),m(Ct.$$.fragment),zo=d(),St=o("ol"),P=o("li"),Cp=s("An end-to-end example of using "),Fs=o("code"),Sp=s("megatron_generate"),Ip=s(` method for Megatron-LM GPT model is available at
`),It=o("a"),Gp=s("megatron_gpt2_generation.py"),Np=s(` with
config file `),Gt=o("a"),Rp=s("megatron_lm_gpt_generate_config.yaml"),Fp=s(`.
The bash script with accelerate launch command is available at `),Nt=o("a"),Bp=s("megatron_lm_gpt_generate.sh"),Up=s(`.
The output logs of the script are available at `),Rt=o("a"),Wp=s("megatron_lm_gpt_generate.log"),Hp=s("."),Co=d(),le=o("h2"),Te=o("a"),Bs=o("span"),m(Ft.$$.fragment),Vp=d(),Us=o("span"),Yp=s("Support for ROPE and ALiBi Positional embeddings and Multi-Query Attention"),So=d(),ma=o("ol"),G=o("li"),Qp=s("For ROPE/ALiBi attention, pass "),Ws=o("code"),Kp=s("position_embedding_type"),Xp=s(" with "),Hs=o("code"),Jp=s('("absolute" | "rotary" | "alibi")'),Zp=s(" to "),Vs=o("code"),ec=s("MegatronLMPlugin"),tc=s(" as shown below."),Io=d(),m(Bt.$$.fragment),Go=d(),Ut=o("ol"),N=o("li"),ac=s("For Multi-Query Attention, pass "),Ys=o("code"),sc=s("attention_head_type"),rc=s(" with "),Qs=o("code"),oc=s('("multihead" | "multiquery")'),nc=s(" to "),Ks=o("code"),lc=s("MegatronLMPlugin"),ic=s(" as shown below."),No=d(),m(Wt.$$.fragment),Ro=d(),ie=o("h2"),Pe=o("a"),Xs=o("span"),m(Ht.$$.fragment),pc=d(),Js=o("span"),cc=s("Caveats"),Fo=d(),k=o("ol"),Zs=o("li"),er=o("p"),dc=s(`Supports Transformers GPT2, Megatron-BERT and T5 models.
This covers Decoder only, Encode only and Encoder-Decoder model classes.`),hc=d(),tr=o("li"),R=o("p"),mc=s(`Only loss is returned from model forward pass as
there is quite complex interplay of pipeline, tensor and data parallelsim behind the scenes.
The `),ar=o("code"),uc=s("model(**batch_data)"),_c=s(` call return loss(es) averaged across the data parallel ranks.
This is fine for most cases wherein pre-training jobs are run using Megatron-LM features and
you can easily compute the `),sr=o("code"),gc=s("perplexity"),fc=s(` using the loss.
For GPT model, returning logits in addition to loss(es) is supported.
These logits aren\u2019t gathered across data prallel ranks. Use `),rr=o("code"),vc=s("accelerator.utils.gather_across_data_parallel_groups"),bc=s(`
to gather logits across data parallel ranks. These logits along with labels can be used for computing various
performance metrics.`),wc=d(),or=o("li"),F=o("p"),yc=s(`The main process is the last rank as the losses/logits are available in the last stage of pipeline.
`),nr=o("code"),kc=s("accelerator.is_main_process"),Ec=s(" and "),lr=o("code"),Mc=s("accelerator.is_local_main_process"),jc=s(" return "),ir=o("code"),$c=s("True"),Tc=s(` for last rank when using
Megatron-LM integration.`),Pc=d(),pr=o("li"),pe=o("p"),Lc=s("In "),cr=o("code"),qc=s("accelerator.prepare"),xc=s(` call, a Megatron-LM model corresponding to a given Transformers model is created
with random weights. Please use `),dr=o("code"),Oc=s("accelerator.load_state"),Dc=s(" to load the Megatron-LM checkpoint with matching TP, PP and DP partitions."),Ac=d(),hr=o("li"),mr=o("p"),zc=s(`Currently, checkpoint reshaping and interoperability support is only available for GPT.
Soon it will be extended to BERT and T5.`),Cc=d(),ur=o("li"),ua=o("p"),_r=o("code"),Sc=s("gradient_accumulation_steps"),Ic=s(` needs to be 1. When using Megatron-LM, micro batches in pipeline parallelism
setting is synonymous with gradient accumulation.`),Gc=d(),gr=o("li"),ce=o("p"),Nc=s("When using Megatron-LM, use "),fr=o("code"),Rc=s("accelerator.save_state"),Fc=s(" and "),vr=o("code"),Bc=s("accelerator.load_state"),Uc=s(" for saving and loading checkpoints."),Wc=d(),br=o("li"),wr=o("p"),Hc=s(`Below are the mapping from Megatron-LM model architectures to the the equivalent \u{1F917} transformers model architectures.
Only these \u{1F917} transformers model architectures are supported.`),Bo=d(),B=o("p"),Vc=s("a. Megatron-LM "),Vt=o("a"),Yc=s("BertModel"),Qc=s(` :
\u{1F917} transformers models with `),yr=o("code"),Kc=s("megatron-bert"),Xc=s(` in config\u2019s model type, e.g.,
`),Yt=o("a"),Jc=s("MegatronBERT"),Uo=d(),U=o("p"),Zc=s("b. Megatron-LM "),Qt=o("a"),ed=s("GPTModel"),td=s(` :
\u{1F917} transformers models with `),kr=o("code"),ad=s("gpt2"),sd=s(` in config\u2019s model type, e.g.,
`),Kt=o("a"),rd=s("OpenAI GPT2"),Wo=d(),D=o("p"),od=s("c. Megatron-LM "),Xt=o("a"),nd=s("T5Model"),ld=s(` :
\u{1F917} transformers models with `),Er=o("code"),id=s("t5"),pd=s(` in  config\u2019s model type, e.g.,
`),Jt=o("a"),cd=s("T5"),dd=s(` and
`),Zt=o("a"),hd=s("MT5"),this.h()},l(e){const i=Tu('[data-svelte="svelte-1phssyn"]',document.head);Z=n(i,"META",{name:!0,content:!0}),i.forEach(a),jr=h(e),ee=n(e,"H1",{class:!0});var Vo=l(ee);me=n(Vo,"A",{id:!0,class:!0,href:!0});var fd=l(me);ya=n(fd,"SPAN",{});var vd=l(ya);u(ze.$$.fragment,vd),vd.forEach(a),fd.forEach(a),hn=h(Vo),ka=n(Vo,"SPAN",{});var bd=l(ka);mn=r(bd,"Megatron-LM"),bd.forEach(a),Vo.forEach(a),$r=h(e),T=n(e,"P",{});var W=l(T);Ce=n(W,"A",{href:!0,rel:!0});var wd=l(Ce);un=r(wd,"Megatron-LM"),wd.forEach(a),_n=r(W,` enables training large transformer language models at scale.
It provides efficient tensor, pipeline and sequence based model parallelism for pre-training transformer based
Language Models such as `),Se=n(W,"A",{href:!0,rel:!0});var yd=l(Se);gn=r(yd,"GPT"),yd.forEach(a),fn=r(W," (Decoder Only), "),Ie=n(W,"A",{href:!0,rel:!0});var kd=l(Ie);vn=r(kd,"BERT"),kd.forEach(a),bn=r(W," (Encoder Only) and "),Ge=n(W,"A",{href:!0,rel:!0});var Ed=l(Ge);wn=r(Ed,"T5"),Ed.forEach(a),yn=r(W,` (Encoder-Decoder).
For detailed information and how things work behind the scene please refer the github `),Ne=n(W,"A",{href:!0,rel:!0});var Md=l(Ne);kn=r(Md,"repo"),Md.forEach(a),En=r(W,"."),W.forEach(a),Tr=h(e),te=n(e,"H2",{class:!0});var Yo=l(te);ue=n(Yo,"A",{id:!0,class:!0,href:!0});var jd=l(ue);Ea=n(jd,"SPAN",{});var $d=l(Ea);u(Re.$$.fragment,$d),$d.forEach(a),jd.forEach(a),Mn=h(Yo),Ma=n(Yo,"SPAN",{});var Td=l(Ma);jn=r(Td,"What is integrated?"),Td.forEach(a),Yo.forEach(a),Pr=h(e),aa=n(e,"P",{});var Pd=l(aa);$n=r(Pd,`Accelerate integrates following feature of Megatron-LM to enable large scale pre-training/finetuning
of BERT (Encoder), GPT (Decoder) or T5 models (Encoder and Decoder):`),Pd.forEach(a),Lr=h(e),M=n(e,"P",{});var z=l(M);Tn=r(z,"a. "),ja=n(z,"STRONG",{});var Ld=l(ja);Pn=r(Ld,"Tensor Parallelism (TP)"),Ld.forEach(a),Ln=r(z,`: Reduces memory footprint without much additional communication on intra-node ranks.
Each tensor is split into multiple chunks with each shard residing on separate GPU. At each step, the same mini-batch of data is processed
independently and in parallel by each shard followed by syncing across all GPUs (`),$a=n(z,"CODE",{});var qd=l($a);qn=r(qd,"all-reduce"),qd.forEach(a),xn=r(z,` operation).
In a simple transformer layer, this leads to 2 `),Ta=n(z,"CODE",{});var xd=l(Ta);On=r(xd,"all-reduces"),xd.forEach(a),Dn=r(z,` in the forward path and 2 in the backward path.
For more details, please refer research paper `),Fe=n(z,"A",{href:!0,rel:!0});var Od=l(Fe);An=r(Od,`Megatron-LM: Training Multi-Billion Parameter Language Models Using
Model Parallelism`),Od.forEach(a),zn=r(z,` and
this section of \u{1F917} blogpost `),Be=n(z,"A",{href:!0,rel:!0});var Dd=l(Be);Cn=r(Dd,"The Technology Behind BLOOM Training"),Dd.forEach(a),Sn=r(z,"."),z.forEach(a),qr=h(e),E=n(e,"P",{});var x=l(E);In=r(x,"b. "),Pa=n(x,"STRONG",{});var Ad=l(Pa);Gn=r(Ad,"Pipeline Parallelism (PP)"),Ad.forEach(a),Nn=r(x,`: Reduces memory footprint and enables large scale training via inter-node parallelization.
Reduces the bubble of naive PP via PipeDream-Flush schedule/1F1B schedule and Interleaved 1F1B schedule.
Layers are distributed uniformly across PP stages. For example, if a model has `),La=n(x,"CODE",{});var zd=l(La);Rn=r(zd,"24"),zd.forEach(a),Fn=r(x," layers and we have "),qa=n(x,"CODE",{});var Cd=l(qa);Bn=r(Cd,"4"),Cd.forEach(a),Un=r(x,` GPUs for
pipeline parallelism, each GPU will have `),xa=n(x,"CODE",{});var Sd=l(xa);Wn=r(Sd,"6"),Sd.forEach(a),Hn=r(x,` layers (24/4). For more details on schedules to reduce the idle time of PP,
please refer to the research paper `),Ue=n(x,"A",{href:!0,rel:!0});var Id=l(Ue);Vn=r(Id,`Efficient Large-Scale Language Model Training on GPU Clusters
Using Megatron-LM`),Id.forEach(a),Yn=r(x,` and
this section of \u{1F917} blogpost `),We=n(x,"A",{href:!0,rel:!0});var Gd=l(We);Qn=r(Gd,"The Technology Behind BLOOM Training"),Gd.forEach(a),Kn=r(x,"."),x.forEach(a),xr=h(e),b=n(e,"P",{});var w=l(b);Xn=r(w,"c. "),Oa=n(w,"STRONG",{});var Nd=l(Oa);Jn=r(Nd,"Sequence Parallelism (SP)"),Nd.forEach(a),Zn=r(w,`: Reduces memory footprint without any additional communication. Only applicable when using TP.
It reduces activation memory required as it prevents the same copies to be on the tensor parallel ranks
post `),Da=n(w,"CODE",{});var Rd=l(Da);el=r(Rd,"all-reduce"),Rd.forEach(a),tl=r(w," by replacing then with "),Aa=n(w,"CODE",{});var Fd=l(Aa);al=r(Fd,"reduce-scatter"),Fd.forEach(a),sl=r(w," and "),za=n(w,"CODE",{});var Bd=l(za);rl=r(Bd,"no-op"),Bd.forEach(a),ol=r(w," operation would be replaced by "),Ca=n(w,"CODE",{});var Ud=l(Ca);nl=r(Ud,"all-gather"),Ud.forEach(a),ll=r(w,`.
As `),Sa=n(w,"CODE",{});var Wd=l(Sa);il=r(Wd,"all-reduce = reduce-scatter + all-gather"),Wd.forEach(a),pl=r(w,`, this saves a ton of activation memory at no added communication cost.
To put it simply, it shards the outputs of each transformer layer along sequence dimension, e.g.,
if the sequence length is `),Ia=n(w,"CODE",{});var Hd=l(Ia);cl=r(Hd,"1024"),Hd.forEach(a),dl=r(w," and the TP size is "),Ga=n(w,"CODE",{});var Vd=l(Ga);hl=r(Vd,"4"),Vd.forEach(a),ml=r(w,", each GPU will have "),Na=n(w,"CODE",{});var Yd=l(Na);ul=r(Yd,"256"),Yd.forEach(a),_l=r(w,` tokens (1024/4) for each sample.
This increases the batch size that can be supported for training. For more details, please refer to the research paper
`),He=n(w,"A",{href:!0,rel:!0});var Qd=l(He);gl=r(Qd,"Reducing Activation Recomputation in Large Transformer Models"),Qd.forEach(a),fl=r(w,"."),w.forEach(a),Or=h(e),A=n(e,"P",{});var Le=l(A);vl=r(Le,"d. "),Ra=n(Le,"STRONG",{});var Kd=l(Ra);bl=r(Kd,"Data Parallelism (DP)"),Kd.forEach(a),wl=r(Le,` via Distributed Optimizer: Reduces the memory footprint by sharding optimizer states and gradients across DP ranks
(versus the traditional method of replicating the optimizer state across data parallel ranks).
For example, when using Adam optimizer with mixed-precision training, each parameter accounts for 12 bytes of memory.
This gets distributed equally across the GPUs, i.e., each parameter would account for 3 bytes (12/4) if we have 4 GPUs.
For more details, please refer the research paper `),Ve=n(Le,"A",{href:!0,rel:!0});var Xd=l(Ve);yl=r(Xd,`ZeRO: Memory Optimizations Toward Training Trillion
Parameter Models`),Xd.forEach(a),kl=r(Le,` and following section of \u{1F917} blog
`),Ye=n(Le,"A",{href:!0,rel:!0});var Jd=l(Ye);El=r(Jd,"The Technology Behind BLOOM Training"),Jd.forEach(a),Ml=r(Le,"."),Le.forEach(a),Dr=h(e),H=n(e,"P",{});var _a=l(H);jl=r(_a,"e. "),Fa=n(_a,"STRONG",{});var Zd=l(Fa);$l=r(Zd,"Selective Activation Recomputation"),Zd.forEach(a),Tl=r(_a,`: Reduces the memory footprint of activations significantly via smart activation checkpointing.
It doesn\u2019t store activations occupying large memory while being fast to recompute thereby achieving great tradeoff between memory and recomputation.
For example, for GPT-3, this leads to 70% reduction in required memory for activations at the expense of
only 2.7% FLOPs overhead for recomputation of activations. For more details, please refer to the research paper
`),Qe=n(_a,"A",{href:!0,rel:!0});var eh=l(Qe);Pl=r(eh,"Reducing Activation Recomputation in Large Transformer Models"),eh.forEach(a),Ll=r(_a,"."),_a.forEach(a),Ar=h(e),_e=n(e,"P",{});var Qo=l(_e);ql=r(Qo,"f. "),Ba=n(Qo,"STRONG",{});var th=l(Ba);xl=r(th,"Fused Kernels"),th.forEach(a),Ol=r(Qo,`: Fused Softmax, Mixed Precision Fused Layer Norm and  Fused gradient accumulation to weight gradient computation of linear layer.
PyTorch JIT compiled Fused GeLU and Fused Bias+Dropout+Residual addition.`),Qo.forEach(a),zr=h(e),L=n(e,"P",{});var K=l(L);Dl=r(K,"g. "),Ua=n(K,"STRONG",{});var ah=l(Ua);Al=r(ah,"Support for Indexed datasets"),ah.forEach(a),zl=r(K,": Efficient binary format of datasets for large scale training. Support for the "),Wa=n(K,"CODE",{});var sh=l(Wa);Cl=r(sh,"mmap"),sh.forEach(a),Sl=r(K,", "),Ha=n(K,"CODE",{});var rh=l(Ha);Il=r(rh,"cached"),rh.forEach(a),Gl=r(K," index file and the "),Va=n(K,"CODE",{});var oh=l(Va);Nl=r(oh,"lazy"),oh.forEach(a),Rl=r(K," loader format."),K.forEach(a),Cr=h(e),ge=n(e,"P",{});var Ko=l(ge);Fl=r(Ko,"h. "),Ya=n(Ko,"STRONG",{});var nh=l(Ya);Bl=r(nh,"Checkpoint reshaping and interoperability"),nh.forEach(a),Ul=r(Ko,`: Utility for reshaping Megatron-LM checkpoints of variable
tensor and pipeline parallel sizes to the beloved \u{1F917} Transformers sharded checkpoints as it has great support with plethora of tools
such as \u{1F917} Accelerate Big Model Inference, Megatron-DeepSpeed Inference etc.
Support is also available for converting \u{1F917} Transformers sharded checkpoints to Megatron-LM checkpoint of variable tensor and pipeline parallel sizes
for large scale training.`),Ko.forEach(a),Sr=h(e),ae=n(e,"H2",{class:!0});var Xo=l(ae);fe=n(Xo,"A",{id:!0,class:!0,href:!0});var lh=l(fe);Qa=n(lh,"SPAN",{});var ih=l(Qa);u(Ke.$$.fragment,ih),ih.forEach(a),lh.forEach(a),Wl=h(Xo),Ka=n(Xo,"SPAN",{});var ph=l(Ka);Hl=r(ph,"Pre-Requisites"),ph.forEach(a),Xo.forEach(a),Ir=h(e),V=n(e,"P",{});var ga=l(V);Vl=r(ga,"You will need to install the latest pytorch, cuda, nccl, and NVIDIA "),Xe=n(ga,"A",{href:!0,rel:!0});var ch=l(Xe);Yl=r(ch,"APEX"),ch.forEach(a),Ql=r(ga,` releases and the nltk library.
See `),Je=n(ga,"A",{href:!0,rel:!0});var dh=l(Je);Kl=r(dh,"documentation"),dh.forEach(a),Xl=r(ga,` for more details.
Another way to setup the environment is to pull an NVIDIA PyTorch Container that comes with all the required installations from NGC.`),ga.forEach(a),Gr=h(e),sa=n(e,"P",{});var hh=l(sa);Jl=r(hh,"Below is a step-by-step method to set up the conda environment:"),hh.forEach(a),Nr=h(e),ra=n(e,"OL",{});var mh=l(ra);Xa=n(mh,"LI",{});var uh=l(Xa);Zl=r(uh,"Create a virtual environment"),uh.forEach(a),mh.forEach(a),Rr=h(e),u(Ze.$$.fragment,e),Fr=h(e),et=n(e,"OL",{start:!0});var _h=l(et);Ja=n(_h,"LI",{});var gh=l(Ja);ei=r(gh,"Assuming that the machine has CUDA 11.3 installed, installing the corresponding PyTorch GPU Version"),gh.forEach(a),_h.forEach(a),Br=h(e),u(tt.$$.fragment,e),Ur=h(e),at=n(e,"OL",{start:!0});var fh=l(at);Za=n(fh,"LI",{});var vh=l(Za);ti=r(vh,"Install Nvidia APEX"),vh.forEach(a),fh.forEach(a),Wr=h(e),u(st.$$.fragment,e),Hr=h(e),rt=n(e,"OL",{start:!0});var bh=l(rt);es=n(bh,"LI",{});var wh=l(es);ai=r(wh,"Installing Megatron-LM"),wh.forEach(a),bh.forEach(a),Vr=h(e),u(ot.$$.fragment,e),Yr=h(e),se=n(e,"H2",{class:!0});var Jo=l(se);ve=n(Jo,"A",{id:!0,class:!0,href:!0});var yh=l(ve);ts=n(yh,"SPAN",{});var kh=l(ts);u(nt.$$.fragment,kh),kh.forEach(a),yh.forEach(a),si=h(Jo),as=n(Jo,"SPAN",{});var Eh=l(as);ri=r(Eh,"Accelerate Megatron-LM Plugin"),Eh.forEach(a),Jo.forEach(a),Qr=h(e),be=n(e,"P",{});var Zo=l(be);oi=r(Zo,"Important features are directly supported via the "),ss=n(Zo,"CODE",{});var Mh=l(ss);ni=r(Mh,"accelerate config"),Mh.forEach(a),li=r(Zo,` command.
An example of thr corresponding questions for using Megatron-LM features is shown below:`),Zo.forEach(a),Kr=h(e),u(lt.$$.fragment,e),Xr=h(e),oa=n(e,"P",{});var jh=l(oa);ii=r(jh,"The resulting config is shown below:"),jh.forEach(a),Jr=h(e),u(it.$$.fragment,e),Zr=h(e),we=n(e,"P",{});var en=l(we);pi=r(en,"We will take the example of GPT pre-training. The minimal changes required to the official "),rs=n(en,"CODE",{});var $h=l(rs);ci=r($h,"run_clm_no_trainer.py"),$h.forEach(a),di=r(en,`
to use Megatron-LM are as follows:`),en.forEach(a),eo=h(e),na=n(e,"OL",{});var Th=l(na);pt=n(Th,"LI",{});var tn=l(pt);hi=r(tn,`As Megatron-LM uses its own implementation of Optimizer, the corresponding scheduler compatible with it needs to be used.
As such, support for only the Megatron-LM\u2019s scheduler is present. User will need to create `),os=n(tn,"CODE",{});var Ph=l(os);mi=r(Ph,"accelerate.utils.MegatronLMDummyScheduler"),Ph.forEach(a),ui=r(tn,`.
Example is given below:`),tn.forEach(a),Th.forEach(a),to=h(e),u(ct.$$.fragment,e),ao=h(e),dt=n(e,"OL",{start:!0});var Lh=l(dt);ns=n(Lh,"LI",{});var qh=l(ns);_i=r(qh,`Getting the details of the total batch size now needs to be cognization of tensor and pipeline parallel sizes.
Example of getting the effective total batch size is shown below:`),qh.forEach(a),Lh.forEach(a),so=h(e),u(ht.$$.fragment,e),ro=h(e),mt=n(e,"OL",{start:!0});var xh=l(mt);ls=n(xh,"LI",{});var Oh=l(ls);gi=r(Oh,"When using Megatron-LM, the losses are already averaged across the data parallel group"),Oh.forEach(a),xh.forEach(a),oo=h(e),u(ut.$$.fragment,e),no=h(e),_t=n(e,"OL",{start:!0});var Dh=l(_t);la=n(Dh,"LI",{});var md=l(la);fi=r(md,"For Megatron-LM, we need to save the model using "),is=n(md,"CODE",{});var Ah=l(is);vi=r(Ah,"accelerator.save_state"),Ah.forEach(a),md.forEach(a),Dh.forEach(a),lo=h(e),u(gt.$$.fragment,e),io=h(e),Y=n(e,"P",{});var fa=l(Y);bi=r(fa,"That\u2019s it! We are good to go \u{1F680}. Please find the example script in the examples folder at the path "),ps=n(fa,"CODE",{});var zh=l(ps);wi=r(zh,"accelerate/examples/by_feature/megatron_lm_gpt_pretraining.py"),zh.forEach(a),yi=r(fa,`.
Let\u2019s run it for `),cs=n(fa,"CODE",{});var Ch=l(cs);ki=r(Ch,"gpt-large"),Ch.forEach(a),Ei=r(fa," model architecture using 4 A100-80GB GPUs."),fa.forEach(a),po=h(e),u(ft.$$.fragment,e),co=h(e),ia=n(e,"P",{});var Sh=l(ia);Mi=r(Sh,"Below are some important excerpts from the output logs:"),Sh.forEach(a),ho=h(e),u(vt.$$.fragment,e),mo=h(e),ye=n(e,"P",{});var an=l(ye);ji=r(an,"There are a large number of other options/features that one can set using "),ds=n(an,"CODE",{});var Ih=l(ds);$i=r(Ih,"accelerate.utils.MegatronLMPlugin"),Ih.forEach(a),Ti=r(an,"."),an.forEach(a),uo=h(e),re=n(e,"H2",{class:!0});var sn=l(re);ke=n(sn,"A",{id:!0,class:!0,href:!0});var Gh=l(ke);hs=n(Gh,"SPAN",{});var Nh=l(hs);u(bt.$$.fragment,Nh),Nh.forEach(a),Gh.forEach(a),Pi=h(sn),ms=n(sn,"SPAN",{});var Rh=l(ms);Li=r(Rh,"Advanced features to leverage writing custom train step and Megatron-LM Indexed Datasets"),Rh.forEach(a),sn.forEach(a),_o=h(e),pa=n(e,"P",{});var Fh=l(pa);qi=r(Fh,"For leveraging more features, please go through below details."),Fh.forEach(a),go=h(e),ca=n(e,"OL",{});var Bh=l(ca);O=n(Bh,"LI",{});var X=l(O);xi=r(X,`Below is an example of changes required to customize the Train Step while using Megatron-LM.
You will implement the `),us=n(X,"CODE",{});var Uh=l(us);Oi=r(Uh,"accelerate.utils.AbstractTrainStep"),Uh.forEach(a),Di=r(X,` or inherit from their corresponding children
`),_s=n(X,"CODE",{});var Wh=l(_s);Ai=r(Wh,"accelerate.utils.GPTTrainStep"),Wh.forEach(a),zi=r(X,", "),gs=n(X,"CODE",{});var Hh=l(gs);Ci=r(Hh,"accelerate.utils.BertTrainStep"),Hh.forEach(a),Si=r(X," or "),fs=n(X,"CODE",{});var Vh=l(fs);Ii=r(Vh,"accelerate.utils.T5TrainStep"),Vh.forEach(a),Gi=r(X,"."),X.forEach(a),Bh.forEach(a),fo=h(e),u(wt.$$.fragment,e),vo=h(e),yt=n(e,"OL",{start:!0});var Yh=l(yt);vs=n(Yh,"LI",{});var Qh=l(vs);Ni=r(Qh,`For using the Megatron-LM datasets, a few more changes are required. Dataloaders for these datasets
are available only on rank 0 of each tensor parallel group. As such, there are rank where dataloader won\u2019t be
avaiable and this requires tweaks to the training loop. Being able to do all this shows how
felixble and extensible \u{1F917} Accelerate is. The changes required are as follows.`),Qh.forEach(a),Yh.forEach(a),bo=h(e),q=n(e,"P",{});var J=l(q);Ri=r(J,"a. For Megatron-LM indexed datasets, we need to use "),bs=n(J,"CODE",{});var Kh=l(bs);Fi=r(Kh,"MegatronLMDummyDataLoader"),Kh.forEach(a),Bi=r(J,`
and pass the required dataset args to it such as `),ws=n(J,"CODE",{});var Xh=l(ws);Ui=r(Xh,"data_path"),Xh.forEach(a),Wi=r(J,", "),ys=n(J,"CODE",{});var Jh=l(ys);Hi=r(Jh,"seq_length"),Jh.forEach(a),Vi=r(J,` etc.
See `),kt=n(J,"A",{href:!0,rel:!0});var Zh=l(kt);Yi=r(Zh,"here"),Zh.forEach(a),Qi=r(J," for the list of available args."),J.forEach(a),wo=h(e),u(Et.$$.fragment,e),yo=h(e),Q=n(e,"P",{});var va=l(Q);Ki=r(va,"b. "),ks=n(va,"CODE",{});var em=l(ks);Xi=r(em,"megatron_dataloader"),em.forEach(a),Ji=r(va,` is repeated 3 times to get training, validation and test dataloaders
as per the `),Es=n(va,"CODE",{});var tm=l(Es);Zi=r(tm,"args.splits_string"),tm.forEach(a),ep=r(va," proportions"),va.forEach(a),ko=h(e),u(Mt.$$.fragment,e),Eo=h(e),j=n(e,"P",{});var C=l(j);tp=r(C,`c. Changes to training and evaluation loops as dataloader is only available on tensor parallel ranks 0
So, we need to iterate only if the dataloader isn\u2019t `),Ms=n(C,"CODE",{});var am=l(Ms);ap=r(am,"None"),am.forEach(a),sp=r(C,` else provide empty dict
As such, we loop using `),js=n(C,"CODE",{});var sm=l(js);rp=r(sm,"while"),sm.forEach(a),op=r(C," loop and break when "),$s=n(C,"CODE",{});var rm=l($s);np=r(rm,"completed_steps"),rm.forEach(a),lp=r(C," is equal to "),Ts=n(C,"CODE",{});var om=l(Ts);ip=r(om,"args.max_train_steps"),om.forEach(a),pp=r(C,`
This is similar to the Megatron-LM setup wherein user has to provide `),Ps=n(C,"CODE",{});var nm=l(Ps);cp=r(nm,"max_train_steps"),nm.forEach(a),dp=r(C,` when using Megaton-LM indexed datasets.
This displays how flexible and extensible \u{1F917} Accelerate is.`),C.forEach(a),Mo=h(e),u(jt.$$.fragment,e),jo=h(e),oe=n(e,"H2",{class:!0});var rn=l(oe);Ee=n(rn,"A",{id:!0,class:!0,href:!0});var lm=l(Ee);Ls=n(lm,"SPAN",{});var im=l(Ls);u($t.$$.fragment,im),im.forEach(a),lm.forEach(a),hp=h(rn),qs=n(rn,"SPAN",{});var pm=l(qs);mp=r(pm,"Utility for Checkpoint reshaping and interoperability"),pm.forEach(a),rn.forEach(a),$o=h(e),Me=n(e,"OL",{});var on=l(Me);xs=n(on,"LI",{});var cm=l(xs);da=n(cm,"P",{});var ud=l(da);up=r(ud,`The scripts for these are present in \u{1F917} Transformers library under respective models.
Currently, it is available for GPT model `),Tt=n(ud,"A",{href:!0,rel:!0});var dm=l(Tt);_p=r(dm,"checkpoint_reshaping_and_interoperability.py"),dm.forEach(a),ud.forEach(a),cm.forEach(a),gp=h(on),Os=n(on,"LI",{});var hm=l(Os);Ds=n(hm,"P",{});var mm=l(Ds);fp=r(mm,"Below is an example of conversion of checkpoint from Megatron-LM to universal \u{1F917} Transformers sharded checkpoint."),mm.forEach(a),hm.forEach(a),on.forEach(a),To=h(e),u(Pt.$$.fragment,e),Po=h(e),Lt=n(e,"OL",{start:!0});var um=l(Lt);I=n(um,"LI",{});var qe=l(I);vp=r(qe,"Conversion of checkpoint from transformers to megatron with "),As=n(qe,"CODE",{});var _m=l(As);bp=r(_m,"tp_size=2"),_m.forEach(a),wp=r(qe,", "),zs=n(qe,"CODE",{});var gm=l(zs);yp=r(gm,"pp_size=2"),gm.forEach(a),kp=r(qe," and "),Cs=n(qe,"CODE",{});var fm=l(Cs);Ep=r(fm,"dp_size=2"),fm.forEach(a),Mp=r(qe,"."),qe.forEach(a),um.forEach(a),Lo=h(e),u(qt.$$.fragment,e),qo=h(e),ne=n(e,"H2",{class:!0});var nn=l(ne);je=n(nn,"A",{id:!0,class:!0,href:!0});var vm=l(je);Ss=n(vm,"SPAN",{});var bm=l(Ss);u(xt.$$.fragment,bm),bm.forEach(a),vm.forEach(a),jp=h(nn),Ot=n(nn,"SPAN",{});var ln=l(Ot);$p=r(ln,"Megatron-LM GPT models support returning logits and "),Is=n(ln,"CODE",{});var wm=l(Is);Tp=r(wm,"megatron_generate"),wm.forEach(a),Pp=r(ln," function for text generation"),ln.forEach(a),nn.forEach(a),xo=h(e),ha=n(e,"OL",{});var ym=l(ha);Dt=n(ym,"LI",{});var pn=l(Dt);Lp=r(pn,"Returning logits require setting "),Gs=n(pn,"CODE",{});var km=l(Gs);qp=r(km,"require_logits=True"),km.forEach(a),xp=r(pn,` in MegatronLMPlugin as shown below.
These would be available on the in the last stage of pipeline.`),pn.forEach(a),ym.forEach(a),Oo=h(e),u(At.$$.fragment,e),Do=h(e),zt=n(e,"OL",{start:!0});var Em=l(zt);$e=n(Em,"LI",{});var Mr=l($e);Ns=n(Mr,"CODE",{});var Mm=l(Ns);Op=r(Mm,"megatron_generate"),Mm.forEach(a),Dp=r(Mr,` method for Megatron-LM GPT model: This will use Tensor and Pipeline Parallelism to complete
generations for a batch of inputs when using greedy with/without top_k/top_p sampling and for individual prompt inputs when using beam search decoding.
Only a subset of features of transformers generate is supported. This will help in using large models via tensor and pipeline parallelism
for generation (already does key-value caching and uses fused kernels by default).
This requires data parallel size to be 1, sequence parallelism and activation checkpointing to be disabled.
It also requires specifying path to tokenizer\u2019s vocab file and merges file.
Below example shows how to configure and use `),Rs=n(Mr,"CODE",{});var jm=l(Rs);Ap=r(jm,"megatron_generate"),jm.forEach(a),zp=r(Mr," method for Megatron-LM GPT model."),Mr.forEach(a),Em.forEach(a),Ao=h(e),u(Ct.$$.fragment,e),zo=h(e),St=n(e,"OL",{start:!0});var $m=l(St);P=n($m,"LI",{});var S=l(P);Cp=r(S,"An end-to-end example of using "),Fs=n(S,"CODE",{});var Tm=l(Fs);Sp=r(Tm,"megatron_generate"),Tm.forEach(a),Ip=r(S,` method for Megatron-LM GPT model is available at
`),It=n(S,"A",{href:!0,rel:!0});var Pm=l(It);Gp=r(Pm,"megatron_gpt2_generation.py"),Pm.forEach(a),Np=r(S,` with
config file `),Gt=n(S,"A",{href:!0,rel:!0});var Lm=l(Gt);Rp=r(Lm,"megatron_lm_gpt_generate_config.yaml"),Lm.forEach(a),Fp=r(S,`.
The bash script with accelerate launch command is available at `),Nt=n(S,"A",{href:!0,rel:!0});var qm=l(Nt);Bp=r(qm,"megatron_lm_gpt_generate.sh"),qm.forEach(a),Up=r(S,`.
The output logs of the script are available at `),Rt=n(S,"A",{href:!0,rel:!0});var xm=l(Rt);Wp=r(xm,"megatron_lm_gpt_generate.log"),xm.forEach(a),Hp=r(S,"."),S.forEach(a),$m.forEach(a),Co=h(e),le=n(e,"H2",{class:!0});var cn=l(le);Te=n(cn,"A",{id:!0,class:!0,href:!0});var Om=l(Te);Bs=n(Om,"SPAN",{});var Dm=l(Bs);u(Ft.$$.fragment,Dm),Dm.forEach(a),Om.forEach(a),Vp=h(cn),Us=n(cn,"SPAN",{});var Am=l(Us);Yp=r(Am,"Support for ROPE and ALiBi Positional embeddings and Multi-Query Attention"),Am.forEach(a),cn.forEach(a),So=h(e),ma=n(e,"OL",{});var zm=l(ma);G=n(zm,"LI",{});var xe=l(G);Qp=r(xe,"For ROPE/ALiBi attention, pass "),Ws=n(xe,"CODE",{});var Cm=l(Ws);Kp=r(Cm,"position_embedding_type"),Cm.forEach(a),Xp=r(xe," with "),Hs=n(xe,"CODE",{});var Sm=l(Hs);Jp=r(Sm,'("absolute" | "rotary" | "alibi")'),Sm.forEach(a),Zp=r(xe," to "),Vs=n(xe,"CODE",{});var Im=l(Vs);ec=r(Im,"MegatronLMPlugin"),Im.forEach(a),tc=r(xe," as shown below."),xe.forEach(a),zm.forEach(a),Io=h(e),u(Bt.$$.fragment,e),Go=h(e),Ut=n(e,"OL",{start:!0});var Gm=l(Ut);N=n(Gm,"LI",{});var Oe=l(N);ac=r(Oe,"For Multi-Query Attention, pass "),Ys=n(Oe,"CODE",{});var Nm=l(Ys);sc=r(Nm,"attention_head_type"),Nm.forEach(a),rc=r(Oe," with "),Qs=n(Oe,"CODE",{});var Rm=l(Qs);oc=r(Rm,'("multihead" | "multiquery")'),Rm.forEach(a),nc=r(Oe," to "),Ks=n(Oe,"CODE",{});var Fm=l(Ks);lc=r(Fm,"MegatronLMPlugin"),Fm.forEach(a),ic=r(Oe," as shown below."),Oe.forEach(a),Gm.forEach(a),No=h(e),u(Wt.$$.fragment,e),Ro=h(e),ie=n(e,"H2",{class:!0});var dn=l(ie);Pe=n(dn,"A",{id:!0,class:!0,href:!0});var Bm=l(Pe);Xs=n(Bm,"SPAN",{});var Um=l(Xs);u(Ht.$$.fragment,Um),Um.forEach(a),Bm.forEach(a),pc=h(dn),Js=n(dn,"SPAN",{});var Wm=l(Js);cc=r(Wm,"Caveats"),Wm.forEach(a),dn.forEach(a),Fo=h(e),k=n(e,"OL",{});var $=l(k);Zs=n($,"LI",{});var Hm=l(Zs);er=n(Hm,"P",{});var Vm=l(er);dc=r(Vm,`Supports Transformers GPT2, Megatron-BERT and T5 models.
This covers Decoder only, Encode only and Encoder-Decoder model classes.`),Vm.forEach(a),Hm.forEach(a),hc=h($),tr=n($,"LI",{});var Ym=l(tr);R=n(Ym,"P",{});var De=l(R);mc=r(De,`Only loss is returned from model forward pass as
there is quite complex interplay of pipeline, tensor and data parallelsim behind the scenes.
The `),ar=n(De,"CODE",{});var Qm=l(ar);uc=r(Qm,"model(**batch_data)"),Qm.forEach(a),_c=r(De,` call return loss(es) averaged across the data parallel ranks.
This is fine for most cases wherein pre-training jobs are run using Megatron-LM features and
you can easily compute the `),sr=n(De,"CODE",{});var Km=l(sr);gc=r(Km,"perplexity"),Km.forEach(a),fc=r(De,` using the loss.
For GPT model, returning logits in addition to loss(es) is supported.
These logits aren\u2019t gathered across data prallel ranks. Use `),rr=n(De,"CODE",{});var Xm=l(rr);vc=r(Xm,"accelerator.utils.gather_across_data_parallel_groups"),Xm.forEach(a),bc=r(De,`
to gather logits across data parallel ranks. These logits along with labels can be used for computing various
performance metrics.`),De.forEach(a),Ym.forEach(a),wc=h($),or=n($,"LI",{});var Jm=l(or);F=n(Jm,"P",{});var Ae=l(F);yc=r(Ae,`The main process is the last rank as the losses/logits are available in the last stage of pipeline.
`),nr=n(Ae,"CODE",{});var Zm=l(nr);kc=r(Zm,"accelerator.is_main_process"),Zm.forEach(a),Ec=r(Ae," and "),lr=n(Ae,"CODE",{});var eu=l(lr);Mc=r(eu,"accelerator.is_local_main_process"),eu.forEach(a),jc=r(Ae," return "),ir=n(Ae,"CODE",{});var tu=l(ir);$c=r(tu,"True"),tu.forEach(a),Tc=r(Ae,` for last rank when using
Megatron-LM integration.`),Ae.forEach(a),Jm.forEach(a),Pc=h($),pr=n($,"LI",{});var au=l(pr);pe=n(au,"P",{});var ba=l(pe);Lc=r(ba,"In "),cr=n(ba,"CODE",{});var su=l(cr);qc=r(su,"accelerator.prepare"),su.forEach(a),xc=r(ba,` call, a Megatron-LM model corresponding to a given Transformers model is created
with random weights. Please use `),dr=n(ba,"CODE",{});var ru=l(dr);Oc=r(ru,"accelerator.load_state"),ru.forEach(a),Dc=r(ba," to load the Megatron-LM checkpoint with matching TP, PP and DP partitions."),ba.forEach(a),au.forEach(a),Ac=h($),hr=n($,"LI",{});var ou=l(hr);mr=n(ou,"P",{});var nu=l(mr);zc=r(nu,`Currently, checkpoint reshaping and interoperability support is only available for GPT.
Soon it will be extended to BERT and T5.`),nu.forEach(a),ou.forEach(a),Cc=h($),ur=n($,"LI",{});var lu=l(ur);ua=n(lu,"P",{});var _d=l(ua);_r=n(_d,"CODE",{});var iu=l(_r);Sc=r(iu,"gradient_accumulation_steps"),iu.forEach(a),Ic=r(_d,` needs to be 1. When using Megatron-LM, micro batches in pipeline parallelism
setting is synonymous with gradient accumulation.`),_d.forEach(a),lu.forEach(a),Gc=h($),gr=n($,"LI",{});var pu=l(gr);ce=n(pu,"P",{});var wa=l(ce);Nc=r(wa,"When using Megatron-LM, use "),fr=n(wa,"CODE",{});var cu=l(fr);Rc=r(cu,"accelerator.save_state"),cu.forEach(a),Fc=r(wa," and "),vr=n(wa,"CODE",{});var du=l(vr);Bc=r(du,"accelerator.load_state"),du.forEach(a),Uc=r(wa," for saving and loading checkpoints."),wa.forEach(a),pu.forEach(a),Wc=h($),br=n($,"LI",{});var hu=l(br);wr=n(hu,"P",{});var mu=l(wr);Hc=r(mu,`Below are the mapping from Megatron-LM model architectures to the the equivalent \u{1F917} transformers model architectures.
Only these \u{1F917} transformers model architectures are supported.`),mu.forEach(a),hu.forEach(a),$.forEach(a),Bo=h(e),B=n(e,"P",{});var ea=l(B);Vc=r(ea,"a. Megatron-LM "),Vt=n(ea,"A",{href:!0,rel:!0});var uu=l(Vt);Yc=r(uu,"BertModel"),uu.forEach(a),Qc=r(ea,` :
\u{1F917} transformers models with `),yr=n(ea,"CODE",{});var _u=l(yr);Kc=r(_u,"megatron-bert"),_u.forEach(a),Xc=r(ea,` in config\u2019s model type, e.g.,
`),Yt=n(ea,"A",{href:!0,rel:!0});var gu=l(Yt);Jc=r(gu,"MegatronBERT"),gu.forEach(a),ea.forEach(a),Uo=h(e),U=n(e,"P",{});var ta=l(U);Zc=r(ta,"b. Megatron-LM "),Qt=n(ta,"A",{href:!0,rel:!0});var fu=l(Qt);ed=r(fu,"GPTModel"),fu.forEach(a),td=r(ta,` :
\u{1F917} transformers models with `),kr=n(ta,"CODE",{});var vu=l(kr);ad=r(vu,"gpt2"),vu.forEach(a),sd=r(ta,` in config\u2019s model type, e.g.,
`),Kt=n(ta,"A",{href:!0,rel:!0});var bu=l(Kt);rd=r(bu,"OpenAI GPT2"),bu.forEach(a),ta.forEach(a),Wo=h(e),D=n(e,"P",{});var de=l(D);od=r(de,"c. Megatron-LM "),Xt=n(de,"A",{href:!0,rel:!0});var wu=l(Xt);nd=r(wu,"T5Model"),wu.forEach(a),ld=r(de,` :
\u{1F917} transformers models with `),Er=n(de,"CODE",{});var yu=l(Er);id=r(yu,"t5"),yu.forEach(a),pd=r(de,` in  config\u2019s model type, e.g.,
`),Jt=n(de,"A",{href:!0,rel:!0});var ku=l(Jt);cd=r(ku,"T5"),ku.forEach(a),dd=r(de,` and
`),Zt=n(de,"A",{href:!0,rel:!0});var Eu=l(Zt);hd=r(Eu,"MT5"),Eu.forEach(a),de.forEach(a),this.h()},h(){c(Z,"name","hf:doc:metadata"),c(Z,"content",JSON.stringify(xu)),c(me,"id","megatronlm"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#megatronlm"),c(ee,"class","relative group"),c(Ce,"href","https://github.com/NVIDIA/Megatron-LM"),c(Ce,"rel","nofollow"),c(Se,"href","https://arxiv.org/abs/2005.14165"),c(Se,"rel","nofollow"),c(Ie,"href","https://arxiv.org/pdf/1810.04805.pdf"),c(Ie,"rel","nofollow"),c(Ge,"href","https://arxiv.org/abs/1910.10683"),c(Ge,"rel","nofollow"),c(Ne,"href","https://github.com/NVIDIA/Megatron-LM"),c(Ne,"rel","nofollow"),c(ue,"id","what-is-integrated"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#what-is-integrated"),c(te,"class","relative group"),c(Fe,"href","https://arxiv.org/pdf/1909.08053.pdf"),c(Fe,"rel","nofollow"),c(Be,"href","https://huggingface.co/blog/bloom-megatron-deepspeed#tensor-parallelism"),c(Be,"rel","nofollow"),c(Ue,"href","https://arxiv.org/pdf/2104.04473.pdf"),c(Ue,"rel","nofollow"),c(We,"href","https://huggingface.co/blog/bloom-megatron-deepspeed#pipeline-parallelism"),c(We,"rel","nofollow"),c(He,"href","https://arxiv.org/pdf/2205.05198.pdf"),c(He,"rel","nofollow"),c(Ve,"href","https://arxiv.org/pdf/1910.02054.pdf"),c(Ve,"rel","nofollow"),c(Ye,"href","https://huggingface.co/blog/bloom-megatron-deepspeed#zero-data-parallelism"),c(Ye,"rel","nofollow"),c(Qe,"href","https://arxiv.org/pdf/2205.05198.pdf"),c(Qe,"rel","nofollow"),c(fe,"id","prerequisites"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#prerequisites"),c(ae,"class","relative group"),c(Xe,"href","https://github.com/NVIDIA/apex#quick-start"),c(Xe,"rel","nofollow"),c(Je,"href","https://github.com/NVIDIA/Megatron-LM#setup"),c(Je,"rel","nofollow"),c(et,"start","2"),c(at,"start","3"),c(rt,"start","4"),c(ve,"id","accelerate-megatronlm-plugin"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#accelerate-megatronlm-plugin"),c(se,"class","relative group"),c(dt,"start","2"),c(mt,"start","3"),c(_t,"start","4"),c(ke,"id","advanced-features-to-leverage-writing-custom-train-step-and-megatronlm-indexed-datasets"),c(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ke,"href","#advanced-features-to-leverage-writing-custom-train-step-and-megatronlm-indexed-datasets"),c(re,"class","relative group"),c(yt,"start","2"),c(kt,"href","https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/arguments.py#L804"),c(kt,"rel","nofollow"),c(Ee,"id","utility-for-checkpoint-reshaping-and-interoperability"),c(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ee,"href","#utility-for-checkpoint-reshaping-and-interoperability"),c(oe,"class","relative group"),c(Tt,"href","https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py"),c(Tt,"rel","nofollow"),c(Lt,"start","3"),c(je,"id","megatronlm-gpt-models-support-returning-logits-and-megatrongenerate-function-for-text-generation"),c(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(je,"href","#megatronlm-gpt-models-support-returning-logits-and-megatrongenerate-function-for-text-generation"),c(ne,"class","relative group"),c(zt,"start","2"),c(It,"href","https://github.com/pacman100/accelerate-megatron-test/blob/main/src/inference/megatron_gpt2_generation.py"),c(It,"rel","nofollow"),c(Gt,"href","https://github.com/pacman100/accelerate-megatron-test/blob/main/src/Configs/megatron_lm_gpt_generate_config.yaml"),c(Gt,"rel","nofollow"),c(Nt,"href","https://github.com/pacman100/accelerate-megatron-test/blob/main/megatron_lm_gpt_generate.sh"),c(Nt,"rel","nofollow"),c(Rt,"href","https://github.com/pacman100/accelerate-megatron-test/blob/main/output_logs/megatron_lm_gpt_generate.log"),c(Rt,"rel","nofollow"),c(St,"start","3"),c(Te,"id","support-for-rope-and-alibi-positional-embeddings-and-multiquery-attention"),c(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Te,"href","#support-for-rope-and-alibi-positional-embeddings-and-multiquery-attention"),c(le,"class","relative group"),c(Ut,"start","2"),c(Pe,"id","caveats"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#caveats"),c(ie,"class","relative group"),c(Vt,"href","https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/bert_model.py"),c(Vt,"rel","nofollow"),c(Yt,"href","https://huggingface.co/docs/transformers/model_doc/megatron-bert"),c(Yt,"rel","nofollow"),c(Qt,"href","https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py"),c(Qt,"rel","nofollow"),c(Kt,"href","https://huggingface.co/docs/transformers/model_doc/gpt2"),c(Kt,"rel","nofollow"),c(Xt,"href","https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/t5_model.py"),c(Xt,"rel","nofollow"),c(Jt,"href","https://huggingface.co/docs/transformers/model_doc/t5"),c(Jt,"rel","nofollow"),c(Zt,"href","https://huggingface.co/docs/transformers/model_doc/mt5"),c(Zt,"rel","nofollow")},m(e,i){t(document.head,Z),p(e,jr,i),p(e,ee,i),t(ee,me),t(me,ya),_(ze,ya,null),t(ee,hn),t(ee,ka),t(ka,mn),p(e,$r,i),p(e,T,i),t(T,Ce),t(Ce,un),t(T,_n),t(T,Se),t(Se,gn),t(T,fn),t(T,Ie),t(Ie,vn),t(T,bn),t(T,Ge),t(Ge,wn),t(T,yn),t(T,Ne),t(Ne,kn),t(T,En),p(e,Tr,i),p(e,te,i),t(te,ue),t(ue,Ea),_(Re,Ea,null),t(te,Mn),t(te,Ma),t(Ma,jn),p(e,Pr,i),p(e,aa,i),t(aa,$n),p(e,Lr,i),p(e,M,i),t(M,Tn),t(M,ja),t(ja,Pn),t(M,Ln),t(M,$a),t($a,qn),t(M,xn),t(M,Ta),t(Ta,On),t(M,Dn),t(M,Fe),t(Fe,An),t(M,zn),t(M,Be),t(Be,Cn),t(M,Sn),p(e,qr,i),p(e,E,i),t(E,In),t(E,Pa),t(Pa,Gn),t(E,Nn),t(E,La),t(La,Rn),t(E,Fn),t(E,qa),t(qa,Bn),t(E,Un),t(E,xa),t(xa,Wn),t(E,Hn),t(E,Ue),t(Ue,Vn),t(E,Yn),t(E,We),t(We,Qn),t(E,Kn),p(e,xr,i),p(e,b,i),t(b,Xn),t(b,Oa),t(Oa,Jn),t(b,Zn),t(b,Da),t(Da,el),t(b,tl),t(b,Aa),t(Aa,al),t(b,sl),t(b,za),t(za,rl),t(b,ol),t(b,Ca),t(Ca,nl),t(b,ll),t(b,Sa),t(Sa,il),t(b,pl),t(b,Ia),t(Ia,cl),t(b,dl),t(b,Ga),t(Ga,hl),t(b,ml),t(b,Na),t(Na,ul),t(b,_l),t(b,He),t(He,gl),t(b,fl),p(e,Or,i),p(e,A,i),t(A,vl),t(A,Ra),t(Ra,bl),t(A,wl),t(A,Ve),t(Ve,yl),t(A,kl),t(A,Ye),t(Ye,El),t(A,Ml),p(e,Dr,i),p(e,H,i),t(H,jl),t(H,Fa),t(Fa,$l),t(H,Tl),t(H,Qe),t(Qe,Pl),t(H,Ll),p(e,Ar,i),p(e,_e,i),t(_e,ql),t(_e,Ba),t(Ba,xl),t(_e,Ol),p(e,zr,i),p(e,L,i),t(L,Dl),t(L,Ua),t(Ua,Al),t(L,zl),t(L,Wa),t(Wa,Cl),t(L,Sl),t(L,Ha),t(Ha,Il),t(L,Gl),t(L,Va),t(Va,Nl),t(L,Rl),p(e,Cr,i),p(e,ge,i),t(ge,Fl),t(ge,Ya),t(Ya,Bl),t(ge,Ul),p(e,Sr,i),p(e,ae,i),t(ae,fe),t(fe,Qa),_(Ke,Qa,null),t(ae,Wl),t(ae,Ka),t(Ka,Hl),p(e,Ir,i),p(e,V,i),t(V,Vl),t(V,Xe),t(Xe,Yl),t(V,Ql),t(V,Je),t(Je,Kl),t(V,Xl),p(e,Gr,i),p(e,sa,i),t(sa,Jl),p(e,Nr,i),p(e,ra,i),t(ra,Xa),t(Xa,Zl),p(e,Rr,i),_(Ze,e,i),p(e,Fr,i),p(e,et,i),t(et,Ja),t(Ja,ei),p(e,Br,i),_(tt,e,i),p(e,Ur,i),p(e,at,i),t(at,Za),t(Za,ti),p(e,Wr,i),_(st,e,i),p(e,Hr,i),p(e,rt,i),t(rt,es),t(es,ai),p(e,Vr,i),_(ot,e,i),p(e,Yr,i),p(e,se,i),t(se,ve),t(ve,ts),_(nt,ts,null),t(se,si),t(se,as),t(as,ri),p(e,Qr,i),p(e,be,i),t(be,oi),t(be,ss),t(ss,ni),t(be,li),p(e,Kr,i),_(lt,e,i),p(e,Xr,i),p(e,oa,i),t(oa,ii),p(e,Jr,i),_(it,e,i),p(e,Zr,i),p(e,we,i),t(we,pi),t(we,rs),t(rs,ci),t(we,di),p(e,eo,i),p(e,na,i),t(na,pt),t(pt,hi),t(pt,os),t(os,mi),t(pt,ui),p(e,to,i),_(ct,e,i),p(e,ao,i),p(e,dt,i),t(dt,ns),t(ns,_i),p(e,so,i),_(ht,e,i),p(e,ro,i),p(e,mt,i),t(mt,ls),t(ls,gi),p(e,oo,i),_(ut,e,i),p(e,no,i),p(e,_t,i),t(_t,la),t(la,fi),t(la,is),t(is,vi),p(e,lo,i),_(gt,e,i),p(e,io,i),p(e,Y,i),t(Y,bi),t(Y,ps),t(ps,wi),t(Y,yi),t(Y,cs),t(cs,ki),t(Y,Ei),p(e,po,i),_(ft,e,i),p(e,co,i),p(e,ia,i),t(ia,Mi),p(e,ho,i),_(vt,e,i),p(e,mo,i),p(e,ye,i),t(ye,ji),t(ye,ds),t(ds,$i),t(ye,Ti),p(e,uo,i),p(e,re,i),t(re,ke),t(ke,hs),_(bt,hs,null),t(re,Pi),t(re,ms),t(ms,Li),p(e,_o,i),p(e,pa,i),t(pa,qi),p(e,go,i),p(e,ca,i),t(ca,O),t(O,xi),t(O,us),t(us,Oi),t(O,Di),t(O,_s),t(_s,Ai),t(O,zi),t(O,gs),t(gs,Ci),t(O,Si),t(O,fs),t(fs,Ii),t(O,Gi),p(e,fo,i),_(wt,e,i),p(e,vo,i),p(e,yt,i),t(yt,vs),t(vs,Ni),p(e,bo,i),p(e,q,i),t(q,Ri),t(q,bs),t(bs,Fi),t(q,Bi),t(q,ws),t(ws,Ui),t(q,Wi),t(q,ys),t(ys,Hi),t(q,Vi),t(q,kt),t(kt,Yi),t(q,Qi),p(e,wo,i),_(Et,e,i),p(e,yo,i),p(e,Q,i),t(Q,Ki),t(Q,ks),t(ks,Xi),t(Q,Ji),t(Q,Es),t(Es,Zi),t(Q,ep),p(e,ko,i),_(Mt,e,i),p(e,Eo,i),p(e,j,i),t(j,tp),t(j,Ms),t(Ms,ap),t(j,sp),t(j,js),t(js,rp),t(j,op),t(j,$s),t($s,np),t(j,lp),t(j,Ts),t(Ts,ip),t(j,pp),t(j,Ps),t(Ps,cp),t(j,dp),p(e,Mo,i),_(jt,e,i),p(e,jo,i),p(e,oe,i),t(oe,Ee),t(Ee,Ls),_($t,Ls,null),t(oe,hp),t(oe,qs),t(qs,mp),p(e,$o,i),p(e,Me,i),t(Me,xs),t(xs,da),t(da,up),t(da,Tt),t(Tt,_p),t(Me,gp),t(Me,Os),t(Os,Ds),t(Ds,fp),p(e,To,i),_(Pt,e,i),p(e,Po,i),p(e,Lt,i),t(Lt,I),t(I,vp),t(I,As),t(As,bp),t(I,wp),t(I,zs),t(zs,yp),t(I,kp),t(I,Cs),t(Cs,Ep),t(I,Mp),p(e,Lo,i),_(qt,e,i),p(e,qo,i),p(e,ne,i),t(ne,je),t(je,Ss),_(xt,Ss,null),t(ne,jp),t(ne,Ot),t(Ot,$p),t(Ot,Is),t(Is,Tp),t(Ot,Pp),p(e,xo,i),p(e,ha,i),t(ha,Dt),t(Dt,Lp),t(Dt,Gs),t(Gs,qp),t(Dt,xp),p(e,Oo,i),_(At,e,i),p(e,Do,i),p(e,zt,i),t(zt,$e),t($e,Ns),t(Ns,Op),t($e,Dp),t($e,Rs),t(Rs,Ap),t($e,zp),p(e,Ao,i),_(Ct,e,i),p(e,zo,i),p(e,St,i),t(St,P),t(P,Cp),t(P,Fs),t(Fs,Sp),t(P,Ip),t(P,It),t(It,Gp),t(P,Np),t(P,Gt),t(Gt,Rp),t(P,Fp),t(P,Nt),t(Nt,Bp),t(P,Up),t(P,Rt),t(Rt,Wp),t(P,Hp),p(e,Co,i),p(e,le,i),t(le,Te),t(Te,Bs),_(Ft,Bs,null),t(le,Vp),t(le,Us),t(Us,Yp),p(e,So,i),p(e,ma,i),t(ma,G),t(G,Qp),t(G,Ws),t(Ws,Kp),t(G,Xp),t(G,Hs),t(Hs,Jp),t(G,Zp),t(G,Vs),t(Vs,ec),t(G,tc),p(e,Io,i),_(Bt,e,i),p(e,Go,i),p(e,Ut,i),t(Ut,N),t(N,ac),t(N,Ys),t(Ys,sc),t(N,rc),t(N,Qs),t(Qs,oc),t(N,nc),t(N,Ks),t(Ks,lc),t(N,ic),p(e,No,i),_(Wt,e,i),p(e,Ro,i),p(e,ie,i),t(ie,Pe),t(Pe,Xs),_(Ht,Xs,null),t(ie,pc),t(ie,Js),t(Js,cc),p(e,Fo,i),p(e,k,i),t(k,Zs),t(Zs,er),t(er,dc),t(k,hc),t(k,tr),t(tr,R),t(R,mc),t(R,ar),t(ar,uc),t(R,_c),t(R,sr),t(sr,gc),t(R,fc),t(R,rr),t(rr,vc),t(R,bc),t(k,wc),t(k,or),t(or,F),t(F,yc),t(F,nr),t(nr,kc),t(F,Ec),t(F,lr),t(lr,Mc),t(F,jc),t(F,ir),t(ir,$c),t(F,Tc),t(k,Pc),t(k,pr),t(pr,pe),t(pe,Lc),t(pe,cr),t(cr,qc),t(pe,xc),t(pe,dr),t(dr,Oc),t(pe,Dc),t(k,Ac),t(k,hr),t(hr,mr),t(mr,zc),t(k,Cc),t(k,ur),t(ur,ua),t(ua,_r),t(_r,Sc),t(ua,Ic),t(k,Gc),t(k,gr),t(gr,ce),t(ce,Nc),t(ce,fr),t(fr,Rc),t(ce,Fc),t(ce,vr),t(vr,Bc),t(ce,Uc),t(k,Wc),t(k,br),t(br,wr),t(wr,Hc),p(e,Bo,i),p(e,B,i),t(B,Vc),t(B,Vt),t(Vt,Yc),t(B,Qc),t(B,yr),t(yr,Kc),t(B,Xc),t(B,Yt),t(Yt,Jc),p(e,Uo,i),p(e,U,i),t(U,Zc),t(U,Qt),t(Qt,ed),t(U,td),t(U,kr),t(kr,ad),t(U,sd),t(U,Kt),t(Kt,rd),p(e,Wo,i),p(e,D,i),t(D,od),t(D,Xt),t(Xt,nd),t(D,ld),t(D,Er),t(Er,id),t(D,pd),t(D,Jt),t(Jt,cd),t(D,dd),t(D,Zt),t(Zt,hd),Ho=!0},p:Pu,i(e){Ho||(g(ze.$$.fragment,e),g(Re.$$.fragment,e),g(Ke.$$.fragment,e),g(Ze.$$.fragment,e),g(tt.$$.fragment,e),g(st.$$.fragment,e),g(ot.$$.fragment,e),g(nt.$$.fragment,e),g(lt.$$.fragment,e),g(it.$$.fragment,e),g(ct.$$.fragment,e),g(ht.$$.fragment,e),g(ut.$$.fragment,e),g(gt.$$.fragment,e),g(ft.$$.fragment,e),g(vt.$$.fragment,e),g(bt.$$.fragment,e),g(wt.$$.fragment,e),g(Et.$$.fragment,e),g(Mt.$$.fragment,e),g(jt.$$.fragment,e),g($t.$$.fragment,e),g(Pt.$$.fragment,e),g(qt.$$.fragment,e),g(xt.$$.fragment,e),g(At.$$.fragment,e),g(Ct.$$.fragment,e),g(Ft.$$.fragment,e),g(Bt.$$.fragment,e),g(Wt.$$.fragment,e),g(Ht.$$.fragment,e),Ho=!0)},o(e){f(ze.$$.fragment,e),f(Re.$$.fragment,e),f(Ke.$$.fragment,e),f(Ze.$$.fragment,e),f(tt.$$.fragment,e),f(st.$$.fragment,e),f(ot.$$.fragment,e),f(nt.$$.fragment,e),f(lt.$$.fragment,e),f(it.$$.fragment,e),f(ct.$$.fragment,e),f(ht.$$.fragment,e),f(ut.$$.fragment,e),f(gt.$$.fragment,e),f(ft.$$.fragment,e),f(vt.$$.fragment,e),f(bt.$$.fragment,e),f(wt.$$.fragment,e),f(Et.$$.fragment,e),f(Mt.$$.fragment,e),f(jt.$$.fragment,e),f($t.$$.fragment,e),f(Pt.$$.fragment,e),f(qt.$$.fragment,e),f(xt.$$.fragment,e),f(At.$$.fragment,e),f(Ct.$$.fragment,e),f(Ft.$$.fragment,e),f(Bt.$$.fragment,e),f(Wt.$$.fragment,e),f(Ht.$$.fragment,e),Ho=!1},d(e){a(Z),e&&a(jr),e&&a(ee),v(ze),e&&a($r),e&&a(T),e&&a(Tr),e&&a(te),v(Re),e&&a(Pr),e&&a(aa),e&&a(Lr),e&&a(M),e&&a(qr),e&&a(E),e&&a(xr),e&&a(b),e&&a(Or),e&&a(A),e&&a(Dr),e&&a(H),e&&a(Ar),e&&a(_e),e&&a(zr),e&&a(L),e&&a(Cr),e&&a(ge),e&&a(Sr),e&&a(ae),v(Ke),e&&a(Ir),e&&a(V),e&&a(Gr),e&&a(sa),e&&a(Nr),e&&a(ra),e&&a(Rr),v(Ze,e),e&&a(Fr),e&&a(et),e&&a(Br),v(tt,e),e&&a(Ur),e&&a(at),e&&a(Wr),v(st,e),e&&a(Hr),e&&a(rt),e&&a(Vr),v(ot,e),e&&a(Yr),e&&a(se),v(nt),e&&a(Qr),e&&a(be),e&&a(Kr),v(lt,e),e&&a(Xr),e&&a(oa),e&&a(Jr),v(it,e),e&&a(Zr),e&&a(we),e&&a(eo),e&&a(na),e&&a(to),v(ct,e),e&&a(ao),e&&a(dt),e&&a(so),v(ht,e),e&&a(ro),e&&a(mt),e&&a(oo),v(ut,e),e&&a(no),e&&a(_t),e&&a(lo),v(gt,e),e&&a(io),e&&a(Y),e&&a(po),v(ft,e),e&&a(co),e&&a(ia),e&&a(ho),v(vt,e),e&&a(mo),e&&a(ye),e&&a(uo),e&&a(re),v(bt),e&&a(_o),e&&a(pa),e&&a(go),e&&a(ca),e&&a(fo),v(wt,e),e&&a(vo),e&&a(yt),e&&a(bo),e&&a(q),e&&a(wo),v(Et,e),e&&a(yo),e&&a(Q),e&&a(ko),v(Mt,e),e&&a(Eo),e&&a(j),e&&a(Mo),v(jt,e),e&&a(jo),e&&a(oe),v($t),e&&a($o),e&&a(Me),e&&a(To),v(Pt,e),e&&a(Po),e&&a(Lt),e&&a(Lo),v(qt,e),e&&a(qo),e&&a(ne),v(xt),e&&a(xo),e&&a(ha),e&&a(Oo),v(At,e),e&&a(Do),e&&a(zt),e&&a(Ao),v(Ct,e),e&&a(zo),e&&a(St),e&&a(Co),e&&a(le),v(Ft),e&&a(So),e&&a(ma),e&&a(Io),v(Bt,e),e&&a(Go),e&&a(Ut),e&&a(No),v(Wt,e),e&&a(Ro),e&&a(ie),v(Ht),e&&a(Fo),e&&a(k),e&&a(Bo),e&&a(B),e&&a(Uo),e&&a(U),e&&a(Wo),e&&a(D)}}}const xu={local:"megatronlm",sections:[{local:"what-is-integrated",title:"What is integrated?"},{local:"prerequisites",title:"Pre-Requisites "},{local:"accelerate-megatronlm-plugin",title:"Accelerate Megatron-LM Plugin"},{local:"advanced-features-to-leverage-writing-custom-train-step-and-megatronlm-indexed-datasets",title:"Advanced features to leverage writing custom train step and Megatron-LM Indexed Datasets"},{local:"utility-for-checkpoint-reshaping-and-interoperability",title:"Utility for Checkpoint reshaping and interoperability"},{local:"megatronlm-gpt-models-support-returning-logits-and-megatrongenerate-function-for-text-generation",title:"Megatron-LM GPT models support returning logits and `megatron_generate` function for text generation"},{local:"support-for-rope-and-alibi-positional-embeddings-and-multiquery-attention",title:"Support for ROPE and ALiBi Positional embeddings and Multi-Query Attention"},{local:"caveats",title:"Caveats"}],title:"Megatron-LM"};function Ou(gd){return Lu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Cu extends Mu{constructor(Z){super();ju(this,Z,Ou,qu,$u,{})}}export{Cu as default,xu as metadata};
