import{S as Ng,i as Ug,s as Mg,e as a,k as d,w as U,t as l,M as kg,c,d as t,m as f,a as i,x as M,h as r,b as _,G as e,g as n,y as k,L as Rg,q as R,o as G,B as W,v as Gg}from"../../chunks/vendor-hf-doc-builder.js";import{I as Bo}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as zi}from"../../chunks/CodeBlock-hf-doc-builder.js";function Wg(Em){let oe,Bi,le,ne,Ko,Lt,$n,jo,Nn,Ki,Co,Un,ji,re,de,Yo,It,Mn,Zo,kn,Yi,At,qo,Rn,Gn,Zi,fe,Xo,Wn,xn,Jo,Fn,qi,he,Hn,Qo,zn,Bn,Xi,$t,Vo,Kn,jn,Ji,Nt,Qi,Ut,el,Yn,Zn,Vi,_e,m,tl,qn,Xn,ol,Jn,Qn,ll,Vn,ed,rl,td,od,al,ld,rd,cl,ad,cd,id,x,il,sd,nd,sl,dd,fd,nl,hd,_d,es,ae,ue,dl,Mt,ud,fl,pd,ts,kt,hl,Ed,vd,os,pe,_l,md,gd,ul,Cd,ls,Ee,Od,Rt,Dd,wd,rs,Gt,pl,bd,Sd,as,Wt,cs,xt,El,Pd,yd,is,ve,g,vl,Td,Ld,ml,Id,Ad,gl,$d,Nd,Cl,Ud,Md,Ol,kd,Rd,Dl,Gd,Wd,xd,F,wl,Fd,Hd,bl,zd,Bd,Sl,Kd,jd,ss,ce,me,Pl,Ft,Yd,yl,Zd,ns,Ht,Tl,qd,Xd,ds,ge,Ll,Jd,Qd,Il,Vd,fs,Oo,ef,hs,zt,Al,tf,of,_s,Bt,us,Kt,$l,lf,rf,ps,Ce,Do,Nl,af,cf,sf,wo,Ul,nf,df,Es,jt,Ml,ff,hf,vs,D,H,kl,_f,uf,Rl,pf,Ef,Gl,vf,mf,gf,Oe,Wl,Cf,Of,xl,Df,wf,bf,z,Fl,Sf,Pf,Hl,yf,Tf,zl,Lf,If,Af,De,Bl,$f,Nf,Kl,Uf,Mf,kf,we,jl,Rf,Gf,Yl,Wf,xf,ms,B,Ff,Zl,Hf,zf,ql,Bf,Kf,gs,Yt,Xl,jf,Yf,Cs,I,be,Jl,Zf,qf,Ql,Xf,Jf,Qf,Se,Vl,Vf,eh,er,th,oh,lh,Pe,tr,rh,ah,or,ch,ih,sh,ye,lr,nh,dh,rr,fh,hh,Os,Zt,ar,_h,uh,Ds,bo,ph,ws,A,Te,cr,Eh,vh,ir,mh,gh,Ch,Le,sr,Oh,Dh,nr,wh,bh,Sh,Ie,dr,Ph,yh,fr,Th,Lh,Ih,Ae,hr,Ah,$h,_r,Nh,Uh,bs,qt,ur,Mh,kh,Ss,So,Rh,Ps,K,$e,pr,Gh,Wh,Er,xh,Fh,Hh,Ne,vr,zh,Bh,mr,Kh,jh,Yh,Ue,gr,Zh,qh,Cr,Xh,Jh,ys,Xt,Or,Qh,Vh,Ts,j,e_,Dr,t_,o_,wr,l_,r_,Ls,p,Me,br,a_,c_,Sr,i_,s_,n_,ke,Pr,d_,f_,yr,h_,__,u_,Re,Tr,p_,E_,Lr,v_,m_,g_,Ge,Ir,C_,O_,Ar,D_,w_,b_,We,$r,S_,P_,Nr,y_,T_,L_,xe,Ur,I_,A_,Mr,$_,N_,U_,Fe,kr,M_,k_,Rr,R_,G_,W_,He,Gr,x_,F_,Wr,H_,z_,Is,Jt,xr,B_,K_,As,Y,j_,Fr,Y_,Z_,Hr,q_,X_,$s,ze,Be,zr,J_,Q_,Br,V_,eu,tu,Ke,Kr,ou,lu,jr,ru,au,Ns,Qt,Yr,cu,iu,Us,$,su,Zr,nu,du,qr,fu,hu,Xr,_u,uu,Ms,h,je,Jr,pu,Eu,Qr,vu,mu,gu,Ye,Vr,Cu,Ou,ea,Du,wu,bu,Ze,ta,Su,Pu,oa,yu,Tu,Lu,qe,la,Iu,Au,ra,$u,Nu,Uu,Xe,aa,Mu,ku,ca,Ru,Gu,Wu,Je,ia,xu,Fu,sa,Hu,zu,Bu,Z,na,Ku,ju,da,Yu,Zu,fa,qu,Xu,Ju,Qe,ha,Qu,Vu,_a,ep,tp,op,Ve,ua,lp,rp,pa,ap,cp,ip,et,Ea,sp,np,va,dp,fp,hp,tt,ma,_p,up,ga,pp,Ep,vp,ot,Ca,mp,gp,Oa,Cp,Op,ks,Vt,Da,Dp,wp,Rs,q,bp,wa,Sp,Pp,ba,yp,Tp,Gs,E,lt,Sa,Lp,Ip,Pa,Ap,$p,Np,rt,ya,Up,Mp,Ta,kp,Rp,Gp,at,La,Wp,xp,Ia,Fp,Hp,zp,ct,Aa,Bp,Kp,$a,jp,Yp,Zp,w,Na,qp,Xp,Ua,Jp,Qp,Ma,Vp,e1,ka,t1,o1,Ra,l1,r1,a1,it,Ga,c1,i1,Wa,s1,n1,d1,st,xa,f1,h1,Fa,_1,u1,Ws,eo,Ha,p1,E1,xs,X,v1,za,m1,g1,Ba,C1,O1,Fs,v,Po,Ka,D1,w1,b1,yo,ja,S1,P1,y1,To,Ya,T1,L1,I1,Lo,Za,A1,$1,N1,Io,qa,U1,M1,k1,Ao,Xa,R1,G1,W1,$o,Ja,x1,F1,Hs,to,Qa,H1,z1,zs,No,B1,Bs,nt,dt,Va,K1,j1,ec,Y1,Z1,q1,ft,tc,X1,J1,oc,Q1,V1,Ks,ie,ht,lc,oo,eE,rc,tE,js,Uo,ac,oE,Ys,lo,cc,lE,rE,Zs,ro,qs,ao,ic,aE,cE,Xs,Mo,J,sc,iE,sE,nc,nE,dE,dc,fE,hE,Js,co,fc,_E,uE,Qs,_t,pE,hc,EE,vE,Vs,Q,ut,_c,mE,gE,uc,CE,OE,DE,pt,pc,wE,bE,Ec,SE,PE,yE,Et,vc,TE,LE,mc,IE,AE,en,io,gc,$E,NE,tn,ko,UE,on,b,vt,Cc,ME,kE,Oc,RE,GE,WE,mt,Dc,xE,FE,wc,HE,zE,BE,gt,bc,KE,jE,Sc,YE,ZE,qE,Ct,Pc,XE,JE,yc,QE,VE,ev,Ot,Tc,tv,ov,Lc,lv,rv,ln,se,Dt,Ic,so,av,Ac,cv,rn,wt,$c,iv,sv,Nc,nv,an,bt,dv,Uc,fv,hv,cn,no,Mc,_v,uv,sn,fo,nn,ho,kc,pv,Ev,dn,St,C,Rc,vv,mv,Gc,gv,Cv,Wc,Ov,Dv,xc,wv,bv,Fc,Sv,Pv,Hc,yv,Tv,Lv,V,zc,Iv,Av,Bc,$v,Nv,Kc,Uv,Mv,fn;return Lt=new Bo({}),It=new Bo({}),Nt=new zi({props:{code:"accelerate config [arguments]",highlighted:"accelerate config [arguments]"}}),Mt=new Bo({}),Wt=new zi({props:{code:"accelerate env [arguments]",highlighted:'accelerate <span class="hljs-built_in">env</span> [arguments]'}}),Ft=new Bo({}),Bt=new zi({props:{code:"accelerate launch [arguments] {training_script} --{training_script-argument-1} --{training_script-argument-2} ...",highlighted:"accelerate launch [arguments] {training_script} --{training_script-argument-1} --{training_script-argument-2} ..."}}),oo=new Bo({}),ro=new zi({props:{code:"accelerate tpu-config [arguments]",highlighted:"accelerate tpu-config [arguments]"}}),so=new Bo({}),fo=new zi({props:{code:"accelerate test [arguments]",highlighted:'accelerate <span class="hljs-built_in">test</span> [arguments]'}}),{c(){oe=a("meta"),Bi=d(),le=a("h1"),ne=a("a"),Ko=a("span"),U(Lt.$$.fragment),$n=d(),jo=a("span"),Nn=l("The Command Line"),Ki=d(),Co=a("p"),Un=l("Below is a list of all the available commands \u{1F917} Accelerate with their parameters"),ji=d(),re=a("h2"),de=a("a"),Yo=a("span"),U(It.$$.fragment),Mn=d(),Zo=a("span"),kn=l("accelerate config"),Yi=d(),At=a("p"),qo=a("strong"),Rn=l("Command"),Gn=l(":"),Zi=d(),fe=a("p"),Xo=a("code"),Wn=l("accelerate config"),xn=l(" or "),Jo=a("code"),Fn=l("accelerate-config"),qi=d(),he=a("p"),Hn=l("Launches a series of prompts to create and save a "),Qo=a("code"),zn=l("default_config.yml"),Bn=l(` configuration file for your training system. Should
always be ran first on your machine.`),Xi=d(),$t=a("p"),Vo=a("strong"),Kn=l("Usage"),jn=l(":"),Ji=d(),U(Nt.$$.fragment),Qi=d(),Ut=a("p"),el=a("strong"),Yn=l("Optional Arguments"),Zn=l(":"),Vi=d(),_e=a("ul"),m=a("li"),tl=a("code"),qn=l("--config_file CONFIG_FILE"),Xn=l(" ("),ol=a("code"),Jn=l("str"),Qn=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),ll=a("code"),Vn=l("HF_HOME"),ed=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),rl=a("code"),td=l("~/.cache"),od=l(" or the content of "),al=a("code"),ld=l("XDG_CACHE_HOME"),rd=l(") suffixed with "),cl=a("code"),ad=l("huggingface"),cd=l("."),id=d(),x=a("li"),il=a("code"),sd=l("-h"),nd=l(", "),sl=a("code"),dd=l("--help"),fd=l(" ("),nl=a("code"),hd=l("bool"),_d=l(") \u2014 Show a help message and exit"),es=d(),ae=a("h2"),ue=a("a"),dl=a("span"),U(Mt.$$.fragment),ud=d(),fl=a("span"),pd=l("accelerate env"),ts=d(),kt=a("p"),hl=a("strong"),Ed=l("Command"),vd=l(":"),os=d(),pe=a("p"),_l=a("code"),md=l("accelerate env"),gd=l(" or "),ul=a("code"),Cd=l("accelerate-env"),ls=d(),Ee=a("p"),Od=l("Lists the contents of the passed \u{1F917} Accelerate configuration file. Should always be used when opening an issue on the "),Rt=a("a"),Dd=l("GitHub repository"),wd=l("."),rs=d(),Gt=a("p"),pl=a("strong"),bd=l("Usage"),Sd=l(":"),as=d(),U(Wt.$$.fragment),cs=d(),xt=a("p"),El=a("strong"),Pd=l("Optional Arguments"),yd=l(":"),is=d(),ve=a("ul"),g=a("li"),vl=a("code"),Td=l("--config_file CONFIG_FILE"),Ld=l(" ("),ml=a("code"),Id=l("str"),Ad=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),gl=a("code"),$d=l("HF_HOME"),Nd=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),Cl=a("code"),Ud=l("~/.cache"),Md=l(" or the content of "),Ol=a("code"),kd=l("XDG_CACHE_HOME"),Rd=l(") suffixed with "),Dl=a("code"),Gd=l("huggingface"),Wd=l("."),xd=d(),F=a("li"),wl=a("code"),Fd=l("-h"),Hd=l(", "),bl=a("code"),zd=l("--help"),Bd=l(" ("),Sl=a("code"),Kd=l("bool"),jd=l(") \u2014 Show a help message and exit"),ss=d(),ce=a("h2"),me=a("a"),Pl=a("span"),U(Ft.$$.fragment),Yd=d(),yl=a("span"),Zd=l("accelerate launch"),ns=d(),Ht=a("p"),Tl=a("strong"),qd=l("Command"),Xd=l(":"),ds=d(),ge=a("p"),Ll=a("code"),Jd=l("accelerate launch"),Qd=l(" or "),Il=a("code"),Vd=l("accelerate-launch"),fs=d(),Oo=a("p"),ef=l("Launches a specified script on a distributed system with the right parameters."),hs=d(),zt=a("p"),Al=a("strong"),tf=l("Usage"),of=l(":"),_s=d(),U(Bt.$$.fragment),us=d(),Kt=a("p"),$l=a("strong"),lf=l("Positional Arguments"),rf=l(":"),ps=d(),Ce=a("ul"),Do=a("li"),Nl=a("code"),af=l("{training_script}"),cf=l(" \u2014 The full path to the script to be launched in parallel"),sf=d(),wo=a("li"),Ul=a("code"),nf=l("--{training_script-argument-1}"),df=l(" \u2014 Arguments of the training script"),Es=d(),jt=a("p"),Ml=a("strong"),ff=l("Optional Arguments"),hf=l(":"),vs=d(),D=a("ul"),H=a("li"),kl=a("code"),_f=l("-h"),uf=l(", "),Rl=a("code"),pf=l("--help"),Ef=l(" ("),Gl=a("code"),vf=l("bool"),mf=l(") \u2014 Show a help message and exit"),gf=d(),Oe=a("li"),Wl=a("code"),Cf=l("--config_file CONFIG_FILE"),Of=l(" ("),xl=a("code"),Df=l("str"),wf=l(")\u2014 The config file to use for the default values in the launching script."),bf=d(),z=a("li"),Fl=a("code"),Sf=l("-m"),Pf=l(", "),Hl=a("code"),yf=l("--module"),Tf=l(" ("),zl=a("code"),Lf=l("bool"),If=l(") \u2014 Change each process to interpret the launch script as a Python module, executing with the same behavior as \u2018python -m\u2019."),Af=d(),De=a("li"),Bl=a("code"),$f=l("--no_python"),Nf=l(" ("),Kl=a("code"),Uf=l("bool"),Mf=l(") \u2014 Skip prepending the training script with \u2018python\u2019 - just execute it directly. Useful when the script is not a Python script."),kf=d(),we=a("li"),jl=a("code"),Rf=l("--debug"),Gf=l(" ("),Yl=a("code"),Wf=l("bool"),xf=l(") \u2014 Whether to print out the torch.distributed stack trace when something fails."),ms=d(),B=a("p"),Ff=l("The rest of these arguments are configured through "),Zl=a("code"),Hf=l("accelerate config"),zf=l(" and are read in from the specified "),ql=a("code"),Bf=l("--config_file"),Kf=l(` (or default configuration) for their
values. They can also be passed in manually.`),gs=d(),Yt=a("p"),Xl=a("strong"),jf=l("Hardware Selection Arguments"),Yf=l(":"),Cs=d(),I=a("ul"),be=a("li"),Jl=a("code"),Zf=l("--cpu"),qf=l(" ("),Ql=a("code"),Xf=l("bool"),Jf=l(") \u2014 Whether or not to force the training on the CPU."),Qf=d(),Se=a("li"),Vl=a("code"),Vf=l("--multi_gpu"),eh=l(" ("),er=a("code"),th=l("bool"),oh=l(") \u2014 Whether or not this should launch a distributed GPU training."),lh=d(),Pe=a("li"),tr=a("code"),rh=l("--mps"),ah=l(" ("),or=a("code"),ch=l("bool"),ih=l(") \u2014 Whether or not this should use MPS-enabled GPU device on MacOS machines."),sh=d(),ye=a("li"),lr=a("code"),nh=l("--tpu"),dh=l(" ("),rr=a("code"),fh=l("bool"),hh=l(") \u2014 Whether or not this should launch a TPU training."),Os=d(),Zt=a("p"),ar=a("strong"),_h=l("Resource Selection Arguments"),uh=l(":"),Ds=d(),bo=a("p"),ph=l("The following arguments are useful for fine-tuning how available hardware should be used"),ws=d(),A=a("ul"),Te=a("li"),cr=a("code"),Eh=l("--mixed_precision {no,fp16,bf16}"),vh=l(" ("),ir=a("code"),mh=l("str"),gh=l(") \u2014 Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later."),Ch=d(),Le=a("li"),sr=a("code"),Oh=l("--num_processes NUM_PROCESSES"),Dh=l(" ("),nr=a("code"),wh=l("int"),bh=l(") \u2014 The total number of processes to be launched in parallel."),Sh=d(),Ie=a("li"),dr=a("code"),Ph=l("--num_machines NUM_MACHINES"),yh=l(" ("),fr=a("code"),Th=l("int"),Lh=l(") \u2014 The total number of machines used in this training."),Ih=d(),Ae=a("li"),hr=a("code"),Ah=l("--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS"),$h=l(" ("),_r=a("code"),Nh=l("int"),Uh=l(") \u2014 The number of CPU threads per process. Can be tuned for optimal performance."),bs=d(),qt=a("p"),ur=a("strong"),Mh=l("Training Paradigm Arguments"),kh=l(":"),Ss=d(),So=a("p"),Rh=l("The following arguments are useful for selecting which training paradigm to use."),Ps=d(),K=a("ul"),$e=a("li"),pr=a("code"),Gh=l("--use_deepspeed"),Wh=l(" ("),Er=a("code"),xh=l("bool"),Fh=l(") \u2014 Whether or not to use DeepSpeed for training."),Hh=d(),Ne=a("li"),vr=a("code"),zh=l("--use_fsdp"),Bh=l(" ("),mr=a("code"),Kh=l("bool"),jh=l(") \u2014 Whether or not to use FullyShardedDataParallel for training."),Yh=d(),Ue=a("li"),gr=a("code"),Zh=l("--use_megatron_lm"),qh=l(" ("),Cr=a("code"),Xh=l("bool"),Jh=l(") \u2014 Whether or not to use Megatron-LM for training."),ys=d(),Xt=a("p"),Or=a("strong"),Qh=l("Distributed GPU Arguments"),Vh=l(":"),Ts=d(),j=a("p"),e_=l("The following arguments are only useful when "),Dr=a("code"),t_=l("multi_gpu"),o_=l(" is passed or multi-gpu training is configured through "),wr=a("code"),l_=l("accelerate config"),r_=l(":"),Ls=d(),p=a("ul"),Me=a("li"),br=a("code"),a_=l("--gpu_ids"),c_=l(" ("),Sr=a("code"),i_=l("str"),s_=l(") \u2014 What GPUs (by id) should be used for training on this machine as a comma-seperated list"),n_=d(),ke=a("li"),Pr=a("code"),d_=l("--same_network"),f_=l(" ("),yr=a("code"),h_=l("bool"),__=l(") \u2014 Whether all machines used for multinode training exist on the same local network."),u_=d(),Re=a("li"),Tr=a("code"),p_=l("--machine_rank MACHINE_RANK"),E_=l(" ("),Lr=a("code"),v_=l("int"),m_=l(") \u2014 The rank of the machine on which this script is launched."),g_=d(),Ge=a("li"),Ir=a("code"),C_=l("--main_process_ip MAIN_PROCESS_IP"),O_=l(" ("),Ar=a("code"),D_=l("str"),w_=l(") \u2014 The IP address of the machine of rank 0."),b_=d(),We=a("li"),$r=a("code"),S_=l("--main_process_port MAIN_PROCESS_PORT"),P_=l(" ("),Nr=a("code"),y_=l("int"),T_=l(") \u2014 The port to use to communicate with the machine of rank 0."),L_=d(),xe=a("li"),Ur=a("code"),I_=l("--rdzv_conf"),A_=l(" ("),Mr=a("code"),$_=l("str"),N_=l(") \u2014 Additional rendezvous configuration (<key1>=<value1>,<key2>=<value2>,\u2026)."),U_=d(),Fe=a("li"),kr=a("code"),M_=l("--max_restarts"),k_=l(" ("),Rr=a("code"),R_=l("int"),G_=l(") \u2014 Maximum number of worker group restarts before failing."),W_=d(),He=a("li"),Gr=a("code"),x_=l("--monitor_interval"),F_=l(" ("),Wr=a("code"),H_=l("float"),z_=l(") \u2014 Interval, in seconds, to monitor the state of workers."),Is=d(),Jt=a("p"),xr=a("strong"),B_=l("TPU Arguments"),K_=l(":"),As=d(),Y=a("p"),j_=l("The following arguments are only useful when "),Fr=a("code"),Y_=l("tpu"),Z_=l(" is passed or TPU training is configured through "),Hr=a("code"),q_=l("accelerate config"),X_=l(":"),$s=d(),ze=a("ul"),Be=a("li"),zr=a("code"),J_=l("--main_training_function MAIN_TRAINING_FUNCTION"),Q_=l(" ("),Br=a("code"),V_=l("str"),eu=l(") \u2014 The name of the main function to be executed in your script."),tu=d(),Ke=a("li"),Kr=a("code"),ou=l("--downcast_bf16"),lu=l(" ("),jr=a("code"),ru=l("bool"),au=l(") \u2014 Whether when using bf16 precision on TPUs if both float and double tensors are cast to bfloat16 or if double tensors remain as float32."),Ns=d(),Qt=a("p"),Yr=a("strong"),cu=l("DeepSpeed Arguments"),iu=l(":"),Us=d(),$=a("p"),su=l("The following arguments are only useful when "),Zr=a("code"),nu=l("use_deepspeed"),du=l(" is passed or "),qr=a("code"),fu=l("deepspeed"),hu=l(" is configured through "),Xr=a("code"),_u=l("accelerate config"),uu=l(":"),Ms=d(),h=a("ul"),je=a("li"),Jr=a("code"),pu=l("--deepspeed_config_file"),Eu=l(" ("),Qr=a("code"),vu=l("str"),mu=l(") \u2014 DeepSpeed config file."),gu=d(),Ye=a("li"),Vr=a("code"),Cu=l("--zero_stage"),Ou=l(" ("),ea=a("code"),Du=l("int"),wu=l(") \u2014 DeepSpeed\u2019s ZeRO optimization stage."),bu=d(),Ze=a("li"),ta=a("code"),Su=l("--offload_optimizer_device"),Pu=l(" ("),oa=a("code"),yu=l("str"),Tu=l(") \u2014 Decides where (none|cpu|nvme) to offload optimizer states."),Lu=d(),qe=a("li"),la=a("code"),Iu=l("--offload_param_device"),Au=l(" ("),ra=a("code"),$u=l("str"),Nu=l(") \u2014 Decides where (none|cpu|nvme) to offload parameters."),Uu=d(),Xe=a("li"),aa=a("code"),Mu=l("--gradient_accumulation_steps"),ku=l(" ("),ca=a("code"),Ru=l("int"),Gu=l(") \u2014 No of gradient_accumulation_steps used in your training script."),Wu=d(),Je=a("li"),ia=a("code"),xu=l("--gradient_clipping"),Fu=l(" ("),sa=a("code"),Hu=l("float"),zu=l(") \u2014 Gradient clipping value used in your training script."),Bu=d(),Z=a("li"),na=a("code"),Ku=l("--zero3_init_flag"),ju=l(" ("),da=a("code"),Yu=l("str"),Zu=l(") \u2014 Decides Whether (true|false) to enable "),fa=a("code"),qu=l("deepspeed.zero.Init"),Xu=l(" for constructing massive models. Only applicable with DeepSpeed ZeRO Stage-3."),Ju=d(),Qe=a("li"),ha=a("code"),Qu=l("--zero3_save_16bit_model"),Vu=l(" ("),_a=a("code"),ep=l("str"),tp=l(") \u2014 Decides Whether (true|false) to save 16-bit model weights when using ZeRO Stage-3. Only applicable with DeepSpeed ZeRO Stage-3."),op=d(),Ve=a("li"),ua=a("code"),lp=l("--deepspeed_hostfile"),rp=l(" ("),pa=a("code"),ap=l("str"),cp=l(") \u2014 DeepSpeed hostfile for configuring multi-node compute resources."),ip=d(),et=a("li"),Ea=a("code"),sp=l("--deepspeed_exclusion_filter"),np=l(" ("),va=a("code"),dp=l("str"),fp=l(") \u2014 DeepSpeed exclusion filter string when using mutli-node setup."),hp=d(),tt=a("li"),ma=a("code"),_p=l("--deepspeed_inclusion_filter"),up=l(" ("),ga=a("code"),pp=l("str"),Ep=l(") \u2014 DeepSpeed inclusion filter string when using mutli-node setup."),vp=d(),ot=a("li"),Ca=a("code"),mp=l("--deepspeed_multinode_launcher"),gp=l(" ("),Oa=a("code"),Cp=l("str"),Op=l(") \u2014 DeepSpeed multi-node launcher to use."),ks=d(),Vt=a("p"),Da=a("strong"),Dp=l("Fully Sharded Data Parallelism Arguments"),wp=l(":"),Rs=d(),q=a("p"),bp=l("The following arguments are only useful when "),wa=a("code"),Sp=l("use_fdsp"),Pp=l(" is passed or Fully Sharded Data Parallelism is configured through "),ba=a("code"),yp=l("accelerate config"),Tp=l(":"),Gs=d(),E=a("ul"),lt=a("li"),Sa=a("code"),Lp=l("--fsdp_offload_params"),Ip=l(" ("),Pa=a("code"),Ap=l("str"),$p=l(") \u2014 Decides Whether (true|false) to offload parameters and gradients to CPU."),Np=d(),rt=a("li"),ya=a("code"),Up=l("--fsdp_min_num_params"),Mp=l(" ("),Ta=a("code"),kp=l("int"),Rp=l(") \u2014 FSDP\u2019s minimum number of parameters for Default Auto Wrapping."),Gp=d(),at=a("li"),La=a("code"),Wp=l("--fsdp_sharding_strategy"),xp=l(" ("),Ia=a("code"),Fp=l("int"),Hp=l(") \u2014 FSDP\u2019s Sharding Strategy."),zp=d(),ct=a("li"),Aa=a("code"),Bp=l("--fsdp_auto_wrap_policy"),Kp=l(" ("),$a=a("code"),jp=l("str"),Yp=l(") \u2014 FSDP\u2019s auto wrap policy."),Zp=d(),w=a("li"),Na=a("code"),qp=l("--fsdp_transformer_layer_cls_to_wrap"),Xp=l(" ("),Ua=a("code"),Jp=l("str"),Qp=l(") \u2014 Transformer layer class name (case-sensitive) to wrap, e.g, "),Ma=a("code"),Vp=l("BertLayer"),e1=l(", "),ka=a("code"),t1=l("GPTJBlock"),o1=l(", "),Ra=a("code"),l1=l("T5Block"),r1=l(" \u2026"),a1=d(),it=a("li"),Ga=a("code"),c1=l("--fsdp_backward_prefetch_policy"),i1=l(" ("),Wa=a("code"),s1=l("str"),n1=l(") \u2014 FSDP\u2019s backward prefetch policy."),d1=d(),st=a("li"),xa=a("code"),f1=l("--fsdp_state_dict_type"),h1=l(" ("),Fa=a("code"),_1=l("str"),u1=l(") \u2014 FSDP\u2019s state dict type."),Ws=d(),eo=a("p"),Ha=a("strong"),p1=l("Megatron-LM Arguments"),E1=l(":"),xs=d(),X=a("p"),v1=l("The following arguments are only useful when "),za=a("code"),m1=l("use_megatron_lm"),g1=l(" is passed or Megatron-LM is configured through "),Ba=a("code"),C1=l("accelerate config"),O1=l(":"),Fs=d(),v=a("ul"),Po=a("li"),Ka=a("code"),D1=l("--megatron_lm_tp_degree"),w1=l(" (\u201C) \u2014 Megatron-LM\u2019s Tensor Parallelism (TP) degree."),b1=d(),yo=a("li"),ja=a("code"),S1=l("--megatron_lm_pp_degree"),P1=l(" (\u201C) \u2014 Megatron-LM\u2019s Pipeline Parallelism (PP) degree."),y1=d(),To=a("li"),Ya=a("code"),T1=l("--megatron_lm_num_micro_batches"),L1=l(" (\u201C) \u2014 Megatron-LM\u2019s number of micro batches when PP degree > 1."),I1=d(),Lo=a("li"),Za=a("code"),A1=l("--megatron_lm_sequence_parallelism"),$1=l(" (\u201C) \u2014 Decides Whether (true|false) to enable Sequence Parallelism when TP degree > 1."),N1=d(),Io=a("li"),qa=a("code"),U1=l("--megatron_lm_recompute_activations"),M1=l(" (\u201C) \u2014 Decides Whether (true|false) to enable Selective Activation Recomputation."),k1=d(),Ao=a("li"),Xa=a("code"),R1=l("--megatron_lm_use_distributed_optimizer"),G1=l(" (\u201C) \u2014 Decides Whether (true|false) to use distributed optimizer which shards optimizer state and gradients across Data Pralellel (DP) ranks."),W1=d(),$o=a("li"),Ja=a("code"),x1=l("--megatron_lm_gradient_clipping"),F1=l(" (\u201C) \u2014 Megatron-LM\u2019s gradient clipping value based on global L2 Norm (0 to disable)."),Hs=d(),to=a("p"),Qa=a("strong"),H1=l("AWS SageMaker Arguments"),z1=l(":"),zs=d(),No=a("p"),B1=l("The following arguments are only useful when training in SageMaker"),Bs=d(),nt=a("ul"),dt=a("li"),Va=a("code"),K1=l("--aws_access_key_id AWS_ACCESS_KEY_ID"),j1=l(" ("),ec=a("code"),Y1=l("str"),Z1=l(") \u2014 The AWS_ACCESS_KEY_ID used to launch the Amazon SageMaker training job"),q1=d(),ft=a("li"),tc=a("code"),X1=l("--aws_secret_access_key AWS_SECRET_ACCESS_KEY"),J1=l(" ("),oc=a("code"),Q1=l("str"),V1=l(") \u2014 The AWS_SECRET_ACCESS_KEY used to launch the Amazon SageMaker training job"),Ks=d(),ie=a("h2"),ht=a("a"),lc=a("span"),U(oo.$$.fragment),eE=d(),rc=a("span"),tE=l("accelerate tpu-config"),js=d(),Uo=a("p"),ac=a("code"),oE=l("accelerate tpu-config"),Ys=d(),lo=a("p"),cc=a("strong"),lE=l("Usage"),rE=l(":"),Zs=d(),U(ro.$$.fragment),qs=d(),ao=a("p"),ic=a("strong"),aE=l("Optional Arguments"),cE=l(":"),Xs=d(),Mo=a("ul"),J=a("li"),sc=a("code"),iE=l("-h"),sE=l(", "),nc=a("code"),nE=l("--help"),dE=l(" ("),dc=a("code"),fE=l("bool"),hE=l(") \u2014 Show a help message and exit"),Js=d(),co=a("p"),fc=a("strong"),_E=l("Config Arguments"),uE=l(":"),Qs=d(),_t=a("p"),pE=l("Arguments that can be configured through "),hc=a("code"),EE=l("accelerate config"),vE=l("."),Vs=d(),Q=a("ul"),ut=a("li"),_c=a("code"),mE=l("--config_file"),gE=l(" ("),uc=a("code"),CE=l("str"),OE=l(") \u2014 Path to the config file to use for accelerate."),DE=d(),pt=a("li"),pc=a("code"),wE=l("--tpu_name"),bE=l(" ("),Ec=a("code"),SE=l("str"),PE=l(") \u2014 The name of the TPU to use. If not specified, will use the TPU specified in the config file."),yE=d(),Et=a("li"),vc=a("code"),TE=l("--tpu_zone"),LE=l(" ("),mc=a("code"),IE=l("str"),AE=l(") \u2014 The zone of the TPU to use. If not specified, will use the zone specified in the config file."),en=d(),io=a("p"),gc=a("strong"),$E=l("TPU Arguments"),NE=l(":"),tn=d(),ko=a("p"),UE=l("Arguments for options ran inside the TPU."),on=d(),b=a("ul"),vt=a("li"),Cc=a("code"),ME=l("--command_file"),kE=l(" ("),Oc=a("code"),RE=l("str"),GE=l(") \u2014 The path to the file containing the commands to run on the pod on startup."),WE=d(),mt=a("li"),Dc=a("code"),xE=l("--command"),FE=l(" ("),wc=a("code"),HE=l("str"),zE=l(") \u2014 A command to run on the pod. Can be passed multiple times."),BE=d(),gt=a("li"),bc=a("code"),KE=l("--install_accelerate"),jE=l(" ("),Sc=a("code"),YE=l("bool"),ZE=l(") \u2014 Whether to install accelerate on the pod. Defaults to False."),qE=d(),Ct=a("li"),Pc=a("code"),XE=l("--accelerate_version"),JE=l(" ("),yc=a("code"),QE=l("str"),VE=l(") \u2014 The version of accelerate to install on the pod. If not specified, will use the latest pypi version. Specify \u2018dev\u2019 to install from GitHub."),ev=d(),Ot=a("li"),Tc=a("code"),tv=l("--debug"),ov=l(" ("),Lc=a("code"),lv=l("bool"),rv=l(") \u2014 If set, will print the command that would be run instead of running it."),ln=d(),se=a("h2"),Dt=a("a"),Ic=a("span"),U(so.$$.fragment),av=d(),Ac=a("span"),cv=l("accelerate test"),rn=d(),wt=a("p"),$c=a("code"),iv=l("accelerate test"),sv=l(" or "),Nc=a("code"),nv=l("accelerate-test"),an=d(),bt=a("p"),dv=l("Runs "),Uc=a("code"),fv=l("accelerate/test_utils/test_script.py"),hv=l(" to verify that \u{1F917} Accelerate has been properly configured on your system and runs."),cn=d(),no=a("p"),Mc=a("strong"),_v=l("Usage"),uv=l(":"),sn=d(),U(fo.$$.fragment),nn=d(),ho=a("p"),kc=a("strong"),pv=l("Optional Arguments"),Ev=l(":"),dn=d(),St=a("ul"),C=a("li"),Rc=a("code"),vv=l("--config_file CONFIG_FILE"),mv=l(" ("),Gc=a("code"),gv=l("str"),Cv=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Wc=a("code"),Ov=l("HF_HOME"),Dv=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),xc=a("code"),wv=l("~/.cache"),bv=l(" or the content of "),Fc=a("code"),Sv=l("XDG_CACHE_HOME"),Pv=l(") suffixed with "),Hc=a("code"),yv=l("huggingface"),Tv=l("."),Lv=d(),V=a("li"),zc=a("code"),Iv=l("-h"),Av=l(", "),Bc=a("code"),$v=l("--help"),Nv=l(" ("),Kc=a("code"),Uv=l("bool"),Mv=l(") \u2014 Show a help message and exit"),this.h()},l(o){const s=kg('[data-svelte="svelte-1phssyn"]',document.head);oe=c(s,"META",{name:!0,content:!0}),s.forEach(t),Bi=f(o),le=c(o,"H1",{class:!0});var hn=i(le);ne=c(hn,"A",{id:!0,class:!0,href:!0});var vm=i(ne);Ko=c(vm,"SPAN",{});var mm=i(Ko);M(Lt.$$.fragment,mm),mm.forEach(t),vm.forEach(t),$n=f(hn),jo=c(hn,"SPAN",{});var gm=i(jo);Nn=r(gm,"The Command Line"),gm.forEach(t),hn.forEach(t),Ki=f(o),Co=c(o,"P",{});var Cm=i(Co);Un=r(Cm,"Below is a list of all the available commands \u{1F917} Accelerate with their parameters"),Cm.forEach(t),ji=f(o),re=c(o,"H2",{class:!0});var _n=i(re);de=c(_n,"A",{id:!0,class:!0,href:!0});var Om=i(de);Yo=c(Om,"SPAN",{});var Dm=i(Yo);M(It.$$.fragment,Dm),Dm.forEach(t),Om.forEach(t),Mn=f(_n),Zo=c(_n,"SPAN",{});var wm=i(Zo);kn=r(wm,"accelerate config"),wm.forEach(t),_n.forEach(t),Yi=f(o),At=c(o,"P",{});var kv=i(At);qo=c(kv,"STRONG",{});var bm=i(qo);Rn=r(bm,"Command"),bm.forEach(t),Gn=r(kv,":"),kv.forEach(t),Zi=f(o),fe=c(o,"P",{});var un=i(fe);Xo=c(un,"CODE",{});var Sm=i(Xo);Wn=r(Sm,"accelerate config"),Sm.forEach(t),xn=r(un," or "),Jo=c(un,"CODE",{});var Pm=i(Jo);Fn=r(Pm,"accelerate-config"),Pm.forEach(t),un.forEach(t),qi=f(o),he=c(o,"P",{});var pn=i(he);Hn=r(pn,"Launches a series of prompts to create and save a "),Qo=c(pn,"CODE",{});var ym=i(Qo);zn=r(ym,"default_config.yml"),ym.forEach(t),Bn=r(pn,` configuration file for your training system. Should
always be ran first on your machine.`),pn.forEach(t),Xi=f(o),$t=c(o,"P",{});var Rv=i($t);Vo=c(Rv,"STRONG",{});var Tm=i(Vo);Kn=r(Tm,"Usage"),Tm.forEach(t),jn=r(Rv,":"),Rv.forEach(t),Ji=f(o),M(Nt.$$.fragment,o),Qi=f(o),Ut=c(o,"P",{});var Gv=i(Ut);el=c(Gv,"STRONG",{});var Lm=i(el);Yn=r(Lm,"Optional Arguments"),Lm.forEach(t),Zn=r(Gv,":"),Gv.forEach(t),Vi=f(o),_e=c(o,"UL",{});var En=i(_e);m=c(En,"LI",{});var y=i(m);tl=c(y,"CODE",{});var Im=i(tl);qn=r(Im,"--config_file CONFIG_FILE"),Im.forEach(t),Xn=r(y," ("),ol=c(y,"CODE",{});var Am=i(ol);Jn=r(Am,"str"),Am.forEach(t),Qn=r(y,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),ll=c(y,"CODE",{});var $m=i(ll);Vn=r($m,"HF_HOME"),$m.forEach(t),ed=r(y,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),rl=c(y,"CODE",{});var Nm=i(rl);td=r(Nm,"~/.cache"),Nm.forEach(t),od=r(y," or the content of "),al=c(y,"CODE",{});var Um=i(al);ld=r(Um,"XDG_CACHE_HOME"),Um.forEach(t),rd=r(y,") suffixed with "),cl=c(y,"CODE",{});var Mm=i(cl);ad=r(Mm,"huggingface"),Mm.forEach(t),cd=r(y,"."),y.forEach(t),id=f(En),x=c(En,"LI",{});var _o=i(x);il=c(_o,"CODE",{});var km=i(il);sd=r(km,"-h"),km.forEach(t),nd=r(_o,", "),sl=c(_o,"CODE",{});var Rm=i(sl);dd=r(Rm,"--help"),Rm.forEach(t),fd=r(_o," ("),nl=c(_o,"CODE",{});var Gm=i(nl);hd=r(Gm,"bool"),Gm.forEach(t),_d=r(_o,") \u2014 Show a help message and exit"),_o.forEach(t),En.forEach(t),es=f(o),ae=c(o,"H2",{class:!0});var vn=i(ae);ue=c(vn,"A",{id:!0,class:!0,href:!0});var Wm=i(ue);dl=c(Wm,"SPAN",{});var xm=i(dl);M(Mt.$$.fragment,xm),xm.forEach(t),Wm.forEach(t),ud=f(vn),fl=c(vn,"SPAN",{});var Fm=i(fl);pd=r(Fm,"accelerate env"),Fm.forEach(t),vn.forEach(t),ts=f(o),kt=c(o,"P",{});var Wv=i(kt);hl=c(Wv,"STRONG",{});var Hm=i(hl);Ed=r(Hm,"Command"),Hm.forEach(t),vd=r(Wv,":"),Wv.forEach(t),os=f(o),pe=c(o,"P",{});var mn=i(pe);_l=c(mn,"CODE",{});var zm=i(_l);md=r(zm,"accelerate env"),zm.forEach(t),gd=r(mn," or "),ul=c(mn,"CODE",{});var Bm=i(ul);Cd=r(Bm,"accelerate-env"),Bm.forEach(t),mn.forEach(t),ls=f(o),Ee=c(o,"P",{});var gn=i(Ee);Od=r(gn,"Lists the contents of the passed \u{1F917} Accelerate configuration file. Should always be used when opening an issue on the "),Rt=c(gn,"A",{href:!0,rel:!0});var Km=i(Rt);Dd=r(Km,"GitHub repository"),Km.forEach(t),wd=r(gn,"."),gn.forEach(t),rs=f(o),Gt=c(o,"P",{});var xv=i(Gt);pl=c(xv,"STRONG",{});var jm=i(pl);bd=r(jm,"Usage"),jm.forEach(t),Sd=r(xv,":"),xv.forEach(t),as=f(o),M(Wt.$$.fragment,o),cs=f(o),xt=c(o,"P",{});var Fv=i(xt);El=c(Fv,"STRONG",{});var Ym=i(El);Pd=r(Ym,"Optional Arguments"),Ym.forEach(t),yd=r(Fv,":"),Fv.forEach(t),is=f(o),ve=c(o,"UL",{});var Cn=i(ve);g=c(Cn,"LI",{});var T=i(g);vl=c(T,"CODE",{});var Zm=i(vl);Td=r(Zm,"--config_file CONFIG_FILE"),Zm.forEach(t),Ld=r(T," ("),ml=c(T,"CODE",{});var qm=i(ml);Id=r(qm,"str"),qm.forEach(t),Ad=r(T,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),gl=c(T,"CODE",{});var Xm=i(gl);$d=r(Xm,"HF_HOME"),Xm.forEach(t),Nd=r(T,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),Cl=c(T,"CODE",{});var Jm=i(Cl);Ud=r(Jm,"~/.cache"),Jm.forEach(t),Md=r(T," or the content of "),Ol=c(T,"CODE",{});var Qm=i(Ol);kd=r(Qm,"XDG_CACHE_HOME"),Qm.forEach(t),Rd=r(T,") suffixed with "),Dl=c(T,"CODE",{});var Vm=i(Dl);Gd=r(Vm,"huggingface"),Vm.forEach(t),Wd=r(T,"."),T.forEach(t),xd=f(Cn),F=c(Cn,"LI",{});var uo=i(F);wl=c(uo,"CODE",{});var e2=i(wl);Fd=r(e2,"-h"),e2.forEach(t),Hd=r(uo,", "),bl=c(uo,"CODE",{});var t2=i(bl);zd=r(t2,"--help"),t2.forEach(t),Bd=r(uo," ("),Sl=c(uo,"CODE",{});var o2=i(Sl);Kd=r(o2,"bool"),o2.forEach(t),jd=r(uo,") \u2014 Show a help message and exit"),uo.forEach(t),Cn.forEach(t),ss=f(o),ce=c(o,"H2",{class:!0});var On=i(ce);me=c(On,"A",{id:!0,class:!0,href:!0});var l2=i(me);Pl=c(l2,"SPAN",{});var r2=i(Pl);M(Ft.$$.fragment,r2),r2.forEach(t),l2.forEach(t),Yd=f(On),yl=c(On,"SPAN",{});var a2=i(yl);Zd=r(a2,"accelerate launch"),a2.forEach(t),On.forEach(t),ns=f(o),Ht=c(o,"P",{});var Hv=i(Ht);Tl=c(Hv,"STRONG",{});var c2=i(Tl);qd=r(c2,"Command"),c2.forEach(t),Xd=r(Hv,":"),Hv.forEach(t),ds=f(o),ge=c(o,"P",{});var Dn=i(ge);Ll=c(Dn,"CODE",{});var i2=i(Ll);Jd=r(i2,"accelerate launch"),i2.forEach(t),Qd=r(Dn," or "),Il=c(Dn,"CODE",{});var s2=i(Il);Vd=r(s2,"accelerate-launch"),s2.forEach(t),Dn.forEach(t),fs=f(o),Oo=c(o,"P",{});var n2=i(Oo);ef=r(n2,"Launches a specified script on a distributed system with the right parameters."),n2.forEach(t),hs=f(o),zt=c(o,"P",{});var zv=i(zt);Al=c(zv,"STRONG",{});var d2=i(Al);tf=r(d2,"Usage"),d2.forEach(t),of=r(zv,":"),zv.forEach(t),_s=f(o),M(Bt.$$.fragment,o),us=f(o),Kt=c(o,"P",{});var Bv=i(Kt);$l=c(Bv,"STRONG",{});var f2=i($l);lf=r(f2,"Positional Arguments"),f2.forEach(t),rf=r(Bv,":"),Bv.forEach(t),ps=f(o),Ce=c(o,"UL",{});var wn=i(Ce);Do=c(wn,"LI",{});var Kv=i(Do);Nl=c(Kv,"CODE",{});var h2=i(Nl);af=r(h2,"{training_script}"),h2.forEach(t),cf=r(Kv," \u2014 The full path to the script to be launched in parallel"),Kv.forEach(t),sf=f(wn),wo=c(wn,"LI",{});var jv=i(wo);Ul=c(jv,"CODE",{});var _2=i(Ul);nf=r(_2,"--{training_script-argument-1}"),_2.forEach(t),df=r(jv," \u2014 Arguments of the training script"),jv.forEach(t),wn.forEach(t),Es=f(o),jt=c(o,"P",{});var Yv=i(jt);Ml=c(Yv,"STRONG",{});var u2=i(Ml);ff=r(u2,"Optional Arguments"),u2.forEach(t),hf=r(Yv,":"),Yv.forEach(t),vs=f(o),D=c(o,"UL",{});var ee=i(D);H=c(ee,"LI",{});var po=i(H);kl=c(po,"CODE",{});var p2=i(kl);_f=r(p2,"-h"),p2.forEach(t),uf=r(po,", "),Rl=c(po,"CODE",{});var E2=i(Rl);pf=r(E2,"--help"),E2.forEach(t),Ef=r(po," ("),Gl=c(po,"CODE",{});var v2=i(Gl);vf=r(v2,"bool"),v2.forEach(t),mf=r(po,") \u2014 Show a help message and exit"),po.forEach(t),gf=f(ee),Oe=c(ee,"LI",{});var jc=i(Oe);Wl=c(jc,"CODE",{});var m2=i(Wl);Cf=r(m2,"--config_file CONFIG_FILE"),m2.forEach(t),Of=r(jc," ("),xl=c(jc,"CODE",{});var g2=i(xl);Df=r(g2,"str"),g2.forEach(t),wf=r(jc,")\u2014 The config file to use for the default values in the launching script."),jc.forEach(t),bf=f(ee),z=c(ee,"LI",{});var Eo=i(z);Fl=c(Eo,"CODE",{});var C2=i(Fl);Sf=r(C2,"-m"),C2.forEach(t),Pf=r(Eo,", "),Hl=c(Eo,"CODE",{});var O2=i(Hl);yf=r(O2,"--module"),O2.forEach(t),Tf=r(Eo," ("),zl=c(Eo,"CODE",{});var D2=i(zl);Lf=r(D2,"bool"),D2.forEach(t),If=r(Eo,") \u2014 Change each process to interpret the launch script as a Python module, executing with the same behavior as \u2018python -m\u2019."),Eo.forEach(t),Af=f(ee),De=c(ee,"LI",{});var Yc=i(De);Bl=c(Yc,"CODE",{});var w2=i(Bl);$f=r(w2,"--no_python"),w2.forEach(t),Nf=r(Yc," ("),Kl=c(Yc,"CODE",{});var b2=i(Kl);Uf=r(b2,"bool"),b2.forEach(t),Mf=r(Yc,") \u2014 Skip prepending the training script with \u2018python\u2019 - just execute it directly. Useful when the script is not a Python script."),Yc.forEach(t),kf=f(ee),we=c(ee,"LI",{});var Zc=i(we);jl=c(Zc,"CODE",{});var S2=i(jl);Rf=r(S2,"--debug"),S2.forEach(t),Gf=r(Zc," ("),Yl=c(Zc,"CODE",{});var P2=i(Yl);Wf=r(P2,"bool"),P2.forEach(t),xf=r(Zc,") \u2014 Whether to print out the torch.distributed stack trace when something fails."),Zc.forEach(t),ee.forEach(t),ms=f(o),B=c(o,"P",{});var Ro=i(B);Ff=r(Ro,"The rest of these arguments are configured through "),Zl=c(Ro,"CODE",{});var y2=i(Zl);Hf=r(y2,"accelerate config"),y2.forEach(t),zf=r(Ro," and are read in from the specified "),ql=c(Ro,"CODE",{});var T2=i(ql);Bf=r(T2,"--config_file"),T2.forEach(t),Kf=r(Ro,` (or default configuration) for their
values. They can also be passed in manually.`),Ro.forEach(t),gs=f(o),Yt=c(o,"P",{});var Zv=i(Yt);Xl=c(Zv,"STRONG",{});var L2=i(Xl);jf=r(L2,"Hardware Selection Arguments"),L2.forEach(t),Yf=r(Zv,":"),Zv.forEach(t),Cs=f(o),I=c(o,"UL",{});var Pt=i(I);be=c(Pt,"LI",{});var qc=i(be);Jl=c(qc,"CODE",{});var I2=i(Jl);Zf=r(I2,"--cpu"),I2.forEach(t),qf=r(qc," ("),Ql=c(qc,"CODE",{});var A2=i(Ql);Xf=r(A2,"bool"),A2.forEach(t),Jf=r(qc,") \u2014 Whether or not to force the training on the CPU."),qc.forEach(t),Qf=f(Pt),Se=c(Pt,"LI",{});var Xc=i(Se);Vl=c(Xc,"CODE",{});var $2=i(Vl);Vf=r($2,"--multi_gpu"),$2.forEach(t),eh=r(Xc," ("),er=c(Xc,"CODE",{});var N2=i(er);th=r(N2,"bool"),N2.forEach(t),oh=r(Xc,") \u2014 Whether or not this should launch a distributed GPU training."),Xc.forEach(t),lh=f(Pt),Pe=c(Pt,"LI",{});var Jc=i(Pe);tr=c(Jc,"CODE",{});var U2=i(tr);rh=r(U2,"--mps"),U2.forEach(t),ah=r(Jc," ("),or=c(Jc,"CODE",{});var M2=i(or);ch=r(M2,"bool"),M2.forEach(t),ih=r(Jc,") \u2014 Whether or not this should use MPS-enabled GPU device on MacOS machines."),Jc.forEach(t),sh=f(Pt),ye=c(Pt,"LI",{});var Qc=i(ye);lr=c(Qc,"CODE",{});var k2=i(lr);nh=r(k2,"--tpu"),k2.forEach(t),dh=r(Qc," ("),rr=c(Qc,"CODE",{});var R2=i(rr);fh=r(R2,"bool"),R2.forEach(t),hh=r(Qc,") \u2014 Whether or not this should launch a TPU training."),Qc.forEach(t),Pt.forEach(t),Os=f(o),Zt=c(o,"P",{});var qv=i(Zt);ar=c(qv,"STRONG",{});var G2=i(ar);_h=r(G2,"Resource Selection Arguments"),G2.forEach(t),uh=r(qv,":"),qv.forEach(t),Ds=f(o),bo=c(o,"P",{});var W2=i(bo);ph=r(W2,"The following arguments are useful for fine-tuning how available hardware should be used"),W2.forEach(t),ws=f(o),A=c(o,"UL",{});var yt=i(A);Te=c(yt,"LI",{});var Vc=i(Te);cr=c(Vc,"CODE",{});var x2=i(cr);Eh=r(x2,"--mixed_precision {no,fp16,bf16}"),x2.forEach(t),vh=r(Vc," ("),ir=c(Vc,"CODE",{});var F2=i(ir);mh=r(F2,"str"),F2.forEach(t),gh=r(Vc,") \u2014 Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later."),Vc.forEach(t),Ch=f(yt),Le=c(yt,"LI",{});var ei=i(Le);sr=c(ei,"CODE",{});var H2=i(sr);Oh=r(H2,"--num_processes NUM_PROCESSES"),H2.forEach(t),Dh=r(ei," ("),nr=c(ei,"CODE",{});var z2=i(nr);wh=r(z2,"int"),z2.forEach(t),bh=r(ei,") \u2014 The total number of processes to be launched in parallel."),ei.forEach(t),Sh=f(yt),Ie=c(yt,"LI",{});var ti=i(Ie);dr=c(ti,"CODE",{});var B2=i(dr);Ph=r(B2,"--num_machines NUM_MACHINES"),B2.forEach(t),yh=r(ti," ("),fr=c(ti,"CODE",{});var K2=i(fr);Th=r(K2,"int"),K2.forEach(t),Lh=r(ti,") \u2014 The total number of machines used in this training."),ti.forEach(t),Ih=f(yt),Ae=c(yt,"LI",{});var oi=i(Ae);hr=c(oi,"CODE",{});var j2=i(hr);Ah=r(j2,"--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS"),j2.forEach(t),$h=r(oi," ("),_r=c(oi,"CODE",{});var Y2=i(_r);Nh=r(Y2,"int"),Y2.forEach(t),Uh=r(oi,") \u2014 The number of CPU threads per process. Can be tuned for optimal performance."),oi.forEach(t),yt.forEach(t),bs=f(o),qt=c(o,"P",{});var Xv=i(qt);ur=c(Xv,"STRONG",{});var Z2=i(ur);Mh=r(Z2,"Training Paradigm Arguments"),Z2.forEach(t),kh=r(Xv,":"),Xv.forEach(t),Ss=f(o),So=c(o,"P",{});var q2=i(So);Rh=r(q2,"The following arguments are useful for selecting which training paradigm to use."),q2.forEach(t),Ps=f(o),K=c(o,"UL",{});var Go=i(K);$e=c(Go,"LI",{});var li=i($e);pr=c(li,"CODE",{});var X2=i(pr);Gh=r(X2,"--use_deepspeed"),X2.forEach(t),Wh=r(li," ("),Er=c(li,"CODE",{});var J2=i(Er);xh=r(J2,"bool"),J2.forEach(t),Fh=r(li,") \u2014 Whether or not to use DeepSpeed for training."),li.forEach(t),Hh=f(Go),Ne=c(Go,"LI",{});var ri=i(Ne);vr=c(ri,"CODE",{});var Q2=i(vr);zh=r(Q2,"--use_fsdp"),Q2.forEach(t),Bh=r(ri," ("),mr=c(ri,"CODE",{});var V2=i(mr);Kh=r(V2,"bool"),V2.forEach(t),jh=r(ri,") \u2014 Whether or not to use FullyShardedDataParallel for training."),ri.forEach(t),Yh=f(Go),Ue=c(Go,"LI",{});var ai=i(Ue);gr=c(ai,"CODE",{});var e3=i(gr);Zh=r(e3,"--use_megatron_lm"),e3.forEach(t),qh=r(ai," ("),Cr=c(ai,"CODE",{});var t3=i(Cr);Xh=r(t3,"bool"),t3.forEach(t),Jh=r(ai,") \u2014 Whether or not to use Megatron-LM for training."),ai.forEach(t),Go.forEach(t),ys=f(o),Xt=c(o,"P",{});var Jv=i(Xt);Or=c(Jv,"STRONG",{});var o3=i(Or);Qh=r(o3,"Distributed GPU Arguments"),o3.forEach(t),Vh=r(Jv,":"),Jv.forEach(t),Ts=f(o),j=c(o,"P",{});var Wo=i(j);e_=r(Wo,"The following arguments are only useful when "),Dr=c(Wo,"CODE",{});var l3=i(Dr);t_=r(l3,"multi_gpu"),l3.forEach(t),o_=r(Wo," is passed or multi-gpu training is configured through "),wr=c(Wo,"CODE",{});var r3=i(wr);l_=r(r3,"accelerate config"),r3.forEach(t),r_=r(Wo,":"),Wo.forEach(t),Ls=f(o),p=c(o,"UL",{});var O=i(p);Me=c(O,"LI",{});var ci=i(Me);br=c(ci,"CODE",{});var a3=i(br);a_=r(a3,"--gpu_ids"),a3.forEach(t),c_=r(ci," ("),Sr=c(ci,"CODE",{});var c3=i(Sr);i_=r(c3,"str"),c3.forEach(t),s_=r(ci,") \u2014 What GPUs (by id) should be used for training on this machine as a comma-seperated list"),ci.forEach(t),n_=f(O),ke=c(O,"LI",{});var ii=i(ke);Pr=c(ii,"CODE",{});var i3=i(Pr);d_=r(i3,"--same_network"),i3.forEach(t),f_=r(ii," ("),yr=c(ii,"CODE",{});var s3=i(yr);h_=r(s3,"bool"),s3.forEach(t),__=r(ii,") \u2014 Whether all machines used for multinode training exist on the same local network."),ii.forEach(t),u_=f(O),Re=c(O,"LI",{});var si=i(Re);Tr=c(si,"CODE",{});var n3=i(Tr);p_=r(n3,"--machine_rank MACHINE_RANK"),n3.forEach(t),E_=r(si," ("),Lr=c(si,"CODE",{});var d3=i(Lr);v_=r(d3,"int"),d3.forEach(t),m_=r(si,") \u2014 The rank of the machine on which this script is launched."),si.forEach(t),g_=f(O),Ge=c(O,"LI",{});var ni=i(Ge);Ir=c(ni,"CODE",{});var f3=i(Ir);C_=r(f3,"--main_process_ip MAIN_PROCESS_IP"),f3.forEach(t),O_=r(ni," ("),Ar=c(ni,"CODE",{});var h3=i(Ar);D_=r(h3,"str"),h3.forEach(t),w_=r(ni,") \u2014 The IP address of the machine of rank 0."),ni.forEach(t),b_=f(O),We=c(O,"LI",{});var di=i(We);$r=c(di,"CODE",{});var _3=i($r);S_=r(_3,"--main_process_port MAIN_PROCESS_PORT"),_3.forEach(t),P_=r(di," ("),Nr=c(di,"CODE",{});var u3=i(Nr);y_=r(u3,"int"),u3.forEach(t),T_=r(di,") \u2014 The port to use to communicate with the machine of rank 0."),di.forEach(t),L_=f(O),xe=c(O,"LI",{});var fi=i(xe);Ur=c(fi,"CODE",{});var p3=i(Ur);I_=r(p3,"--rdzv_conf"),p3.forEach(t),A_=r(fi," ("),Mr=c(fi,"CODE",{});var E3=i(Mr);$_=r(E3,"str"),E3.forEach(t),N_=r(fi,") \u2014 Additional rendezvous configuration (<key1>=<value1>,<key2>=<value2>,\u2026)."),fi.forEach(t),U_=f(O),Fe=c(O,"LI",{});var hi=i(Fe);kr=c(hi,"CODE",{});var v3=i(kr);M_=r(v3,"--max_restarts"),v3.forEach(t),k_=r(hi," ("),Rr=c(hi,"CODE",{});var m3=i(Rr);R_=r(m3,"int"),m3.forEach(t),G_=r(hi,") \u2014 Maximum number of worker group restarts before failing."),hi.forEach(t),W_=f(O),He=c(O,"LI",{});var _i=i(He);Gr=c(_i,"CODE",{});var g3=i(Gr);x_=r(g3,"--monitor_interval"),g3.forEach(t),F_=r(_i," ("),Wr=c(_i,"CODE",{});var C3=i(Wr);H_=r(C3,"float"),C3.forEach(t),z_=r(_i,") \u2014 Interval, in seconds, to monitor the state of workers."),_i.forEach(t),O.forEach(t),Is=f(o),Jt=c(o,"P",{});var Qv=i(Jt);xr=c(Qv,"STRONG",{});var O3=i(xr);B_=r(O3,"TPU Arguments"),O3.forEach(t),K_=r(Qv,":"),Qv.forEach(t),As=f(o),Y=c(o,"P",{});var xo=i(Y);j_=r(xo,"The following arguments are only useful when "),Fr=c(xo,"CODE",{});var D3=i(Fr);Y_=r(D3,"tpu"),D3.forEach(t),Z_=r(xo," is passed or TPU training is configured through "),Hr=c(xo,"CODE",{});var w3=i(Hr);q_=r(w3,"accelerate config"),w3.forEach(t),X_=r(xo,":"),xo.forEach(t),$s=f(o),ze=c(o,"UL",{});var bn=i(ze);Be=c(bn,"LI",{});var ui=i(Be);zr=c(ui,"CODE",{});var b3=i(zr);J_=r(b3,"--main_training_function MAIN_TRAINING_FUNCTION"),b3.forEach(t),Q_=r(ui," ("),Br=c(ui,"CODE",{});var S3=i(Br);V_=r(S3,"str"),S3.forEach(t),eu=r(ui,") \u2014 The name of the main function to be executed in your script."),ui.forEach(t),tu=f(bn),Ke=c(bn,"LI",{});var pi=i(Ke);Kr=c(pi,"CODE",{});var P3=i(Kr);ou=r(P3,"--downcast_bf16"),P3.forEach(t),lu=r(pi," ("),jr=c(pi,"CODE",{});var y3=i(jr);ru=r(y3,"bool"),y3.forEach(t),au=r(pi,") \u2014 Whether when using bf16 precision on TPUs if both float and double tensors are cast to bfloat16 or if double tensors remain as float32."),pi.forEach(t),bn.forEach(t),Ns=f(o),Qt=c(o,"P",{});var Vv=i(Qt);Yr=c(Vv,"STRONG",{});var T3=i(Yr);cu=r(T3,"DeepSpeed Arguments"),T3.forEach(t),iu=r(Vv,":"),Vv.forEach(t),Us=f(o),$=c(o,"P",{});var Tt=i($);su=r(Tt,"The following arguments are only useful when "),Zr=c(Tt,"CODE",{});var L3=i(Zr);nu=r(L3,"use_deepspeed"),L3.forEach(t),du=r(Tt," is passed or "),qr=c(Tt,"CODE",{});var I3=i(qr);fu=r(I3,"deepspeed"),I3.forEach(t),hu=r(Tt," is configured through "),Xr=c(Tt,"CODE",{});var A3=i(Xr);_u=r(A3,"accelerate config"),A3.forEach(t),uu=r(Tt,":"),Tt.forEach(t),Ms=f(o),h=c(o,"UL",{});var u=i(h);je=c(u,"LI",{});var Ei=i(je);Jr=c(Ei,"CODE",{});var $3=i(Jr);pu=r($3,"--deepspeed_config_file"),$3.forEach(t),Eu=r(Ei," ("),Qr=c(Ei,"CODE",{});var N3=i(Qr);vu=r(N3,"str"),N3.forEach(t),mu=r(Ei,") \u2014 DeepSpeed config file."),Ei.forEach(t),gu=f(u),Ye=c(u,"LI",{});var vi=i(Ye);Vr=c(vi,"CODE",{});var U3=i(Vr);Cu=r(U3,"--zero_stage"),U3.forEach(t),Ou=r(vi," ("),ea=c(vi,"CODE",{});var M3=i(ea);Du=r(M3,"int"),M3.forEach(t),wu=r(vi,") \u2014 DeepSpeed\u2019s ZeRO optimization stage."),vi.forEach(t),bu=f(u),Ze=c(u,"LI",{});var mi=i(Ze);ta=c(mi,"CODE",{});var k3=i(ta);Su=r(k3,"--offload_optimizer_device"),k3.forEach(t),Pu=r(mi," ("),oa=c(mi,"CODE",{});var R3=i(oa);yu=r(R3,"str"),R3.forEach(t),Tu=r(mi,") \u2014 Decides where (none|cpu|nvme) to offload optimizer states."),mi.forEach(t),Lu=f(u),qe=c(u,"LI",{});var gi=i(qe);la=c(gi,"CODE",{});var G3=i(la);Iu=r(G3,"--offload_param_device"),G3.forEach(t),Au=r(gi," ("),ra=c(gi,"CODE",{});var W3=i(ra);$u=r(W3,"str"),W3.forEach(t),Nu=r(gi,") \u2014 Decides where (none|cpu|nvme) to offload parameters."),gi.forEach(t),Uu=f(u),Xe=c(u,"LI",{});var Ci=i(Xe);aa=c(Ci,"CODE",{});var x3=i(aa);Mu=r(x3,"--gradient_accumulation_steps"),x3.forEach(t),ku=r(Ci," ("),ca=c(Ci,"CODE",{});var F3=i(ca);Ru=r(F3,"int"),F3.forEach(t),Gu=r(Ci,") \u2014 No of gradient_accumulation_steps used in your training script."),Ci.forEach(t),Wu=f(u),Je=c(u,"LI",{});var Oi=i(Je);ia=c(Oi,"CODE",{});var H3=i(ia);xu=r(H3,"--gradient_clipping"),H3.forEach(t),Fu=r(Oi," ("),sa=c(Oi,"CODE",{});var z3=i(sa);Hu=r(z3,"float"),z3.forEach(t),zu=r(Oi,") \u2014 Gradient clipping value used in your training script."),Oi.forEach(t),Bu=f(u),Z=c(u,"LI",{});var vo=i(Z);na=c(vo,"CODE",{});var B3=i(na);Ku=r(B3,"--zero3_init_flag"),B3.forEach(t),ju=r(vo," ("),da=c(vo,"CODE",{});var K3=i(da);Yu=r(K3,"str"),K3.forEach(t),Zu=r(vo,") \u2014 Decides Whether (true|false) to enable "),fa=c(vo,"CODE",{});var j3=i(fa);qu=r(j3,"deepspeed.zero.Init"),j3.forEach(t),Xu=r(vo," for constructing massive models. Only applicable with DeepSpeed ZeRO Stage-3."),vo.forEach(t),Ju=f(u),Qe=c(u,"LI",{});var Di=i(Qe);ha=c(Di,"CODE",{});var Y3=i(ha);Qu=r(Y3,"--zero3_save_16bit_model"),Y3.forEach(t),Vu=r(Di," ("),_a=c(Di,"CODE",{});var Z3=i(_a);ep=r(Z3,"str"),Z3.forEach(t),tp=r(Di,") \u2014 Decides Whether (true|false) to save 16-bit model weights when using ZeRO Stage-3. Only applicable with DeepSpeed ZeRO Stage-3."),Di.forEach(t),op=f(u),Ve=c(u,"LI",{});var wi=i(Ve);ua=c(wi,"CODE",{});var q3=i(ua);lp=r(q3,"--deepspeed_hostfile"),q3.forEach(t),rp=r(wi," ("),pa=c(wi,"CODE",{});var X3=i(pa);ap=r(X3,"str"),X3.forEach(t),cp=r(wi,") \u2014 DeepSpeed hostfile for configuring multi-node compute resources."),wi.forEach(t),ip=f(u),et=c(u,"LI",{});var bi=i(et);Ea=c(bi,"CODE",{});var J3=i(Ea);sp=r(J3,"--deepspeed_exclusion_filter"),J3.forEach(t),np=r(bi," ("),va=c(bi,"CODE",{});var Q3=i(va);dp=r(Q3,"str"),Q3.forEach(t),fp=r(bi,") \u2014 DeepSpeed exclusion filter string when using mutli-node setup."),bi.forEach(t),hp=f(u),tt=c(u,"LI",{});var Si=i(tt);ma=c(Si,"CODE",{});var V3=i(ma);_p=r(V3,"--deepspeed_inclusion_filter"),V3.forEach(t),up=r(Si," ("),ga=c(Si,"CODE",{});var e4=i(ga);pp=r(e4,"str"),e4.forEach(t),Ep=r(Si,") \u2014 DeepSpeed inclusion filter string when using mutli-node setup."),Si.forEach(t),vp=f(u),ot=c(u,"LI",{});var Pi=i(ot);Ca=c(Pi,"CODE",{});var t4=i(Ca);mp=r(t4,"--deepspeed_multinode_launcher"),t4.forEach(t),gp=r(Pi," ("),Oa=c(Pi,"CODE",{});var o4=i(Oa);Cp=r(o4,"str"),o4.forEach(t),Op=r(Pi,") \u2014 DeepSpeed multi-node launcher to use."),Pi.forEach(t),u.forEach(t),ks=f(o),Vt=c(o,"P",{});var em=i(Vt);Da=c(em,"STRONG",{});var l4=i(Da);Dp=r(l4,"Fully Sharded Data Parallelism Arguments"),l4.forEach(t),wp=r(em,":"),em.forEach(t),Rs=f(o),q=c(o,"P",{});var Fo=i(q);bp=r(Fo,"The following arguments are only useful when "),wa=c(Fo,"CODE",{});var r4=i(wa);Sp=r(r4,"use_fdsp"),r4.forEach(t),Pp=r(Fo," is passed or Fully Sharded Data Parallelism is configured through "),ba=c(Fo,"CODE",{});var a4=i(ba);yp=r(a4,"accelerate config"),a4.forEach(t),Tp=r(Fo,":"),Fo.forEach(t),Gs=f(o),E=c(o,"UL",{});var S=i(E);lt=c(S,"LI",{});var yi=i(lt);Sa=c(yi,"CODE",{});var c4=i(Sa);Lp=r(c4,"--fsdp_offload_params"),c4.forEach(t),Ip=r(yi," ("),Pa=c(yi,"CODE",{});var i4=i(Pa);Ap=r(i4,"str"),i4.forEach(t),$p=r(yi,") \u2014 Decides Whether (true|false) to offload parameters and gradients to CPU."),yi.forEach(t),Np=f(S),rt=c(S,"LI",{});var Ti=i(rt);ya=c(Ti,"CODE",{});var s4=i(ya);Up=r(s4,"--fsdp_min_num_params"),s4.forEach(t),Mp=r(Ti," ("),Ta=c(Ti,"CODE",{});var n4=i(Ta);kp=r(n4,"int"),n4.forEach(t),Rp=r(Ti,") \u2014 FSDP\u2019s minimum number of parameters for Default Auto Wrapping."),Ti.forEach(t),Gp=f(S),at=c(S,"LI",{});var Li=i(at);La=c(Li,"CODE",{});var d4=i(La);Wp=r(d4,"--fsdp_sharding_strategy"),d4.forEach(t),xp=r(Li," ("),Ia=c(Li,"CODE",{});var f4=i(Ia);Fp=r(f4,"int"),f4.forEach(t),Hp=r(Li,") \u2014 FSDP\u2019s Sharding Strategy."),Li.forEach(t),zp=f(S),ct=c(S,"LI",{});var Ii=i(ct);Aa=c(Ii,"CODE",{});var h4=i(Aa);Bp=r(h4,"--fsdp_auto_wrap_policy"),h4.forEach(t),Kp=r(Ii," ("),$a=c(Ii,"CODE",{});var _4=i($a);jp=r(_4,"str"),_4.forEach(t),Yp=r(Ii,") \u2014 FSDP\u2019s auto wrap policy."),Ii.forEach(t),Zp=f(S),w=c(S,"LI",{});var N=i(w);Na=c(N,"CODE",{});var u4=i(Na);qp=r(u4,"--fsdp_transformer_layer_cls_to_wrap"),u4.forEach(t),Xp=r(N," ("),Ua=c(N,"CODE",{});var p4=i(Ua);Jp=r(p4,"str"),p4.forEach(t),Qp=r(N,") \u2014 Transformer layer class name (case-sensitive) to wrap, e.g, "),Ma=c(N,"CODE",{});var E4=i(Ma);Vp=r(E4,"BertLayer"),E4.forEach(t),e1=r(N,", "),ka=c(N,"CODE",{});var v4=i(ka);t1=r(v4,"GPTJBlock"),v4.forEach(t),o1=r(N,", "),Ra=c(N,"CODE",{});var m4=i(Ra);l1=r(m4,"T5Block"),m4.forEach(t),r1=r(N," \u2026"),N.forEach(t),a1=f(S),it=c(S,"LI",{});var Ai=i(it);Ga=c(Ai,"CODE",{});var g4=i(Ga);c1=r(g4,"--fsdp_backward_prefetch_policy"),g4.forEach(t),i1=r(Ai," ("),Wa=c(Ai,"CODE",{});var C4=i(Wa);s1=r(C4,"str"),C4.forEach(t),n1=r(Ai,") \u2014 FSDP\u2019s backward prefetch policy."),Ai.forEach(t),d1=f(S),st=c(S,"LI",{});var $i=i(st);xa=c($i,"CODE",{});var O4=i(xa);f1=r(O4,"--fsdp_state_dict_type"),O4.forEach(t),h1=r($i," ("),Fa=c($i,"CODE",{});var D4=i(Fa);_1=r(D4,"str"),D4.forEach(t),u1=r($i,") \u2014 FSDP\u2019s state dict type."),$i.forEach(t),S.forEach(t),Ws=f(o),eo=c(o,"P",{});var tm=i(eo);Ha=c(tm,"STRONG",{});var w4=i(Ha);p1=r(w4,"Megatron-LM Arguments"),w4.forEach(t),E1=r(tm,":"),tm.forEach(t),xs=f(o),X=c(o,"P",{});var Ho=i(X);v1=r(Ho,"The following arguments are only useful when "),za=c(Ho,"CODE",{});var b4=i(za);m1=r(b4,"use_megatron_lm"),b4.forEach(t),g1=r(Ho," is passed or Megatron-LM is configured through "),Ba=c(Ho,"CODE",{});var S4=i(Ba);C1=r(S4,"accelerate config"),S4.forEach(t),O1=r(Ho,":"),Ho.forEach(t),Fs=f(o),v=c(o,"UL",{});var P=i(v);Po=c(P,"LI",{});var om=i(Po);Ka=c(om,"CODE",{});var P4=i(Ka);D1=r(P4,"--megatron_lm_tp_degree"),P4.forEach(t),w1=r(om," (\u201C) \u2014 Megatron-LM\u2019s Tensor Parallelism (TP) degree."),om.forEach(t),b1=f(P),yo=c(P,"LI",{});var lm=i(yo);ja=c(lm,"CODE",{});var y4=i(ja);S1=r(y4,"--megatron_lm_pp_degree"),y4.forEach(t),P1=r(lm," (\u201C) \u2014 Megatron-LM\u2019s Pipeline Parallelism (PP) degree."),lm.forEach(t),y1=f(P),To=c(P,"LI",{});var rm=i(To);Ya=c(rm,"CODE",{});var T4=i(Ya);T1=r(T4,"--megatron_lm_num_micro_batches"),T4.forEach(t),L1=r(rm," (\u201C) \u2014 Megatron-LM\u2019s number of micro batches when PP degree > 1."),rm.forEach(t),I1=f(P),Lo=c(P,"LI",{});var am=i(Lo);Za=c(am,"CODE",{});var L4=i(Za);A1=r(L4,"--megatron_lm_sequence_parallelism"),L4.forEach(t),$1=r(am," (\u201C) \u2014 Decides Whether (true|false) to enable Sequence Parallelism when TP degree > 1."),am.forEach(t),N1=f(P),Io=c(P,"LI",{});var cm=i(Io);qa=c(cm,"CODE",{});var I4=i(qa);U1=r(I4,"--megatron_lm_recompute_activations"),I4.forEach(t),M1=r(cm," (\u201C) \u2014 Decides Whether (true|false) to enable Selective Activation Recomputation."),cm.forEach(t),k1=f(P),Ao=c(P,"LI",{});var im=i(Ao);Xa=c(im,"CODE",{});var A4=i(Xa);R1=r(A4,"--megatron_lm_use_distributed_optimizer"),A4.forEach(t),G1=r(im," (\u201C) \u2014 Decides Whether (true|false) to use distributed optimizer which shards optimizer state and gradients across Data Pralellel (DP) ranks."),im.forEach(t),W1=f(P),$o=c(P,"LI",{});var sm=i($o);Ja=c(sm,"CODE",{});var $4=i(Ja);x1=r($4,"--megatron_lm_gradient_clipping"),$4.forEach(t),F1=r(sm," (\u201C) \u2014 Megatron-LM\u2019s gradient clipping value based on global L2 Norm (0 to disable)."),sm.forEach(t),P.forEach(t),Hs=f(o),to=c(o,"P",{});var nm=i(to);Qa=c(nm,"STRONG",{});var N4=i(Qa);H1=r(N4,"AWS SageMaker Arguments"),N4.forEach(t),z1=r(nm,":"),nm.forEach(t),zs=f(o),No=c(o,"P",{});var U4=i(No);B1=r(U4,"The following arguments are only useful when training in SageMaker"),U4.forEach(t),Bs=f(o),nt=c(o,"UL",{});var Sn=i(nt);dt=c(Sn,"LI",{});var Ni=i(dt);Va=c(Ni,"CODE",{});var M4=i(Va);K1=r(M4,"--aws_access_key_id AWS_ACCESS_KEY_ID"),M4.forEach(t),j1=r(Ni," ("),ec=c(Ni,"CODE",{});var k4=i(ec);Y1=r(k4,"str"),k4.forEach(t),Z1=r(Ni,") \u2014 The AWS_ACCESS_KEY_ID used to launch the Amazon SageMaker training job"),Ni.forEach(t),q1=f(Sn),ft=c(Sn,"LI",{});var Ui=i(ft);tc=c(Ui,"CODE",{});var R4=i(tc);X1=r(R4,"--aws_secret_access_key AWS_SECRET_ACCESS_KEY"),R4.forEach(t),J1=r(Ui," ("),oc=c(Ui,"CODE",{});var G4=i(oc);Q1=r(G4,"str"),G4.forEach(t),V1=r(Ui,") \u2014 The AWS_SECRET_ACCESS_KEY used to launch the Amazon SageMaker training job"),Ui.forEach(t),Sn.forEach(t),Ks=f(o),ie=c(o,"H2",{class:!0});var Pn=i(ie);ht=c(Pn,"A",{id:!0,class:!0,href:!0});var W4=i(ht);lc=c(W4,"SPAN",{});var x4=i(lc);M(oo.$$.fragment,x4),x4.forEach(t),W4.forEach(t),eE=f(Pn),rc=c(Pn,"SPAN",{});var F4=i(rc);tE=r(F4,"accelerate tpu-config"),F4.forEach(t),Pn.forEach(t),js=f(o),Uo=c(o,"P",{});var H4=i(Uo);ac=c(H4,"CODE",{});var z4=i(ac);oE=r(z4,"accelerate tpu-config"),z4.forEach(t),H4.forEach(t),Ys=f(o),lo=c(o,"P",{});var dm=i(lo);cc=c(dm,"STRONG",{});var B4=i(cc);lE=r(B4,"Usage"),B4.forEach(t),rE=r(dm,":"),dm.forEach(t),Zs=f(o),M(ro.$$.fragment,o),qs=f(o),ao=c(o,"P",{});var fm=i(ao);ic=c(fm,"STRONG",{});var K4=i(ic);aE=r(K4,"Optional Arguments"),K4.forEach(t),cE=r(fm,":"),fm.forEach(t),Xs=f(o),Mo=c(o,"UL",{});var j4=i(Mo);J=c(j4,"LI",{});var mo=i(J);sc=c(mo,"CODE",{});var Y4=i(sc);iE=r(Y4,"-h"),Y4.forEach(t),sE=r(mo,", "),nc=c(mo,"CODE",{});var Z4=i(nc);nE=r(Z4,"--help"),Z4.forEach(t),dE=r(mo," ("),dc=c(mo,"CODE",{});var q4=i(dc);fE=r(q4,"bool"),q4.forEach(t),hE=r(mo,") \u2014 Show a help message and exit"),mo.forEach(t),j4.forEach(t),Js=f(o),co=c(o,"P",{});var hm=i(co);fc=c(hm,"STRONG",{});var X4=i(fc);_E=r(X4,"Config Arguments"),X4.forEach(t),uE=r(hm,":"),hm.forEach(t),Qs=f(o),_t=c(o,"P",{});var yn=i(_t);pE=r(yn,"Arguments that can be configured through "),hc=c(yn,"CODE",{});var J4=i(hc);EE=r(J4,"accelerate config"),J4.forEach(t),vE=r(yn,"."),yn.forEach(t),Vs=f(o),Q=c(o,"UL",{});var zo=i(Q);ut=c(zo,"LI",{});var Mi=i(ut);_c=c(Mi,"CODE",{});var Q4=i(_c);mE=r(Q4,"--config_file"),Q4.forEach(t),gE=r(Mi," ("),uc=c(Mi,"CODE",{});var V4=i(uc);CE=r(V4,"str"),V4.forEach(t),OE=r(Mi,") \u2014 Path to the config file to use for accelerate."),Mi.forEach(t),DE=f(zo),pt=c(zo,"LI",{});var ki=i(pt);pc=c(ki,"CODE",{});var eg=i(pc);wE=r(eg,"--tpu_name"),eg.forEach(t),bE=r(ki," ("),Ec=c(ki,"CODE",{});var tg=i(Ec);SE=r(tg,"str"),tg.forEach(t),PE=r(ki,") \u2014 The name of the TPU to use. If not specified, will use the TPU specified in the config file."),ki.forEach(t),yE=f(zo),Et=c(zo,"LI",{});var Ri=i(Et);vc=c(Ri,"CODE",{});var og=i(vc);TE=r(og,"--tpu_zone"),og.forEach(t),LE=r(Ri," ("),mc=c(Ri,"CODE",{});var lg=i(mc);IE=r(lg,"str"),lg.forEach(t),AE=r(Ri,") \u2014 The zone of the TPU to use. If not specified, will use the zone specified in the config file."),Ri.forEach(t),zo.forEach(t),en=f(o),io=c(o,"P",{});var _m=i(io);gc=c(_m,"STRONG",{});var rg=i(gc);$E=r(rg,"TPU Arguments"),rg.forEach(t),NE=r(_m,":"),_m.forEach(t),tn=f(o),ko=c(o,"P",{});var ag=i(ko);UE=r(ag,"Arguments for options ran inside the TPU."),ag.forEach(t),on=f(o),b=c(o,"UL",{});var te=i(b);vt=c(te,"LI",{});var Gi=i(vt);Cc=c(Gi,"CODE",{});var cg=i(Cc);ME=r(cg,"--command_file"),cg.forEach(t),kE=r(Gi," ("),Oc=c(Gi,"CODE",{});var ig=i(Oc);RE=r(ig,"str"),ig.forEach(t),GE=r(Gi,") \u2014 The path to the file containing the commands to run on the pod on startup."),Gi.forEach(t),WE=f(te),mt=c(te,"LI",{});var Wi=i(mt);Dc=c(Wi,"CODE",{});var sg=i(Dc);xE=r(sg,"--command"),sg.forEach(t),FE=r(Wi," ("),wc=c(Wi,"CODE",{});var ng=i(wc);HE=r(ng,"str"),ng.forEach(t),zE=r(Wi,") \u2014 A command to run on the pod. Can be passed multiple times."),Wi.forEach(t),BE=f(te),gt=c(te,"LI",{});var xi=i(gt);bc=c(xi,"CODE",{});var dg=i(bc);KE=r(dg,"--install_accelerate"),dg.forEach(t),jE=r(xi," ("),Sc=c(xi,"CODE",{});var fg=i(Sc);YE=r(fg,"bool"),fg.forEach(t),ZE=r(xi,") \u2014 Whether to install accelerate on the pod. Defaults to False."),xi.forEach(t),qE=f(te),Ct=c(te,"LI",{});var Fi=i(Ct);Pc=c(Fi,"CODE",{});var hg=i(Pc);XE=r(hg,"--accelerate_version"),hg.forEach(t),JE=r(Fi," ("),yc=c(Fi,"CODE",{});var _g=i(yc);QE=r(_g,"str"),_g.forEach(t),VE=r(Fi,") \u2014 The version of accelerate to install on the pod. If not specified, will use the latest pypi version. Specify \u2018dev\u2019 to install from GitHub."),Fi.forEach(t),ev=f(te),Ot=c(te,"LI",{});var Hi=i(Ot);Tc=c(Hi,"CODE",{});var ug=i(Tc);tv=r(ug,"--debug"),ug.forEach(t),ov=r(Hi," ("),Lc=c(Hi,"CODE",{});var pg=i(Lc);lv=r(pg,"bool"),pg.forEach(t),rv=r(Hi,") \u2014 If set, will print the command that would be run instead of running it."),Hi.forEach(t),te.forEach(t),ln=f(o),se=c(o,"H2",{class:!0});var Tn=i(se);Dt=c(Tn,"A",{id:!0,class:!0,href:!0});var Eg=i(Dt);Ic=c(Eg,"SPAN",{});var vg=i(Ic);M(so.$$.fragment,vg),vg.forEach(t),Eg.forEach(t),av=f(Tn),Ac=c(Tn,"SPAN",{});var mg=i(Ac);cv=r(mg,"accelerate test"),mg.forEach(t),Tn.forEach(t),rn=f(o),wt=c(o,"P",{});var Ln=i(wt);$c=c(Ln,"CODE",{});var gg=i($c);iv=r(gg,"accelerate test"),gg.forEach(t),sv=r(Ln," or "),Nc=c(Ln,"CODE",{});var Cg=i(Nc);nv=r(Cg,"accelerate-test"),Cg.forEach(t),Ln.forEach(t),an=f(o),bt=c(o,"P",{});var In=i(bt);dv=r(In,"Runs "),Uc=c(In,"CODE",{});var Og=i(Uc);fv=r(Og,"accelerate/test_utils/test_script.py"),Og.forEach(t),hv=r(In," to verify that \u{1F917} Accelerate has been properly configured on your system and runs."),In.forEach(t),cn=f(o),no=c(o,"P",{});var um=i(no);Mc=c(um,"STRONG",{});var Dg=i(Mc);_v=r(Dg,"Usage"),Dg.forEach(t),uv=r(um,":"),um.forEach(t),sn=f(o),M(fo.$$.fragment,o),nn=f(o),ho=c(o,"P",{});var pm=i(ho);kc=c(pm,"STRONG",{});var wg=i(kc);pv=r(wg,"Optional Arguments"),wg.forEach(t),Ev=r(pm,":"),pm.forEach(t),dn=f(o),St=c(o,"UL",{});var An=i(St);C=c(An,"LI",{});var L=i(C);Rc=c(L,"CODE",{});var bg=i(Rc);vv=r(bg,"--config_file CONFIG_FILE"),bg.forEach(t),mv=r(L," ("),Gc=c(L,"CODE",{});var Sg=i(Gc);gv=r(Sg,"str"),Sg.forEach(t),Cv=r(L,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Wc=c(L,"CODE",{});var Pg=i(Wc);Ov=r(Pg,"HF_HOME"),Pg.forEach(t),Dv=r(L,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),xc=c(L,"CODE",{});var yg=i(xc);wv=r(yg,"~/.cache"),yg.forEach(t),bv=r(L," or the content of "),Fc=c(L,"CODE",{});var Tg=i(Fc);Sv=r(Tg,"XDG_CACHE_HOME"),Tg.forEach(t),Pv=r(L,") suffixed with "),Hc=c(L,"CODE",{});var Lg=i(Hc);yv=r(Lg,"huggingface"),Lg.forEach(t),Tv=r(L,"."),L.forEach(t),Lv=f(An),V=c(An,"LI",{});var go=i(V);zc=c(go,"CODE",{});var Ig=i(zc);Iv=r(Ig,"-h"),Ig.forEach(t),Av=r(go,", "),Bc=c(go,"CODE",{});var Ag=i(Bc);$v=r(Ag,"--help"),Ag.forEach(t),Nv=r(go," ("),Kc=c(go,"CODE",{});var $g=i(Kc);Uv=r($g,"bool"),$g.forEach(t),Mv=r(go,") \u2014 Show a help message and exit"),go.forEach(t),An.forEach(t),this.h()},h(){_(oe,"name","hf:doc:metadata"),_(oe,"content",JSON.stringify(xg)),_(ne,"id","the-command-line"),_(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(ne,"href","#the-command-line"),_(le,"class","relative group"),_(de,"id","accelerate-config"),_(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(de,"href","#accelerate-config"),_(re,"class","relative group"),_(ue,"id","accelerate-env"),_(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(ue,"href","#accelerate-env"),_(ae,"class","relative group"),_(Rt,"href","https://github.com/huggingface/accelerate"),_(Rt,"rel","nofollow"),_(me,"id","accelerate-launch"),_(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(me,"href","#accelerate-launch"),_(ce,"class","relative group"),_(ht,"id","accelerate-tpuconfig"),_(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(ht,"href","#accelerate-tpuconfig"),_(ie,"class","relative group"),_(Dt,"id","accelerate-test"),_(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(Dt,"href","#accelerate-test"),_(se,"class","relative group")},m(o,s){e(document.head,oe),n(o,Bi,s),n(o,le,s),e(le,ne),e(ne,Ko),k(Lt,Ko,null),e(le,$n),e(le,jo),e(jo,Nn),n(o,Ki,s),n(o,Co,s),e(Co,Un),n(o,ji,s),n(o,re,s),e(re,de),e(de,Yo),k(It,Yo,null),e(re,Mn),e(re,Zo),e(Zo,kn),n(o,Yi,s),n(o,At,s),e(At,qo),e(qo,Rn),e(At,Gn),n(o,Zi,s),n(o,fe,s),e(fe,Xo),e(Xo,Wn),e(fe,xn),e(fe,Jo),e(Jo,Fn),n(o,qi,s),n(o,he,s),e(he,Hn),e(he,Qo),e(Qo,zn),e(he,Bn),n(o,Xi,s),n(o,$t,s),e($t,Vo),e(Vo,Kn),e($t,jn),n(o,Ji,s),k(Nt,o,s),n(o,Qi,s),n(o,Ut,s),e(Ut,el),e(el,Yn),e(Ut,Zn),n(o,Vi,s),n(o,_e,s),e(_e,m),e(m,tl),e(tl,qn),e(m,Xn),e(m,ol),e(ol,Jn),e(m,Qn),e(m,ll),e(ll,Vn),e(m,ed),e(m,rl),e(rl,td),e(m,od),e(m,al),e(al,ld),e(m,rd),e(m,cl),e(cl,ad),e(m,cd),e(_e,id),e(_e,x),e(x,il),e(il,sd),e(x,nd),e(x,sl),e(sl,dd),e(x,fd),e(x,nl),e(nl,hd),e(x,_d),n(o,es,s),n(o,ae,s),e(ae,ue),e(ue,dl),k(Mt,dl,null),e(ae,ud),e(ae,fl),e(fl,pd),n(o,ts,s),n(o,kt,s),e(kt,hl),e(hl,Ed),e(kt,vd),n(o,os,s),n(o,pe,s),e(pe,_l),e(_l,md),e(pe,gd),e(pe,ul),e(ul,Cd),n(o,ls,s),n(o,Ee,s),e(Ee,Od),e(Ee,Rt),e(Rt,Dd),e(Ee,wd),n(o,rs,s),n(o,Gt,s),e(Gt,pl),e(pl,bd),e(Gt,Sd),n(o,as,s),k(Wt,o,s),n(o,cs,s),n(o,xt,s),e(xt,El),e(El,Pd),e(xt,yd),n(o,is,s),n(o,ve,s),e(ve,g),e(g,vl),e(vl,Td),e(g,Ld),e(g,ml),e(ml,Id),e(g,Ad),e(g,gl),e(gl,$d),e(g,Nd),e(g,Cl),e(Cl,Ud),e(g,Md),e(g,Ol),e(Ol,kd),e(g,Rd),e(g,Dl),e(Dl,Gd),e(g,Wd),e(ve,xd),e(ve,F),e(F,wl),e(wl,Fd),e(F,Hd),e(F,bl),e(bl,zd),e(F,Bd),e(F,Sl),e(Sl,Kd),e(F,jd),n(o,ss,s),n(o,ce,s),e(ce,me),e(me,Pl),k(Ft,Pl,null),e(ce,Yd),e(ce,yl),e(yl,Zd),n(o,ns,s),n(o,Ht,s),e(Ht,Tl),e(Tl,qd),e(Ht,Xd),n(o,ds,s),n(o,ge,s),e(ge,Ll),e(Ll,Jd),e(ge,Qd),e(ge,Il),e(Il,Vd),n(o,fs,s),n(o,Oo,s),e(Oo,ef),n(o,hs,s),n(o,zt,s),e(zt,Al),e(Al,tf),e(zt,of),n(o,_s,s),k(Bt,o,s),n(o,us,s),n(o,Kt,s),e(Kt,$l),e($l,lf),e(Kt,rf),n(o,ps,s),n(o,Ce,s),e(Ce,Do),e(Do,Nl),e(Nl,af),e(Do,cf),e(Ce,sf),e(Ce,wo),e(wo,Ul),e(Ul,nf),e(wo,df),n(o,Es,s),n(o,jt,s),e(jt,Ml),e(Ml,ff),e(jt,hf),n(o,vs,s),n(o,D,s),e(D,H),e(H,kl),e(kl,_f),e(H,uf),e(H,Rl),e(Rl,pf),e(H,Ef),e(H,Gl),e(Gl,vf),e(H,mf),e(D,gf),e(D,Oe),e(Oe,Wl),e(Wl,Cf),e(Oe,Of),e(Oe,xl),e(xl,Df),e(Oe,wf),e(D,bf),e(D,z),e(z,Fl),e(Fl,Sf),e(z,Pf),e(z,Hl),e(Hl,yf),e(z,Tf),e(z,zl),e(zl,Lf),e(z,If),e(D,Af),e(D,De),e(De,Bl),e(Bl,$f),e(De,Nf),e(De,Kl),e(Kl,Uf),e(De,Mf),e(D,kf),e(D,we),e(we,jl),e(jl,Rf),e(we,Gf),e(we,Yl),e(Yl,Wf),e(we,xf),n(o,ms,s),n(o,B,s),e(B,Ff),e(B,Zl),e(Zl,Hf),e(B,zf),e(B,ql),e(ql,Bf),e(B,Kf),n(o,gs,s),n(o,Yt,s),e(Yt,Xl),e(Xl,jf),e(Yt,Yf),n(o,Cs,s),n(o,I,s),e(I,be),e(be,Jl),e(Jl,Zf),e(be,qf),e(be,Ql),e(Ql,Xf),e(be,Jf),e(I,Qf),e(I,Se),e(Se,Vl),e(Vl,Vf),e(Se,eh),e(Se,er),e(er,th),e(Se,oh),e(I,lh),e(I,Pe),e(Pe,tr),e(tr,rh),e(Pe,ah),e(Pe,or),e(or,ch),e(Pe,ih),e(I,sh),e(I,ye),e(ye,lr),e(lr,nh),e(ye,dh),e(ye,rr),e(rr,fh),e(ye,hh),n(o,Os,s),n(o,Zt,s),e(Zt,ar),e(ar,_h),e(Zt,uh),n(o,Ds,s),n(o,bo,s),e(bo,ph),n(o,ws,s),n(o,A,s),e(A,Te),e(Te,cr),e(cr,Eh),e(Te,vh),e(Te,ir),e(ir,mh),e(Te,gh),e(A,Ch),e(A,Le),e(Le,sr),e(sr,Oh),e(Le,Dh),e(Le,nr),e(nr,wh),e(Le,bh),e(A,Sh),e(A,Ie),e(Ie,dr),e(dr,Ph),e(Ie,yh),e(Ie,fr),e(fr,Th),e(Ie,Lh),e(A,Ih),e(A,Ae),e(Ae,hr),e(hr,Ah),e(Ae,$h),e(Ae,_r),e(_r,Nh),e(Ae,Uh),n(o,bs,s),n(o,qt,s),e(qt,ur),e(ur,Mh),e(qt,kh),n(o,Ss,s),n(o,So,s),e(So,Rh),n(o,Ps,s),n(o,K,s),e(K,$e),e($e,pr),e(pr,Gh),e($e,Wh),e($e,Er),e(Er,xh),e($e,Fh),e(K,Hh),e(K,Ne),e(Ne,vr),e(vr,zh),e(Ne,Bh),e(Ne,mr),e(mr,Kh),e(Ne,jh),e(K,Yh),e(K,Ue),e(Ue,gr),e(gr,Zh),e(Ue,qh),e(Ue,Cr),e(Cr,Xh),e(Ue,Jh),n(o,ys,s),n(o,Xt,s),e(Xt,Or),e(Or,Qh),e(Xt,Vh),n(o,Ts,s),n(o,j,s),e(j,e_),e(j,Dr),e(Dr,t_),e(j,o_),e(j,wr),e(wr,l_),e(j,r_),n(o,Ls,s),n(o,p,s),e(p,Me),e(Me,br),e(br,a_),e(Me,c_),e(Me,Sr),e(Sr,i_),e(Me,s_),e(p,n_),e(p,ke),e(ke,Pr),e(Pr,d_),e(ke,f_),e(ke,yr),e(yr,h_),e(ke,__),e(p,u_),e(p,Re),e(Re,Tr),e(Tr,p_),e(Re,E_),e(Re,Lr),e(Lr,v_),e(Re,m_),e(p,g_),e(p,Ge),e(Ge,Ir),e(Ir,C_),e(Ge,O_),e(Ge,Ar),e(Ar,D_),e(Ge,w_),e(p,b_),e(p,We),e(We,$r),e($r,S_),e(We,P_),e(We,Nr),e(Nr,y_),e(We,T_),e(p,L_),e(p,xe),e(xe,Ur),e(Ur,I_),e(xe,A_),e(xe,Mr),e(Mr,$_),e(xe,N_),e(p,U_),e(p,Fe),e(Fe,kr),e(kr,M_),e(Fe,k_),e(Fe,Rr),e(Rr,R_),e(Fe,G_),e(p,W_),e(p,He),e(He,Gr),e(Gr,x_),e(He,F_),e(He,Wr),e(Wr,H_),e(He,z_),n(o,Is,s),n(o,Jt,s),e(Jt,xr),e(xr,B_),e(Jt,K_),n(o,As,s),n(o,Y,s),e(Y,j_),e(Y,Fr),e(Fr,Y_),e(Y,Z_),e(Y,Hr),e(Hr,q_),e(Y,X_),n(o,$s,s),n(o,ze,s),e(ze,Be),e(Be,zr),e(zr,J_),e(Be,Q_),e(Be,Br),e(Br,V_),e(Be,eu),e(ze,tu),e(ze,Ke),e(Ke,Kr),e(Kr,ou),e(Ke,lu),e(Ke,jr),e(jr,ru),e(Ke,au),n(o,Ns,s),n(o,Qt,s),e(Qt,Yr),e(Yr,cu),e(Qt,iu),n(o,Us,s),n(o,$,s),e($,su),e($,Zr),e(Zr,nu),e($,du),e($,qr),e(qr,fu),e($,hu),e($,Xr),e(Xr,_u),e($,uu),n(o,Ms,s),n(o,h,s),e(h,je),e(je,Jr),e(Jr,pu),e(je,Eu),e(je,Qr),e(Qr,vu),e(je,mu),e(h,gu),e(h,Ye),e(Ye,Vr),e(Vr,Cu),e(Ye,Ou),e(Ye,ea),e(ea,Du),e(Ye,wu),e(h,bu),e(h,Ze),e(Ze,ta),e(ta,Su),e(Ze,Pu),e(Ze,oa),e(oa,yu),e(Ze,Tu),e(h,Lu),e(h,qe),e(qe,la),e(la,Iu),e(qe,Au),e(qe,ra),e(ra,$u),e(qe,Nu),e(h,Uu),e(h,Xe),e(Xe,aa),e(aa,Mu),e(Xe,ku),e(Xe,ca),e(ca,Ru),e(Xe,Gu),e(h,Wu),e(h,Je),e(Je,ia),e(ia,xu),e(Je,Fu),e(Je,sa),e(sa,Hu),e(Je,zu),e(h,Bu),e(h,Z),e(Z,na),e(na,Ku),e(Z,ju),e(Z,da),e(da,Yu),e(Z,Zu),e(Z,fa),e(fa,qu),e(Z,Xu),e(h,Ju),e(h,Qe),e(Qe,ha),e(ha,Qu),e(Qe,Vu),e(Qe,_a),e(_a,ep),e(Qe,tp),e(h,op),e(h,Ve),e(Ve,ua),e(ua,lp),e(Ve,rp),e(Ve,pa),e(pa,ap),e(Ve,cp),e(h,ip),e(h,et),e(et,Ea),e(Ea,sp),e(et,np),e(et,va),e(va,dp),e(et,fp),e(h,hp),e(h,tt),e(tt,ma),e(ma,_p),e(tt,up),e(tt,ga),e(ga,pp),e(tt,Ep),e(h,vp),e(h,ot),e(ot,Ca),e(Ca,mp),e(ot,gp),e(ot,Oa),e(Oa,Cp),e(ot,Op),n(o,ks,s),n(o,Vt,s),e(Vt,Da),e(Da,Dp),e(Vt,wp),n(o,Rs,s),n(o,q,s),e(q,bp),e(q,wa),e(wa,Sp),e(q,Pp),e(q,ba),e(ba,yp),e(q,Tp),n(o,Gs,s),n(o,E,s),e(E,lt),e(lt,Sa),e(Sa,Lp),e(lt,Ip),e(lt,Pa),e(Pa,Ap),e(lt,$p),e(E,Np),e(E,rt),e(rt,ya),e(ya,Up),e(rt,Mp),e(rt,Ta),e(Ta,kp),e(rt,Rp),e(E,Gp),e(E,at),e(at,La),e(La,Wp),e(at,xp),e(at,Ia),e(Ia,Fp),e(at,Hp),e(E,zp),e(E,ct),e(ct,Aa),e(Aa,Bp),e(ct,Kp),e(ct,$a),e($a,jp),e(ct,Yp),e(E,Zp),e(E,w),e(w,Na),e(Na,qp),e(w,Xp),e(w,Ua),e(Ua,Jp),e(w,Qp),e(w,Ma),e(Ma,Vp),e(w,e1),e(w,ka),e(ka,t1),e(w,o1),e(w,Ra),e(Ra,l1),e(w,r1),e(E,a1),e(E,it),e(it,Ga),e(Ga,c1),e(it,i1),e(it,Wa),e(Wa,s1),e(it,n1),e(E,d1),e(E,st),e(st,xa),e(xa,f1),e(st,h1),e(st,Fa),e(Fa,_1),e(st,u1),n(o,Ws,s),n(o,eo,s),e(eo,Ha),e(Ha,p1),e(eo,E1),n(o,xs,s),n(o,X,s),e(X,v1),e(X,za),e(za,m1),e(X,g1),e(X,Ba),e(Ba,C1),e(X,O1),n(o,Fs,s),n(o,v,s),e(v,Po),e(Po,Ka),e(Ka,D1),e(Po,w1),e(v,b1),e(v,yo),e(yo,ja),e(ja,S1),e(yo,P1),e(v,y1),e(v,To),e(To,Ya),e(Ya,T1),e(To,L1),e(v,I1),e(v,Lo),e(Lo,Za),e(Za,A1),e(Lo,$1),e(v,N1),e(v,Io),e(Io,qa),e(qa,U1),e(Io,M1),e(v,k1),e(v,Ao),e(Ao,Xa),e(Xa,R1),e(Ao,G1),e(v,W1),e(v,$o),e($o,Ja),e(Ja,x1),e($o,F1),n(o,Hs,s),n(o,to,s),e(to,Qa),e(Qa,H1),e(to,z1),n(o,zs,s),n(o,No,s),e(No,B1),n(o,Bs,s),n(o,nt,s),e(nt,dt),e(dt,Va),e(Va,K1),e(dt,j1),e(dt,ec),e(ec,Y1),e(dt,Z1),e(nt,q1),e(nt,ft),e(ft,tc),e(tc,X1),e(ft,J1),e(ft,oc),e(oc,Q1),e(ft,V1),n(o,Ks,s),n(o,ie,s),e(ie,ht),e(ht,lc),k(oo,lc,null),e(ie,eE),e(ie,rc),e(rc,tE),n(o,js,s),n(o,Uo,s),e(Uo,ac),e(ac,oE),n(o,Ys,s),n(o,lo,s),e(lo,cc),e(cc,lE),e(lo,rE),n(o,Zs,s),k(ro,o,s),n(o,qs,s),n(o,ao,s),e(ao,ic),e(ic,aE),e(ao,cE),n(o,Xs,s),n(o,Mo,s),e(Mo,J),e(J,sc),e(sc,iE),e(J,sE),e(J,nc),e(nc,nE),e(J,dE),e(J,dc),e(dc,fE),e(J,hE),n(o,Js,s),n(o,co,s),e(co,fc),e(fc,_E),e(co,uE),n(o,Qs,s),n(o,_t,s),e(_t,pE),e(_t,hc),e(hc,EE),e(_t,vE),n(o,Vs,s),n(o,Q,s),e(Q,ut),e(ut,_c),e(_c,mE),e(ut,gE),e(ut,uc),e(uc,CE),e(ut,OE),e(Q,DE),e(Q,pt),e(pt,pc),e(pc,wE),e(pt,bE),e(pt,Ec),e(Ec,SE),e(pt,PE),e(Q,yE),e(Q,Et),e(Et,vc),e(vc,TE),e(Et,LE),e(Et,mc),e(mc,IE),e(Et,AE),n(o,en,s),n(o,io,s),e(io,gc),e(gc,$E),e(io,NE),n(o,tn,s),n(o,ko,s),e(ko,UE),n(o,on,s),n(o,b,s),e(b,vt),e(vt,Cc),e(Cc,ME),e(vt,kE),e(vt,Oc),e(Oc,RE),e(vt,GE),e(b,WE),e(b,mt),e(mt,Dc),e(Dc,xE),e(mt,FE),e(mt,wc),e(wc,HE),e(mt,zE),e(b,BE),e(b,gt),e(gt,bc),e(bc,KE),e(gt,jE),e(gt,Sc),e(Sc,YE),e(gt,ZE),e(b,qE),e(b,Ct),e(Ct,Pc),e(Pc,XE),e(Ct,JE),e(Ct,yc),e(yc,QE),e(Ct,VE),e(b,ev),e(b,Ot),e(Ot,Tc),e(Tc,tv),e(Ot,ov),e(Ot,Lc),e(Lc,lv),e(Ot,rv),n(o,ln,s),n(o,se,s),e(se,Dt),e(Dt,Ic),k(so,Ic,null),e(se,av),e(se,Ac),e(Ac,cv),n(o,rn,s),n(o,wt,s),e(wt,$c),e($c,iv),e(wt,sv),e(wt,Nc),e(Nc,nv),n(o,an,s),n(o,bt,s),e(bt,dv),e(bt,Uc),e(Uc,fv),e(bt,hv),n(o,cn,s),n(o,no,s),e(no,Mc),e(Mc,_v),e(no,uv),n(o,sn,s),k(fo,o,s),n(o,nn,s),n(o,ho,s),e(ho,kc),e(kc,pv),e(ho,Ev),n(o,dn,s),n(o,St,s),e(St,C),e(C,Rc),e(Rc,vv),e(C,mv),e(C,Gc),e(Gc,gv),e(C,Cv),e(C,Wc),e(Wc,Ov),e(C,Dv),e(C,xc),e(xc,wv),e(C,bv),e(C,Fc),e(Fc,Sv),e(C,Pv),e(C,Hc),e(Hc,yv),e(C,Tv),e(St,Lv),e(St,V),e(V,zc),e(zc,Iv),e(V,Av),e(V,Bc),e(Bc,$v),e(V,Nv),e(V,Kc),e(Kc,Uv),e(V,Mv),fn=!0},p:Rg,i(o){fn||(R(Lt.$$.fragment,o),R(It.$$.fragment,o),R(Nt.$$.fragment,o),R(Mt.$$.fragment,o),R(Wt.$$.fragment,o),R(Ft.$$.fragment,o),R(Bt.$$.fragment,o),R(oo.$$.fragment,o),R(ro.$$.fragment,o),R(so.$$.fragment,o),R(fo.$$.fragment,o),fn=!0)},o(o){G(Lt.$$.fragment,o),G(It.$$.fragment,o),G(Nt.$$.fragment,o),G(Mt.$$.fragment,o),G(Wt.$$.fragment,o),G(Ft.$$.fragment,o),G(Bt.$$.fragment,o),G(oo.$$.fragment,o),G(ro.$$.fragment,o),G(so.$$.fragment,o),G(fo.$$.fragment,o),fn=!1},d(o){t(oe),o&&t(Bi),o&&t(le),W(Lt),o&&t(Ki),o&&t(Co),o&&t(ji),o&&t(re),W(It),o&&t(Yi),o&&t(At),o&&t(Zi),o&&t(fe),o&&t(qi),o&&t(he),o&&t(Xi),o&&t($t),o&&t(Ji),W(Nt,o),o&&t(Qi),o&&t(Ut),o&&t(Vi),o&&t(_e),o&&t(es),o&&t(ae),W(Mt),o&&t(ts),o&&t(kt),o&&t(os),o&&t(pe),o&&t(ls),o&&t(Ee),o&&t(rs),o&&t(Gt),o&&t(as),W(Wt,o),o&&t(cs),o&&t(xt),o&&t(is),o&&t(ve),o&&t(ss),o&&t(ce),W(Ft),o&&t(ns),o&&t(Ht),o&&t(ds),o&&t(ge),o&&t(fs),o&&t(Oo),o&&t(hs),o&&t(zt),o&&t(_s),W(Bt,o),o&&t(us),o&&t(Kt),o&&t(ps),o&&t(Ce),o&&t(Es),o&&t(jt),o&&t(vs),o&&t(D),o&&t(ms),o&&t(B),o&&t(gs),o&&t(Yt),o&&t(Cs),o&&t(I),o&&t(Os),o&&t(Zt),o&&t(Ds),o&&t(bo),o&&t(ws),o&&t(A),o&&t(bs),o&&t(qt),o&&t(Ss),o&&t(So),o&&t(Ps),o&&t(K),o&&t(ys),o&&t(Xt),o&&t(Ts),o&&t(j),o&&t(Ls),o&&t(p),o&&t(Is),o&&t(Jt),o&&t(As),o&&t(Y),o&&t($s),o&&t(ze),o&&t(Ns),o&&t(Qt),o&&t(Us),o&&t($),o&&t(Ms),o&&t(h),o&&t(ks),o&&t(Vt),o&&t(Rs),o&&t(q),o&&t(Gs),o&&t(E),o&&t(Ws),o&&t(eo),o&&t(xs),o&&t(X),o&&t(Fs),o&&t(v),o&&t(Hs),o&&t(to),o&&t(zs),o&&t(No),o&&t(Bs),o&&t(nt),o&&t(Ks),o&&t(ie),W(oo),o&&t(js),o&&t(Uo),o&&t(Ys),o&&t(lo),o&&t(Zs),W(ro,o),o&&t(qs),o&&t(ao),o&&t(Xs),o&&t(Mo),o&&t(Js),o&&t(co),o&&t(Qs),o&&t(_t),o&&t(Vs),o&&t(Q),o&&t(en),o&&t(io),o&&t(tn),o&&t(ko),o&&t(on),o&&t(b),o&&t(ln),o&&t(se),W(so),o&&t(rn),o&&t(wt),o&&t(an),o&&t(bt),o&&t(cn),o&&t(no),o&&t(sn),W(fo,o),o&&t(nn),o&&t(ho),o&&t(dn),o&&t(St)}}}const xg={local:"the-command-line",sections:[{local:"accelerate-config",title:"accelerate config"},{local:"accelerate-env",title:"accelerate env"},{local:"accelerate-launch",title:"accelerate launch"},{local:"accelerate-tpuconfig",title:"accelerate tpu-config"},{local:"accelerate-test",title:"accelerate test"}],title:"The Command Line "};function Fg(Em){return Gg(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Kg extends Ng{constructor(oe){super();Ug(this,oe,Fg,Wg,Mg,{})}}export{Kg as default,xg as metadata};
