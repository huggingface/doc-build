import{S as To,i as Ao,s as Oo,e as o,k as i,w as E,t as c,M as Lo,c as r,d as a,m as h,a as s,x as S,h as n,b as m,G as e,g as u,y as x,q as T,o as A,B as O,v as Po}from"../../chunks/vendor-hf-doc-builder.js";import{T as xo}from"../../chunks/Tip-hf-doc-builder.js";import{D as ve}from"../../chunks/Docstring-hf-doc-builder.js";import{I as Et}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Io(we){let d,_,f,v,b,y,I,g,k;return{c(){d=o("p"),_=o("code"),f=c("BatchSampler"),v=c("s with varying batch sizes are not enabled by default. To enable this behaviour, set "),b=o("code"),y=c("even_batches"),I=c(`
equal to `),g=o("code"),k=c("False")},l(w){d=r(w,"P",{});var p=s(d);_=r(p,"CODE",{});var B=s(_);f=n(B,"BatchSampler"),B.forEach(a),v=n(p,"s with varying batch sizes are not enabled by default. To enable this behaviour, set "),b=r(p,"CODE",{});var L=s(b);y=n(L,"even_batches"),L.forEach(a),I=n(p,`
equal to `),g=r(p,"CODE",{});var G=s(g);k=n(G,"False"),G.forEach(a),p.forEach(a)},m(w,p){u(w,d,p),e(d,_),e(_,f),e(d,v),e(d,b),e(b,y),e(d,I),e(d,g),e(g,k)},d(w){w&&a(d)}}}function ko(we){let d,_,f,v,b,y,I,g,k;return{c(){d=o("p"),_=o("code"),f=c("BatchSampler"),v=c("s with varying batch sizes are not enabled by default. To enable this behaviour, set "),b=o("code"),y=c("even_batches"),I=c(`
equal to `),g=o("code"),k=c("False")},l(w){d=r(w,"P",{});var p=s(d);_=r(p,"CODE",{});var B=s(_);f=n(B,"BatchSampler"),B.forEach(a),v=n(p,"s with varying batch sizes are not enabled by default. To enable this behaviour, set "),b=r(p,"CODE",{});var L=s(b);y=n(L,"even_batches"),L.forEach(a),I=n(p,`
equal to `),g=r(p,"CODE",{});var G=s(g);k=n(G,"False"),G.forEach(a),p.forEach(a)},m(w,p){u(w,d,p),e(d,_),e(_,f),e(d,v),e(d,b),e(b,y),e(d,I),e(d,g),e(g,k)},d(w){w&&a(d)}}}function qo(we){let d,_,f,v,b,y,I,g,k,w,p,B,L,G,St,ct,U,X,Se,oe,xt,xe,Tt,nt,z,re,At,se,Ot,Te,Lt,Pt,It,R,kt,Ae,qt,Ct,Oe,Wt,Nt,Ft,J,lt,q,ce,Bt,C,Gt,Le,Ut,Rt,Pe,Vt,Ht,Ie,Mt,jt,Xt,K,dt,V,ne,Jt,$,Kt,ke,Qt,Yt,qe,Zt,ea,Ce,ta,aa,We,oa,ra,Ne,sa,ca,it,D,le,na,de,la,Fe,da,ia,ha,Be,Ge,pa,ma,ie,Ue,F,Re,ua,fa,Ve,ba,ga,He,_a,va,wa,Me,Q,je,ya,$a,Xe,za,Da,ht,he,pe,pt,H,Y,Je,me,Ea,Ke,Sa,mt,W,ue,xa,Qe,Ta,Aa,M,Oa,Ye,La,Pa,Ze,Ia,ka,ut,j,Z,et,fe,qa,tt,Ca,ft,N,be,Wa,at,Na,Fa,ot,Ba,bt;return y=new Et({}),oe=new Et({}),re=new ve({props:{name:"accelerate.data_loader.prepare_data_loader",anchor:"accelerate.data_loader.prepare_data_loader",parameters:[{name:"dataloader",val:": DataLoader"},{name:"device",val:": typing.Optional[torch.device] = None"},{name:"num_processes",val:": typing.Optional[int] = None"},{name:"process_index",val:": typing.Optional[int] = None"},{name:"split_batches",val:": bool = False"},{name:"put_on_device",val:": bool = False"},{name:"rng_types",val:": typing.Union[typing.List[typing.Union[str, accelerate.utils.dataclasses.RNGType]], NoneType] = None"},{name:"dispatch_batches",val:": typing.Optional[bool] = None"},{name:"even_batches",val:": bool = True"}],parametersDescription:[{anchor:"accelerate.data_loader.prepare_data_loader.dataloader",description:`<strong>dataloader</strong> (<code>torch.utils.data.dataloader.DataLoader</code>) &#x2014;
The data loader to split across several devices.`,name:"dataloader"},{anchor:"accelerate.data_loader.prepare_data_loader.device",description:`<strong>device</strong> (<code>torch.device</code>) &#x2014;
The target device for the returned <code>DataLoader</code>.`,name:"device"},{anchor:"accelerate.data_loader.prepare_data_loader.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of processes running concurrently. Will default to the value given by
<a href="/docs/accelerate/v0.14.0/en/package_reference/state#accelerate.state.AcceleratorState">AcceleratorState</a>.`,name:"num_processes"},{anchor:"accelerate.data_loader.prepare_data_loader.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The index of the current process. Will default to the value given by <a href="/docs/accelerate/v0.14.0/en/package_reference/state#accelerate.state.AcceleratorState">AcceleratorState</a>.`,name:"process_index"},{anchor:"accelerate.data_loader.prepare_data_loader.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the resulting <code>DataLoader</code> should split the batches of the original data loader across devices or
yield full batches (in which case it will yield batches starting at the <code>process_index</code>-th and advancing of
<code>num_processes</code> batches at each iteration).</p>
<p>Another way to see this is that the observed batch size will be the same as the initial <code>dataloader</code> if
this option is set to <code>True</code>, the batch size of the initial <code>dataloader</code> multiplied by <code>num_processes</code>
otherwise.</p>
<p>Setting this option to <code>True</code> requires that the batch size of the <code>dataloader</code> is a round multiple of
<code>batch_size</code>.`,name:"split_batches"},{anchor:"accelerate.data_loader.prepare_data_loader.put_on_device",description:`<strong>put_on_device</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to put the batches on <code>device</code> (only works if the batches are nested list, tuples or
dictionaries of tensors).`,name:"put_on_device"},{anchor:"accelerate.data_loader.prepare_data_loader.rng_types",description:`<strong>rng_types</strong> (list of <code>str</code> or <code>RNGType</code>) &#x2014;
The list of random number generators to synchronize at the beginning of each iteration. Should be one or
several of:</p>
<ul>
<li><code>&quot;torch&quot;</code>: the base torch random number generator</li>
<li><code>&quot;cuda&quot;</code>: the CUDA random number generator (GPU only)</li>
<li><code>&quot;xla&quot;</code>: the XLA random number generator (TPU only)</li>
<li><code>&quot;generator&quot;</code>: the <code>torch.Generator</code> of the sampler (or batch sampler if there is no sampler in your
dataloader) or of the iterable dataset (if it exists) if the underlying dataset is of that type.</li>
</ul>`,name:"rng_types"},{anchor:"accelerate.data_loader.prepare_data_loader.dispatch_batches",description:`<strong>dispatch_batches</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the datalaoder prepared is only iterated through on the main process and then the batches
are split and broadcast to each process. Will default to <code>True</code> when the underlying dataset is an
<code>IterableDataset</code>, <code>False</code> otherwise.`,name:"dispatch_batches"},{anchor:"accelerate.data_loader.prepare_data_loader.even_batches",description:`<strong>even_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, in cases where the total batch size across all processes does not exactly divide the
dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among
all workers.`,name:"even_batches"}],source:"https://github.com/huggingface/accelerate/blob/v0.14.0/src/accelerate/data_loader.py#L565",returnDescription:`
<p>A new data loader that will yield the portion of the batches</p>
`,returnType:`
<p><code>torch.utils.data.dataloader.DataLoader</code></p>
`}}),J=new xo({props:{warning:!0,$$slots:{default:[Io]},$$scope:{ctx:we}}}),ce=new ve({props:{name:"class accelerate.data_loader.BatchSamplerShard",anchor:"accelerate.data_loader.BatchSamplerShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],parametersDescription:[{anchor:"accelerate.data_loader.BatchSamplerShard.batch_sampler",description:`<strong>batch_sampler</strong> (<code>torch.utils.data.sampler.BatchSampler</code>) &#x2014;
The batch sampler to split in several shards.`,name:"batch_sampler"},{anchor:"accelerate.data_loader.BatchSamplerShard.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of processes running concurrently.`,name:"num_processes"},{anchor:"accelerate.data_loader.BatchSamplerShard.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The index of the current process.`,name:"process_index"},{anchor:"accelerate.data_loader.BatchSamplerShard.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
yielding different full batches on each process.</p>
<p>On two processes with a sampler of <code>[[0, 1, 2, 3], [4, 5, 6, 7]]</code>, this will result in:</p>
<ul>
<li>the sampler on process 0 to yield <code>[0, 1, 2, 3]</code> and the sampler on process 1 to yield <code>[4, 5, 6, 7]</code> if
this argument is set to <code>False</code>.</li>
<li>the sampler on process 0 to yield <code>[0, 1]</code> then <code>[4, 5]</code> and the sampler on process 1 to yield <code>[2, 3]</code>
then <code>[6, 7]</code> if this argument is set to <code>True</code>.</li>
</ul>`,name:"split_batches"},{anchor:"accelerate.data_loader.BatchSamplerShard.even_batches",description:`<strong>even_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to loop back at the beginning of the sampler when the number of samples is not a round
multiple of (original batch size / number of processes).`,name:"even_batches"}],source:"https://github.com/huggingface/accelerate/blob/v0.14.0/src/accelerate/data_loader.py#L91"}}),K=new xo({props:{warning:!0,$$slots:{default:[ko]},$$scope:{ctx:we}}}),ne=new ve({props:{name:"class accelerate.data_loader.IterableDatasetShard",anchor:"accelerate.data_loader.IterableDatasetShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],parametersDescription:[{anchor:"accelerate.data_loader.IterableDatasetShard.dataset",description:`<strong>dataset</strong> (<code>torch.utils.data.dataset.IterableDataset</code>) &#x2014;
The batch sampler to split in several shards.`,name:"dataset"},{anchor:"accelerate.data_loader.IterableDatasetShard.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The size of the batches per shard (if <code>split_batches=False</code>) or the size of the batches (if
<code>split_batches=True</code>).`,name:"batch_size"},{anchor:"accelerate.data_loader.IterableDatasetShard.drop_last",description:`<strong>drop_last</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to drop the last incomplete batch or complete the last batches by using the samples from the
beginning.`,name:"drop_last"},{anchor:"accelerate.data_loader.IterableDatasetShard.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of processes running concurrently.`,name:"num_processes"},{anchor:"accelerate.data_loader.IterableDatasetShard.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The index of the current process.`,name:"process_index"},{anchor:"accelerate.data_loader.IterableDatasetShard.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
yielding different full batches on each process.</p>
<p>On two processes with an iterable dataset yielding of <code>[0, 1, 2, 3, 4, 5, 6, 7]</code>, this will result in:</p>
<ul>
<li>the shard on process 0 to yield <code>[0, 1, 2, 3]</code> and the shard on process 1 to yield <code>[4, 5, 6, 7]</code> if this
argument is set to <code>False</code>.</li>
<li>the shard on process 0 to yield <code>[0, 1, 4, 5]</code> and the sampler on process 1 to yield <code>[2, 3, 6, 7]</code> if
this argument is set to <code>True</code>.</li>
</ul>`,name:"split_batches"}],source:"https://github.com/huggingface/accelerate/blob/v0.14.0/src/accelerate/data_loader.py#L244"}}),le=new ve({props:{name:"class accelerate.data_loader.DataLoaderShard",anchor:"accelerate.data_loader.DataLoaderShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],parametersDescription:[{anchor:"accelerate.data_loader.DataLoaderShard.dataset",description:`<strong>dataset</strong> (<code>torch.utils.data.dataset.Dataset</code>) &#x2014;
The dataset to use to build this datalaoder.`,name:"dataset"},{anchor:"accelerate.data_loader.DataLoaderShard.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
If passed, the device to put all batches on.`,name:"device"},{anchor:"accelerate.data_loader.DataLoaderShard.rng_types",description:`<strong>rng_types</strong> (list of <code>str</code> or <code>RNGType</code>) &#x2014;
The list of random number generators to synchronize at the beginning of each iteration. Should be one or
several of:</p>
<ul>
<li><code>&quot;torch&quot;</code>: the base torch random number generator</li>
<li><code>&quot;cuda&quot;</code>: the CUDA random number generator (GPU only)</li>
<li><code>&quot;xla&quot;</code>: the XLA random number generator (TPU only)</li>
<li><code>&quot;generator&quot;</code>: an optional <code>torch.Generator</code></li>
</ul>`,name:"rng_types"},{anchor:"accelerate.data_loader.DataLoaderShard.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A random number generator to keep synchronized across processes.
kwargs &#x2014;
All other keyword arguments to pass to the regular <code>DataLoader</code> initialization.`,name:"generator"}],source:"https://github.com/huggingface/accelerate/blob/v0.14.0/src/accelerate/data_loader.py#L325"}}),pe=new ve({props:{name:"class accelerate.data_loader.DataLoaderDispatcher",anchor:"accelerate.data_loader.DataLoaderDispatcher",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],parametersDescription:[{anchor:"accelerate.data_loader.DataLoaderDispatcher.Subclass",description:"<strong>Subclass</strong> of a PyTorch <code>DataLoader</code> that will iterate and preprocess on process 0 only, then dispatch on each &#x2014;",name:"Subclass"},{anchor:"accelerate.data_loader.DataLoaderDispatcher.process",description:`<strong>process</strong> their part of the batch. &#x2014;
split_batches (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>):
Whether the resulting <code>DataLoader</code> should split the batches of the original data loader across devices or
yield full batches (in which case it will yield batches starting at the <code>process_index</code>-th and advancing of
<code>num_processes</code> batches at each iteration). Another way to see this is that the observed batch size will be
the same as the initial <code>dataloader</code> if this option is set to <code>True</code>, the batch size of the initial
<code>dataloader</code> multiplied by <code>num_processes</code> otherwise. Setting this option to <code>True</code> requires that the batch
size of the <code>dataloader</code> is a round multiple of <code>batch_size</code>.`,name:"process"},{anchor:"accelerate.data_loader.DataLoaderDispatcher.*Available",description:`*<strong>*Available</strong> attributes &#x2014;**</p>
<ul>
<li>
<p><strong>total_batch_size</strong> (<code>int</code>) &#x2014; Total batch size of the dataloader across all processes.
Equal to the original batch size when <code>split_batches=True</code>; otherwise the original batch size * the total
number of processes</p>
</li>
<li>
<p><strong>total_dataset_length</strong> (<code>int</code>) &#x2014; Total length of the inner dataset across all processes.</p>
</li>
</ul>`,name:"*Available"}],source:"https://github.com/huggingface/accelerate/blob/v0.14.0/src/accelerate/data_loader.py#L409"}}),me=new Et({}),ue=new ve({props:{name:"class accelerate.optimizer.AcceleratedOptimizer",anchor:"accelerate.optimizer.AcceleratedOptimizer",parameters:[{name:"optimizer",val:""},{name:"device_placement",val:" = True"},{name:"scaler",val:" = None"}],parametersDescription:[{anchor:"accelerate.optimizer.AcceleratedOptimizer.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.optimizer.Optimizer</code>) &#x2014;
The optimizer to wrap.`,name:"optimizer"},{anchor:"accelerate.optimizer.AcceleratedOptimizer.device_placement",description:`<strong>device_placement</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the optimizer should handle device placement. If so, it will place the state dictionary of
<code>optimizer</code> on the right device.`,name:"device_placement"},{anchor:"accelerate.optimizer.AcceleratedOptimizer.scaler",description:`<strong>scaler</strong> (<code>torch.cuda.amp.grad_scaler.GradScaler</code>, <em>optional</em>) &#x2014;
The scaler to use in the step function if training with mixed precision.`,name:"scaler"}],source:"https://github.com/huggingface/accelerate/blob/v0.14.0/src/accelerate/optimizer.py#L38"}}),fe=new Et({}),be=new ve({props:{name:"class accelerate.scheduler.AcceleratedScheduler",anchor:"accelerate.scheduler.AcceleratedScheduler",parameters:[{name:"scheduler",val:""},{name:"optimizers",val:""},{name:"step_with_optimizer",val:": bool = True"},{name:"split_batches",val:": bool = False"}],parametersDescription:[{anchor:"accelerate.scheduler.AcceleratedScheduler.scheduler",description:`<strong>scheduler</strong> (<code>torch.optim.lr_scheduler._LRScheduler</code>) &#x2014;
The scheduler to wrap.`,name:"scheduler"},{anchor:"accelerate.scheduler.AcceleratedScheduler.optimizers",description:`<strong>optimizers</strong> (one or a list of <code>torch.optim.Optimizer</code>) &#x2014;
The optimizers used.`,name:"optimizers"},{anchor:"accelerate.scheduler.AcceleratedScheduler.step_with_optimizer",description:`<strong>step_with_optimizer</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the scheduler should be stepped at each optimizer step.`,name:"step_with_optimizer"},{anchor:"accelerate.scheduler.AcceleratedScheduler.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the dataloaders split one batch across the different processes (so batch size is the same
regardless of the number of processes) or create batches on each process (so batch size is the original
batch size multiplied by the number of processes).`,name:"split_batches"}],source:"https://github.com/huggingface/accelerate/blob/v0.14.0/src/accelerate/scheduler.py#L25"}}),{c(){d=o("meta"),_=i(),f=o("h1"),v=o("a"),b=o("span"),E(y.$$.fragment),I=i(),g=o("span"),k=c("Wrapper classes for torch Dataloaders, Optimizers, and Schedulers"),w=i(),p=o("p"),B=c(`The internal classes Accelerate uses to prepare objects for distributed training
when calling `),L=o("a"),G=c("prepare()"),St=c("."),ct=i(),U=o("h2"),X=o("a"),Se=o("span"),E(oe.$$.fragment),xt=i(),xe=o("span"),Tt=c("Datasets and DataLoaders"),nt=i(),z=o("div"),E(re.$$.fragment),At=i(),se=o("p"),Ot=c("Wraps a PyTorch "),Te=o("code"),Lt=c("DataLoader"),Pt=c(" to generate batches for one of the processes only."),It=i(),R=o("p"),kt=c("Depending on the value of the "),Ae=o("code"),qt=c("drop_last"),Ct=c(" attribute of the "),Oe=o("code"),Wt=c("dataloader"),Nt=c(` passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),Ft=i(),E(J.$$.fragment),lt=i(),q=o("div"),E(ce.$$.fragment),Bt=i(),C=o("p"),Gt=c("Wraps a PyTorch "),Le=o("code"),Ut=c("BatchSampler"),Rt=c(` to generate batches for one of the processes only. Instances of this class will
always yield a number of batches that is a round multiple of `),Pe=o("code"),Vt=c("num_processes"),Ht=c(` and that all have the same size.
Depending on the value of the `),Ie=o("code"),Mt=c("drop_last"),jt=c(` attribute of the batch sampler passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),Xt=i(),E(K.$$.fragment),dt=i(),V=o("div"),E(ne.$$.fragment),Jt=i(),$=o("p"),Kt=c("Wraps a PyTorch "),ke=o("code"),Qt=c("IterableDataset"),Yt=c(` to generate samples for one of the processes only. Instances of this class will
always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
`),qe=o("code"),Zt=c("split_batches"),ea=c(", this is either "),Ce=o("code"),ta=c("batch_size"),aa=c(" or "),We=o("code"),oa=c("batch_size x num_processes"),ra=c(`). Depending on the value of the
`),Ne=o("code"),sa=c("drop_last"),ca=c(` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
be too small or loop with indices from the beginning.`),it=i(),D=o("div"),E(le.$$.fragment),na=i(),de=o("p"),la=c("Subclass of a PyTorch "),Fe=o("code"),da=c("DataLoader"),ia=c(" that will deal with device placement and current distributed setup."),ha=i(),Be=o("p"),Ge=o("strong"),pa=c("Available attributes:"),ma=i(),ie=o("ul"),Ue=o("li"),F=o("p"),Re=o("strong"),ua=c("total_batch_size"),fa=c(" ("),Ve=o("code"),ba=c("int"),ga=c(`) \u2014 Total batch size of the dataloader across all processes.
Equal to the original batch size when `),He=o("code"),_a=c("split_batches=True"),va=c(`; otherwise the original batch size * the total
number of processes`),wa=i(),Me=o("li"),Q=o("p"),je=o("strong"),ya=c("total_dataset_length"),$a=c(" ("),Xe=o("code"),za=c("int"),Da=c(") \u2014 Total length of the inner dataset across all processes."),ht=i(),he=o("div"),E(pe.$$.fragment),pt=i(),H=o("h2"),Y=o("a"),Je=o("span"),E(me.$$.fragment),Ea=i(),Ke=o("span"),Sa=c("Optimizers"),mt=i(),W=o("div"),E(ue.$$.fragment),xa=i(),Qe=o("p"),Ta=c("Internal wrapper around a torch optimizer."),Aa=i(),M=o("p"),Oa=c("Conditionally will perform "),Ye=o("code"),La=c("step"),Pa=c(" and "),Ze=o("code"),Ia=c("zero_grad"),ka=c(` if gradients should be synchronized when performing gradient
accumulation.`),ut=i(),j=o("h2"),Z=o("a"),et=o("span"),E(fe.$$.fragment),qa=i(),tt=o("span"),Ca=c("Schedulers"),ft=i(),N=o("div"),E(be.$$.fragment),Wa=i(),at=o("p"),Na=c(`A wrapper around a learning rate scheduler that will only step when the optimizer(s) have a training step. Useful
to avoid making a scheduler step too fast when gradients went overflow and there was no training step (in mixed
precision training)`),Fa=i(),ot=o("p"),Ba=c(`When performing gradient accumulation scheduler lengths should not be changed accordingly, Accelerate will always
step the scheduler to account for it.`),this.h()},l(t){const l=Lo('[data-svelte="svelte-1phssyn"]',document.head);d=r(l,"META",{name:!0,content:!0}),l.forEach(a),_=h(t),f=r(t,"H1",{class:!0});var ge=s(f);v=r(ge,"A",{id:!0,class:!0,href:!0});var rt=s(v);b=r(rt,"SPAN",{});var Ga=s(b);S(y.$$.fragment,Ga),Ga.forEach(a),rt.forEach(a),I=h(ge),g=r(ge,"SPAN",{});var Ua=s(g);k=n(Ua,"Wrapper classes for torch Dataloaders, Optimizers, and Schedulers"),Ua.forEach(a),ge.forEach(a),w=h(t),p=r(t,"P",{});var gt=s(p);B=n(gt,`The internal classes Accelerate uses to prepare objects for distributed training
when calling `),L=r(gt,"A",{href:!0});var Ra=s(L);G=n(Ra,"prepare()"),Ra.forEach(a),St=n(gt,"."),gt.forEach(a),ct=h(t),U=r(t,"H2",{class:!0});var _t=s(U);X=r(_t,"A",{id:!0,class:!0,href:!0});var Va=s(X);Se=r(Va,"SPAN",{});var Ha=s(Se);S(oe.$$.fragment,Ha),Ha.forEach(a),Va.forEach(a),xt=h(_t),xe=r(_t,"SPAN",{});var Ma=s(xe);Tt=n(Ma,"Datasets and DataLoaders"),Ma.forEach(a),_t.forEach(a),nt=h(t),z=r(t,"DIV",{class:!0});var ee=s(z);S(re.$$.fragment,ee),At=h(ee),se=r(ee,"P",{});var vt=s(se);Ot=n(vt,"Wraps a PyTorch "),Te=r(vt,"CODE",{});var ja=s(Te);Lt=n(ja,"DataLoader"),ja.forEach(a),Pt=n(vt," to generate batches for one of the processes only."),vt.forEach(a),It=h(ee),R=r(ee,"P",{});var ye=s(R);kt=n(ye,"Depending on the value of the "),Ae=r(ye,"CODE",{});var Xa=s(Ae);qt=n(Xa,"drop_last"),Xa.forEach(a),Ct=n(ye," attribute of the "),Oe=r(ye,"CODE",{});var Ja=s(Oe);Wt=n(Ja,"dataloader"),Ja.forEach(a),Nt=n(ye,` passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),ye.forEach(a),Ft=h(ee),S(J.$$.fragment,ee),ee.forEach(a),lt=h(t),q=r(t,"DIV",{class:!0});var $e=s(q);S(ce.$$.fragment,$e),Bt=h($e),C=r($e,"P",{});var te=s(C);Gt=n(te,"Wraps a PyTorch "),Le=r(te,"CODE",{});var Ka=s(Le);Ut=n(Ka,"BatchSampler"),Ka.forEach(a),Rt=n(te,` to generate batches for one of the processes only. Instances of this class will
always yield a number of batches that is a round multiple of `),Pe=r(te,"CODE",{});var Qa=s(Pe);Vt=n(Qa,"num_processes"),Qa.forEach(a),Ht=n(te,` and that all have the same size.
Depending on the value of the `),Ie=r(te,"CODE",{});var Ya=s(Ie);Mt=n(Ya,"drop_last"),Ya.forEach(a),jt=n(te,` attribute of the batch sampler passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),te.forEach(a),Xt=h($e),S(K.$$.fragment,$e),$e.forEach(a),dt=h(t),V=r(t,"DIV",{class:!0});var wt=s(V);S(ne.$$.fragment,wt),Jt=h(wt),$=r(wt,"P",{});var P=s($);Kt=n(P,"Wraps a PyTorch "),ke=r(P,"CODE",{});var Za=s(ke);Qt=n(Za,"IterableDataset"),Za.forEach(a),Yt=n(P,` to generate samples for one of the processes only. Instances of this class will
always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
`),qe=r(P,"CODE",{});var eo=s(qe);Zt=n(eo,"split_batches"),eo.forEach(a),ea=n(P,", this is either "),Ce=r(P,"CODE",{});var to=s(Ce);ta=n(to,"batch_size"),to.forEach(a),aa=n(P," or "),We=r(P,"CODE",{});var ao=s(We);oa=n(ao,"batch_size x num_processes"),ao.forEach(a),ra=n(P,`). Depending on the value of the
`),Ne=r(P,"CODE",{});var oo=s(Ne);sa=n(oo,"drop_last"),oo.forEach(a),ca=n(P,` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
be too small or loop with indices from the beginning.`),P.forEach(a),wt.forEach(a),it=h(t),D=r(t,"DIV",{class:!0});var ae=s(D);S(le.$$.fragment,ae),na=h(ae),de=r(ae,"P",{});var yt=s(de);la=n(yt,"Subclass of a PyTorch "),Fe=r(yt,"CODE",{});var ro=s(Fe);da=n(ro,"DataLoader"),ro.forEach(a),ia=n(yt," that will deal with device placement and current distributed setup."),yt.forEach(a),ha=h(ae),Be=r(ae,"P",{});var so=s(Be);Ge=r(so,"STRONG",{});var co=s(Ge);pa=n(co,"Available attributes:"),co.forEach(a),so.forEach(a),ma=h(ae),ie=r(ae,"UL",{});var $t=s(ie);Ue=r($t,"LI",{});var no=s(Ue);F=r(no,"P",{});var _e=s(F);Re=r(_e,"STRONG",{});var lo=s(Re);ua=n(lo,"total_batch_size"),lo.forEach(a),fa=n(_e," ("),Ve=r(_e,"CODE",{});var io=s(Ve);ba=n(io,"int"),io.forEach(a),ga=n(_e,`) \u2014 Total batch size of the dataloader across all processes.
Equal to the original batch size when `),He=r(_e,"CODE",{});var ho=s(He);_a=n(ho,"split_batches=True"),ho.forEach(a),va=n(_e,`; otherwise the original batch size * the total
number of processes`),_e.forEach(a),no.forEach(a),wa=h($t),Me=r($t,"LI",{});var po=s(Me);Q=r(po,"P",{});var st=s(Q);je=r(st,"STRONG",{});var mo=s(je);ya=n(mo,"total_dataset_length"),mo.forEach(a),$a=n(st," ("),Xe=r(st,"CODE",{});var uo=s(Xe);za=n(uo,"int"),uo.forEach(a),Da=n(st,") \u2014 Total length of the inner dataset across all processes."),st.forEach(a),po.forEach(a),$t.forEach(a),ae.forEach(a),ht=h(t),he=r(t,"DIV",{class:!0});var fo=s(he);S(pe.$$.fragment,fo),fo.forEach(a),pt=h(t),H=r(t,"H2",{class:!0});var zt=s(H);Y=r(zt,"A",{id:!0,class:!0,href:!0});var bo=s(Y);Je=r(bo,"SPAN",{});var go=s(Je);S(me.$$.fragment,go),go.forEach(a),bo.forEach(a),Ea=h(zt),Ke=r(zt,"SPAN",{});var _o=s(Ke);Sa=n(_o,"Optimizers"),_o.forEach(a),zt.forEach(a),mt=h(t),W=r(t,"DIV",{class:!0});var ze=s(W);S(ue.$$.fragment,ze),xa=h(ze),Qe=r(ze,"P",{});var vo=s(Qe);Ta=n(vo,"Internal wrapper around a torch optimizer."),vo.forEach(a),Aa=h(ze),M=r(ze,"P",{});var De=s(M);Oa=n(De,"Conditionally will perform "),Ye=r(De,"CODE",{});var wo=s(Ye);La=n(wo,"step"),wo.forEach(a),Pa=n(De," and "),Ze=r(De,"CODE",{});var yo=s(Ze);Ia=n(yo,"zero_grad"),yo.forEach(a),ka=n(De,` if gradients should be synchronized when performing gradient
accumulation.`),De.forEach(a),ze.forEach(a),ut=h(t),j=r(t,"H2",{class:!0});var Dt=s(j);Z=r(Dt,"A",{id:!0,class:!0,href:!0});var $o=s(Z);et=r($o,"SPAN",{});var zo=s(et);S(fe.$$.fragment,zo),zo.forEach(a),$o.forEach(a),qa=h(Dt),tt=r(Dt,"SPAN",{});var Do=s(tt);Ca=n(Do,"Schedulers"),Do.forEach(a),Dt.forEach(a),ft=h(t),N=r(t,"DIV",{class:!0});var Ee=s(N);S(be.$$.fragment,Ee),Wa=h(Ee),at=r(Ee,"P",{});var Eo=s(at);Na=n(Eo,`A wrapper around a learning rate scheduler that will only step when the optimizer(s) have a training step. Useful
to avoid making a scheduler step too fast when gradients went overflow and there was no training step (in mixed
precision training)`),Eo.forEach(a),Fa=h(Ee),ot=r(Ee,"P",{});var So=s(ot);Ba=n(So,`When performing gradient accumulation scheduler lengths should not be changed accordingly, Accelerate will always
step the scheduler to account for it.`),So.forEach(a),Ee.forEach(a),this.h()},h(){m(d,"name","hf:doc:metadata"),m(d,"content",JSON.stringify(Co)),m(v,"id","wrapper-classes-for-torch-dataloaders-optimizers-and-schedulers"),m(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(v,"href","#wrapper-classes-for-torch-dataloaders-optimizers-and-schedulers"),m(f,"class","relative group"),m(L,"href","/docs/accelerate/v0.14.0/en/package_reference/accelerator#accelerate.Accelerator.prepare"),m(X,"id","accelerate.data_loader.prepare_data_loader"),m(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(X,"href","#accelerate.data_loader.prepare_data_loader"),m(U,"class","relative group"),m(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Y,"id","accelerate.optimizer.AcceleratedOptimizer"),m(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Y,"href","#accelerate.optimizer.AcceleratedOptimizer"),m(H,"class","relative group"),m(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Z,"id","accelerate.scheduler.AcceleratedScheduler"),m(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Z,"href","#accelerate.scheduler.AcceleratedScheduler"),m(j,"class","relative group"),m(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,l){e(document.head,d),u(t,_,l),u(t,f,l),e(f,v),e(v,b),x(y,b,null),e(f,I),e(f,g),e(g,k),u(t,w,l),u(t,p,l),e(p,B),e(p,L),e(L,G),e(p,St),u(t,ct,l),u(t,U,l),e(U,X),e(X,Se),x(oe,Se,null),e(U,xt),e(U,xe),e(xe,Tt),u(t,nt,l),u(t,z,l),x(re,z,null),e(z,At),e(z,se),e(se,Ot),e(se,Te),e(Te,Lt),e(se,Pt),e(z,It),e(z,R),e(R,kt),e(R,Ae),e(Ae,qt),e(R,Ct),e(R,Oe),e(Oe,Wt),e(R,Nt),e(z,Ft),x(J,z,null),u(t,lt,l),u(t,q,l),x(ce,q,null),e(q,Bt),e(q,C),e(C,Gt),e(C,Le),e(Le,Ut),e(C,Rt),e(C,Pe),e(Pe,Vt),e(C,Ht),e(C,Ie),e(Ie,Mt),e(C,jt),e(q,Xt),x(K,q,null),u(t,dt,l),u(t,V,l),x(ne,V,null),e(V,Jt),e(V,$),e($,Kt),e($,ke),e(ke,Qt),e($,Yt),e($,qe),e(qe,Zt),e($,ea),e($,Ce),e(Ce,ta),e($,aa),e($,We),e(We,oa),e($,ra),e($,Ne),e(Ne,sa),e($,ca),u(t,it,l),u(t,D,l),x(le,D,null),e(D,na),e(D,de),e(de,la),e(de,Fe),e(Fe,da),e(de,ia),e(D,ha),e(D,Be),e(Be,Ge),e(Ge,pa),e(D,ma),e(D,ie),e(ie,Ue),e(Ue,F),e(F,Re),e(Re,ua),e(F,fa),e(F,Ve),e(Ve,ba),e(F,ga),e(F,He),e(He,_a),e(F,va),e(ie,wa),e(ie,Me),e(Me,Q),e(Q,je),e(je,ya),e(Q,$a),e(Q,Xe),e(Xe,za),e(Q,Da),u(t,ht,l),u(t,he,l),x(pe,he,null),u(t,pt,l),u(t,H,l),e(H,Y),e(Y,Je),x(me,Je,null),e(H,Ea),e(H,Ke),e(Ke,Sa),u(t,mt,l),u(t,W,l),x(ue,W,null),e(W,xa),e(W,Qe),e(Qe,Ta),e(W,Aa),e(W,M),e(M,Oa),e(M,Ye),e(Ye,La),e(M,Pa),e(M,Ze),e(Ze,Ia),e(M,ka),u(t,ut,l),u(t,j,l),e(j,Z),e(Z,et),x(fe,et,null),e(j,qa),e(j,tt),e(tt,Ca),u(t,ft,l),u(t,N,l),x(be,N,null),e(N,Wa),e(N,at),e(at,Na),e(N,Fa),e(N,ot),e(ot,Ba),bt=!0},p(t,[l]){const ge={};l&2&&(ge.$$scope={dirty:l,ctx:t}),J.$set(ge);const rt={};l&2&&(rt.$$scope={dirty:l,ctx:t}),K.$set(rt)},i(t){bt||(T(y.$$.fragment,t),T(oe.$$.fragment,t),T(re.$$.fragment,t),T(J.$$.fragment,t),T(ce.$$.fragment,t),T(K.$$.fragment,t),T(ne.$$.fragment,t),T(le.$$.fragment,t),T(pe.$$.fragment,t),T(me.$$.fragment,t),T(ue.$$.fragment,t),T(fe.$$.fragment,t),T(be.$$.fragment,t),bt=!0)},o(t){A(y.$$.fragment,t),A(oe.$$.fragment,t),A(re.$$.fragment,t),A(J.$$.fragment,t),A(ce.$$.fragment,t),A(K.$$.fragment,t),A(ne.$$.fragment,t),A(le.$$.fragment,t),A(pe.$$.fragment,t),A(me.$$.fragment,t),A(ue.$$.fragment,t),A(fe.$$.fragment,t),A(be.$$.fragment,t),bt=!1},d(t){a(d),t&&a(_),t&&a(f),O(y),t&&a(w),t&&a(p),t&&a(ct),t&&a(U),O(oe),t&&a(nt),t&&a(z),O(re),O(J),t&&a(lt),t&&a(q),O(ce),O(K),t&&a(dt),t&&a(V),O(ne),t&&a(it),t&&a(D),O(le),t&&a(ht),t&&a(he),O(pe),t&&a(pt),t&&a(H),O(me),t&&a(mt),t&&a(W),O(ue),t&&a(ut),t&&a(j),O(fe),t&&a(ft),t&&a(N),O(be)}}}const Co={local:"wrapper-classes-for-torch-dataloaders-optimizers-and-schedulers",sections:[{local:"accelerate.data_loader.prepare_data_loader",title:"Datasets and DataLoaders"},{local:"accelerate.optimizer.AcceleratedOptimizer",title:"Optimizers "},{local:"accelerate.scheduler.AcceleratedScheduler",title:"Schedulers "}],title:"Wrapper classes for torch Dataloaders, Optimizers, and Schedulers"};function Wo(we){return Po(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Uo extends To{constructor(d){super();Ao(this,d,Wo,qo,Oo,{})}}export{Uo as default,Co as metadata};
