import{S as M6,i as k6,s as G6,e as a,k as d,w as $,t as l,M as x6,c,d as t,m as f,a as i,x as N,h as r,b as h,G as e,g as n,y as U,L as R6,q as M,o as k,B as G,v as F6}from"../../chunks/vendor-hf-doc-builder.js";import{I as Uo}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as il}from"../../chunks/CodeBlock-hf-doc-builder.js";function W6(Z2){let ce,Ts,ie,_e,sl,Gt,Sd,nl,yd,Is,Mo,Td,Ls,se,pe,dl,xt,Id,fl,Ld,As,Rt,hl,Ad,$d,$s,Ee,ul,Nd,Ud,_l,Md,Ns,ve,kd,pl,Gd,xd,Us,Ft,El,Rd,Fd,Ms,Wt,ks,Ht,vl,Wd,Hd,Gs,me,m,ml,zd,Bd,gl,Kd,jd,Cl,Xd,Yd,Ol,Zd,qd,Dl,Jd,Qd,wl,Vd,ef,tf,H,bl,of,lf,Pl,rf,af,Sl,cf,sf,xs,ne,ge,yl,zt,nf,Tl,df,Rs,Bt,Il,ff,hf,Fs,Ce,Ll,uf,_f,Al,pf,Ws,ko,Ef,Hs,Kt,$l,vf,mf,zs,jt,Bs,Xt,Nl,gf,Cf,Ks,z,Ul,g,Ml,Of,Df,kl,wf,bf,Gl,Pf,Sf,xl,yf,Tf,Rl,If,Lf,Fl,Af,$f,Nf,Wl,B,Hl,Uf,Mf,zl,kf,Gf,Bl,xf,Rf,Ff,Kl,Oe,jl,Wf,Hf,Xl,zf,Bf,js,de,De,Yl,Yt,Kf,Zl,jf,Xs,Zt,ql,Xf,Yf,Ys,we,Jl,Zf,qf,Ql,Jf,Zs,be,Qf,qt,Vf,eh,qs,Jt,Vl,th,oh,Js,Qt,Qs,Vt,er,lh,rh,Vs,Pe,C,tr,ah,ch,or,ih,sh,lr,nh,dh,rr,fh,hh,ar,uh,_h,cr,ph,Eh,vh,K,ir,mh,gh,sr,Ch,Oh,nr,Dh,wh,en,fe,Se,dr,eo,bh,fr,Ph,tn,to,hr,Sh,yh,on,ye,ur,Th,Ih,_r,Lh,ln,Go,Ah,rn,oo,pr,$h,Nh,an,lo,cn,ro,Er,Uh,Mh,sn,Te,xo,vr,kh,Gh,xh,Ro,mr,Rh,Fh,nn,ao,gr,Wh,Hh,dn,w,j,Cr,zh,Bh,Or,Kh,jh,Dr,Xh,Yh,Zh,Ie,wr,qh,Jh,br,Qh,Vh,eu,X,Pr,tu,ou,Sr,lu,ru,yr,au,cu,iu,Le,Tr,su,nu,Ir,du,fu,hu,Ae,Lr,uu,_u,Ar,pu,Eu,fn,Y,vu,$r,mu,gu,Nr,Cu,Ou,hn,co,Ur,Du,wu,un,x,$e,Mr,bu,Pu,kr,Su,yu,Tu,Ne,Gr,Iu,Lu,xr,Au,$u,Nu,Ue,Rr,Uu,Mu,Fr,ku,Gu,xu,Me,Wr,Ru,Fu,Hr,Wu,Hu,_n,io,zr,zu,Bu,pn,Fo,Ku,En,R,ke,Br,ju,Xu,Kr,Yu,Zu,qu,Ge,jr,Ju,Qu,Xr,Vu,e_,t_,xe,Yr,o_,l_,Zr,r_,a_,c_,Re,qr,i_,s_,Jr,n_,d_,vn,so,Qr,f_,h_,mn,Wo,u_,gn,Z,Fe,Vr,__,p_,ea,E_,v_,m_,We,ta,g_,C_,oa,O_,D_,w_,He,la,b_,P_,ra,S_,y_,Cn,no,aa,T_,I_,On,q,L_,ca,A_,$_,ia,N_,U_,Dn,p,ze,sa,M_,k_,na,G_,x_,R_,Be,da,F_,W_,fa,H_,z_,B_,Ke,ha,K_,j_,ua,X_,Y_,Z_,je,_a,q_,J_,pa,Q_,V_,ep,Xe,Ea,tp,op,va,lp,rp,ap,Ye,ma,cp,ip,ga,sp,np,dp,Ze,Ca,fp,hp,Oa,up,_p,pp,qe,Da,Ep,vp,wa,mp,gp,wn,fo,ba,Cp,Op,bn,J,Dp,Pa,wp,bp,Sa,Pp,Sp,Pn,Je,Qe,ya,yp,Tp,Ta,Ip,Lp,Ap,Ve,Ia,$p,Np,La,Up,Mp,Sn,ho,Aa,kp,Gp,yn,F,xp,$a,Rp,Fp,Na,Wp,Hp,Ua,zp,Bp,Tn,u,et,Ma,Kp,jp,ka,Xp,Yp,Zp,tt,Ga,qp,Jp,xa,Qp,Vp,e1,ot,Ra,t1,o1,Fa,l1,r1,a1,lt,Wa,c1,i1,Ha,s1,n1,d1,rt,za,f1,h1,Ba,u1,_1,p1,at,Ka,E1,v1,ja,m1,g1,C1,Q,Xa,O1,D1,Ya,w1,b1,Za,P1,S1,y1,ct,qa,T1,I1,Ja,L1,A1,$1,it,Qa,N1,U1,Va,M1,k1,G1,st,ec,x1,R1,tc,F1,W1,H1,nt,oc,z1,B1,lc,K1,j1,X1,dt,rc,Y1,Z1,ac,q1,J1,In,uo,cc,Q1,V1,Ln,V,eE,ic,tE,oE,sc,lE,rE,An,E,ft,nc,aE,cE,dc,iE,sE,nE,ht,fc,dE,fE,hc,hE,uE,_E,ut,uc,pE,EE,_c,vE,mE,gE,_t,pc,CE,OE,Ec,DE,wE,bE,b,vc,PE,SE,mc,yE,TE,gc,IE,LE,Cc,AE,$E,Oc,NE,UE,ME,pt,Dc,kE,GE,wc,xE,RE,FE,Et,bc,WE,HE,Pc,zE,BE,$n,_o,Sc,KE,jE,Nn,ee,XE,yc,YE,ZE,Tc,qE,JE,Un,v,Ho,Ic,QE,VE,ev,zo,Lc,tv,ov,lv,Bo,Ac,rv,av,cv,Ko,$c,iv,sv,nv,jo,Nc,dv,fv,hv,Xo,Uc,uv,_v,pv,Yo,Mc,Ev,vv,Mn,po,kc,mv,gv,kn,Zo,Cv,Gn,vt,mt,Gc,Ov,Dv,xc,wv,bv,Pv,gt,Rc,Sv,yv,Fc,Tv,Iv,xn,he,Ct,Wc,Eo,Lv,Hc,Av,Rn,qo,zc,$v,Fn,vo,Bc,Nv,Uv,Wn,mo,Hn,go,Kc,Mv,kv,zn,Jo,te,jc,Gv,xv,Xc,Rv,Fv,Yc,Wv,Hv,Bn,Co,Zc,zv,Bv,Kn,Ot,Kv,qc,jv,Xv,jn,oe,Dt,Jc,Yv,Zv,Qc,qv,Jv,Qv,wt,Vc,Vv,em,ei,tm,om,lm,bt,ti,rm,am,oi,cm,im,Xn,Oo,li,sm,nm,Yn,Qo,dm,Zn,P,Pt,ri,fm,hm,ai,um,_m,pm,St,ci,Em,vm,ii,mm,gm,Cm,yt,si,Om,Dm,ni,wm,bm,Pm,Tt,di,Sm,ym,fi,Tm,Im,Lm,It,hi,Am,$m,ui,Nm,Um,qn,ue,Lt,_i,Do,Mm,pi,km,Jn,At,Ei,Gm,xm,vi,Rm,Qn,$t,Fm,mi,Wm,Hm,Vn,wo,gi,zm,Bm,ed,bo,td,Po,Ci,Km,jm,od,Nt,O,Oi,Xm,Ym,Di,Zm,qm,wi,Jm,Qm,bi,Vm,e2,Pi,t2,o2,Si,l2,r2,a2,le,yi,c2,i2,Ti,s2,n2,Ii,d2,f2,ld;return Gt=new Uo({}),xt=new Uo({}),Wt=new il({props:{code:"accelerate config [arguments]",highlighted:"accelerate config [arguments]"}}),zt=new Uo({}),jt=new il({props:{code:"accelerate default-config [arguments]",highlighted:"accelerate default-config [arguments]"}}),Yt=new Uo({}),Qt=new il({props:{code:"accelerate env [arguments]",highlighted:'accelerate <span class="hljs-built_in">env</span> [arguments]'}}),eo=new Uo({}),lo=new il({props:{code:"accelerate launch [arguments] {training_script} --{training_script-argument-1} --{training_script-argument-2} ...",highlighted:"accelerate launch [arguments] {training_script} --{training_script-argument-1} --{training_script-argument-2} ..."}}),Eo=new Uo({}),mo=new il({props:{code:"accelerate tpu-config [arguments]",highlighted:"accelerate tpu-config [arguments]"}}),Do=new Uo({}),bo=new il({props:{code:"accelerate test [arguments]",highlighted:'accelerate <span class="hljs-built_in">test</span> [arguments]'}}),{c(){ce=a("meta"),Ts=d(),ie=a("h1"),_e=a("a"),sl=a("span"),$(Gt.$$.fragment),Sd=d(),nl=a("span"),yd=l("The Command Line"),Is=d(),Mo=a("p"),Td=l("Below is a list of all the available commands \u{1F917} Accelerate with their parameters"),Ls=d(),se=a("h2"),pe=a("a"),dl=a("span"),$(xt.$$.fragment),Id=d(),fl=a("span"),Ld=l("accelerate config"),As=d(),Rt=a("p"),hl=a("strong"),Ad=l("Command"),$d=l(":"),$s=d(),Ee=a("p"),ul=a("code"),Nd=l("accelerate config"),Ud=l(" or "),_l=a("code"),Md=l("accelerate-config"),Ns=d(),ve=a("p"),kd=l("Launches a series of prompts to create and save a "),pl=a("code"),Gd=l("default_config.yml"),xd=l(` configuration file for your training system. Should
always be ran first on your machine.`),Us=d(),Ft=a("p"),El=a("strong"),Rd=l("Usage"),Fd=l(":"),Ms=d(),$(Wt.$$.fragment),ks=d(),Ht=a("p"),vl=a("strong"),Wd=l("Optional Arguments"),Hd=l(":"),Gs=d(),me=a("ul"),m=a("li"),ml=a("code"),zd=l("--config_file CONFIG_FILE"),Bd=l(" ("),gl=a("code"),Kd=l("str"),jd=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Cl=a("code"),Xd=l("HF_HOME"),Yd=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),Ol=a("code"),Zd=l("~/.cache"),qd=l(" or the content of "),Dl=a("code"),Jd=l("XDG_CACHE_HOME"),Qd=l(") suffixed with "),wl=a("code"),Vd=l("huggingface"),ef=l("."),tf=d(),H=a("li"),bl=a("code"),of=l("-h"),lf=l(", "),Pl=a("code"),rf=l("--help"),af=l(" ("),Sl=a("code"),cf=l("bool"),sf=l(") \u2014 Show a help message and exit"),xs=d(),ne=a("h2"),ge=a("a"),yl=a("span"),$(zt.$$.fragment),nf=d(),Tl=a("span"),df=l("accelerate config default"),Rs=d(),Bt=a("p"),Il=a("strong"),ff=l("Command"),hf=l(":"),Fs=d(),Ce=a("p"),Ll=a("code"),uf=l("accelerate config default"),_f=l(" or "),Al=a("code"),pf=l("accelerate-config default"),Ws=d(),ko=a("p"),Ef=l("Create a default config file for Accelerate with only a few flags set."),Hs=d(),Kt=a("p"),$l=a("strong"),vf=l("Usage"),mf=l(":"),zs=d(),$(jt.$$.fragment),Bs=d(),Xt=a("p"),Nl=a("strong"),gf=l("Optional Arguments"),Cf=l(":"),Ks=d(),z=a("ul"),Ul=a("li"),g=a("p"),Ml=a("code"),Of=l("--config_file CONFIG_FILE"),Df=l(" ("),kl=a("code"),wf=l("str"),bf=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Gl=a("code"),Pf=l("HF_HOME"),Sf=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),xl=a("code"),yf=l("~/.cache"),Tf=l(" or the content of "),Rl=a("code"),If=l("XDG_CACHE_HOME"),Lf=l(") suffixed with "),Fl=a("code"),Af=l("huggingface"),$f=l("."),Nf=d(),Wl=a("li"),B=a("p"),Hl=a("code"),Uf=l("-h"),Mf=l(", "),zl=a("code"),kf=l("--help"),Gf=l(" ("),Bl=a("code"),xf=l("bool"),Rf=l(") \u2014 Show a help message and exit"),Ff=d(),Kl=a("li"),Oe=a("p"),jl=a("code"),Wf=l("--mixed_precision {no,fp16,bf16}"),Hf=l(" ("),Xl=a("code"),zf=l("str"),Bf=l(") \u2014 Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later."),js=d(),de=a("h2"),De=a("a"),Yl=a("span"),$(Yt.$$.fragment),Kf=d(),Zl=a("span"),jf=l("accelerate env"),Xs=d(),Zt=a("p"),ql=a("strong"),Xf=l("Command"),Yf=l(":"),Ys=d(),we=a("p"),Jl=a("code"),Zf=l("accelerate env"),qf=l(" or "),Ql=a("code"),Jf=l("accelerate-env"),Zs=d(),be=a("p"),Qf=l("Lists the contents of the passed \u{1F917} Accelerate configuration file. Should always be used when opening an issue on the "),qt=a("a"),Vf=l("GitHub repository"),eh=l("."),qs=d(),Jt=a("p"),Vl=a("strong"),th=l("Usage"),oh=l(":"),Js=d(),$(Qt.$$.fragment),Qs=d(),Vt=a("p"),er=a("strong"),lh=l("Optional Arguments"),rh=l(":"),Vs=d(),Pe=a("ul"),C=a("li"),tr=a("code"),ah=l("--config_file CONFIG_FILE"),ch=l(" ("),or=a("code"),ih=l("str"),sh=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),lr=a("code"),nh=l("HF_HOME"),dh=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),rr=a("code"),fh=l("~/.cache"),hh=l(" or the content of "),ar=a("code"),uh=l("XDG_CACHE_HOME"),_h=l(") suffixed with "),cr=a("code"),ph=l("huggingface"),Eh=l("."),vh=d(),K=a("li"),ir=a("code"),mh=l("-h"),gh=l(", "),sr=a("code"),Ch=l("--help"),Oh=l(" ("),nr=a("code"),Dh=l("bool"),wh=l(") \u2014 Show a help message and exit"),en=d(),fe=a("h2"),Se=a("a"),dr=a("span"),$(eo.$$.fragment),bh=d(),fr=a("span"),Ph=l("accelerate launch"),tn=d(),to=a("p"),hr=a("strong"),Sh=l("Command"),yh=l(":"),on=d(),ye=a("p"),ur=a("code"),Th=l("accelerate launch"),Ih=l(" or "),_r=a("code"),Lh=l("accelerate-launch"),ln=d(),Go=a("p"),Ah=l("Launches a specified script on a distributed system with the right parameters."),rn=d(),oo=a("p"),pr=a("strong"),$h=l("Usage"),Nh=l(":"),an=d(),$(lo.$$.fragment),cn=d(),ro=a("p"),Er=a("strong"),Uh=l("Positional Arguments"),Mh=l(":"),sn=d(),Te=a("ul"),xo=a("li"),vr=a("code"),kh=l("{training_script}"),Gh=l(" \u2014 The full path to the script to be launched in parallel"),xh=d(),Ro=a("li"),mr=a("code"),Rh=l("--{training_script-argument-1}"),Fh=l(" \u2014 Arguments of the training script"),nn=d(),ao=a("p"),gr=a("strong"),Wh=l("Optional Arguments"),Hh=l(":"),dn=d(),w=a("ul"),j=a("li"),Cr=a("code"),zh=l("-h"),Bh=l(", "),Or=a("code"),Kh=l("--help"),jh=l(" ("),Dr=a("code"),Xh=l("bool"),Yh=l(") \u2014 Show a help message and exit"),Zh=d(),Ie=a("li"),wr=a("code"),qh=l("--config_file CONFIG_FILE"),Jh=l(" ("),br=a("code"),Qh=l("str"),Vh=l(")\u2014 The config file to use for the default values in the launching script."),eu=d(),X=a("li"),Pr=a("code"),tu=l("-m"),ou=l(", "),Sr=a("code"),lu=l("--module"),ru=l(" ("),yr=a("code"),au=l("bool"),cu=l(") \u2014 Change each process to interpret the launch script as a Python module, executing with the same behavior as \u2018python -m\u2019."),iu=d(),Le=a("li"),Tr=a("code"),su=l("--no_python"),nu=l(" ("),Ir=a("code"),du=l("bool"),fu=l(") \u2014 Skip prepending the training script with \u2018python\u2019 - just execute it directly. Useful when the script is not a Python script."),hu=d(),Ae=a("li"),Lr=a("code"),uu=l("--debug"),_u=l(" ("),Ar=a("code"),pu=l("bool"),Eu=l(") \u2014 Whether to print out the torch.distributed stack trace when something fails."),fn=d(),Y=a("p"),vu=l("The rest of these arguments are configured through "),$r=a("code"),mu=l("accelerate config"),gu=l(" and are read in from the specified "),Nr=a("code"),Cu=l("--config_file"),Ou=l(` (or default configuration) for their
values. They can also be passed in manually.`),hn=d(),co=a("p"),Ur=a("strong"),Du=l("Hardware Selection Arguments"),wu=l(":"),un=d(),x=a("ul"),$e=a("li"),Mr=a("code"),bu=l("--cpu"),Pu=l(" ("),kr=a("code"),Su=l("bool"),yu=l(") \u2014 Whether or not to force the training on the CPU."),Tu=d(),Ne=a("li"),Gr=a("code"),Iu=l("--multi_gpu"),Lu=l(" ("),xr=a("code"),Au=l("bool"),$u=l(") \u2014 Whether or not this should launch a distributed GPU training."),Nu=d(),Ue=a("li"),Rr=a("code"),Uu=l("--mps"),Mu=l(" ("),Fr=a("code"),ku=l("bool"),Gu=l(") \u2014 Whether or not this should use MPS-enabled GPU device on MacOS machines."),xu=d(),Me=a("li"),Wr=a("code"),Ru=l("--tpu"),Fu=l(" ("),Hr=a("code"),Wu=l("bool"),Hu=l(") \u2014 Whether or not this should launch a TPU training."),_n=d(),io=a("p"),zr=a("strong"),zu=l("Resource Selection Arguments"),Bu=l(":"),pn=d(),Fo=a("p"),Ku=l("The following arguments are useful for fine-tuning how available hardware should be used"),En=d(),R=a("ul"),ke=a("li"),Br=a("code"),ju=l("--mixed_precision {no,fp16,bf16}"),Xu=l(" ("),Kr=a("code"),Yu=l("str"),Zu=l(") \u2014 Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later."),qu=d(),Ge=a("li"),jr=a("code"),Ju=l("--num_processes NUM_PROCESSES"),Qu=l(" ("),Xr=a("code"),Vu=l("int"),e_=l(") \u2014 The total number of processes to be launched in parallel."),t_=d(),xe=a("li"),Yr=a("code"),o_=l("--num_machines NUM_MACHINES"),l_=l(" ("),Zr=a("code"),r_=l("int"),a_=l(") \u2014 The total number of machines used in this training."),c_=d(),Re=a("li"),qr=a("code"),i_=l("--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS"),s_=l(" ("),Jr=a("code"),n_=l("int"),d_=l(") \u2014 The number of CPU threads per process. Can be tuned for optimal performance."),vn=d(),so=a("p"),Qr=a("strong"),f_=l("Training Paradigm Arguments"),h_=l(":"),mn=d(),Wo=a("p"),u_=l("The following arguments are useful for selecting which training paradigm to use."),gn=d(),Z=a("ul"),Fe=a("li"),Vr=a("code"),__=l("--use_deepspeed"),p_=l(" ("),ea=a("code"),E_=l("bool"),v_=l(") \u2014 Whether or not to use DeepSpeed for training."),m_=d(),We=a("li"),ta=a("code"),g_=l("--use_fsdp"),C_=l(" ("),oa=a("code"),O_=l("bool"),D_=l(") \u2014 Whether or not to use FullyShardedDataParallel for training."),w_=d(),He=a("li"),la=a("code"),b_=l("--use_megatron_lm"),P_=l(" ("),ra=a("code"),S_=l("bool"),y_=l(") \u2014 Whether or not to use Megatron-LM for training."),Cn=d(),no=a("p"),aa=a("strong"),T_=l("Distributed GPU Arguments"),I_=l(":"),On=d(),q=a("p"),L_=l("The following arguments are only useful when "),ca=a("code"),A_=l("multi_gpu"),$_=l(" is passed or multi-gpu training is configured through "),ia=a("code"),N_=l("accelerate config"),U_=l(":"),Dn=d(),p=a("ul"),ze=a("li"),sa=a("code"),M_=l("--gpu_ids"),k_=l(" ("),na=a("code"),G_=l("str"),x_=l(") \u2014 What GPUs (by id) should be used for training on this machine as a comma-seperated list"),R_=d(),Be=a("li"),da=a("code"),F_=l("--same_network"),W_=l(" ("),fa=a("code"),H_=l("bool"),z_=l(") \u2014 Whether all machines used for multinode training exist on the same local network."),B_=d(),Ke=a("li"),ha=a("code"),K_=l("--machine_rank MACHINE_RANK"),j_=l(" ("),ua=a("code"),X_=l("int"),Y_=l(") \u2014 The rank of the machine on which this script is launched."),Z_=d(),je=a("li"),_a=a("code"),q_=l("--main_process_ip MAIN_PROCESS_IP"),J_=l(" ("),pa=a("code"),Q_=l("str"),V_=l(") \u2014 The IP address of the machine of rank 0."),ep=d(),Xe=a("li"),Ea=a("code"),tp=l("--main_process_port MAIN_PROCESS_PORT"),op=l(" ("),va=a("code"),lp=l("int"),rp=l(") \u2014 The port to use to communicate with the machine of rank 0."),ap=d(),Ye=a("li"),ma=a("code"),cp=l("--rdzv_conf"),ip=l(" ("),ga=a("code"),sp=l("str"),np=l(") \u2014 Additional rendezvous configuration (<key1>=<value1>,<key2>=<value2>,\u2026)."),dp=d(),Ze=a("li"),Ca=a("code"),fp=l("--max_restarts"),hp=l(" ("),Oa=a("code"),up=l("int"),_p=l(") \u2014 Maximum number of worker group restarts before failing."),pp=d(),qe=a("li"),Da=a("code"),Ep=l("--monitor_interval"),vp=l(" ("),wa=a("code"),mp=l("float"),gp=l(") \u2014 Interval, in seconds, to monitor the state of workers."),wn=d(),fo=a("p"),ba=a("strong"),Cp=l("TPU Arguments"),Op=l(":"),bn=d(),J=a("p"),Dp=l("The following arguments are only useful when "),Pa=a("code"),wp=l("tpu"),bp=l(" is passed or TPU training is configured through "),Sa=a("code"),Pp=l("accelerate config"),Sp=l(":"),Pn=d(),Je=a("ul"),Qe=a("li"),ya=a("code"),yp=l("--main_training_function MAIN_TRAINING_FUNCTION"),Tp=l(" ("),Ta=a("code"),Ip=l("str"),Lp=l(") \u2014 The name of the main function to be executed in your script."),Ap=d(),Ve=a("li"),Ia=a("code"),$p=l("--downcast_bf16"),Np=l(" ("),La=a("code"),Up=l("bool"),Mp=l(") \u2014 Whether when using bf16 precision on TPUs if both float and double tensors are cast to bfloat16 or if double tensors remain as float32."),Sn=d(),ho=a("p"),Aa=a("strong"),kp=l("DeepSpeed Arguments"),Gp=l(":"),yn=d(),F=a("p"),xp=l("The following arguments are only useful when "),$a=a("code"),Rp=l("use_deepspeed"),Fp=l(" is passed or "),Na=a("code"),Wp=l("deepspeed"),Hp=l(" is configured through "),Ua=a("code"),zp=l("accelerate config"),Bp=l(":"),Tn=d(),u=a("ul"),et=a("li"),Ma=a("code"),Kp=l("--deepspeed_config_file"),jp=l(" ("),ka=a("code"),Xp=l("str"),Yp=l(") \u2014 DeepSpeed config file."),Zp=d(),tt=a("li"),Ga=a("code"),qp=l("--zero_stage"),Jp=l(" ("),xa=a("code"),Qp=l("int"),Vp=l(") \u2014 DeepSpeed\u2019s ZeRO optimization stage."),e1=d(),ot=a("li"),Ra=a("code"),t1=l("--offload_optimizer_device"),o1=l(" ("),Fa=a("code"),l1=l("str"),r1=l(") \u2014 Decides where (none|cpu|nvme) to offload optimizer states."),a1=d(),lt=a("li"),Wa=a("code"),c1=l("--offload_param_device"),i1=l(" ("),Ha=a("code"),s1=l("str"),n1=l(") \u2014 Decides where (none|cpu|nvme) to offload parameters."),d1=d(),rt=a("li"),za=a("code"),f1=l("--gradient_accumulation_steps"),h1=l(" ("),Ba=a("code"),u1=l("int"),_1=l(") \u2014 No of gradient_accumulation_steps used in your training script."),p1=d(),at=a("li"),Ka=a("code"),E1=l("--gradient_clipping"),v1=l(" ("),ja=a("code"),m1=l("float"),g1=l(") \u2014 Gradient clipping value used in your training script."),C1=d(),Q=a("li"),Xa=a("code"),O1=l("--zero3_init_flag"),D1=l(" ("),Ya=a("code"),w1=l("str"),b1=l(") \u2014 Decides Whether (true|false) to enable "),Za=a("code"),P1=l("deepspeed.zero.Init"),S1=l(" for constructing massive models. Only applicable with DeepSpeed ZeRO Stage-3."),y1=d(),ct=a("li"),qa=a("code"),T1=l("--zero3_save_16bit_model"),I1=l(" ("),Ja=a("code"),L1=l("str"),A1=l(") \u2014 Decides Whether (true|false) to save 16-bit model weights when using ZeRO Stage-3. Only applicable with DeepSpeed ZeRO Stage-3."),$1=d(),it=a("li"),Qa=a("code"),N1=l("--deepspeed_hostfile"),U1=l(" ("),Va=a("code"),M1=l("str"),k1=l(") \u2014 DeepSpeed hostfile for configuring multi-node compute resources."),G1=d(),st=a("li"),ec=a("code"),x1=l("--deepspeed_exclusion_filter"),R1=l(" ("),tc=a("code"),F1=l("str"),W1=l(") \u2014 DeepSpeed exclusion filter string when using mutli-node setup."),H1=d(),nt=a("li"),oc=a("code"),z1=l("--deepspeed_inclusion_filter"),B1=l(" ("),lc=a("code"),K1=l("str"),j1=l(") \u2014 DeepSpeed inclusion filter string when using mutli-node setup."),X1=d(),dt=a("li"),rc=a("code"),Y1=l("--deepspeed_multinode_launcher"),Z1=l(" ("),ac=a("code"),q1=l("str"),J1=l(") \u2014 DeepSpeed multi-node launcher to use."),In=d(),uo=a("p"),cc=a("strong"),Q1=l("Fully Sharded Data Parallelism Arguments"),V1=l(":"),Ln=d(),V=a("p"),eE=l("The following arguments are only useful when "),ic=a("code"),tE=l("use_fdsp"),oE=l(" is passed or Fully Sharded Data Parallelism is configured through "),sc=a("code"),lE=l("accelerate config"),rE=l(":"),An=d(),E=a("ul"),ft=a("li"),nc=a("code"),aE=l("--fsdp_offload_params"),cE=l(" ("),dc=a("code"),iE=l("str"),sE=l(") \u2014 Decides Whether (true|false) to offload parameters and gradients to CPU."),nE=d(),ht=a("li"),fc=a("code"),dE=l("--fsdp_min_num_params"),fE=l(" ("),hc=a("code"),hE=l("int"),uE=l(") \u2014 FSDP\u2019s minimum number of parameters for Default Auto Wrapping."),_E=d(),ut=a("li"),uc=a("code"),pE=l("--fsdp_sharding_strategy"),EE=l(" ("),_c=a("code"),vE=l("int"),mE=l(") \u2014 FSDP\u2019s Sharding Strategy."),gE=d(),_t=a("li"),pc=a("code"),CE=l("--fsdp_auto_wrap_policy"),OE=l(" ("),Ec=a("code"),DE=l("str"),wE=l(") \u2014 FSDP\u2019s auto wrap policy."),bE=d(),b=a("li"),vc=a("code"),PE=l("--fsdp_transformer_layer_cls_to_wrap"),SE=l(" ("),mc=a("code"),yE=l("str"),TE=l(") \u2014 Transformer layer class name (case-sensitive) to wrap, e.g, "),gc=a("code"),IE=l("BertLayer"),LE=l(", "),Cc=a("code"),AE=l("GPTJBlock"),$E=l(", "),Oc=a("code"),NE=l("T5Block"),UE=l(" \u2026"),ME=d(),pt=a("li"),Dc=a("code"),kE=l("--fsdp_backward_prefetch_policy"),GE=l(" ("),wc=a("code"),xE=l("str"),RE=l(") \u2014 FSDP\u2019s backward prefetch policy."),FE=d(),Et=a("li"),bc=a("code"),WE=l("--fsdp_state_dict_type"),HE=l(" ("),Pc=a("code"),zE=l("str"),BE=l(") \u2014 FSDP\u2019s state dict type."),$n=d(),_o=a("p"),Sc=a("strong"),KE=l("Megatron-LM Arguments"),jE=l(":"),Nn=d(),ee=a("p"),XE=l("The following arguments are only useful when "),yc=a("code"),YE=l("use_megatron_lm"),ZE=l(" is passed or Megatron-LM is configured through "),Tc=a("code"),qE=l("accelerate config"),JE=l(":"),Un=d(),v=a("ul"),Ho=a("li"),Ic=a("code"),QE=l("--megatron_lm_tp_degree"),VE=l(" (\u201C) \u2014 Megatron-LM\u2019s Tensor Parallelism (TP) degree."),ev=d(),zo=a("li"),Lc=a("code"),tv=l("--megatron_lm_pp_degree"),ov=l(" (\u201C) \u2014 Megatron-LM\u2019s Pipeline Parallelism (PP) degree."),lv=d(),Bo=a("li"),Ac=a("code"),rv=l("--megatron_lm_num_micro_batches"),av=l(" (\u201C) \u2014 Megatron-LM\u2019s number of micro batches when PP degree > 1."),cv=d(),Ko=a("li"),$c=a("code"),iv=l("--megatron_lm_sequence_parallelism"),sv=l(" (\u201C) \u2014 Decides Whether (true|false) to enable Sequence Parallelism when TP degree > 1."),nv=d(),jo=a("li"),Nc=a("code"),dv=l("--megatron_lm_recompute_activations"),fv=l(" (\u201C) \u2014 Decides Whether (true|false) to enable Selective Activation Recomputation."),hv=d(),Xo=a("li"),Uc=a("code"),uv=l("--megatron_lm_use_distributed_optimizer"),_v=l(" (\u201C) \u2014 Decides Whether (true|false) to use distributed optimizer which shards optimizer state and gradients across Data Pralellel (DP) ranks."),pv=d(),Yo=a("li"),Mc=a("code"),Ev=l("--megatron_lm_gradient_clipping"),vv=l(" (\u201C) \u2014 Megatron-LM\u2019s gradient clipping value based on global L2 Norm (0 to disable)."),Mn=d(),po=a("p"),kc=a("strong"),mv=l("AWS SageMaker Arguments"),gv=l(":"),kn=d(),Zo=a("p"),Cv=l("The following arguments are only useful when training in SageMaker"),Gn=d(),vt=a("ul"),mt=a("li"),Gc=a("code"),Ov=l("--aws_access_key_id AWS_ACCESS_KEY_ID"),Dv=l(" ("),xc=a("code"),wv=l("str"),bv=l(") \u2014 The AWS_ACCESS_KEY_ID used to launch the Amazon SageMaker training job"),Pv=d(),gt=a("li"),Rc=a("code"),Sv=l("--aws_secret_access_key AWS_SECRET_ACCESS_KEY"),yv=l(" ("),Fc=a("code"),Tv=l("str"),Iv=l(") \u2014 The AWS_SECRET_ACCESS_KEY used to launch the Amazon SageMaker training job"),xn=d(),he=a("h2"),Ct=a("a"),Wc=a("span"),$(Eo.$$.fragment),Lv=d(),Hc=a("span"),Av=l("accelerate tpu-config"),Rn=d(),qo=a("p"),zc=a("code"),$v=l("accelerate tpu-config"),Fn=d(),vo=a("p"),Bc=a("strong"),Nv=l("Usage"),Uv=l(":"),Wn=d(),$(mo.$$.fragment),Hn=d(),go=a("p"),Kc=a("strong"),Mv=l("Optional Arguments"),kv=l(":"),zn=d(),Jo=a("ul"),te=a("li"),jc=a("code"),Gv=l("-h"),xv=l(", "),Xc=a("code"),Rv=l("--help"),Fv=l(" ("),Yc=a("code"),Wv=l("bool"),Hv=l(") \u2014 Show a help message and exit"),Bn=d(),Co=a("p"),Zc=a("strong"),zv=l("Config Arguments"),Bv=l(":"),Kn=d(),Ot=a("p"),Kv=l("Arguments that can be configured through "),qc=a("code"),jv=l("accelerate config"),Xv=l("."),jn=d(),oe=a("ul"),Dt=a("li"),Jc=a("code"),Yv=l("--config_file"),Zv=l(" ("),Qc=a("code"),qv=l("str"),Jv=l(") \u2014 Path to the config file to use for accelerate."),Qv=d(),wt=a("li"),Vc=a("code"),Vv=l("--tpu_name"),em=l(" ("),ei=a("code"),tm=l("str"),om=l(") \u2014 The name of the TPU to use. If not specified, will use the TPU specified in the config file."),lm=d(),bt=a("li"),ti=a("code"),rm=l("--tpu_zone"),am=l(" ("),oi=a("code"),cm=l("str"),im=l(") \u2014 The zone of the TPU to use. If not specified, will use the zone specified in the config file."),Xn=d(),Oo=a("p"),li=a("strong"),sm=l("TPU Arguments"),nm=l(":"),Yn=d(),Qo=a("p"),dm=l("Arguments for options ran inside the TPU."),Zn=d(),P=a("ul"),Pt=a("li"),ri=a("code"),fm=l("--command_file"),hm=l(" ("),ai=a("code"),um=l("str"),_m=l(") \u2014 The path to the file containing the commands to run on the pod on startup."),pm=d(),St=a("li"),ci=a("code"),Em=l("--command"),vm=l(" ("),ii=a("code"),mm=l("str"),gm=l(") \u2014 A command to run on the pod. Can be passed multiple times."),Cm=d(),yt=a("li"),si=a("code"),Om=l("--install_accelerate"),Dm=l(" ("),ni=a("code"),wm=l("bool"),bm=l(") \u2014 Whether to install accelerate on the pod. Defaults to False."),Pm=d(),Tt=a("li"),di=a("code"),Sm=l("--accelerate_version"),ym=l(" ("),fi=a("code"),Tm=l("str"),Im=l(") \u2014 The version of accelerate to install on the pod. If not specified, will use the latest pypi version. Specify \u2018dev\u2019 to install from GitHub."),Lm=d(),It=a("li"),hi=a("code"),Am=l("--debug"),$m=l(" ("),ui=a("code"),Nm=l("bool"),Um=l(") \u2014 If set, will print the command that would be run instead of running it."),qn=d(),ue=a("h2"),Lt=a("a"),_i=a("span"),$(Do.$$.fragment),Mm=d(),pi=a("span"),km=l("accelerate test"),Jn=d(),At=a("p"),Ei=a("code"),Gm=l("accelerate test"),xm=l(" or "),vi=a("code"),Rm=l("accelerate-test"),Qn=d(),$t=a("p"),Fm=l("Runs "),mi=a("code"),Wm=l("accelerate/test_utils/test_script.py"),Hm=l(" to verify that \u{1F917} Accelerate has been properly configured on your system and runs."),Vn=d(),wo=a("p"),gi=a("strong"),zm=l("Usage"),Bm=l(":"),ed=d(),$(bo.$$.fragment),td=d(),Po=a("p"),Ci=a("strong"),Km=l("Optional Arguments"),jm=l(":"),od=d(),Nt=a("ul"),O=a("li"),Oi=a("code"),Xm=l("--config_file CONFIG_FILE"),Ym=l(" ("),Di=a("code"),Zm=l("str"),qm=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),wi=a("code"),Jm=l("HF_HOME"),Qm=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),bi=a("code"),Vm=l("~/.cache"),e2=l(" or the content of "),Pi=a("code"),t2=l("XDG_CACHE_HOME"),o2=l(") suffixed with "),Si=a("code"),l2=l("huggingface"),r2=l("."),a2=d(),le=a("li"),yi=a("code"),c2=l("-h"),i2=l(", "),Ti=a("code"),s2=l("--help"),n2=l(" ("),Ii=a("code"),d2=l("bool"),f2=l(") \u2014 Show a help message and exit"),this.h()},l(o){const s=x6('[data-svelte="svelte-1phssyn"]',document.head);ce=c(s,"META",{name:!0,content:!0}),s.forEach(t),Ts=f(o),ie=c(o,"H1",{class:!0});var rd=i(ie);_e=c(rd,"A",{id:!0,class:!0,href:!0});var q2=i(_e);sl=c(q2,"SPAN",{});var J2=i(sl);N(Gt.$$.fragment,J2),J2.forEach(t),q2.forEach(t),Sd=f(rd),nl=c(rd,"SPAN",{});var Q2=i(nl);yd=r(Q2,"The Command Line"),Q2.forEach(t),rd.forEach(t),Is=f(o),Mo=c(o,"P",{});var V2=i(Mo);Td=r(V2,"Below is a list of all the available commands \u{1F917} Accelerate with their parameters"),V2.forEach(t),Ls=f(o),se=c(o,"H2",{class:!0});var ad=i(se);pe=c(ad,"A",{id:!0,class:!0,href:!0});var e3=i(pe);dl=c(e3,"SPAN",{});var t3=i(dl);N(xt.$$.fragment,t3),t3.forEach(t),e3.forEach(t),Id=f(ad),fl=c(ad,"SPAN",{});var o3=i(fl);Ld=r(o3,"accelerate config"),o3.forEach(t),ad.forEach(t),As=f(o),Rt=c(o,"P",{});var h2=i(Rt);hl=c(h2,"STRONG",{});var l3=i(hl);Ad=r(l3,"Command"),l3.forEach(t),$d=r(h2,":"),h2.forEach(t),$s=f(o),Ee=c(o,"P",{});var cd=i(Ee);ul=c(cd,"CODE",{});var r3=i(ul);Nd=r(r3,"accelerate config"),r3.forEach(t),Ud=r(cd," or "),_l=c(cd,"CODE",{});var a3=i(_l);Md=r(a3,"accelerate-config"),a3.forEach(t),cd.forEach(t),Ns=f(o),ve=c(o,"P",{});var id=i(ve);kd=r(id,"Launches a series of prompts to create and save a "),pl=c(id,"CODE",{});var c3=i(pl);Gd=r(c3,"default_config.yml"),c3.forEach(t),xd=r(id,` configuration file for your training system. Should
always be ran first on your machine.`),id.forEach(t),Us=f(o),Ft=c(o,"P",{});var u2=i(Ft);El=c(u2,"STRONG",{});var i3=i(El);Rd=r(i3,"Usage"),i3.forEach(t),Fd=r(u2,":"),u2.forEach(t),Ms=f(o),N(Wt.$$.fragment,o),ks=f(o),Ht=c(o,"P",{});var _2=i(Ht);vl=c(_2,"STRONG",{});var s3=i(vl);Wd=r(s3,"Optional Arguments"),s3.forEach(t),Hd=r(_2,":"),_2.forEach(t),Gs=f(o),me=c(o,"UL",{});var sd=i(me);m=c(sd,"LI",{});var T=i(m);ml=c(T,"CODE",{});var n3=i(ml);zd=r(n3,"--config_file CONFIG_FILE"),n3.forEach(t),Bd=r(T," ("),gl=c(T,"CODE",{});var d3=i(gl);Kd=r(d3,"str"),d3.forEach(t),jd=r(T,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Cl=c(T,"CODE",{});var f3=i(Cl);Xd=r(f3,"HF_HOME"),f3.forEach(t),Yd=r(T,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),Ol=c(T,"CODE",{});var h3=i(Ol);Zd=r(h3,"~/.cache"),h3.forEach(t),qd=r(T," or the content of "),Dl=c(T,"CODE",{});var u3=i(Dl);Jd=r(u3,"XDG_CACHE_HOME"),u3.forEach(t),Qd=r(T,") suffixed with "),wl=c(T,"CODE",{});var _3=i(wl);Vd=r(_3,"huggingface"),_3.forEach(t),ef=r(T,"."),T.forEach(t),tf=f(sd),H=c(sd,"LI",{});var So=i(H);bl=c(So,"CODE",{});var p3=i(bl);of=r(p3,"-h"),p3.forEach(t),lf=r(So,", "),Pl=c(So,"CODE",{});var E3=i(Pl);rf=r(E3,"--help"),E3.forEach(t),af=r(So," ("),Sl=c(So,"CODE",{});var v3=i(Sl);cf=r(v3,"bool"),v3.forEach(t),sf=r(So,") \u2014 Show a help message and exit"),So.forEach(t),sd.forEach(t),xs=f(o),ne=c(o,"H2",{class:!0});var nd=i(ne);ge=c(nd,"A",{id:!0,class:!0,href:!0});var m3=i(ge);yl=c(m3,"SPAN",{});var g3=i(yl);N(zt.$$.fragment,g3),g3.forEach(t),m3.forEach(t),nf=f(nd),Tl=c(nd,"SPAN",{});var C3=i(Tl);df=r(C3,"accelerate config default"),C3.forEach(t),nd.forEach(t),Rs=f(o),Bt=c(o,"P",{});var p2=i(Bt);Il=c(p2,"STRONG",{});var O3=i(Il);ff=r(O3,"Command"),O3.forEach(t),hf=r(p2,":"),p2.forEach(t),Fs=f(o),Ce=c(o,"P",{});var dd=i(Ce);Ll=c(dd,"CODE",{});var D3=i(Ll);uf=r(D3,"accelerate config default"),D3.forEach(t),_f=r(dd," or "),Al=c(dd,"CODE",{});var w3=i(Al);pf=r(w3,"accelerate-config default"),w3.forEach(t),dd.forEach(t),Ws=f(o),ko=c(o,"P",{});var b3=i(ko);Ef=r(b3,"Create a default config file for Accelerate with only a few flags set."),b3.forEach(t),Hs=f(o),Kt=c(o,"P",{});var E2=i(Kt);$l=c(E2,"STRONG",{});var P3=i($l);vf=r(P3,"Usage"),P3.forEach(t),mf=r(E2,":"),E2.forEach(t),zs=f(o),N(jt.$$.fragment,o),Bs=f(o),Xt=c(o,"P",{});var v2=i(Xt);Nl=c(v2,"STRONG",{});var S3=i(Nl);gf=r(S3,"Optional Arguments"),S3.forEach(t),Cf=r(v2,":"),v2.forEach(t),Ks=f(o),z=c(o,"UL",{});var Vo=i(z);Ul=c(Vo,"LI",{});var y3=i(Ul);g=c(y3,"P",{});var I=i(g);Ml=c(I,"CODE",{});var T3=i(Ml);Of=r(T3,"--config_file CONFIG_FILE"),T3.forEach(t),Df=r(I," ("),kl=c(I,"CODE",{});var I3=i(kl);wf=r(I3,"str"),I3.forEach(t),bf=r(I,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Gl=c(I,"CODE",{});var L3=i(Gl);Pf=r(L3,"HF_HOME"),L3.forEach(t),Sf=r(I,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),xl=c(I,"CODE",{});var A3=i(xl);yf=r(A3,"~/.cache"),A3.forEach(t),Tf=r(I," or the content of "),Rl=c(I,"CODE",{});var $3=i(Rl);If=r($3,"XDG_CACHE_HOME"),$3.forEach(t),Lf=r(I,") suffixed with "),Fl=c(I,"CODE",{});var N3=i(Fl);Af=r(N3,"huggingface"),N3.forEach(t),$f=r(I,"."),I.forEach(t),y3.forEach(t),Nf=f(Vo),Wl=c(Vo,"LI",{});var U3=i(Wl);B=c(U3,"P",{});var yo=i(B);Hl=c(yo,"CODE",{});var M3=i(Hl);Uf=r(M3,"-h"),M3.forEach(t),Mf=r(yo,", "),zl=c(yo,"CODE",{});var k3=i(zl);kf=r(k3,"--help"),k3.forEach(t),Gf=r(yo," ("),Bl=c(yo,"CODE",{});var G3=i(Bl);xf=r(G3,"bool"),G3.forEach(t),Rf=r(yo,") \u2014 Show a help message and exit"),yo.forEach(t),U3.forEach(t),Ff=f(Vo),Kl=c(Vo,"LI",{});var x3=i(Kl);Oe=c(x3,"P",{});var Li=i(Oe);jl=c(Li,"CODE",{});var R3=i(jl);Wf=r(R3,"--mixed_precision {no,fp16,bf16}"),R3.forEach(t),Hf=r(Li," ("),Xl=c(Li,"CODE",{});var F3=i(Xl);zf=r(F3,"str"),F3.forEach(t),Bf=r(Li,") \u2014 Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later."),Li.forEach(t),x3.forEach(t),Vo.forEach(t),js=f(o),de=c(o,"H2",{class:!0});var fd=i(de);De=c(fd,"A",{id:!0,class:!0,href:!0});var W3=i(De);Yl=c(W3,"SPAN",{});var H3=i(Yl);N(Yt.$$.fragment,H3),H3.forEach(t),W3.forEach(t),Kf=f(fd),Zl=c(fd,"SPAN",{});var z3=i(Zl);jf=r(z3,"accelerate env"),z3.forEach(t),fd.forEach(t),Xs=f(o),Zt=c(o,"P",{});var m2=i(Zt);ql=c(m2,"STRONG",{});var B3=i(ql);Xf=r(B3,"Command"),B3.forEach(t),Yf=r(m2,":"),m2.forEach(t),Ys=f(o),we=c(o,"P",{});var hd=i(we);Jl=c(hd,"CODE",{});var K3=i(Jl);Zf=r(K3,"accelerate env"),K3.forEach(t),qf=r(hd," or "),Ql=c(hd,"CODE",{});var j3=i(Ql);Jf=r(j3,"accelerate-env"),j3.forEach(t),hd.forEach(t),Zs=f(o),be=c(o,"P",{});var ud=i(be);Qf=r(ud,"Lists the contents of the passed \u{1F917} Accelerate configuration file. Should always be used when opening an issue on the "),qt=c(ud,"A",{href:!0,rel:!0});var X3=i(qt);Vf=r(X3,"GitHub repository"),X3.forEach(t),eh=r(ud,"."),ud.forEach(t),qs=f(o),Jt=c(o,"P",{});var g2=i(Jt);Vl=c(g2,"STRONG",{});var Y3=i(Vl);th=r(Y3,"Usage"),Y3.forEach(t),oh=r(g2,":"),g2.forEach(t),Js=f(o),N(Qt.$$.fragment,o),Qs=f(o),Vt=c(o,"P",{});var C2=i(Vt);er=c(C2,"STRONG",{});var Z3=i(er);lh=r(Z3,"Optional Arguments"),Z3.forEach(t),rh=r(C2,":"),C2.forEach(t),Vs=f(o),Pe=c(o,"UL",{});var _d=i(Pe);C=c(_d,"LI",{});var L=i(C);tr=c(L,"CODE",{});var q3=i(tr);ah=r(q3,"--config_file CONFIG_FILE"),q3.forEach(t),ch=r(L," ("),or=c(L,"CODE",{});var J3=i(or);ih=r(J3,"str"),J3.forEach(t),sh=r(L,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),lr=c(L,"CODE",{});var Q3=i(lr);nh=r(Q3,"HF_HOME"),Q3.forEach(t),dh=r(L,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),rr=c(L,"CODE",{});var V3=i(rr);fh=r(V3,"~/.cache"),V3.forEach(t),hh=r(L," or the content of "),ar=c(L,"CODE",{});var e5=i(ar);uh=r(e5,"XDG_CACHE_HOME"),e5.forEach(t),_h=r(L,") suffixed with "),cr=c(L,"CODE",{});var t5=i(cr);ph=r(t5,"huggingface"),t5.forEach(t),Eh=r(L,"."),L.forEach(t),vh=f(_d),K=c(_d,"LI",{});var To=i(K);ir=c(To,"CODE",{});var o5=i(ir);mh=r(o5,"-h"),o5.forEach(t),gh=r(To,", "),sr=c(To,"CODE",{});var l5=i(sr);Ch=r(l5,"--help"),l5.forEach(t),Oh=r(To," ("),nr=c(To,"CODE",{});var r5=i(nr);Dh=r(r5,"bool"),r5.forEach(t),wh=r(To,") \u2014 Show a help message and exit"),To.forEach(t),_d.forEach(t),en=f(o),fe=c(o,"H2",{class:!0});var pd=i(fe);Se=c(pd,"A",{id:!0,class:!0,href:!0});var a5=i(Se);dr=c(a5,"SPAN",{});var c5=i(dr);N(eo.$$.fragment,c5),c5.forEach(t),a5.forEach(t),bh=f(pd),fr=c(pd,"SPAN",{});var i5=i(fr);Ph=r(i5,"accelerate launch"),i5.forEach(t),pd.forEach(t),tn=f(o),to=c(o,"P",{});var O2=i(to);hr=c(O2,"STRONG",{});var s5=i(hr);Sh=r(s5,"Command"),s5.forEach(t),yh=r(O2,":"),O2.forEach(t),on=f(o),ye=c(o,"P",{});var Ed=i(ye);ur=c(Ed,"CODE",{});var n5=i(ur);Th=r(n5,"accelerate launch"),n5.forEach(t),Ih=r(Ed," or "),_r=c(Ed,"CODE",{});var d5=i(_r);Lh=r(d5,"accelerate-launch"),d5.forEach(t),Ed.forEach(t),ln=f(o),Go=c(o,"P",{});var f5=i(Go);Ah=r(f5,"Launches a specified script on a distributed system with the right parameters."),f5.forEach(t),rn=f(o),oo=c(o,"P",{});var D2=i(oo);pr=c(D2,"STRONG",{});var h5=i(pr);$h=r(h5,"Usage"),h5.forEach(t),Nh=r(D2,":"),D2.forEach(t),an=f(o),N(lo.$$.fragment,o),cn=f(o),ro=c(o,"P",{});var w2=i(ro);Er=c(w2,"STRONG",{});var u5=i(Er);Uh=r(u5,"Positional Arguments"),u5.forEach(t),Mh=r(w2,":"),w2.forEach(t),sn=f(o),Te=c(o,"UL",{});var vd=i(Te);xo=c(vd,"LI",{});var b2=i(xo);vr=c(b2,"CODE",{});var _5=i(vr);kh=r(_5,"{training_script}"),_5.forEach(t),Gh=r(b2," \u2014 The full path to the script to be launched in parallel"),b2.forEach(t),xh=f(vd),Ro=c(vd,"LI",{});var P2=i(Ro);mr=c(P2,"CODE",{});var p5=i(mr);Rh=r(p5,"--{training_script-argument-1}"),p5.forEach(t),Fh=r(P2," \u2014 Arguments of the training script"),P2.forEach(t),vd.forEach(t),nn=f(o),ao=c(o,"P",{});var S2=i(ao);gr=c(S2,"STRONG",{});var E5=i(gr);Wh=r(E5,"Optional Arguments"),E5.forEach(t),Hh=r(S2,":"),S2.forEach(t),dn=f(o),w=c(o,"UL",{});var re=i(w);j=c(re,"LI",{});var Io=i(j);Cr=c(Io,"CODE",{});var v5=i(Cr);zh=r(v5,"-h"),v5.forEach(t),Bh=r(Io,", "),Or=c(Io,"CODE",{});var m5=i(Or);Kh=r(m5,"--help"),m5.forEach(t),jh=r(Io," ("),Dr=c(Io,"CODE",{});var g5=i(Dr);Xh=r(g5,"bool"),g5.forEach(t),Yh=r(Io,") \u2014 Show a help message and exit"),Io.forEach(t),Zh=f(re),Ie=c(re,"LI",{});var Ai=i(Ie);wr=c(Ai,"CODE",{});var C5=i(wr);qh=r(C5,"--config_file CONFIG_FILE"),C5.forEach(t),Jh=r(Ai," ("),br=c(Ai,"CODE",{});var O5=i(br);Qh=r(O5,"str"),O5.forEach(t),Vh=r(Ai,")\u2014 The config file to use for the default values in the launching script."),Ai.forEach(t),eu=f(re),X=c(re,"LI",{});var Lo=i(X);Pr=c(Lo,"CODE",{});var D5=i(Pr);tu=r(D5,"-m"),D5.forEach(t),ou=r(Lo,", "),Sr=c(Lo,"CODE",{});var w5=i(Sr);lu=r(w5,"--module"),w5.forEach(t),ru=r(Lo," ("),yr=c(Lo,"CODE",{});var b5=i(yr);au=r(b5,"bool"),b5.forEach(t),cu=r(Lo,") \u2014 Change each process to interpret the launch script as a Python module, executing with the same behavior as \u2018python -m\u2019."),Lo.forEach(t),iu=f(re),Le=c(re,"LI",{});var $i=i(Le);Tr=c($i,"CODE",{});var P5=i(Tr);su=r(P5,"--no_python"),P5.forEach(t),nu=r($i," ("),Ir=c($i,"CODE",{});var S5=i(Ir);du=r(S5,"bool"),S5.forEach(t),fu=r($i,") \u2014 Skip prepending the training script with \u2018python\u2019 - just execute it directly. Useful when the script is not a Python script."),$i.forEach(t),hu=f(re),Ae=c(re,"LI",{});var Ni=i(Ae);Lr=c(Ni,"CODE",{});var y5=i(Lr);uu=r(y5,"--debug"),y5.forEach(t),_u=r(Ni," ("),Ar=c(Ni,"CODE",{});var T5=i(Ar);pu=r(T5,"bool"),T5.forEach(t),Eu=r(Ni,") \u2014 Whether to print out the torch.distributed stack trace when something fails."),Ni.forEach(t),re.forEach(t),fn=f(o),Y=c(o,"P",{});var el=i(Y);vu=r(el,"The rest of these arguments are configured through "),$r=c(el,"CODE",{});var I5=i($r);mu=r(I5,"accelerate config"),I5.forEach(t),gu=r(el," and are read in from the specified "),Nr=c(el,"CODE",{});var L5=i(Nr);Cu=r(L5,"--config_file"),L5.forEach(t),Ou=r(el,` (or default configuration) for their
values. They can also be passed in manually.`),el.forEach(t),hn=f(o),co=c(o,"P",{});var y2=i(co);Ur=c(y2,"STRONG",{});var A5=i(Ur);Du=r(A5,"Hardware Selection Arguments"),A5.forEach(t),wu=r(y2,":"),y2.forEach(t),un=f(o),x=c(o,"UL",{});var Ut=i(x);$e=c(Ut,"LI",{});var Ui=i($e);Mr=c(Ui,"CODE",{});var $5=i(Mr);bu=r($5,"--cpu"),$5.forEach(t),Pu=r(Ui," ("),kr=c(Ui,"CODE",{});var N5=i(kr);Su=r(N5,"bool"),N5.forEach(t),yu=r(Ui,") \u2014 Whether or not to force the training on the CPU."),Ui.forEach(t),Tu=f(Ut),Ne=c(Ut,"LI",{});var Mi=i(Ne);Gr=c(Mi,"CODE",{});var U5=i(Gr);Iu=r(U5,"--multi_gpu"),U5.forEach(t),Lu=r(Mi," ("),xr=c(Mi,"CODE",{});var M5=i(xr);Au=r(M5,"bool"),M5.forEach(t),$u=r(Mi,") \u2014 Whether or not this should launch a distributed GPU training."),Mi.forEach(t),Nu=f(Ut),Ue=c(Ut,"LI",{});var ki=i(Ue);Rr=c(ki,"CODE",{});var k5=i(Rr);Uu=r(k5,"--mps"),k5.forEach(t),Mu=r(ki," ("),Fr=c(ki,"CODE",{});var G5=i(Fr);ku=r(G5,"bool"),G5.forEach(t),Gu=r(ki,") \u2014 Whether or not this should use MPS-enabled GPU device on MacOS machines."),ki.forEach(t),xu=f(Ut),Me=c(Ut,"LI",{});var Gi=i(Me);Wr=c(Gi,"CODE",{});var x5=i(Wr);Ru=r(x5,"--tpu"),x5.forEach(t),Fu=r(Gi," ("),Hr=c(Gi,"CODE",{});var R5=i(Hr);Wu=r(R5,"bool"),R5.forEach(t),Hu=r(Gi,") \u2014 Whether or not this should launch a TPU training."),Gi.forEach(t),Ut.forEach(t),_n=f(o),io=c(o,"P",{});var T2=i(io);zr=c(T2,"STRONG",{});var F5=i(zr);zu=r(F5,"Resource Selection Arguments"),F5.forEach(t),Bu=r(T2,":"),T2.forEach(t),pn=f(o),Fo=c(o,"P",{});var W5=i(Fo);Ku=r(W5,"The following arguments are useful for fine-tuning how available hardware should be used"),W5.forEach(t),En=f(o),R=c(o,"UL",{});var Mt=i(R);ke=c(Mt,"LI",{});var xi=i(ke);Br=c(xi,"CODE",{});var H5=i(Br);ju=r(H5,"--mixed_precision {no,fp16,bf16}"),H5.forEach(t),Xu=r(xi," ("),Kr=c(xi,"CODE",{});var z5=i(Kr);Yu=r(z5,"str"),z5.forEach(t),Zu=r(xi,") \u2014 Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later."),xi.forEach(t),qu=f(Mt),Ge=c(Mt,"LI",{});var Ri=i(Ge);jr=c(Ri,"CODE",{});var B5=i(jr);Ju=r(B5,"--num_processes NUM_PROCESSES"),B5.forEach(t),Qu=r(Ri," ("),Xr=c(Ri,"CODE",{});var K5=i(Xr);Vu=r(K5,"int"),K5.forEach(t),e_=r(Ri,") \u2014 The total number of processes to be launched in parallel."),Ri.forEach(t),t_=f(Mt),xe=c(Mt,"LI",{});var Fi=i(xe);Yr=c(Fi,"CODE",{});var j5=i(Yr);o_=r(j5,"--num_machines NUM_MACHINES"),j5.forEach(t),l_=r(Fi," ("),Zr=c(Fi,"CODE",{});var X5=i(Zr);r_=r(X5,"int"),X5.forEach(t),a_=r(Fi,") \u2014 The total number of machines used in this training."),Fi.forEach(t),c_=f(Mt),Re=c(Mt,"LI",{});var Wi=i(Re);qr=c(Wi,"CODE",{});var Y5=i(qr);i_=r(Y5,"--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS"),Y5.forEach(t),s_=r(Wi," ("),Jr=c(Wi,"CODE",{});var Z5=i(Jr);n_=r(Z5,"int"),Z5.forEach(t),d_=r(Wi,") \u2014 The number of CPU threads per process. Can be tuned for optimal performance."),Wi.forEach(t),Mt.forEach(t),vn=f(o),so=c(o,"P",{});var I2=i(so);Qr=c(I2,"STRONG",{});var q5=i(Qr);f_=r(q5,"Training Paradigm Arguments"),q5.forEach(t),h_=r(I2,":"),I2.forEach(t),mn=f(o),Wo=c(o,"P",{});var J5=i(Wo);u_=r(J5,"The following arguments are useful for selecting which training paradigm to use."),J5.forEach(t),gn=f(o),Z=c(o,"UL",{});var tl=i(Z);Fe=c(tl,"LI",{});var Hi=i(Fe);Vr=c(Hi,"CODE",{});var Q5=i(Vr);__=r(Q5,"--use_deepspeed"),Q5.forEach(t),p_=r(Hi," ("),ea=c(Hi,"CODE",{});var V5=i(ea);E_=r(V5,"bool"),V5.forEach(t),v_=r(Hi,") \u2014 Whether or not to use DeepSpeed for training."),Hi.forEach(t),m_=f(tl),We=c(tl,"LI",{});var zi=i(We);ta=c(zi,"CODE",{});var e4=i(ta);g_=r(e4,"--use_fsdp"),e4.forEach(t),C_=r(zi," ("),oa=c(zi,"CODE",{});var t4=i(oa);O_=r(t4,"bool"),t4.forEach(t),D_=r(zi,") \u2014 Whether or not to use FullyShardedDataParallel for training."),zi.forEach(t),w_=f(tl),He=c(tl,"LI",{});var Bi=i(He);la=c(Bi,"CODE",{});var o4=i(la);b_=r(o4,"--use_megatron_lm"),o4.forEach(t),P_=r(Bi," ("),ra=c(Bi,"CODE",{});var l4=i(ra);S_=r(l4,"bool"),l4.forEach(t),y_=r(Bi,") \u2014 Whether or not to use Megatron-LM for training."),Bi.forEach(t),tl.forEach(t),Cn=f(o),no=c(o,"P",{});var L2=i(no);aa=c(L2,"STRONG",{});var r4=i(aa);T_=r(r4,"Distributed GPU Arguments"),r4.forEach(t),I_=r(L2,":"),L2.forEach(t),On=f(o),q=c(o,"P",{});var ol=i(q);L_=r(ol,"The following arguments are only useful when "),ca=c(ol,"CODE",{});var a4=i(ca);A_=r(a4,"multi_gpu"),a4.forEach(t),$_=r(ol," is passed or multi-gpu training is configured through "),ia=c(ol,"CODE",{});var c4=i(ia);N_=r(c4,"accelerate config"),c4.forEach(t),U_=r(ol,":"),ol.forEach(t),Dn=f(o),p=c(o,"UL",{});var D=i(p);ze=c(D,"LI",{});var Ki=i(ze);sa=c(Ki,"CODE",{});var i4=i(sa);M_=r(i4,"--gpu_ids"),i4.forEach(t),k_=r(Ki," ("),na=c(Ki,"CODE",{});var s4=i(na);G_=r(s4,"str"),s4.forEach(t),x_=r(Ki,") \u2014 What GPUs (by id) should be used for training on this machine as a comma-seperated list"),Ki.forEach(t),R_=f(D),Be=c(D,"LI",{});var ji=i(Be);da=c(ji,"CODE",{});var n4=i(da);F_=r(n4,"--same_network"),n4.forEach(t),W_=r(ji," ("),fa=c(ji,"CODE",{});var d4=i(fa);H_=r(d4,"bool"),d4.forEach(t),z_=r(ji,") \u2014 Whether all machines used for multinode training exist on the same local network."),ji.forEach(t),B_=f(D),Ke=c(D,"LI",{});var Xi=i(Ke);ha=c(Xi,"CODE",{});var f4=i(ha);K_=r(f4,"--machine_rank MACHINE_RANK"),f4.forEach(t),j_=r(Xi," ("),ua=c(Xi,"CODE",{});var h4=i(ua);X_=r(h4,"int"),h4.forEach(t),Y_=r(Xi,") \u2014 The rank of the machine on which this script is launched."),Xi.forEach(t),Z_=f(D),je=c(D,"LI",{});var Yi=i(je);_a=c(Yi,"CODE",{});var u4=i(_a);q_=r(u4,"--main_process_ip MAIN_PROCESS_IP"),u4.forEach(t),J_=r(Yi," ("),pa=c(Yi,"CODE",{});var _4=i(pa);Q_=r(_4,"str"),_4.forEach(t),V_=r(Yi,") \u2014 The IP address of the machine of rank 0."),Yi.forEach(t),ep=f(D),Xe=c(D,"LI",{});var Zi=i(Xe);Ea=c(Zi,"CODE",{});var p4=i(Ea);tp=r(p4,"--main_process_port MAIN_PROCESS_PORT"),p4.forEach(t),op=r(Zi," ("),va=c(Zi,"CODE",{});var E4=i(va);lp=r(E4,"int"),E4.forEach(t),rp=r(Zi,") \u2014 The port to use to communicate with the machine of rank 0."),Zi.forEach(t),ap=f(D),Ye=c(D,"LI",{});var qi=i(Ye);ma=c(qi,"CODE",{});var v4=i(ma);cp=r(v4,"--rdzv_conf"),v4.forEach(t),ip=r(qi," ("),ga=c(qi,"CODE",{});var m4=i(ga);sp=r(m4,"str"),m4.forEach(t),np=r(qi,") \u2014 Additional rendezvous configuration (<key1>=<value1>,<key2>=<value2>,\u2026)."),qi.forEach(t),dp=f(D),Ze=c(D,"LI",{});var Ji=i(Ze);Ca=c(Ji,"CODE",{});var g4=i(Ca);fp=r(g4,"--max_restarts"),g4.forEach(t),hp=r(Ji," ("),Oa=c(Ji,"CODE",{});var C4=i(Oa);up=r(C4,"int"),C4.forEach(t),_p=r(Ji,") \u2014 Maximum number of worker group restarts before failing."),Ji.forEach(t),pp=f(D),qe=c(D,"LI",{});var Qi=i(qe);Da=c(Qi,"CODE",{});var O4=i(Da);Ep=r(O4,"--monitor_interval"),O4.forEach(t),vp=r(Qi," ("),wa=c(Qi,"CODE",{});var D4=i(wa);mp=r(D4,"float"),D4.forEach(t),gp=r(Qi,") \u2014 Interval, in seconds, to monitor the state of workers."),Qi.forEach(t),D.forEach(t),wn=f(o),fo=c(o,"P",{});var A2=i(fo);ba=c(A2,"STRONG",{});var w4=i(ba);Cp=r(w4,"TPU Arguments"),w4.forEach(t),Op=r(A2,":"),A2.forEach(t),bn=f(o),J=c(o,"P",{});var ll=i(J);Dp=r(ll,"The following arguments are only useful when "),Pa=c(ll,"CODE",{});var b4=i(Pa);wp=r(b4,"tpu"),b4.forEach(t),bp=r(ll," is passed or TPU training is configured through "),Sa=c(ll,"CODE",{});var P4=i(Sa);Pp=r(P4,"accelerate config"),P4.forEach(t),Sp=r(ll,":"),ll.forEach(t),Pn=f(o),Je=c(o,"UL",{});var md=i(Je);Qe=c(md,"LI",{});var Vi=i(Qe);ya=c(Vi,"CODE",{});var S4=i(ya);yp=r(S4,"--main_training_function MAIN_TRAINING_FUNCTION"),S4.forEach(t),Tp=r(Vi," ("),Ta=c(Vi,"CODE",{});var y4=i(Ta);Ip=r(y4,"str"),y4.forEach(t),Lp=r(Vi,") \u2014 The name of the main function to be executed in your script."),Vi.forEach(t),Ap=f(md),Ve=c(md,"LI",{});var es=i(Ve);Ia=c(es,"CODE",{});var T4=i(Ia);$p=r(T4,"--downcast_bf16"),T4.forEach(t),Np=r(es," ("),La=c(es,"CODE",{});var I4=i(La);Up=r(I4,"bool"),I4.forEach(t),Mp=r(es,") \u2014 Whether when using bf16 precision on TPUs if both float and double tensors are cast to bfloat16 or if double tensors remain as float32."),es.forEach(t),md.forEach(t),Sn=f(o),ho=c(o,"P",{});var $2=i(ho);Aa=c($2,"STRONG",{});var L4=i(Aa);kp=r(L4,"DeepSpeed Arguments"),L4.forEach(t),Gp=r($2,":"),$2.forEach(t),yn=f(o),F=c(o,"P",{});var kt=i(F);xp=r(kt,"The following arguments are only useful when "),$a=c(kt,"CODE",{});var A4=i($a);Rp=r(A4,"use_deepspeed"),A4.forEach(t),Fp=r(kt," is passed or "),Na=c(kt,"CODE",{});var $4=i(Na);Wp=r($4,"deepspeed"),$4.forEach(t),Hp=r(kt," is configured through "),Ua=c(kt,"CODE",{});var N4=i(Ua);zp=r(N4,"accelerate config"),N4.forEach(t),Bp=r(kt,":"),kt.forEach(t),Tn=f(o),u=c(o,"UL",{});var _=i(u);et=c(_,"LI",{});var ts=i(et);Ma=c(ts,"CODE",{});var U4=i(Ma);Kp=r(U4,"--deepspeed_config_file"),U4.forEach(t),jp=r(ts," ("),ka=c(ts,"CODE",{});var M4=i(ka);Xp=r(M4,"str"),M4.forEach(t),Yp=r(ts,") \u2014 DeepSpeed config file."),ts.forEach(t),Zp=f(_),tt=c(_,"LI",{});var os=i(tt);Ga=c(os,"CODE",{});var k4=i(Ga);qp=r(k4,"--zero_stage"),k4.forEach(t),Jp=r(os," ("),xa=c(os,"CODE",{});var G4=i(xa);Qp=r(G4,"int"),G4.forEach(t),Vp=r(os,") \u2014 DeepSpeed\u2019s ZeRO optimization stage."),os.forEach(t),e1=f(_),ot=c(_,"LI",{});var ls=i(ot);Ra=c(ls,"CODE",{});var x4=i(Ra);t1=r(x4,"--offload_optimizer_device"),x4.forEach(t),o1=r(ls," ("),Fa=c(ls,"CODE",{});var R4=i(Fa);l1=r(R4,"str"),R4.forEach(t),r1=r(ls,") \u2014 Decides where (none|cpu|nvme) to offload optimizer states."),ls.forEach(t),a1=f(_),lt=c(_,"LI",{});var rs=i(lt);Wa=c(rs,"CODE",{});var F4=i(Wa);c1=r(F4,"--offload_param_device"),F4.forEach(t),i1=r(rs," ("),Ha=c(rs,"CODE",{});var W4=i(Ha);s1=r(W4,"str"),W4.forEach(t),n1=r(rs,") \u2014 Decides where (none|cpu|nvme) to offload parameters."),rs.forEach(t),d1=f(_),rt=c(_,"LI",{});var as=i(rt);za=c(as,"CODE",{});var H4=i(za);f1=r(H4,"--gradient_accumulation_steps"),H4.forEach(t),h1=r(as," ("),Ba=c(as,"CODE",{});var z4=i(Ba);u1=r(z4,"int"),z4.forEach(t),_1=r(as,") \u2014 No of gradient_accumulation_steps used in your training script."),as.forEach(t),p1=f(_),at=c(_,"LI",{});var cs=i(at);Ka=c(cs,"CODE",{});var B4=i(Ka);E1=r(B4,"--gradient_clipping"),B4.forEach(t),v1=r(cs," ("),ja=c(cs,"CODE",{});var K4=i(ja);m1=r(K4,"float"),K4.forEach(t),g1=r(cs,") \u2014 Gradient clipping value used in your training script."),cs.forEach(t),C1=f(_),Q=c(_,"LI",{});var Ao=i(Q);Xa=c(Ao,"CODE",{});var j4=i(Xa);O1=r(j4,"--zero3_init_flag"),j4.forEach(t),D1=r(Ao," ("),Ya=c(Ao,"CODE",{});var X4=i(Ya);w1=r(X4,"str"),X4.forEach(t),b1=r(Ao,") \u2014 Decides Whether (true|false) to enable "),Za=c(Ao,"CODE",{});var Y4=i(Za);P1=r(Y4,"deepspeed.zero.Init"),Y4.forEach(t),S1=r(Ao," for constructing massive models. Only applicable with DeepSpeed ZeRO Stage-3."),Ao.forEach(t),y1=f(_),ct=c(_,"LI",{});var is=i(ct);qa=c(is,"CODE",{});var Z4=i(qa);T1=r(Z4,"--zero3_save_16bit_model"),Z4.forEach(t),I1=r(is," ("),Ja=c(is,"CODE",{});var q4=i(Ja);L1=r(q4,"str"),q4.forEach(t),A1=r(is,") \u2014 Decides Whether (true|false) to save 16-bit model weights when using ZeRO Stage-3. Only applicable with DeepSpeed ZeRO Stage-3."),is.forEach(t),$1=f(_),it=c(_,"LI",{});var ss=i(it);Qa=c(ss,"CODE",{});var J4=i(Qa);N1=r(J4,"--deepspeed_hostfile"),J4.forEach(t),U1=r(ss," ("),Va=c(ss,"CODE",{});var Q4=i(Va);M1=r(Q4,"str"),Q4.forEach(t),k1=r(ss,") \u2014 DeepSpeed hostfile for configuring multi-node compute resources."),ss.forEach(t),G1=f(_),st=c(_,"LI",{});var ns=i(st);ec=c(ns,"CODE",{});var V4=i(ec);x1=r(V4,"--deepspeed_exclusion_filter"),V4.forEach(t),R1=r(ns," ("),tc=c(ns,"CODE",{});var eg=i(tc);F1=r(eg,"str"),eg.forEach(t),W1=r(ns,") \u2014 DeepSpeed exclusion filter string when using mutli-node setup."),ns.forEach(t),H1=f(_),nt=c(_,"LI",{});var ds=i(nt);oc=c(ds,"CODE",{});var tg=i(oc);z1=r(tg,"--deepspeed_inclusion_filter"),tg.forEach(t),B1=r(ds," ("),lc=c(ds,"CODE",{});var og=i(lc);K1=r(og,"str"),og.forEach(t),j1=r(ds,") \u2014 DeepSpeed inclusion filter string when using mutli-node setup."),ds.forEach(t),X1=f(_),dt=c(_,"LI",{});var fs=i(dt);rc=c(fs,"CODE",{});var lg=i(rc);Y1=r(lg,"--deepspeed_multinode_launcher"),lg.forEach(t),Z1=r(fs," ("),ac=c(fs,"CODE",{});var rg=i(ac);q1=r(rg,"str"),rg.forEach(t),J1=r(fs,") \u2014 DeepSpeed multi-node launcher to use."),fs.forEach(t),_.forEach(t),In=f(o),uo=c(o,"P",{});var N2=i(uo);cc=c(N2,"STRONG",{});var ag=i(cc);Q1=r(ag,"Fully Sharded Data Parallelism Arguments"),ag.forEach(t),V1=r(N2,":"),N2.forEach(t),Ln=f(o),V=c(o,"P",{});var rl=i(V);eE=r(rl,"The following arguments are only useful when "),ic=c(rl,"CODE",{});var cg=i(ic);tE=r(cg,"use_fdsp"),cg.forEach(t),oE=r(rl," is passed or Fully Sharded Data Parallelism is configured through "),sc=c(rl,"CODE",{});var ig=i(sc);lE=r(ig,"accelerate config"),ig.forEach(t),rE=r(rl,":"),rl.forEach(t),An=f(o),E=c(o,"UL",{});var S=i(E);ft=c(S,"LI",{});var hs=i(ft);nc=c(hs,"CODE",{});var sg=i(nc);aE=r(sg,"--fsdp_offload_params"),sg.forEach(t),cE=r(hs," ("),dc=c(hs,"CODE",{});var ng=i(dc);iE=r(ng,"str"),ng.forEach(t),sE=r(hs,") \u2014 Decides Whether (true|false) to offload parameters and gradients to CPU."),hs.forEach(t),nE=f(S),ht=c(S,"LI",{});var us=i(ht);fc=c(us,"CODE",{});var dg=i(fc);dE=r(dg,"--fsdp_min_num_params"),dg.forEach(t),fE=r(us," ("),hc=c(us,"CODE",{});var fg=i(hc);hE=r(fg,"int"),fg.forEach(t),uE=r(us,") \u2014 FSDP\u2019s minimum number of parameters for Default Auto Wrapping."),us.forEach(t),_E=f(S),ut=c(S,"LI",{});var _s=i(ut);uc=c(_s,"CODE",{});var hg=i(uc);pE=r(hg,"--fsdp_sharding_strategy"),hg.forEach(t),EE=r(_s," ("),_c=c(_s,"CODE",{});var ug=i(_c);vE=r(ug,"int"),ug.forEach(t),mE=r(_s,") \u2014 FSDP\u2019s Sharding Strategy."),_s.forEach(t),gE=f(S),_t=c(S,"LI",{});var ps=i(_t);pc=c(ps,"CODE",{});var _g=i(pc);CE=r(_g,"--fsdp_auto_wrap_policy"),_g.forEach(t),OE=r(ps," ("),Ec=c(ps,"CODE",{});var pg=i(Ec);DE=r(pg,"str"),pg.forEach(t),wE=r(ps,") \u2014 FSDP\u2019s auto wrap policy."),ps.forEach(t),bE=f(S),b=c(S,"LI",{});var W=i(b);vc=c(W,"CODE",{});var Eg=i(vc);PE=r(Eg,"--fsdp_transformer_layer_cls_to_wrap"),Eg.forEach(t),SE=r(W," ("),mc=c(W,"CODE",{});var vg=i(mc);yE=r(vg,"str"),vg.forEach(t),TE=r(W,") \u2014 Transformer layer class name (case-sensitive) to wrap, e.g, "),gc=c(W,"CODE",{});var mg=i(gc);IE=r(mg,"BertLayer"),mg.forEach(t),LE=r(W,", "),Cc=c(W,"CODE",{});var gg=i(Cc);AE=r(gg,"GPTJBlock"),gg.forEach(t),$E=r(W,", "),Oc=c(W,"CODE",{});var Cg=i(Oc);NE=r(Cg,"T5Block"),Cg.forEach(t),UE=r(W," \u2026"),W.forEach(t),ME=f(S),pt=c(S,"LI",{});var Es=i(pt);Dc=c(Es,"CODE",{});var Og=i(Dc);kE=r(Og,"--fsdp_backward_prefetch_policy"),Og.forEach(t),GE=r(Es," ("),wc=c(Es,"CODE",{});var Dg=i(wc);xE=r(Dg,"str"),Dg.forEach(t),RE=r(Es,") \u2014 FSDP\u2019s backward prefetch policy."),Es.forEach(t),FE=f(S),Et=c(S,"LI",{});var vs=i(Et);bc=c(vs,"CODE",{});var wg=i(bc);WE=r(wg,"--fsdp_state_dict_type"),wg.forEach(t),HE=r(vs," ("),Pc=c(vs,"CODE",{});var bg=i(Pc);zE=r(bg,"str"),bg.forEach(t),BE=r(vs,") \u2014 FSDP\u2019s state dict type."),vs.forEach(t),S.forEach(t),$n=f(o),_o=c(o,"P",{});var U2=i(_o);Sc=c(U2,"STRONG",{});var Pg=i(Sc);KE=r(Pg,"Megatron-LM Arguments"),Pg.forEach(t),jE=r(U2,":"),U2.forEach(t),Nn=f(o),ee=c(o,"P",{});var al=i(ee);XE=r(al,"The following arguments are only useful when "),yc=c(al,"CODE",{});var Sg=i(yc);YE=r(Sg,"use_megatron_lm"),Sg.forEach(t),ZE=r(al," is passed or Megatron-LM is configured through "),Tc=c(al,"CODE",{});var yg=i(Tc);qE=r(yg,"accelerate config"),yg.forEach(t),JE=r(al,":"),al.forEach(t),Un=f(o),v=c(o,"UL",{});var y=i(v);Ho=c(y,"LI",{});var M2=i(Ho);Ic=c(M2,"CODE",{});var Tg=i(Ic);QE=r(Tg,"--megatron_lm_tp_degree"),Tg.forEach(t),VE=r(M2," (\u201C) \u2014 Megatron-LM\u2019s Tensor Parallelism (TP) degree."),M2.forEach(t),ev=f(y),zo=c(y,"LI",{});var k2=i(zo);Lc=c(k2,"CODE",{});var Ig=i(Lc);tv=r(Ig,"--megatron_lm_pp_degree"),Ig.forEach(t),ov=r(k2," (\u201C) \u2014 Megatron-LM\u2019s Pipeline Parallelism (PP) degree."),k2.forEach(t),lv=f(y),Bo=c(y,"LI",{});var G2=i(Bo);Ac=c(G2,"CODE",{});var Lg=i(Ac);rv=r(Lg,"--megatron_lm_num_micro_batches"),Lg.forEach(t),av=r(G2," (\u201C) \u2014 Megatron-LM\u2019s number of micro batches when PP degree > 1."),G2.forEach(t),cv=f(y),Ko=c(y,"LI",{});var x2=i(Ko);$c=c(x2,"CODE",{});var Ag=i($c);iv=r(Ag,"--megatron_lm_sequence_parallelism"),Ag.forEach(t),sv=r(x2," (\u201C) \u2014 Decides Whether (true|false) to enable Sequence Parallelism when TP degree > 1."),x2.forEach(t),nv=f(y),jo=c(y,"LI",{});var R2=i(jo);Nc=c(R2,"CODE",{});var $g=i(Nc);dv=r($g,"--megatron_lm_recompute_activations"),$g.forEach(t),fv=r(R2," (\u201C) \u2014 Decides Whether (true|false) to enable Selective Activation Recomputation."),R2.forEach(t),hv=f(y),Xo=c(y,"LI",{});var F2=i(Xo);Uc=c(F2,"CODE",{});var Ng=i(Uc);uv=r(Ng,"--megatron_lm_use_distributed_optimizer"),Ng.forEach(t),_v=r(F2," (\u201C) \u2014 Decides Whether (true|false) to use distributed optimizer which shards optimizer state and gradients across Data Pralellel (DP) ranks."),F2.forEach(t),pv=f(y),Yo=c(y,"LI",{});var W2=i(Yo);Mc=c(W2,"CODE",{});var Ug=i(Mc);Ev=r(Ug,"--megatron_lm_gradient_clipping"),Ug.forEach(t),vv=r(W2," (\u201C) \u2014 Megatron-LM\u2019s gradient clipping value based on global L2 Norm (0 to disable)."),W2.forEach(t),y.forEach(t),Mn=f(o),po=c(o,"P",{});var H2=i(po);kc=c(H2,"STRONG",{});var Mg=i(kc);mv=r(Mg,"AWS SageMaker Arguments"),Mg.forEach(t),gv=r(H2,":"),H2.forEach(t),kn=f(o),Zo=c(o,"P",{});var kg=i(Zo);Cv=r(kg,"The following arguments are only useful when training in SageMaker"),kg.forEach(t),Gn=f(o),vt=c(o,"UL",{});var gd=i(vt);mt=c(gd,"LI",{});var ms=i(mt);Gc=c(ms,"CODE",{});var Gg=i(Gc);Ov=r(Gg,"--aws_access_key_id AWS_ACCESS_KEY_ID"),Gg.forEach(t),Dv=r(ms," ("),xc=c(ms,"CODE",{});var xg=i(xc);wv=r(xg,"str"),xg.forEach(t),bv=r(ms,") \u2014 The AWS_ACCESS_KEY_ID used to launch the Amazon SageMaker training job"),ms.forEach(t),Pv=f(gd),gt=c(gd,"LI",{});var gs=i(gt);Rc=c(gs,"CODE",{});var Rg=i(Rc);Sv=r(Rg,"--aws_secret_access_key AWS_SECRET_ACCESS_KEY"),Rg.forEach(t),yv=r(gs," ("),Fc=c(gs,"CODE",{});var Fg=i(Fc);Tv=r(Fg,"str"),Fg.forEach(t),Iv=r(gs,") \u2014 The AWS_SECRET_ACCESS_KEY used to launch the Amazon SageMaker training job"),gs.forEach(t),gd.forEach(t),xn=f(o),he=c(o,"H2",{class:!0});var Cd=i(he);Ct=c(Cd,"A",{id:!0,class:!0,href:!0});var Wg=i(Ct);Wc=c(Wg,"SPAN",{});var Hg=i(Wc);N(Eo.$$.fragment,Hg),Hg.forEach(t),Wg.forEach(t),Lv=f(Cd),Hc=c(Cd,"SPAN",{});var zg=i(Hc);Av=r(zg,"accelerate tpu-config"),zg.forEach(t),Cd.forEach(t),Rn=f(o),qo=c(o,"P",{});var Bg=i(qo);zc=c(Bg,"CODE",{});var Kg=i(zc);$v=r(Kg,"accelerate tpu-config"),Kg.forEach(t),Bg.forEach(t),Fn=f(o),vo=c(o,"P",{});var z2=i(vo);Bc=c(z2,"STRONG",{});var jg=i(Bc);Nv=r(jg,"Usage"),jg.forEach(t),Uv=r(z2,":"),z2.forEach(t),Wn=f(o),N(mo.$$.fragment,o),Hn=f(o),go=c(o,"P",{});var B2=i(go);Kc=c(B2,"STRONG",{});var Xg=i(Kc);Mv=r(Xg,"Optional Arguments"),Xg.forEach(t),kv=r(B2,":"),B2.forEach(t),zn=f(o),Jo=c(o,"UL",{});var Yg=i(Jo);te=c(Yg,"LI",{});var $o=i(te);jc=c($o,"CODE",{});var Zg=i(jc);Gv=r(Zg,"-h"),Zg.forEach(t),xv=r($o,", "),Xc=c($o,"CODE",{});var qg=i(Xc);Rv=r(qg,"--help"),qg.forEach(t),Fv=r($o," ("),Yc=c($o,"CODE",{});var Jg=i(Yc);Wv=r(Jg,"bool"),Jg.forEach(t),Hv=r($o,") \u2014 Show a help message and exit"),$o.forEach(t),Yg.forEach(t),Bn=f(o),Co=c(o,"P",{});var K2=i(Co);Zc=c(K2,"STRONG",{});var Qg=i(Zc);zv=r(Qg,"Config Arguments"),Qg.forEach(t),Bv=r(K2,":"),K2.forEach(t),Kn=f(o),Ot=c(o,"P",{});var Od=i(Ot);Kv=r(Od,"Arguments that can be configured through "),qc=c(Od,"CODE",{});var Vg=i(qc);jv=r(Vg,"accelerate config"),Vg.forEach(t),Xv=r(Od,"."),Od.forEach(t),jn=f(o),oe=c(o,"UL",{});var cl=i(oe);Dt=c(cl,"LI",{});var Cs=i(Dt);Jc=c(Cs,"CODE",{});var e6=i(Jc);Yv=r(e6,"--config_file"),e6.forEach(t),Zv=r(Cs," ("),Qc=c(Cs,"CODE",{});var t6=i(Qc);qv=r(t6,"str"),t6.forEach(t),Jv=r(Cs,") \u2014 Path to the config file to use for accelerate."),Cs.forEach(t),Qv=f(cl),wt=c(cl,"LI",{});var Os=i(wt);Vc=c(Os,"CODE",{});var o6=i(Vc);Vv=r(o6,"--tpu_name"),o6.forEach(t),em=r(Os," ("),ei=c(Os,"CODE",{});var l6=i(ei);tm=r(l6,"str"),l6.forEach(t),om=r(Os,") \u2014 The name of the TPU to use. If not specified, will use the TPU specified in the config file."),Os.forEach(t),lm=f(cl),bt=c(cl,"LI",{});var Ds=i(bt);ti=c(Ds,"CODE",{});var r6=i(ti);rm=r(r6,"--tpu_zone"),r6.forEach(t),am=r(Ds," ("),oi=c(Ds,"CODE",{});var a6=i(oi);cm=r(a6,"str"),a6.forEach(t),im=r(Ds,") \u2014 The zone of the TPU to use. If not specified, will use the zone specified in the config file."),Ds.forEach(t),cl.forEach(t),Xn=f(o),Oo=c(o,"P",{});var j2=i(Oo);li=c(j2,"STRONG",{});var c6=i(li);sm=r(c6,"TPU Arguments"),c6.forEach(t),nm=r(j2,":"),j2.forEach(t),Yn=f(o),Qo=c(o,"P",{});var i6=i(Qo);dm=r(i6,"Arguments for options ran inside the TPU."),i6.forEach(t),Zn=f(o),P=c(o,"UL",{});var ae=i(P);Pt=c(ae,"LI",{});var ws=i(Pt);ri=c(ws,"CODE",{});var s6=i(ri);fm=r(s6,"--command_file"),s6.forEach(t),hm=r(ws," ("),ai=c(ws,"CODE",{});var n6=i(ai);um=r(n6,"str"),n6.forEach(t),_m=r(ws,") \u2014 The path to the file containing the commands to run on the pod on startup."),ws.forEach(t),pm=f(ae),St=c(ae,"LI",{});var bs=i(St);ci=c(bs,"CODE",{});var d6=i(ci);Em=r(d6,"--command"),d6.forEach(t),vm=r(bs," ("),ii=c(bs,"CODE",{});var f6=i(ii);mm=r(f6,"str"),f6.forEach(t),gm=r(bs,") \u2014 A command to run on the pod. Can be passed multiple times."),bs.forEach(t),Cm=f(ae),yt=c(ae,"LI",{});var Ps=i(yt);si=c(Ps,"CODE",{});var h6=i(si);Om=r(h6,"--install_accelerate"),h6.forEach(t),Dm=r(Ps," ("),ni=c(Ps,"CODE",{});var u6=i(ni);wm=r(u6,"bool"),u6.forEach(t),bm=r(Ps,") \u2014 Whether to install accelerate on the pod. Defaults to False."),Ps.forEach(t),Pm=f(ae),Tt=c(ae,"LI",{});var Ss=i(Tt);di=c(Ss,"CODE",{});var _6=i(di);Sm=r(_6,"--accelerate_version"),_6.forEach(t),ym=r(Ss," ("),fi=c(Ss,"CODE",{});var p6=i(fi);Tm=r(p6,"str"),p6.forEach(t),Im=r(Ss,") \u2014 The version of accelerate to install on the pod. If not specified, will use the latest pypi version. Specify \u2018dev\u2019 to install from GitHub."),Ss.forEach(t),Lm=f(ae),It=c(ae,"LI",{});var ys=i(It);hi=c(ys,"CODE",{});var E6=i(hi);Am=r(E6,"--debug"),E6.forEach(t),$m=r(ys," ("),ui=c(ys,"CODE",{});var v6=i(ui);Nm=r(v6,"bool"),v6.forEach(t),Um=r(ys,") \u2014 If set, will print the command that would be run instead of running it."),ys.forEach(t),ae.forEach(t),qn=f(o),ue=c(o,"H2",{class:!0});var Dd=i(ue);Lt=c(Dd,"A",{id:!0,class:!0,href:!0});var m6=i(Lt);_i=c(m6,"SPAN",{});var g6=i(_i);N(Do.$$.fragment,g6),g6.forEach(t),m6.forEach(t),Mm=f(Dd),pi=c(Dd,"SPAN",{});var C6=i(pi);km=r(C6,"accelerate test"),C6.forEach(t),Dd.forEach(t),Jn=f(o),At=c(o,"P",{});var wd=i(At);Ei=c(wd,"CODE",{});var O6=i(Ei);Gm=r(O6,"accelerate test"),O6.forEach(t),xm=r(wd," or "),vi=c(wd,"CODE",{});var D6=i(vi);Rm=r(D6,"accelerate-test"),D6.forEach(t),wd.forEach(t),Qn=f(o),$t=c(o,"P",{});var bd=i($t);Fm=r(bd,"Runs "),mi=c(bd,"CODE",{});var w6=i(mi);Wm=r(w6,"accelerate/test_utils/test_script.py"),w6.forEach(t),Hm=r(bd," to verify that \u{1F917} Accelerate has been properly configured on your system and runs."),bd.forEach(t),Vn=f(o),wo=c(o,"P",{});var X2=i(wo);gi=c(X2,"STRONG",{});var b6=i(gi);zm=r(b6,"Usage"),b6.forEach(t),Bm=r(X2,":"),X2.forEach(t),ed=f(o),N(bo.$$.fragment,o),td=f(o),Po=c(o,"P",{});var Y2=i(Po);Ci=c(Y2,"STRONG",{});var P6=i(Ci);Km=r(P6,"Optional Arguments"),P6.forEach(t),jm=r(Y2,":"),Y2.forEach(t),od=f(o),Nt=c(o,"UL",{});var Pd=i(Nt);O=c(Pd,"LI",{});var A=i(O);Oi=c(A,"CODE",{});var S6=i(Oi);Xm=r(S6,"--config_file CONFIG_FILE"),S6.forEach(t),Ym=r(A," ("),Di=c(A,"CODE",{});var y6=i(Di);Zm=r(y6,"str"),y6.forEach(t),qm=r(A,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),wi=c(A,"CODE",{});var T6=i(wi);Jm=r(T6,"HF_HOME"),T6.forEach(t),Qm=r(A,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),bi=c(A,"CODE",{});var I6=i(bi);Vm=r(I6,"~/.cache"),I6.forEach(t),e2=r(A," or the content of "),Pi=c(A,"CODE",{});var L6=i(Pi);t2=r(L6,"XDG_CACHE_HOME"),L6.forEach(t),o2=r(A,") suffixed with "),Si=c(A,"CODE",{});var A6=i(Si);l2=r(A6,"huggingface"),A6.forEach(t),r2=r(A,"."),A.forEach(t),a2=f(Pd),le=c(Pd,"LI",{});var No=i(le);yi=c(No,"CODE",{});var $6=i(yi);c2=r($6,"-h"),$6.forEach(t),i2=r(No,", "),Ti=c(No,"CODE",{});var N6=i(Ti);s2=r(N6,"--help"),N6.forEach(t),n2=r(No," ("),Ii=c(No,"CODE",{});var U6=i(Ii);d2=r(U6,"bool"),U6.forEach(t),f2=r(No,") \u2014 Show a help message and exit"),No.forEach(t),Pd.forEach(t),this.h()},h(){h(ce,"name","hf:doc:metadata"),h(ce,"content",JSON.stringify(H6)),h(_e,"id","the-command-line"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#the-command-line"),h(ie,"class","relative group"),h(pe,"id","accelerate-config"),h(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(pe,"href","#accelerate-config"),h(se,"class","relative group"),h(ge,"id","accelerate-config-default"),h(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ge,"href","#accelerate-config-default"),h(ne,"class","relative group"),h(De,"id","accelerate-env"),h(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(De,"href","#accelerate-env"),h(de,"class","relative group"),h(qt,"href","https://github.com/huggingface/accelerate"),h(qt,"rel","nofollow"),h(Se,"id","accelerate-launch"),h(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Se,"href","#accelerate-launch"),h(fe,"class","relative group"),h(Ct,"id","accelerate-tpuconfig"),h(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ct,"href","#accelerate-tpuconfig"),h(he,"class","relative group"),h(Lt,"id","accelerate-test"),h(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Lt,"href","#accelerate-test"),h(ue,"class","relative group")},m(o,s){e(document.head,ce),n(o,Ts,s),n(o,ie,s),e(ie,_e),e(_e,sl),U(Gt,sl,null),e(ie,Sd),e(ie,nl),e(nl,yd),n(o,Is,s),n(o,Mo,s),e(Mo,Td),n(o,Ls,s),n(o,se,s),e(se,pe),e(pe,dl),U(xt,dl,null),e(se,Id),e(se,fl),e(fl,Ld),n(o,As,s),n(o,Rt,s),e(Rt,hl),e(hl,Ad),e(Rt,$d),n(o,$s,s),n(o,Ee,s),e(Ee,ul),e(ul,Nd),e(Ee,Ud),e(Ee,_l),e(_l,Md),n(o,Ns,s),n(o,ve,s),e(ve,kd),e(ve,pl),e(pl,Gd),e(ve,xd),n(o,Us,s),n(o,Ft,s),e(Ft,El),e(El,Rd),e(Ft,Fd),n(o,Ms,s),U(Wt,o,s),n(o,ks,s),n(o,Ht,s),e(Ht,vl),e(vl,Wd),e(Ht,Hd),n(o,Gs,s),n(o,me,s),e(me,m),e(m,ml),e(ml,zd),e(m,Bd),e(m,gl),e(gl,Kd),e(m,jd),e(m,Cl),e(Cl,Xd),e(m,Yd),e(m,Ol),e(Ol,Zd),e(m,qd),e(m,Dl),e(Dl,Jd),e(m,Qd),e(m,wl),e(wl,Vd),e(m,ef),e(me,tf),e(me,H),e(H,bl),e(bl,of),e(H,lf),e(H,Pl),e(Pl,rf),e(H,af),e(H,Sl),e(Sl,cf),e(H,sf),n(o,xs,s),n(o,ne,s),e(ne,ge),e(ge,yl),U(zt,yl,null),e(ne,nf),e(ne,Tl),e(Tl,df),n(o,Rs,s),n(o,Bt,s),e(Bt,Il),e(Il,ff),e(Bt,hf),n(o,Fs,s),n(o,Ce,s),e(Ce,Ll),e(Ll,uf),e(Ce,_f),e(Ce,Al),e(Al,pf),n(o,Ws,s),n(o,ko,s),e(ko,Ef),n(o,Hs,s),n(o,Kt,s),e(Kt,$l),e($l,vf),e(Kt,mf),n(o,zs,s),U(jt,o,s),n(o,Bs,s),n(o,Xt,s),e(Xt,Nl),e(Nl,gf),e(Xt,Cf),n(o,Ks,s),n(o,z,s),e(z,Ul),e(Ul,g),e(g,Ml),e(Ml,Of),e(g,Df),e(g,kl),e(kl,wf),e(g,bf),e(g,Gl),e(Gl,Pf),e(g,Sf),e(g,xl),e(xl,yf),e(g,Tf),e(g,Rl),e(Rl,If),e(g,Lf),e(g,Fl),e(Fl,Af),e(g,$f),e(z,Nf),e(z,Wl),e(Wl,B),e(B,Hl),e(Hl,Uf),e(B,Mf),e(B,zl),e(zl,kf),e(B,Gf),e(B,Bl),e(Bl,xf),e(B,Rf),e(z,Ff),e(z,Kl),e(Kl,Oe),e(Oe,jl),e(jl,Wf),e(Oe,Hf),e(Oe,Xl),e(Xl,zf),e(Oe,Bf),n(o,js,s),n(o,de,s),e(de,De),e(De,Yl),U(Yt,Yl,null),e(de,Kf),e(de,Zl),e(Zl,jf),n(o,Xs,s),n(o,Zt,s),e(Zt,ql),e(ql,Xf),e(Zt,Yf),n(o,Ys,s),n(o,we,s),e(we,Jl),e(Jl,Zf),e(we,qf),e(we,Ql),e(Ql,Jf),n(o,Zs,s),n(o,be,s),e(be,Qf),e(be,qt),e(qt,Vf),e(be,eh),n(o,qs,s),n(o,Jt,s),e(Jt,Vl),e(Vl,th),e(Jt,oh),n(o,Js,s),U(Qt,o,s),n(o,Qs,s),n(o,Vt,s),e(Vt,er),e(er,lh),e(Vt,rh),n(o,Vs,s),n(o,Pe,s),e(Pe,C),e(C,tr),e(tr,ah),e(C,ch),e(C,or),e(or,ih),e(C,sh),e(C,lr),e(lr,nh),e(C,dh),e(C,rr),e(rr,fh),e(C,hh),e(C,ar),e(ar,uh),e(C,_h),e(C,cr),e(cr,ph),e(C,Eh),e(Pe,vh),e(Pe,K),e(K,ir),e(ir,mh),e(K,gh),e(K,sr),e(sr,Ch),e(K,Oh),e(K,nr),e(nr,Dh),e(K,wh),n(o,en,s),n(o,fe,s),e(fe,Se),e(Se,dr),U(eo,dr,null),e(fe,bh),e(fe,fr),e(fr,Ph),n(o,tn,s),n(o,to,s),e(to,hr),e(hr,Sh),e(to,yh),n(o,on,s),n(o,ye,s),e(ye,ur),e(ur,Th),e(ye,Ih),e(ye,_r),e(_r,Lh),n(o,ln,s),n(o,Go,s),e(Go,Ah),n(o,rn,s),n(o,oo,s),e(oo,pr),e(pr,$h),e(oo,Nh),n(o,an,s),U(lo,o,s),n(o,cn,s),n(o,ro,s),e(ro,Er),e(Er,Uh),e(ro,Mh),n(o,sn,s),n(o,Te,s),e(Te,xo),e(xo,vr),e(vr,kh),e(xo,Gh),e(Te,xh),e(Te,Ro),e(Ro,mr),e(mr,Rh),e(Ro,Fh),n(o,nn,s),n(o,ao,s),e(ao,gr),e(gr,Wh),e(ao,Hh),n(o,dn,s),n(o,w,s),e(w,j),e(j,Cr),e(Cr,zh),e(j,Bh),e(j,Or),e(Or,Kh),e(j,jh),e(j,Dr),e(Dr,Xh),e(j,Yh),e(w,Zh),e(w,Ie),e(Ie,wr),e(wr,qh),e(Ie,Jh),e(Ie,br),e(br,Qh),e(Ie,Vh),e(w,eu),e(w,X),e(X,Pr),e(Pr,tu),e(X,ou),e(X,Sr),e(Sr,lu),e(X,ru),e(X,yr),e(yr,au),e(X,cu),e(w,iu),e(w,Le),e(Le,Tr),e(Tr,su),e(Le,nu),e(Le,Ir),e(Ir,du),e(Le,fu),e(w,hu),e(w,Ae),e(Ae,Lr),e(Lr,uu),e(Ae,_u),e(Ae,Ar),e(Ar,pu),e(Ae,Eu),n(o,fn,s),n(o,Y,s),e(Y,vu),e(Y,$r),e($r,mu),e(Y,gu),e(Y,Nr),e(Nr,Cu),e(Y,Ou),n(o,hn,s),n(o,co,s),e(co,Ur),e(Ur,Du),e(co,wu),n(o,un,s),n(o,x,s),e(x,$e),e($e,Mr),e(Mr,bu),e($e,Pu),e($e,kr),e(kr,Su),e($e,yu),e(x,Tu),e(x,Ne),e(Ne,Gr),e(Gr,Iu),e(Ne,Lu),e(Ne,xr),e(xr,Au),e(Ne,$u),e(x,Nu),e(x,Ue),e(Ue,Rr),e(Rr,Uu),e(Ue,Mu),e(Ue,Fr),e(Fr,ku),e(Ue,Gu),e(x,xu),e(x,Me),e(Me,Wr),e(Wr,Ru),e(Me,Fu),e(Me,Hr),e(Hr,Wu),e(Me,Hu),n(o,_n,s),n(o,io,s),e(io,zr),e(zr,zu),e(io,Bu),n(o,pn,s),n(o,Fo,s),e(Fo,Ku),n(o,En,s),n(o,R,s),e(R,ke),e(ke,Br),e(Br,ju),e(ke,Xu),e(ke,Kr),e(Kr,Yu),e(ke,Zu),e(R,qu),e(R,Ge),e(Ge,jr),e(jr,Ju),e(Ge,Qu),e(Ge,Xr),e(Xr,Vu),e(Ge,e_),e(R,t_),e(R,xe),e(xe,Yr),e(Yr,o_),e(xe,l_),e(xe,Zr),e(Zr,r_),e(xe,a_),e(R,c_),e(R,Re),e(Re,qr),e(qr,i_),e(Re,s_),e(Re,Jr),e(Jr,n_),e(Re,d_),n(o,vn,s),n(o,so,s),e(so,Qr),e(Qr,f_),e(so,h_),n(o,mn,s),n(o,Wo,s),e(Wo,u_),n(o,gn,s),n(o,Z,s),e(Z,Fe),e(Fe,Vr),e(Vr,__),e(Fe,p_),e(Fe,ea),e(ea,E_),e(Fe,v_),e(Z,m_),e(Z,We),e(We,ta),e(ta,g_),e(We,C_),e(We,oa),e(oa,O_),e(We,D_),e(Z,w_),e(Z,He),e(He,la),e(la,b_),e(He,P_),e(He,ra),e(ra,S_),e(He,y_),n(o,Cn,s),n(o,no,s),e(no,aa),e(aa,T_),e(no,I_),n(o,On,s),n(o,q,s),e(q,L_),e(q,ca),e(ca,A_),e(q,$_),e(q,ia),e(ia,N_),e(q,U_),n(o,Dn,s),n(o,p,s),e(p,ze),e(ze,sa),e(sa,M_),e(ze,k_),e(ze,na),e(na,G_),e(ze,x_),e(p,R_),e(p,Be),e(Be,da),e(da,F_),e(Be,W_),e(Be,fa),e(fa,H_),e(Be,z_),e(p,B_),e(p,Ke),e(Ke,ha),e(ha,K_),e(Ke,j_),e(Ke,ua),e(ua,X_),e(Ke,Y_),e(p,Z_),e(p,je),e(je,_a),e(_a,q_),e(je,J_),e(je,pa),e(pa,Q_),e(je,V_),e(p,ep),e(p,Xe),e(Xe,Ea),e(Ea,tp),e(Xe,op),e(Xe,va),e(va,lp),e(Xe,rp),e(p,ap),e(p,Ye),e(Ye,ma),e(ma,cp),e(Ye,ip),e(Ye,ga),e(ga,sp),e(Ye,np),e(p,dp),e(p,Ze),e(Ze,Ca),e(Ca,fp),e(Ze,hp),e(Ze,Oa),e(Oa,up),e(Ze,_p),e(p,pp),e(p,qe),e(qe,Da),e(Da,Ep),e(qe,vp),e(qe,wa),e(wa,mp),e(qe,gp),n(o,wn,s),n(o,fo,s),e(fo,ba),e(ba,Cp),e(fo,Op),n(o,bn,s),n(o,J,s),e(J,Dp),e(J,Pa),e(Pa,wp),e(J,bp),e(J,Sa),e(Sa,Pp),e(J,Sp),n(o,Pn,s),n(o,Je,s),e(Je,Qe),e(Qe,ya),e(ya,yp),e(Qe,Tp),e(Qe,Ta),e(Ta,Ip),e(Qe,Lp),e(Je,Ap),e(Je,Ve),e(Ve,Ia),e(Ia,$p),e(Ve,Np),e(Ve,La),e(La,Up),e(Ve,Mp),n(o,Sn,s),n(o,ho,s),e(ho,Aa),e(Aa,kp),e(ho,Gp),n(o,yn,s),n(o,F,s),e(F,xp),e(F,$a),e($a,Rp),e(F,Fp),e(F,Na),e(Na,Wp),e(F,Hp),e(F,Ua),e(Ua,zp),e(F,Bp),n(o,Tn,s),n(o,u,s),e(u,et),e(et,Ma),e(Ma,Kp),e(et,jp),e(et,ka),e(ka,Xp),e(et,Yp),e(u,Zp),e(u,tt),e(tt,Ga),e(Ga,qp),e(tt,Jp),e(tt,xa),e(xa,Qp),e(tt,Vp),e(u,e1),e(u,ot),e(ot,Ra),e(Ra,t1),e(ot,o1),e(ot,Fa),e(Fa,l1),e(ot,r1),e(u,a1),e(u,lt),e(lt,Wa),e(Wa,c1),e(lt,i1),e(lt,Ha),e(Ha,s1),e(lt,n1),e(u,d1),e(u,rt),e(rt,za),e(za,f1),e(rt,h1),e(rt,Ba),e(Ba,u1),e(rt,_1),e(u,p1),e(u,at),e(at,Ka),e(Ka,E1),e(at,v1),e(at,ja),e(ja,m1),e(at,g1),e(u,C1),e(u,Q),e(Q,Xa),e(Xa,O1),e(Q,D1),e(Q,Ya),e(Ya,w1),e(Q,b1),e(Q,Za),e(Za,P1),e(Q,S1),e(u,y1),e(u,ct),e(ct,qa),e(qa,T1),e(ct,I1),e(ct,Ja),e(Ja,L1),e(ct,A1),e(u,$1),e(u,it),e(it,Qa),e(Qa,N1),e(it,U1),e(it,Va),e(Va,M1),e(it,k1),e(u,G1),e(u,st),e(st,ec),e(ec,x1),e(st,R1),e(st,tc),e(tc,F1),e(st,W1),e(u,H1),e(u,nt),e(nt,oc),e(oc,z1),e(nt,B1),e(nt,lc),e(lc,K1),e(nt,j1),e(u,X1),e(u,dt),e(dt,rc),e(rc,Y1),e(dt,Z1),e(dt,ac),e(ac,q1),e(dt,J1),n(o,In,s),n(o,uo,s),e(uo,cc),e(cc,Q1),e(uo,V1),n(o,Ln,s),n(o,V,s),e(V,eE),e(V,ic),e(ic,tE),e(V,oE),e(V,sc),e(sc,lE),e(V,rE),n(o,An,s),n(o,E,s),e(E,ft),e(ft,nc),e(nc,aE),e(ft,cE),e(ft,dc),e(dc,iE),e(ft,sE),e(E,nE),e(E,ht),e(ht,fc),e(fc,dE),e(ht,fE),e(ht,hc),e(hc,hE),e(ht,uE),e(E,_E),e(E,ut),e(ut,uc),e(uc,pE),e(ut,EE),e(ut,_c),e(_c,vE),e(ut,mE),e(E,gE),e(E,_t),e(_t,pc),e(pc,CE),e(_t,OE),e(_t,Ec),e(Ec,DE),e(_t,wE),e(E,bE),e(E,b),e(b,vc),e(vc,PE),e(b,SE),e(b,mc),e(mc,yE),e(b,TE),e(b,gc),e(gc,IE),e(b,LE),e(b,Cc),e(Cc,AE),e(b,$E),e(b,Oc),e(Oc,NE),e(b,UE),e(E,ME),e(E,pt),e(pt,Dc),e(Dc,kE),e(pt,GE),e(pt,wc),e(wc,xE),e(pt,RE),e(E,FE),e(E,Et),e(Et,bc),e(bc,WE),e(Et,HE),e(Et,Pc),e(Pc,zE),e(Et,BE),n(o,$n,s),n(o,_o,s),e(_o,Sc),e(Sc,KE),e(_o,jE),n(o,Nn,s),n(o,ee,s),e(ee,XE),e(ee,yc),e(yc,YE),e(ee,ZE),e(ee,Tc),e(Tc,qE),e(ee,JE),n(o,Un,s),n(o,v,s),e(v,Ho),e(Ho,Ic),e(Ic,QE),e(Ho,VE),e(v,ev),e(v,zo),e(zo,Lc),e(Lc,tv),e(zo,ov),e(v,lv),e(v,Bo),e(Bo,Ac),e(Ac,rv),e(Bo,av),e(v,cv),e(v,Ko),e(Ko,$c),e($c,iv),e(Ko,sv),e(v,nv),e(v,jo),e(jo,Nc),e(Nc,dv),e(jo,fv),e(v,hv),e(v,Xo),e(Xo,Uc),e(Uc,uv),e(Xo,_v),e(v,pv),e(v,Yo),e(Yo,Mc),e(Mc,Ev),e(Yo,vv),n(o,Mn,s),n(o,po,s),e(po,kc),e(kc,mv),e(po,gv),n(o,kn,s),n(o,Zo,s),e(Zo,Cv),n(o,Gn,s),n(o,vt,s),e(vt,mt),e(mt,Gc),e(Gc,Ov),e(mt,Dv),e(mt,xc),e(xc,wv),e(mt,bv),e(vt,Pv),e(vt,gt),e(gt,Rc),e(Rc,Sv),e(gt,yv),e(gt,Fc),e(Fc,Tv),e(gt,Iv),n(o,xn,s),n(o,he,s),e(he,Ct),e(Ct,Wc),U(Eo,Wc,null),e(he,Lv),e(he,Hc),e(Hc,Av),n(o,Rn,s),n(o,qo,s),e(qo,zc),e(zc,$v),n(o,Fn,s),n(o,vo,s),e(vo,Bc),e(Bc,Nv),e(vo,Uv),n(o,Wn,s),U(mo,o,s),n(o,Hn,s),n(o,go,s),e(go,Kc),e(Kc,Mv),e(go,kv),n(o,zn,s),n(o,Jo,s),e(Jo,te),e(te,jc),e(jc,Gv),e(te,xv),e(te,Xc),e(Xc,Rv),e(te,Fv),e(te,Yc),e(Yc,Wv),e(te,Hv),n(o,Bn,s),n(o,Co,s),e(Co,Zc),e(Zc,zv),e(Co,Bv),n(o,Kn,s),n(o,Ot,s),e(Ot,Kv),e(Ot,qc),e(qc,jv),e(Ot,Xv),n(o,jn,s),n(o,oe,s),e(oe,Dt),e(Dt,Jc),e(Jc,Yv),e(Dt,Zv),e(Dt,Qc),e(Qc,qv),e(Dt,Jv),e(oe,Qv),e(oe,wt),e(wt,Vc),e(Vc,Vv),e(wt,em),e(wt,ei),e(ei,tm),e(wt,om),e(oe,lm),e(oe,bt),e(bt,ti),e(ti,rm),e(bt,am),e(bt,oi),e(oi,cm),e(bt,im),n(o,Xn,s),n(o,Oo,s),e(Oo,li),e(li,sm),e(Oo,nm),n(o,Yn,s),n(o,Qo,s),e(Qo,dm),n(o,Zn,s),n(o,P,s),e(P,Pt),e(Pt,ri),e(ri,fm),e(Pt,hm),e(Pt,ai),e(ai,um),e(Pt,_m),e(P,pm),e(P,St),e(St,ci),e(ci,Em),e(St,vm),e(St,ii),e(ii,mm),e(St,gm),e(P,Cm),e(P,yt),e(yt,si),e(si,Om),e(yt,Dm),e(yt,ni),e(ni,wm),e(yt,bm),e(P,Pm),e(P,Tt),e(Tt,di),e(di,Sm),e(Tt,ym),e(Tt,fi),e(fi,Tm),e(Tt,Im),e(P,Lm),e(P,It),e(It,hi),e(hi,Am),e(It,$m),e(It,ui),e(ui,Nm),e(It,Um),n(o,qn,s),n(o,ue,s),e(ue,Lt),e(Lt,_i),U(Do,_i,null),e(ue,Mm),e(ue,pi),e(pi,km),n(o,Jn,s),n(o,At,s),e(At,Ei),e(Ei,Gm),e(At,xm),e(At,vi),e(vi,Rm),n(o,Qn,s),n(o,$t,s),e($t,Fm),e($t,mi),e(mi,Wm),e($t,Hm),n(o,Vn,s),n(o,wo,s),e(wo,gi),e(gi,zm),e(wo,Bm),n(o,ed,s),U(bo,o,s),n(o,td,s),n(o,Po,s),e(Po,Ci),e(Ci,Km),e(Po,jm),n(o,od,s),n(o,Nt,s),e(Nt,O),e(O,Oi),e(Oi,Xm),e(O,Ym),e(O,Di),e(Di,Zm),e(O,qm),e(O,wi),e(wi,Jm),e(O,Qm),e(O,bi),e(bi,Vm),e(O,e2),e(O,Pi),e(Pi,t2),e(O,o2),e(O,Si),e(Si,l2),e(O,r2),e(Nt,a2),e(Nt,le),e(le,yi),e(yi,c2),e(le,i2),e(le,Ti),e(Ti,s2),e(le,n2),e(le,Ii),e(Ii,d2),e(le,f2),ld=!0},p:R6,i(o){ld||(M(Gt.$$.fragment,o),M(xt.$$.fragment,o),M(Wt.$$.fragment,o),M(zt.$$.fragment,o),M(jt.$$.fragment,o),M(Yt.$$.fragment,o),M(Qt.$$.fragment,o),M(eo.$$.fragment,o),M(lo.$$.fragment,o),M(Eo.$$.fragment,o),M(mo.$$.fragment,o),M(Do.$$.fragment,o),M(bo.$$.fragment,o),ld=!0)},o(o){k(Gt.$$.fragment,o),k(xt.$$.fragment,o),k(Wt.$$.fragment,o),k(zt.$$.fragment,o),k(jt.$$.fragment,o),k(Yt.$$.fragment,o),k(Qt.$$.fragment,o),k(eo.$$.fragment,o),k(lo.$$.fragment,o),k(Eo.$$.fragment,o),k(mo.$$.fragment,o),k(Do.$$.fragment,o),k(bo.$$.fragment,o),ld=!1},d(o){t(ce),o&&t(Ts),o&&t(ie),G(Gt),o&&t(Is),o&&t(Mo),o&&t(Ls),o&&t(se),G(xt),o&&t(As),o&&t(Rt),o&&t($s),o&&t(Ee),o&&t(Ns),o&&t(ve),o&&t(Us),o&&t(Ft),o&&t(Ms),G(Wt,o),o&&t(ks),o&&t(Ht),o&&t(Gs),o&&t(me),o&&t(xs),o&&t(ne),G(zt),o&&t(Rs),o&&t(Bt),o&&t(Fs),o&&t(Ce),o&&t(Ws),o&&t(ko),o&&t(Hs),o&&t(Kt),o&&t(zs),G(jt,o),o&&t(Bs),o&&t(Xt),o&&t(Ks),o&&t(z),o&&t(js),o&&t(de),G(Yt),o&&t(Xs),o&&t(Zt),o&&t(Ys),o&&t(we),o&&t(Zs),o&&t(be),o&&t(qs),o&&t(Jt),o&&t(Js),G(Qt,o),o&&t(Qs),o&&t(Vt),o&&t(Vs),o&&t(Pe),o&&t(en),o&&t(fe),G(eo),o&&t(tn),o&&t(to),o&&t(on),o&&t(ye),o&&t(ln),o&&t(Go),o&&t(rn),o&&t(oo),o&&t(an),G(lo,o),o&&t(cn),o&&t(ro),o&&t(sn),o&&t(Te),o&&t(nn),o&&t(ao),o&&t(dn),o&&t(w),o&&t(fn),o&&t(Y),o&&t(hn),o&&t(co),o&&t(un),o&&t(x),o&&t(_n),o&&t(io),o&&t(pn),o&&t(Fo),o&&t(En),o&&t(R),o&&t(vn),o&&t(so),o&&t(mn),o&&t(Wo),o&&t(gn),o&&t(Z),o&&t(Cn),o&&t(no),o&&t(On),o&&t(q),o&&t(Dn),o&&t(p),o&&t(wn),o&&t(fo),o&&t(bn),o&&t(J),o&&t(Pn),o&&t(Je),o&&t(Sn),o&&t(ho),o&&t(yn),o&&t(F),o&&t(Tn),o&&t(u),o&&t(In),o&&t(uo),o&&t(Ln),o&&t(V),o&&t(An),o&&t(E),o&&t($n),o&&t(_o),o&&t(Nn),o&&t(ee),o&&t(Un),o&&t(v),o&&t(Mn),o&&t(po),o&&t(kn),o&&t(Zo),o&&t(Gn),o&&t(vt),o&&t(xn),o&&t(he),G(Eo),o&&t(Rn),o&&t(qo),o&&t(Fn),o&&t(vo),o&&t(Wn),G(mo,o),o&&t(Hn),o&&t(go),o&&t(zn),o&&t(Jo),o&&t(Bn),o&&t(Co),o&&t(Kn),o&&t(Ot),o&&t(jn),o&&t(oe),o&&t(Xn),o&&t(Oo),o&&t(Yn),o&&t(Qo),o&&t(Zn),o&&t(P),o&&t(qn),o&&t(ue),G(Do),o&&t(Jn),o&&t(At),o&&t(Qn),o&&t($t),o&&t(Vn),o&&t(wo),o&&t(ed),G(bo,o),o&&t(td),o&&t(Po),o&&t(od),o&&t(Nt)}}}const H6={local:"the-command-line",sections:[{local:"accelerate-config",title:"accelerate config"},{local:"accelerate-config-default",title:"accelerate config default"},{local:"accelerate-env",title:"accelerate env"},{local:"accelerate-launch",title:"accelerate launch"},{local:"accelerate-tpuconfig",title:"accelerate tpu-config"},{local:"accelerate-test",title:"accelerate test"}],title:"The Command Line "};function z6(Z2){return F6(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class X6 extends M6{constructor(ce){super();k6(this,ce,z6,W6,G6,{})}}export{X6 as default,H6 as metadata};
