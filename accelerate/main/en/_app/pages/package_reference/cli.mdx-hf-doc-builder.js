import{S as y2,i as P2,s as L2,e as a,k as d,w as Q,t as l,M as T2,c,d as t,m as f,a as i,x as V,h as r,b as u,G as e,g as n,y as ee,L as I2,q as te,o as oe,B as le,v as A2}from"../../chunks/vendor-hf-doc-builder.js";import{I as kc}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ns}from"../../chunks/CodeBlock-hf-doc-builder.js";function $2(x1){let j,Uc,Y,re,mo,ut,ds,go,fs,Rc,Jt,hs,Gc,Z,ae,Co,pt,_s,Oo,us,Wc,Et,Do,ps,Es,xc,ce,wo,vs,ms,So,gs,Fc,ie,Cs,bo,Os,Ds,Hc,vt,yo,ws,Ss,zc,mt,Bc,gt,Po,bs,ys,Kc,se,m,Lo,Ps,Ls,To,Ts,Is,Io,As,$s,Ao,Ms,Ns,$o,ks,Us,Mo,Rs,Gs,Ws,M,No,xs,Fs,ko,Hs,zs,Uo,Bs,Ks,jc,q,ne,Ro,Ct,js,Go,Ys,Yc,Ot,Wo,Zs,qs,Zc,de,xo,Xs,Js,Fo,Qs,qc,fe,Vs,Dt,en,tn,Xc,wt,Ho,on,ln,Jc,St,Qc,bt,zo,rn,an,Vc,he,g,Bo,cn,sn,Ko,nn,dn,jo,fn,hn,Yo,_n,un,Zo,pn,En,qo,vn,mn,gn,N,Xo,Cn,On,Jo,Dn,wn,Qo,Sn,bn,ei,X,_e,Vo,yt,yn,el,Pn,ti,Pt,tl,Ln,Tn,oi,ue,ol,In,An,ll,$n,li,Qt,Mn,ri,Lt,rl,Nn,kn,ai,Tt,ci,It,al,Un,Rn,ii,pe,Vt,cl,Gn,Wn,xn,eo,il,Fn,Hn,si,At,sl,zn,Bn,ni,D,k,nl,Kn,jn,dl,Yn,Zn,fl,qn,Xn,Jn,Ee,hl,Qn,Vn,_l,ed,td,od,U,ul,ld,rd,pl,ad,cd,El,id,sd,nd,ve,vl,dd,fd,ml,hd,_d,ud,me,gl,pd,Ed,Cl,vd,md,di,R,gd,Ol,Cd,Od,Dl,Dd,wd,fi,$t,wl,Sd,bd,hi,T,ge,Sl,yd,Pd,bl,Ld,Td,Id,Ce,yl,Ad,$d,Pl,Md,Nd,kd,Oe,Ll,Ud,Rd,Tl,Gd,Wd,xd,De,Il,Fd,Hd,Al,zd,Bd,_i,Mt,$l,Kd,jd,ui,to,Yd,pi,I,we,Ml,Zd,qd,Nl,Xd,Jd,Qd,Se,kl,Vd,ef,Ul,tf,of,lf,be,Rl,rf,af,Gl,cf,sf,nf,ye,Wl,df,ff,xl,hf,_f,Ei,Nt,Fl,uf,pf,vi,oo,Ef,mi,G,Pe,Hl,vf,mf,zl,gf,Cf,Of,Le,Bl,Df,wf,Kl,Sf,bf,yf,Te,jl,Pf,Lf,Yl,Tf,If,gi,kt,Zl,Af,$f,Ci,W,Mf,ql,Nf,kf,Xl,Uf,Rf,Oi,p,Ie,Jl,Gf,Wf,Ql,xf,Ff,Hf,Ae,Vl,zf,Bf,er,Kf,jf,Yf,$e,tr,Zf,qf,or,Xf,Jf,Qf,Me,lr,Vf,eh,rr,th,oh,lh,Ne,ar,rh,ah,cr,ch,ih,sh,ke,ir,nh,dh,sr,fh,hh,_h,Ue,nr,uh,ph,dr,Eh,vh,mh,Re,fr,gh,Ch,hr,Oh,Dh,Di,Ut,_r,wh,Sh,wi,x,bh,ur,yh,Ph,pr,Lh,Th,Si,Ge,We,Er,Ih,Ah,vr,$h,Mh,Nh,xe,mr,kh,Uh,gr,Rh,Gh,bi,Rt,Cr,Wh,xh,yi,A,Fh,Or,Hh,zh,Dr,Bh,Kh,wr,jh,Yh,Pi,h,Fe,Sr,Zh,qh,br,Xh,Jh,Qh,He,yr,Vh,e_,Pr,t_,o_,l_,ze,Lr,r_,a_,Tr,c_,i_,s_,Be,Ir,n_,d_,Ar,f_,h_,__,Ke,$r,u_,p_,Mr,E_,v_,m_,je,Nr,g_,C_,kr,O_,D_,w_,F,Ur,S_,b_,Rr,y_,P_,Gr,L_,T_,I_,Ye,Wr,A_,$_,xr,M_,N_,k_,Ze,Fr,U_,R_,Hr,G_,W_,x_,qe,zr,F_,H_,Br,z_,B_,K_,Xe,Kr,j_,Y_,jr,Z_,q_,X_,Je,Yr,J_,Q_,Zr,V_,eu,Li,Gt,qr,tu,ou,Ti,H,lu,Xr,ru,au,Jr,cu,iu,Ii,E,Qe,Qr,su,nu,Vr,du,fu,hu,Ve,ea,_u,uu,ta,pu,Eu,vu,et,oa,mu,gu,la,Cu,Ou,Du,tt,ra,wu,Su,aa,bu,yu,Pu,w,ca,Lu,Tu,ia,Iu,Au,sa,$u,Mu,na,Nu,ku,da,Uu,Ru,Gu,ot,fa,Wu,xu,ha,Fu,Hu,zu,lt,_a,Bu,Ku,ua,ju,Yu,Ai,Wt,pa,Zu,qu,$i,z,Xu,Ea,Ju,Qu,va,Vu,ep,Mi,v,lo,ma,tp,op,lp,ro,ga,rp,ap,cp,ao,Ca,ip,sp,np,co,Oa,dp,fp,hp,io,Da,_p,up,pp,so,wa,Ep,vp,mp,no,Sa,gp,Cp,Ni,xt,ba,Op,Dp,ki,fo,wp,Ui,rt,at,ya,Sp,bp,Pa,yp,Pp,Lp,ct,La,Tp,Ip,Ta,Ap,$p,Ri,J,it,Ia,Ft,Mp,Aa,Np,Gi,st,$a,kp,Up,Ma,Rp,Wi,nt,Gp,Na,Wp,xp,xi,Ht,ka,Fp,Hp,Fi,zt,Hi,Bt,Ua,zp,Bp,zi,dt,C,Ra,Kp,jp,Ga,Yp,Zp,Wa,qp,Xp,xa,Jp,Qp,Fa,Vp,e1,Ha,t1,o1,l1,B,za,r1,a1,Ba,c1,i1,Ka,s1,n1,Bi;return ut=new kc({}),pt=new kc({}),mt=new ns({props:{code:"accelerate config [arguments]",highlighted:"accelerate config [arguments]"}}),Ct=new kc({}),St=new ns({props:{code:"accelerate env [arguments]",highlighted:'accelerate <span class="hljs-built_in">env</span> [arguments]'}}),yt=new kc({}),Tt=new ns({props:{code:"accelerate launch [arguments] {training_script} --{training_script-argument-1} --{training_script-argument-2} ...",highlighted:"accelerate launch [arguments] {training_script} --{training_script-argument-1} --{training_script-argument-2} ..."}}),Ft=new kc({}),zt=new ns({props:{code:"accelerate test [arguments]",highlighted:'accelerate <span class="hljs-built_in">test</span> [arguments]'}}),{c(){j=a("meta"),Uc=d(),Y=a("h1"),re=a("a"),mo=a("span"),Q(ut.$$.fragment),ds=d(),go=a("span"),fs=l("The Command Line"),Rc=d(),Jt=a("p"),hs=l("Below is a list of all the available commands \u{1F917} Accelerate with their parameters"),Gc=d(),Z=a("h2"),ae=a("a"),Co=a("span"),Q(pt.$$.fragment),_s=d(),Oo=a("span"),us=l("accelerate config"),Wc=d(),Et=a("p"),Do=a("strong"),ps=l("Command"),Es=l(":"),xc=d(),ce=a("p"),wo=a("code"),vs=l("accelerate config"),ms=l(" or "),So=a("code"),gs=l("accelerate-config"),Fc=d(),ie=a("p"),Cs=l("Launches a series of prompts to create and save a "),bo=a("code"),Os=l("default_config.yml"),Ds=l(` configuration file for your training system. Should
always be ran first on your machine.`),Hc=d(),vt=a("p"),yo=a("strong"),ws=l("Usage"),Ss=l(":"),zc=d(),Q(mt.$$.fragment),Bc=d(),gt=a("p"),Po=a("strong"),bs=l("Optional Arguments"),ys=l(":"),Kc=d(),se=a("ul"),m=a("li"),Lo=a("code"),Ps=l("--config_file CONFIG_FILE"),Ls=l(" ("),To=a("code"),Ts=l("str"),Is=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Io=a("code"),As=l("HF_HOME"),$s=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),Ao=a("code"),Ms=l("~/.cache"),Ns=l(" or the content of "),$o=a("code"),ks=l("XDG_CACHE_HOME"),Us=l(") suffixed with "),Mo=a("code"),Rs=l("huggingface"),Gs=l("."),Ws=d(),M=a("li"),No=a("code"),xs=l("-h"),Fs=l(", "),ko=a("code"),Hs=l("--help"),zs=l(" ("),Uo=a("code"),Bs=l("bool"),Ks=l(") \u2014 Show a help message and exit"),jc=d(),q=a("h2"),ne=a("a"),Ro=a("span"),Q(Ct.$$.fragment),js=d(),Go=a("span"),Ys=l("accelerate env"),Yc=d(),Ot=a("p"),Wo=a("strong"),Zs=l("Command"),qs=l(":"),Zc=d(),de=a("p"),xo=a("code"),Xs=l("accelerate env"),Js=l(" or "),Fo=a("code"),Qs=l("accelerate-env"),qc=d(),fe=a("p"),Vs=l("Lists the contents of the passed \u{1F917} Accelerate configuration file. Should always be used when opening an issue on the "),Dt=a("a"),en=l("GitHub repository"),tn=l("."),Xc=d(),wt=a("p"),Ho=a("strong"),on=l("Usage"),ln=l(":"),Jc=d(),Q(St.$$.fragment),Qc=d(),bt=a("p"),zo=a("strong"),rn=l("Optional Arguments"),an=l(":"),Vc=d(),he=a("ul"),g=a("li"),Bo=a("code"),cn=l("--config_file CONFIG_FILE"),sn=l(" ("),Ko=a("code"),nn=l("str"),dn=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),jo=a("code"),fn=l("HF_HOME"),hn=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),Yo=a("code"),_n=l("~/.cache"),un=l(" or the content of "),Zo=a("code"),pn=l("XDG_CACHE_HOME"),En=l(") suffixed with "),qo=a("code"),vn=l("huggingface"),mn=l("."),gn=d(),N=a("li"),Xo=a("code"),Cn=l("-h"),On=l(", "),Jo=a("code"),Dn=l("--help"),wn=l(" ("),Qo=a("code"),Sn=l("bool"),bn=l(") \u2014 Show a help message and exit"),ei=d(),X=a("h2"),_e=a("a"),Vo=a("span"),Q(yt.$$.fragment),yn=d(),el=a("span"),Pn=l("accelerate launch"),ti=d(),Pt=a("p"),tl=a("strong"),Ln=l("Command"),Tn=l(":"),oi=d(),ue=a("p"),ol=a("code"),In=l("accelerate launch"),An=l(" or "),ll=a("code"),$n=l("accelerate-launch"),li=d(),Qt=a("p"),Mn=l("Launches a specified script on a distributed system with the right parameters."),ri=d(),Lt=a("p"),rl=a("strong"),Nn=l("Usage"),kn=l(":"),ai=d(),Q(Tt.$$.fragment),ci=d(),It=a("p"),al=a("strong"),Un=l("Positional Arguments"),Rn=l(":"),ii=d(),pe=a("ul"),Vt=a("li"),cl=a("code"),Gn=l("{training_script}"),Wn=l(" \u2014 The full path to the script to be launched in parallel"),xn=d(),eo=a("li"),il=a("code"),Fn=l("--{training_script-argument-1}"),Hn=l(" \u2014 Arguments of the training script"),si=d(),At=a("p"),sl=a("strong"),zn=l("Optional Arguments"),Bn=l(":"),ni=d(),D=a("ul"),k=a("li"),nl=a("code"),Kn=l("-h"),jn=l(", "),dl=a("code"),Yn=l("--help"),Zn=l(" ("),fl=a("code"),qn=l("bool"),Xn=l(") \u2014 Show a help message and exit"),Jn=d(),Ee=a("li"),hl=a("code"),Qn=l("--config_file CONFIG_FILE"),Vn=l(" ("),_l=a("code"),ed=l("str"),td=l(")\u2014 The config file to use for the default values in the launching script."),od=d(),U=a("li"),ul=a("code"),ld=l("-m"),rd=l(", "),pl=a("code"),ad=l("--module"),cd=l(" ("),El=a("code"),id=l("bool"),sd=l(") \u2014 Change each process to interpret the launch script as a Python module, executing with the same behavior as \u2018python -m\u2019."),nd=d(),ve=a("li"),vl=a("code"),dd=l("--no_python"),fd=l(" ("),ml=a("code"),hd=l("bool"),_d=l(") \u2014 Skip prepending the training script with \u2018python\u2019 - just execute it directly. Useful when the script is not a Python script."),ud=d(),me=a("li"),gl=a("code"),pd=l("--debug"),Ed=l(" ("),Cl=a("code"),vd=l("bool"),md=l(") \u2014 Whether to print out the torch.distributed stack trace when something fails."),di=d(),R=a("p"),gd=l("The rest of these arguments are configured through "),Ol=a("code"),Cd=l("accelerate config"),Od=l(" and are read in from the specified "),Dl=a("code"),Dd=l("--config_file"),wd=l(` (or default configuration) for their
values. They can also be passed in manually.`),fi=d(),$t=a("p"),wl=a("strong"),Sd=l("Hardware Selection Arguments"),bd=l(":"),hi=d(),T=a("ul"),ge=a("li"),Sl=a("code"),yd=l("--cpu"),Pd=l(" ("),bl=a("code"),Ld=l("bool"),Td=l(") \u2014 Whether or not to force the training on the CPU."),Id=d(),Ce=a("li"),yl=a("code"),Ad=l("--multi_gpu"),$d=l(" ("),Pl=a("code"),Md=l("bool"),Nd=l(") \u2014 Whether or not this should launch a distributed GPU training."),kd=d(),Oe=a("li"),Ll=a("code"),Ud=l("--mps"),Rd=l(" ("),Tl=a("code"),Gd=l("bool"),Wd=l(") \u2014 Whether or not this should use MPS-enabled GPU device on MacOS machines."),xd=d(),De=a("li"),Il=a("code"),Fd=l("--tpu"),Hd=l(" ("),Al=a("code"),zd=l("bool"),Bd=l(") \u2014 Whether or not this should launch a TPU training."),_i=d(),Mt=a("p"),$l=a("strong"),Kd=l("Resource Selection Arguments"),jd=l(":"),ui=d(),to=a("p"),Yd=l("The following arguments are useful for fine-tuning how available hardware should be used"),pi=d(),I=a("ul"),we=a("li"),Ml=a("code"),Zd=l("--mixed_precision {no,fp16,bf16}"),qd=l(" ("),Nl=a("code"),Xd=l("str"),Jd=l(") \u2014 Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later."),Qd=d(),Se=a("li"),kl=a("code"),Vd=l("--num_processes NUM_PROCESSES"),ef=l(" ("),Ul=a("code"),tf=l("int"),of=l(") \u2014 The total number of processes to be launched in parallel."),lf=d(),be=a("li"),Rl=a("code"),rf=l("--num_machines NUM_MACHINES"),af=l(" ("),Gl=a("code"),cf=l("int"),sf=l(") \u2014 The total number of machines used in this training."),nf=d(),ye=a("li"),Wl=a("code"),df=l("--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS"),ff=l(" ("),xl=a("code"),hf=l("int"),_f=l(") \u2014 The number of CPU threads per process. Can be tuned for optimal performance."),Ei=d(),Nt=a("p"),Fl=a("strong"),uf=l("Training Paradigm Arguments"),pf=l(":"),vi=d(),oo=a("p"),Ef=l("The following arguments are useful for selecting which training paradigm to use."),mi=d(),G=a("ul"),Pe=a("li"),Hl=a("code"),vf=l("--use_deepspeed"),mf=l(" ("),zl=a("code"),gf=l("bool"),Cf=l(") \u2014 Whether or not to use DeepSpeed for training."),Of=d(),Le=a("li"),Bl=a("code"),Df=l("--use_fsdp"),wf=l(" ("),Kl=a("code"),Sf=l("bool"),bf=l(") \u2014 Whether or not to use FullyShardedDataParallel for training."),yf=d(),Te=a("li"),jl=a("code"),Pf=l("--use_megatron_lm"),Lf=l(" ("),Yl=a("code"),Tf=l("bool"),If=l(") \u2014 Whether or not to use Megatron-LM for training."),gi=d(),kt=a("p"),Zl=a("strong"),Af=l("Distributed GPU Arguments"),$f=l(":"),Ci=d(),W=a("p"),Mf=l("The following arguments are only useful when "),ql=a("code"),Nf=l("multi_gpu"),kf=l(" is passed or multi-gpu training is configured through "),Xl=a("code"),Uf=l("accelerate config"),Rf=l(":"),Oi=d(),p=a("ul"),Ie=a("li"),Jl=a("code"),Gf=l("--gpu_ids"),Wf=l(" ("),Ql=a("code"),xf=l("str"),Ff=l(") \u2014 What GPUs (by id) should be used for training on this machine as a comma-seperated list"),Hf=d(),Ae=a("li"),Vl=a("code"),zf=l("--same_network"),Bf=l(" ("),er=a("code"),Kf=l("bool"),jf=l(") \u2014 Whether all machines used for multinode training exist on the same local network."),Yf=d(),$e=a("li"),tr=a("code"),Zf=l("--machine_rank MACHINE_RANK"),qf=l(" ("),or=a("code"),Xf=l("int"),Jf=l(") \u2014 The rank of the machine on which this script is launched."),Qf=d(),Me=a("li"),lr=a("code"),Vf=l("--main_process_ip MAIN_PROCESS_IP"),eh=l(" ("),rr=a("code"),th=l("str"),oh=l(") \u2014 The IP address of the machine of rank 0."),lh=d(),Ne=a("li"),ar=a("code"),rh=l("--main_process_port MAIN_PROCESS_PORT"),ah=l(" ("),cr=a("code"),ch=l("int"),ih=l(") \u2014 The port to use to communicate with the machine of rank 0."),sh=d(),ke=a("li"),ir=a("code"),nh=l("--rdzv_conf"),dh=l(" ("),sr=a("code"),fh=l("str"),hh=l(") \u2014 Additional rendezvous configuration (<key1>=<value1>,<key2>=<value2>,\u2026)."),_h=d(),Ue=a("li"),nr=a("code"),uh=l("--max_restarts"),ph=l(" ("),dr=a("code"),Eh=l("int"),vh=l(") \u2014 Maximum number of worker group restarts before failing."),mh=d(),Re=a("li"),fr=a("code"),gh=l("--monitor_interval"),Ch=l(" ("),hr=a("code"),Oh=l("float"),Dh=l(") \u2014 Interval, in seconds, to monitor the state of workers."),Di=d(),Ut=a("p"),_r=a("strong"),wh=l("TPU Arguments"),Sh=l(":"),wi=d(),x=a("p"),bh=l("The following arguments are only useful when "),ur=a("code"),yh=l("tpu"),Ph=l(" is passed or TPU training is configured through "),pr=a("code"),Lh=l("accelerate config"),Th=l(":"),Si=d(),Ge=a("ul"),We=a("li"),Er=a("code"),Ih=l("--main_training_function MAIN_TRAINING_FUNCTION"),Ah=l(" ("),vr=a("code"),$h=l("str"),Mh=l(") \u2014 The name of the main function to be executed in your script."),Nh=d(),xe=a("li"),mr=a("code"),kh=l("--downcast_bf16"),Uh=l(" ("),gr=a("code"),Rh=l("bool"),Gh=l(") \u2014 Whether when using bf16 precision on TPUs if both float and double tensors are cast to bfloat16 or if double tensors remain as float32."),bi=d(),Rt=a("p"),Cr=a("strong"),Wh=l("DeepSpeed Arguments"),xh=l(":"),yi=d(),A=a("p"),Fh=l("The following arguments are only useful when "),Or=a("code"),Hh=l("use_deepspeed"),zh=l(" is passed or "),Dr=a("code"),Bh=l("deepspeed"),Kh=l(" is configured through "),wr=a("code"),jh=l("accelerate config"),Yh=l(":"),Pi=d(),h=a("ul"),Fe=a("li"),Sr=a("code"),Zh=l("--deepspeed_config_file"),qh=l(" ("),br=a("code"),Xh=l("str"),Jh=l(") \u2014 DeepSpeed config file."),Qh=d(),He=a("li"),yr=a("code"),Vh=l("--zero_stage"),e_=l(" ("),Pr=a("code"),t_=l("int"),o_=l(") \u2014 DeepSpeed\u2019s ZeRO optimization stage."),l_=d(),ze=a("li"),Lr=a("code"),r_=l("--offload_optimizer_device"),a_=l(" ("),Tr=a("code"),c_=l("str"),i_=l(") \u2014 Decides where (none|cpu|nvme) to offload optimizer states."),s_=d(),Be=a("li"),Ir=a("code"),n_=l("--offload_param_device"),d_=l(" ("),Ar=a("code"),f_=l("str"),h_=l(") \u2014 Decides where (none|cpu|nvme) to offload parameters."),__=d(),Ke=a("li"),$r=a("code"),u_=l("--gradient_accumulation_steps"),p_=l(" ("),Mr=a("code"),E_=l("int"),v_=l(") \u2014 No of gradient_accumulation_steps used in your training script."),m_=d(),je=a("li"),Nr=a("code"),g_=l("--gradient_clipping"),C_=l(" ("),kr=a("code"),O_=l("float"),D_=l(") \u2014 Gradient clipping value used in your training script."),w_=d(),F=a("li"),Ur=a("code"),S_=l("--zero3_init_flag"),b_=l(" ("),Rr=a("code"),y_=l("str"),P_=l(") \u2014 Decides Whether (true|false) to enable "),Gr=a("code"),L_=l("deepspeed.zero.Init"),T_=l(" for constructing massive models. Only applicable with DeepSpeed ZeRO Stage-3."),I_=d(),Ye=a("li"),Wr=a("code"),A_=l("--zero3_save_16bit_model"),$_=l(" ("),xr=a("code"),M_=l("str"),N_=l(") \u2014 Decides Whether (true|false) to save 16-bit model weights when using ZeRO Stage-3. Only applicable with DeepSpeed ZeRO Stage-3."),k_=d(),Ze=a("li"),Fr=a("code"),U_=l("--deepspeed_hostfile"),R_=l(" ("),Hr=a("code"),G_=l("str"),W_=l(") \u2014 DeepSpeed hostfile for configuring multi-node compute resources."),x_=d(),qe=a("li"),zr=a("code"),F_=l("--deepspeed_exclusion_filter"),H_=l(" ("),Br=a("code"),z_=l("str"),B_=l(") \u2014 DeepSpeed exclusion filter string when using mutli-node setup."),K_=d(),Xe=a("li"),Kr=a("code"),j_=l("--deepspeed_inclusion_filter"),Y_=l(" ("),jr=a("code"),Z_=l("str"),q_=l(") \u2014 DeepSpeed inclusion filter string when using mutli-node setup."),X_=d(),Je=a("li"),Yr=a("code"),J_=l("--deepspeed_multinode_launcher"),Q_=l(" ("),Zr=a("code"),V_=l("str"),eu=l(") \u2014 DeepSpeed multi-node launcher to use."),Li=d(),Gt=a("p"),qr=a("strong"),tu=l("Fully Sharded Data Parallelism Arguments"),ou=l(":"),Ti=d(),H=a("p"),lu=l("The following arguments are only useful when "),Xr=a("code"),ru=l("use_fdsp"),au=l(" is passed or Fully Sharded Data Parallelism is configured through "),Jr=a("code"),cu=l("accelerate config"),iu=l(":"),Ii=d(),E=a("ul"),Qe=a("li"),Qr=a("code"),su=l("--fsdp_offload_params"),nu=l(" ("),Vr=a("code"),du=l("str"),fu=l(") \u2014 Decides Whether (true|false) to offload parameters and gradients to CPU."),hu=d(),Ve=a("li"),ea=a("code"),_u=l("--fsdp_min_num_params"),uu=l(" ("),ta=a("code"),pu=l("int"),Eu=l(") \u2014 FSDP\u2019s minimum number of parameters for Default Auto Wrapping."),vu=d(),et=a("li"),oa=a("code"),mu=l("--fsdp_sharding_strategy"),gu=l(" ("),la=a("code"),Cu=l("int"),Ou=l(") \u2014 FSDP\u2019s Sharding Strategy."),Du=d(),tt=a("li"),ra=a("code"),wu=l("--fsdp_auto_wrap_policy"),Su=l(" ("),aa=a("code"),bu=l("str"),yu=l(") \u2014 FSDP\u2019s auto wrap policy."),Pu=d(),w=a("li"),ca=a("code"),Lu=l("--fsdp_transformer_layer_cls_to_wrap"),Tu=l(" ("),ia=a("code"),Iu=l("str"),Au=l(") \u2014 Transformer layer class name (case-sensitive) to wrap, e.g, "),sa=a("code"),$u=l("BertLayer"),Mu=l(", "),na=a("code"),Nu=l("GPTJBlock"),ku=l(", "),da=a("code"),Uu=l("T5Block"),Ru=l(" \u2026"),Gu=d(),ot=a("li"),fa=a("code"),Wu=l("--fsdp_backward_prefetch_policy"),xu=l(" ("),ha=a("code"),Fu=l("str"),Hu=l(") \u2014 FSDP\u2019s backward prefetch policy."),zu=d(),lt=a("li"),_a=a("code"),Bu=l("--fsdp_state_dict_type"),Ku=l(" ("),ua=a("code"),ju=l("str"),Yu=l(") \u2014 FSDP\u2019s state dict type."),Ai=d(),Wt=a("p"),pa=a("strong"),Zu=l("Megatron-LM Arguments"),qu=l(":"),$i=d(),z=a("p"),Xu=l("The following arguments are only useful when "),Ea=a("code"),Ju=l("use_megatron_lm"),Qu=l(" is passed or Megatron-LM is configured through "),va=a("code"),Vu=l("accelerate config"),ep=l(":"),Mi=d(),v=a("ul"),lo=a("li"),ma=a("code"),tp=l("--megatron_lm_tp_degree"),op=l(" (\u201C) \u2014 Megatron-LM\u2019s Tensor Parallelism (TP) degree."),lp=d(),ro=a("li"),ga=a("code"),rp=l("--megatron_lm_pp_degree"),ap=l(" (\u201C) \u2014 Megatron-LM\u2019s Pipeline Parallelism (PP) degree."),cp=d(),ao=a("li"),Ca=a("code"),ip=l("--megatron_lm_num_micro_batches"),sp=l(" (\u201C) \u2014 Megatron-LM\u2019s number of micro batches when PP degree > 1."),np=d(),co=a("li"),Oa=a("code"),dp=l("--megatron_lm_sequence_parallelism"),fp=l(" (\u201C) \u2014 Decides Whether (true|false) to enable Sequence Parallelism when TP degree > 1."),hp=d(),io=a("li"),Da=a("code"),_p=l("--megatron_lm_recompute_activations"),up=l(" (\u201C) \u2014 Decides Whether (true|false) to enable Selective Activation Recomputation."),pp=d(),so=a("li"),wa=a("code"),Ep=l("--megatron_lm_use_distributed_optimizer"),vp=l(" (\u201C) \u2014 Decides Whether (true|false) to use distributed optimizer which shards optimizer state and gradients across Data Pralellel (DP) ranks."),mp=d(),no=a("li"),Sa=a("code"),gp=l("--megatron_lm_gradient_clipping"),Cp=l(" (\u201C) \u2014 Megatron-LM\u2019s gradient clipping value based on global L2 Norm (0 to disable)."),Ni=d(),xt=a("p"),ba=a("strong"),Op=l("AWS SageMaker Arguments"),Dp=l(":"),ki=d(),fo=a("p"),wp=l("The following arguments are only useful when training in SageMaker"),Ui=d(),rt=a("ul"),at=a("li"),ya=a("code"),Sp=l("--aws_access_key_id AWS_ACCESS_KEY_ID"),bp=l(" ("),Pa=a("code"),yp=l("str"),Pp=l(") \u2014 The AWS_ACCESS_KEY_ID used to launch the Amazon SageMaker training job"),Lp=d(),ct=a("li"),La=a("code"),Tp=l("--aws_secret_access_key AWS_SECRET_ACCESS_KEY"),Ip=l(" ("),Ta=a("code"),Ap=l("str"),$p=l(") \u2014 The AWS_SECRET_ACCESS_KEY used to launch the Amazon SageMaker training job"),Ri=d(),J=a("h2"),it=a("a"),Ia=a("span"),Q(Ft.$$.fragment),Mp=d(),Aa=a("span"),Np=l("accelerate test"),Gi=d(),st=a("p"),$a=a("code"),kp=l("accelerate test"),Up=l(" or "),Ma=a("code"),Rp=l("accelerate-test"),Wi=d(),nt=a("p"),Gp=l("Runs "),Na=a("code"),Wp=l("accelerate/test_utils/test_script.py"),xp=l(" to verify that \u{1F917} Accelerate has been properly configured on your system and runs."),xi=d(),Ht=a("p"),ka=a("strong"),Fp=l("Usage"),Hp=l(":"),Fi=d(),Q(zt.$$.fragment),Hi=d(),Bt=a("p"),Ua=a("strong"),zp=l("Optional Arguments"),Bp=l(":"),zi=d(),dt=a("ul"),C=a("li"),Ra=a("code"),Kp=l("--config_file CONFIG_FILE"),jp=l(" ("),Ga=a("code"),Yp=l("str"),Zp=l(`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Wa=a("code"),qp=l("HF_HOME"),Xp=l(` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),xa=a("code"),Jp=l("~/.cache"),Qp=l(" or the content of "),Fa=a("code"),Vp=l("XDG_CACHE_HOME"),e1=l(") suffixed with "),Ha=a("code"),t1=l("huggingface"),o1=l("."),l1=d(),B=a("li"),za=a("code"),r1=l("-h"),a1=l(", "),Ba=a("code"),c1=l("--help"),i1=l(" ("),Ka=a("code"),s1=l("bool"),n1=l(") \u2014 Show a help message and exit"),this.h()},l(o){const s=T2('[data-svelte="svelte-1phssyn"]',document.head);j=c(s,"META",{name:!0,content:!0}),s.forEach(t),Uc=f(o),Y=c(o,"H1",{class:!0});var Ki=i(Y);re=c(Ki,"A",{id:!0,class:!0,href:!0});var F1=i(re);mo=c(F1,"SPAN",{});var H1=i(mo);V(ut.$$.fragment,H1),H1.forEach(t),F1.forEach(t),ds=f(Ki),go=c(Ki,"SPAN",{});var z1=i(go);fs=r(z1,"The Command Line"),z1.forEach(t),Ki.forEach(t),Rc=f(o),Jt=c(o,"P",{});var B1=i(Jt);hs=r(B1,"Below is a list of all the available commands \u{1F917} Accelerate with their parameters"),B1.forEach(t),Gc=f(o),Z=c(o,"H2",{class:!0});var ji=i(Z);ae=c(ji,"A",{id:!0,class:!0,href:!0});var K1=i(ae);Co=c(K1,"SPAN",{});var j1=i(Co);V(pt.$$.fragment,j1),j1.forEach(t),K1.forEach(t),_s=f(ji),Oo=c(ji,"SPAN",{});var Y1=i(Oo);us=r(Y1,"accelerate config"),Y1.forEach(t),ji.forEach(t),Wc=f(o),Et=c(o,"P",{});var d1=i(Et);Do=c(d1,"STRONG",{});var Z1=i(Do);ps=r(Z1,"Command"),Z1.forEach(t),Es=r(d1,":"),d1.forEach(t),xc=f(o),ce=c(o,"P",{});var Yi=i(ce);wo=c(Yi,"CODE",{});var q1=i(wo);vs=r(q1,"accelerate config"),q1.forEach(t),ms=r(Yi," or "),So=c(Yi,"CODE",{});var X1=i(So);gs=r(X1,"accelerate-config"),X1.forEach(t),Yi.forEach(t),Fc=f(o),ie=c(o,"P",{});var Zi=i(ie);Cs=r(Zi,"Launches a series of prompts to create and save a "),bo=c(Zi,"CODE",{});var J1=i(bo);Os=r(J1,"default_config.yml"),J1.forEach(t),Ds=r(Zi,` configuration file for your training system. Should
always be ran first on your machine.`),Zi.forEach(t),Hc=f(o),vt=c(o,"P",{});var f1=i(vt);yo=c(f1,"STRONG",{});var Q1=i(yo);ws=r(Q1,"Usage"),Q1.forEach(t),Ss=r(f1,":"),f1.forEach(t),zc=f(o),V(mt.$$.fragment,o),Bc=f(o),gt=c(o,"P",{});var h1=i(gt);Po=c(h1,"STRONG",{});var V1=i(Po);bs=r(V1,"Optional Arguments"),V1.forEach(t),ys=r(h1,":"),h1.forEach(t),Kc=f(o),se=c(o,"UL",{});var qi=i(se);m=c(qi,"LI",{});var y=i(m);Lo=c(y,"CODE",{});var eE=i(Lo);Ps=r(eE,"--config_file CONFIG_FILE"),eE.forEach(t),Ls=r(y," ("),To=c(y,"CODE",{});var tE=i(To);Ts=r(tE,"str"),tE.forEach(t),Is=r(y,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Io=c(y,"CODE",{});var oE=i(Io);As=r(oE,"HF_HOME"),oE.forEach(t),$s=r(y,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),Ao=c(y,"CODE",{});var lE=i(Ao);Ms=r(lE,"~/.cache"),lE.forEach(t),Ns=r(y," or the content of "),$o=c(y,"CODE",{});var rE=i($o);ks=r(rE,"XDG_CACHE_HOME"),rE.forEach(t),Us=r(y,") suffixed with "),Mo=c(y,"CODE",{});var aE=i(Mo);Rs=r(aE,"huggingface"),aE.forEach(t),Gs=r(y,"."),y.forEach(t),Ws=f(qi),M=c(qi,"LI",{});var Kt=i(M);No=c(Kt,"CODE",{});var cE=i(No);xs=r(cE,"-h"),cE.forEach(t),Fs=r(Kt,", "),ko=c(Kt,"CODE",{});var iE=i(ko);Hs=r(iE,"--help"),iE.forEach(t),zs=r(Kt," ("),Uo=c(Kt,"CODE",{});var sE=i(Uo);Bs=r(sE,"bool"),sE.forEach(t),Ks=r(Kt,") \u2014 Show a help message and exit"),Kt.forEach(t),qi.forEach(t),jc=f(o),q=c(o,"H2",{class:!0});var Xi=i(q);ne=c(Xi,"A",{id:!0,class:!0,href:!0});var nE=i(ne);Ro=c(nE,"SPAN",{});var dE=i(Ro);V(Ct.$$.fragment,dE),dE.forEach(t),nE.forEach(t),js=f(Xi),Go=c(Xi,"SPAN",{});var fE=i(Go);Ys=r(fE,"accelerate env"),fE.forEach(t),Xi.forEach(t),Yc=f(o),Ot=c(o,"P",{});var _1=i(Ot);Wo=c(_1,"STRONG",{});var hE=i(Wo);Zs=r(hE,"Command"),hE.forEach(t),qs=r(_1,":"),_1.forEach(t),Zc=f(o),de=c(o,"P",{});var Ji=i(de);xo=c(Ji,"CODE",{});var _E=i(xo);Xs=r(_E,"accelerate env"),_E.forEach(t),Js=r(Ji," or "),Fo=c(Ji,"CODE",{});var uE=i(Fo);Qs=r(uE,"accelerate-env"),uE.forEach(t),Ji.forEach(t),qc=f(o),fe=c(o,"P",{});var Qi=i(fe);Vs=r(Qi,"Lists the contents of the passed \u{1F917} Accelerate configuration file. Should always be used when opening an issue on the "),Dt=c(Qi,"A",{href:!0,rel:!0});var pE=i(Dt);en=r(pE,"GitHub repository"),pE.forEach(t),tn=r(Qi,"."),Qi.forEach(t),Xc=f(o),wt=c(o,"P",{});var u1=i(wt);Ho=c(u1,"STRONG",{});var EE=i(Ho);on=r(EE,"Usage"),EE.forEach(t),ln=r(u1,":"),u1.forEach(t),Jc=f(o),V(St.$$.fragment,o),Qc=f(o),bt=c(o,"P",{});var p1=i(bt);zo=c(p1,"STRONG",{});var vE=i(zo);rn=r(vE,"Optional Arguments"),vE.forEach(t),an=r(p1,":"),p1.forEach(t),Vc=f(o),he=c(o,"UL",{});var Vi=i(he);g=c(Vi,"LI",{});var P=i(g);Bo=c(P,"CODE",{});var mE=i(Bo);cn=r(mE,"--config_file CONFIG_FILE"),mE.forEach(t),sn=r(P," ("),Ko=c(P,"CODE",{});var gE=i(Ko);nn=r(gE,"str"),gE.forEach(t),dn=r(P,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),jo=c(P,"CODE",{});var CE=i(jo);fn=r(CE,"HF_HOME"),CE.forEach(t),hn=r(P,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),Yo=c(P,"CODE",{});var OE=i(Yo);_n=r(OE,"~/.cache"),OE.forEach(t),un=r(P," or the content of "),Zo=c(P,"CODE",{});var DE=i(Zo);pn=r(DE,"XDG_CACHE_HOME"),DE.forEach(t),En=r(P,") suffixed with "),qo=c(P,"CODE",{});var wE=i(qo);vn=r(wE,"huggingface"),wE.forEach(t),mn=r(P,"."),P.forEach(t),gn=f(Vi),N=c(Vi,"LI",{});var jt=i(N);Xo=c(jt,"CODE",{});var SE=i(Xo);Cn=r(SE,"-h"),SE.forEach(t),On=r(jt,", "),Jo=c(jt,"CODE",{});var bE=i(Jo);Dn=r(bE,"--help"),bE.forEach(t),wn=r(jt," ("),Qo=c(jt,"CODE",{});var yE=i(Qo);Sn=r(yE,"bool"),yE.forEach(t),bn=r(jt,") \u2014 Show a help message and exit"),jt.forEach(t),Vi.forEach(t),ei=f(o),X=c(o,"H2",{class:!0});var es=i(X);_e=c(es,"A",{id:!0,class:!0,href:!0});var PE=i(_e);Vo=c(PE,"SPAN",{});var LE=i(Vo);V(yt.$$.fragment,LE),LE.forEach(t),PE.forEach(t),yn=f(es),el=c(es,"SPAN",{});var TE=i(el);Pn=r(TE,"accelerate launch"),TE.forEach(t),es.forEach(t),ti=f(o),Pt=c(o,"P",{});var E1=i(Pt);tl=c(E1,"STRONG",{});var IE=i(tl);Ln=r(IE,"Command"),IE.forEach(t),Tn=r(E1,":"),E1.forEach(t),oi=f(o),ue=c(o,"P",{});var ts=i(ue);ol=c(ts,"CODE",{});var AE=i(ol);In=r(AE,"accelerate launch"),AE.forEach(t),An=r(ts," or "),ll=c(ts,"CODE",{});var $E=i(ll);$n=r($E,"accelerate-launch"),$E.forEach(t),ts.forEach(t),li=f(o),Qt=c(o,"P",{});var ME=i(Qt);Mn=r(ME,"Launches a specified script on a distributed system with the right parameters."),ME.forEach(t),ri=f(o),Lt=c(o,"P",{});var v1=i(Lt);rl=c(v1,"STRONG",{});var NE=i(rl);Nn=r(NE,"Usage"),NE.forEach(t),kn=r(v1,":"),v1.forEach(t),ai=f(o),V(Tt.$$.fragment,o),ci=f(o),It=c(o,"P",{});var m1=i(It);al=c(m1,"STRONG",{});var kE=i(al);Un=r(kE,"Positional Arguments"),kE.forEach(t),Rn=r(m1,":"),m1.forEach(t),ii=f(o),pe=c(o,"UL",{});var os=i(pe);Vt=c(os,"LI",{});var g1=i(Vt);cl=c(g1,"CODE",{});var UE=i(cl);Gn=r(UE,"{training_script}"),UE.forEach(t),Wn=r(g1," \u2014 The full path to the script to be launched in parallel"),g1.forEach(t),xn=f(os),eo=c(os,"LI",{});var C1=i(eo);il=c(C1,"CODE",{});var RE=i(il);Fn=r(RE,"--{training_script-argument-1}"),RE.forEach(t),Hn=r(C1," \u2014 Arguments of the training script"),C1.forEach(t),os.forEach(t),si=f(o),At=c(o,"P",{});var O1=i(At);sl=c(O1,"STRONG",{});var GE=i(sl);zn=r(GE,"Optional Arguments"),GE.forEach(t),Bn=r(O1,":"),O1.forEach(t),ni=f(o),D=c(o,"UL",{});var K=i(D);k=c(K,"LI",{});var Yt=i(k);nl=c(Yt,"CODE",{});var WE=i(nl);Kn=r(WE,"-h"),WE.forEach(t),jn=r(Yt,", "),dl=c(Yt,"CODE",{});var xE=i(dl);Yn=r(xE,"--help"),xE.forEach(t),Zn=r(Yt," ("),fl=c(Yt,"CODE",{});var FE=i(fl);qn=r(FE,"bool"),FE.forEach(t),Xn=r(Yt,") \u2014 Show a help message and exit"),Yt.forEach(t),Jn=f(K),Ee=c(K,"LI",{});var ja=i(Ee);hl=c(ja,"CODE",{});var HE=i(hl);Qn=r(HE,"--config_file CONFIG_FILE"),HE.forEach(t),Vn=r(ja," ("),_l=c(ja,"CODE",{});var zE=i(_l);ed=r(zE,"str"),zE.forEach(t),td=r(ja,")\u2014 The config file to use for the default values in the launching script."),ja.forEach(t),od=f(K),U=c(K,"LI",{});var Zt=i(U);ul=c(Zt,"CODE",{});var BE=i(ul);ld=r(BE,"-m"),BE.forEach(t),rd=r(Zt,", "),pl=c(Zt,"CODE",{});var KE=i(pl);ad=r(KE,"--module"),KE.forEach(t),cd=r(Zt," ("),El=c(Zt,"CODE",{});var jE=i(El);id=r(jE,"bool"),jE.forEach(t),sd=r(Zt,") \u2014 Change each process to interpret the launch script as a Python module, executing with the same behavior as \u2018python -m\u2019."),Zt.forEach(t),nd=f(K),ve=c(K,"LI",{});var Ya=i(ve);vl=c(Ya,"CODE",{});var YE=i(vl);dd=r(YE,"--no_python"),YE.forEach(t),fd=r(Ya," ("),ml=c(Ya,"CODE",{});var ZE=i(ml);hd=r(ZE,"bool"),ZE.forEach(t),_d=r(Ya,") \u2014 Skip prepending the training script with \u2018python\u2019 - just execute it directly. Useful when the script is not a Python script."),Ya.forEach(t),ud=f(K),me=c(K,"LI",{});var Za=i(me);gl=c(Za,"CODE",{});var qE=i(gl);pd=r(qE,"--debug"),qE.forEach(t),Ed=r(Za," ("),Cl=c(Za,"CODE",{});var XE=i(Cl);vd=r(XE,"bool"),XE.forEach(t),md=r(Za,") \u2014 Whether to print out the torch.distributed stack trace when something fails."),Za.forEach(t),K.forEach(t),di=f(o),R=c(o,"P",{});var ho=i(R);gd=r(ho,"The rest of these arguments are configured through "),Ol=c(ho,"CODE",{});var JE=i(Ol);Cd=r(JE,"accelerate config"),JE.forEach(t),Od=r(ho," and are read in from the specified "),Dl=c(ho,"CODE",{});var QE=i(Dl);Dd=r(QE,"--config_file"),QE.forEach(t),wd=r(ho,` (or default configuration) for their
values. They can also be passed in manually.`),ho.forEach(t),fi=f(o),$t=c(o,"P",{});var D1=i($t);wl=c(D1,"STRONG",{});var VE=i(wl);Sd=r(VE,"Hardware Selection Arguments"),VE.forEach(t),bd=r(D1,":"),D1.forEach(t),hi=f(o),T=c(o,"UL",{});var ft=i(T);ge=c(ft,"LI",{});var qa=i(ge);Sl=c(qa,"CODE",{});var ev=i(Sl);yd=r(ev,"--cpu"),ev.forEach(t),Pd=r(qa," ("),bl=c(qa,"CODE",{});var tv=i(bl);Ld=r(tv,"bool"),tv.forEach(t),Td=r(qa,") \u2014 Whether or not to force the training on the CPU."),qa.forEach(t),Id=f(ft),Ce=c(ft,"LI",{});var Xa=i(Ce);yl=c(Xa,"CODE",{});var ov=i(yl);Ad=r(ov,"--multi_gpu"),ov.forEach(t),$d=r(Xa," ("),Pl=c(Xa,"CODE",{});var lv=i(Pl);Md=r(lv,"bool"),lv.forEach(t),Nd=r(Xa,") \u2014 Whether or not this should launch a distributed GPU training."),Xa.forEach(t),kd=f(ft),Oe=c(ft,"LI",{});var Ja=i(Oe);Ll=c(Ja,"CODE",{});var rv=i(Ll);Ud=r(rv,"--mps"),rv.forEach(t),Rd=r(Ja," ("),Tl=c(Ja,"CODE",{});var av=i(Tl);Gd=r(av,"bool"),av.forEach(t),Wd=r(Ja,") \u2014 Whether or not this should use MPS-enabled GPU device on MacOS machines."),Ja.forEach(t),xd=f(ft),De=c(ft,"LI",{});var Qa=i(De);Il=c(Qa,"CODE",{});var cv=i(Il);Fd=r(cv,"--tpu"),cv.forEach(t),Hd=r(Qa," ("),Al=c(Qa,"CODE",{});var iv=i(Al);zd=r(iv,"bool"),iv.forEach(t),Bd=r(Qa,") \u2014 Whether or not this should launch a TPU training."),Qa.forEach(t),ft.forEach(t),_i=f(o),Mt=c(o,"P",{});var w1=i(Mt);$l=c(w1,"STRONG",{});var sv=i($l);Kd=r(sv,"Resource Selection Arguments"),sv.forEach(t),jd=r(w1,":"),w1.forEach(t),ui=f(o),to=c(o,"P",{});var nv=i(to);Yd=r(nv,"The following arguments are useful for fine-tuning how available hardware should be used"),nv.forEach(t),pi=f(o),I=c(o,"UL",{});var ht=i(I);we=c(ht,"LI",{});var Va=i(we);Ml=c(Va,"CODE",{});var dv=i(Ml);Zd=r(dv,"--mixed_precision {no,fp16,bf16}"),dv.forEach(t),qd=r(Va," ("),Nl=c(Va,"CODE",{});var fv=i(Nl);Xd=r(fv,"str"),fv.forEach(t),Jd=r(Va,") \u2014 Whether or not to use mixed precision training. Choose between FP16 and BF16 (bfloat16) training. BF16 training is only supported on Nvidia Ampere GPUs and PyTorch 1.10 or later."),Va.forEach(t),Qd=f(ht),Se=c(ht,"LI",{});var ec=i(Se);kl=c(ec,"CODE",{});var hv=i(kl);Vd=r(hv,"--num_processes NUM_PROCESSES"),hv.forEach(t),ef=r(ec," ("),Ul=c(ec,"CODE",{});var _v=i(Ul);tf=r(_v,"int"),_v.forEach(t),of=r(ec,") \u2014 The total number of processes to be launched in parallel."),ec.forEach(t),lf=f(ht),be=c(ht,"LI",{});var tc=i(be);Rl=c(tc,"CODE",{});var uv=i(Rl);rf=r(uv,"--num_machines NUM_MACHINES"),uv.forEach(t),af=r(tc," ("),Gl=c(tc,"CODE",{});var pv=i(Gl);cf=r(pv,"int"),pv.forEach(t),sf=r(tc,") \u2014 The total number of machines used in this training."),tc.forEach(t),nf=f(ht),ye=c(ht,"LI",{});var oc=i(ye);Wl=c(oc,"CODE",{});var Ev=i(Wl);df=r(Ev,"--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS"),Ev.forEach(t),ff=r(oc," ("),xl=c(oc,"CODE",{});var vv=i(xl);hf=r(vv,"int"),vv.forEach(t),_f=r(oc,") \u2014 The number of CPU threads per process. Can be tuned for optimal performance."),oc.forEach(t),ht.forEach(t),Ei=f(o),Nt=c(o,"P",{});var S1=i(Nt);Fl=c(S1,"STRONG",{});var mv=i(Fl);uf=r(mv,"Training Paradigm Arguments"),mv.forEach(t),pf=r(S1,":"),S1.forEach(t),vi=f(o),oo=c(o,"P",{});var gv=i(oo);Ef=r(gv,"The following arguments are useful for selecting which training paradigm to use."),gv.forEach(t),mi=f(o),G=c(o,"UL",{});var _o=i(G);Pe=c(_o,"LI",{});var lc=i(Pe);Hl=c(lc,"CODE",{});var Cv=i(Hl);vf=r(Cv,"--use_deepspeed"),Cv.forEach(t),mf=r(lc," ("),zl=c(lc,"CODE",{});var Ov=i(zl);gf=r(Ov,"bool"),Ov.forEach(t),Cf=r(lc,") \u2014 Whether or not to use DeepSpeed for training."),lc.forEach(t),Of=f(_o),Le=c(_o,"LI",{});var rc=i(Le);Bl=c(rc,"CODE",{});var Dv=i(Bl);Df=r(Dv,"--use_fsdp"),Dv.forEach(t),wf=r(rc," ("),Kl=c(rc,"CODE",{});var wv=i(Kl);Sf=r(wv,"bool"),wv.forEach(t),bf=r(rc,") \u2014 Whether or not to use FullyShardedDataParallel for training."),rc.forEach(t),yf=f(_o),Te=c(_o,"LI",{});var ac=i(Te);jl=c(ac,"CODE",{});var Sv=i(jl);Pf=r(Sv,"--use_megatron_lm"),Sv.forEach(t),Lf=r(ac," ("),Yl=c(ac,"CODE",{});var bv=i(Yl);Tf=r(bv,"bool"),bv.forEach(t),If=r(ac,") \u2014 Whether or not to use Megatron-LM for training."),ac.forEach(t),_o.forEach(t),gi=f(o),kt=c(o,"P",{});var b1=i(kt);Zl=c(b1,"STRONG",{});var yv=i(Zl);Af=r(yv,"Distributed GPU Arguments"),yv.forEach(t),$f=r(b1,":"),b1.forEach(t),Ci=f(o),W=c(o,"P",{});var uo=i(W);Mf=r(uo,"The following arguments are only useful when "),ql=c(uo,"CODE",{});var Pv=i(ql);Nf=r(Pv,"multi_gpu"),Pv.forEach(t),kf=r(uo," is passed or multi-gpu training is configured through "),Xl=c(uo,"CODE",{});var Lv=i(Xl);Uf=r(Lv,"accelerate config"),Lv.forEach(t),Rf=r(uo,":"),uo.forEach(t),Oi=f(o),p=c(o,"UL",{});var O=i(p);Ie=c(O,"LI",{});var cc=i(Ie);Jl=c(cc,"CODE",{});var Tv=i(Jl);Gf=r(Tv,"--gpu_ids"),Tv.forEach(t),Wf=r(cc," ("),Ql=c(cc,"CODE",{});var Iv=i(Ql);xf=r(Iv,"str"),Iv.forEach(t),Ff=r(cc,") \u2014 What GPUs (by id) should be used for training on this machine as a comma-seperated list"),cc.forEach(t),Hf=f(O),Ae=c(O,"LI",{});var ic=i(Ae);Vl=c(ic,"CODE",{});var Av=i(Vl);zf=r(Av,"--same_network"),Av.forEach(t),Bf=r(ic," ("),er=c(ic,"CODE",{});var $v=i(er);Kf=r($v,"bool"),$v.forEach(t),jf=r(ic,") \u2014 Whether all machines used for multinode training exist on the same local network."),ic.forEach(t),Yf=f(O),$e=c(O,"LI",{});var sc=i($e);tr=c(sc,"CODE",{});var Mv=i(tr);Zf=r(Mv,"--machine_rank MACHINE_RANK"),Mv.forEach(t),qf=r(sc," ("),or=c(sc,"CODE",{});var Nv=i(or);Xf=r(Nv,"int"),Nv.forEach(t),Jf=r(sc,") \u2014 The rank of the machine on which this script is launched."),sc.forEach(t),Qf=f(O),Me=c(O,"LI",{});var nc=i(Me);lr=c(nc,"CODE",{});var kv=i(lr);Vf=r(kv,"--main_process_ip MAIN_PROCESS_IP"),kv.forEach(t),eh=r(nc," ("),rr=c(nc,"CODE",{});var Uv=i(rr);th=r(Uv,"str"),Uv.forEach(t),oh=r(nc,") \u2014 The IP address of the machine of rank 0."),nc.forEach(t),lh=f(O),Ne=c(O,"LI",{});var dc=i(Ne);ar=c(dc,"CODE",{});var Rv=i(ar);rh=r(Rv,"--main_process_port MAIN_PROCESS_PORT"),Rv.forEach(t),ah=r(dc," ("),cr=c(dc,"CODE",{});var Gv=i(cr);ch=r(Gv,"int"),Gv.forEach(t),ih=r(dc,") \u2014 The port to use to communicate with the machine of rank 0."),dc.forEach(t),sh=f(O),ke=c(O,"LI",{});var fc=i(ke);ir=c(fc,"CODE",{});var Wv=i(ir);nh=r(Wv,"--rdzv_conf"),Wv.forEach(t),dh=r(fc," ("),sr=c(fc,"CODE",{});var xv=i(sr);fh=r(xv,"str"),xv.forEach(t),hh=r(fc,") \u2014 Additional rendezvous configuration (<key1>=<value1>,<key2>=<value2>,\u2026)."),fc.forEach(t),_h=f(O),Ue=c(O,"LI",{});var hc=i(Ue);nr=c(hc,"CODE",{});var Fv=i(nr);uh=r(Fv,"--max_restarts"),Fv.forEach(t),ph=r(hc," ("),dr=c(hc,"CODE",{});var Hv=i(dr);Eh=r(Hv,"int"),Hv.forEach(t),vh=r(hc,") \u2014 Maximum number of worker group restarts before failing."),hc.forEach(t),mh=f(O),Re=c(O,"LI",{});var _c=i(Re);fr=c(_c,"CODE",{});var zv=i(fr);gh=r(zv,"--monitor_interval"),zv.forEach(t),Ch=r(_c," ("),hr=c(_c,"CODE",{});var Bv=i(hr);Oh=r(Bv,"float"),Bv.forEach(t),Dh=r(_c,") \u2014 Interval, in seconds, to monitor the state of workers."),_c.forEach(t),O.forEach(t),Di=f(o),Ut=c(o,"P",{});var y1=i(Ut);_r=c(y1,"STRONG",{});var Kv=i(_r);wh=r(Kv,"TPU Arguments"),Kv.forEach(t),Sh=r(y1,":"),y1.forEach(t),wi=f(o),x=c(o,"P",{});var po=i(x);bh=r(po,"The following arguments are only useful when "),ur=c(po,"CODE",{});var jv=i(ur);yh=r(jv,"tpu"),jv.forEach(t),Ph=r(po," is passed or TPU training is configured through "),pr=c(po,"CODE",{});var Yv=i(pr);Lh=r(Yv,"accelerate config"),Yv.forEach(t),Th=r(po,":"),po.forEach(t),Si=f(o),Ge=c(o,"UL",{});var ls=i(Ge);We=c(ls,"LI",{});var uc=i(We);Er=c(uc,"CODE",{});var Zv=i(Er);Ih=r(Zv,"--main_training_function MAIN_TRAINING_FUNCTION"),Zv.forEach(t),Ah=r(uc," ("),vr=c(uc,"CODE",{});var qv=i(vr);$h=r(qv,"str"),qv.forEach(t),Mh=r(uc,") \u2014 The name of the main function to be executed in your script."),uc.forEach(t),Nh=f(ls),xe=c(ls,"LI",{});var pc=i(xe);mr=c(pc,"CODE",{});var Xv=i(mr);kh=r(Xv,"--downcast_bf16"),Xv.forEach(t),Uh=r(pc," ("),gr=c(pc,"CODE",{});var Jv=i(gr);Rh=r(Jv,"bool"),Jv.forEach(t),Gh=r(pc,") \u2014 Whether when using bf16 precision on TPUs if both float and double tensors are cast to bfloat16 or if double tensors remain as float32."),pc.forEach(t),ls.forEach(t),bi=f(o),Rt=c(o,"P",{});var P1=i(Rt);Cr=c(P1,"STRONG",{});var Qv=i(Cr);Wh=r(Qv,"DeepSpeed Arguments"),Qv.forEach(t),xh=r(P1,":"),P1.forEach(t),yi=f(o),A=c(o,"P",{});var _t=i(A);Fh=r(_t,"The following arguments are only useful when "),Or=c(_t,"CODE",{});var Vv=i(Or);Hh=r(Vv,"use_deepspeed"),Vv.forEach(t),zh=r(_t," is passed or "),Dr=c(_t,"CODE",{});var em=i(Dr);Bh=r(em,"deepspeed"),em.forEach(t),Kh=r(_t," is configured through "),wr=c(_t,"CODE",{});var tm=i(wr);jh=r(tm,"accelerate config"),tm.forEach(t),Yh=r(_t,":"),_t.forEach(t),Pi=f(o),h=c(o,"UL",{});var _=i(h);Fe=c(_,"LI",{});var Ec=i(Fe);Sr=c(Ec,"CODE",{});var om=i(Sr);Zh=r(om,"--deepspeed_config_file"),om.forEach(t),qh=r(Ec," ("),br=c(Ec,"CODE",{});var lm=i(br);Xh=r(lm,"str"),lm.forEach(t),Jh=r(Ec,") \u2014 DeepSpeed config file."),Ec.forEach(t),Qh=f(_),He=c(_,"LI",{});var vc=i(He);yr=c(vc,"CODE",{});var rm=i(yr);Vh=r(rm,"--zero_stage"),rm.forEach(t),e_=r(vc," ("),Pr=c(vc,"CODE",{});var am=i(Pr);t_=r(am,"int"),am.forEach(t),o_=r(vc,") \u2014 DeepSpeed\u2019s ZeRO optimization stage."),vc.forEach(t),l_=f(_),ze=c(_,"LI",{});var mc=i(ze);Lr=c(mc,"CODE",{});var cm=i(Lr);r_=r(cm,"--offload_optimizer_device"),cm.forEach(t),a_=r(mc," ("),Tr=c(mc,"CODE",{});var im=i(Tr);c_=r(im,"str"),im.forEach(t),i_=r(mc,") \u2014 Decides where (none|cpu|nvme) to offload optimizer states."),mc.forEach(t),s_=f(_),Be=c(_,"LI",{});var gc=i(Be);Ir=c(gc,"CODE",{});var sm=i(Ir);n_=r(sm,"--offload_param_device"),sm.forEach(t),d_=r(gc," ("),Ar=c(gc,"CODE",{});var nm=i(Ar);f_=r(nm,"str"),nm.forEach(t),h_=r(gc,") \u2014 Decides where (none|cpu|nvme) to offload parameters."),gc.forEach(t),__=f(_),Ke=c(_,"LI",{});var Cc=i(Ke);$r=c(Cc,"CODE",{});var dm=i($r);u_=r(dm,"--gradient_accumulation_steps"),dm.forEach(t),p_=r(Cc," ("),Mr=c(Cc,"CODE",{});var fm=i(Mr);E_=r(fm,"int"),fm.forEach(t),v_=r(Cc,") \u2014 No of gradient_accumulation_steps used in your training script."),Cc.forEach(t),m_=f(_),je=c(_,"LI",{});var Oc=i(je);Nr=c(Oc,"CODE",{});var hm=i(Nr);g_=r(hm,"--gradient_clipping"),hm.forEach(t),C_=r(Oc," ("),kr=c(Oc,"CODE",{});var _m=i(kr);O_=r(_m,"float"),_m.forEach(t),D_=r(Oc,") \u2014 Gradient clipping value used in your training script."),Oc.forEach(t),w_=f(_),F=c(_,"LI",{});var qt=i(F);Ur=c(qt,"CODE",{});var um=i(Ur);S_=r(um,"--zero3_init_flag"),um.forEach(t),b_=r(qt," ("),Rr=c(qt,"CODE",{});var pm=i(Rr);y_=r(pm,"str"),pm.forEach(t),P_=r(qt,") \u2014 Decides Whether (true|false) to enable "),Gr=c(qt,"CODE",{});var Em=i(Gr);L_=r(Em,"deepspeed.zero.Init"),Em.forEach(t),T_=r(qt," for constructing massive models. Only applicable with DeepSpeed ZeRO Stage-3."),qt.forEach(t),I_=f(_),Ye=c(_,"LI",{});var Dc=i(Ye);Wr=c(Dc,"CODE",{});var vm=i(Wr);A_=r(vm,"--zero3_save_16bit_model"),vm.forEach(t),$_=r(Dc," ("),xr=c(Dc,"CODE",{});var mm=i(xr);M_=r(mm,"str"),mm.forEach(t),N_=r(Dc,") \u2014 Decides Whether (true|false) to save 16-bit model weights when using ZeRO Stage-3. Only applicable with DeepSpeed ZeRO Stage-3."),Dc.forEach(t),k_=f(_),Ze=c(_,"LI",{});var wc=i(Ze);Fr=c(wc,"CODE",{});var gm=i(Fr);U_=r(gm,"--deepspeed_hostfile"),gm.forEach(t),R_=r(wc," ("),Hr=c(wc,"CODE",{});var Cm=i(Hr);G_=r(Cm,"str"),Cm.forEach(t),W_=r(wc,") \u2014 DeepSpeed hostfile for configuring multi-node compute resources."),wc.forEach(t),x_=f(_),qe=c(_,"LI",{});var Sc=i(qe);zr=c(Sc,"CODE",{});var Om=i(zr);F_=r(Om,"--deepspeed_exclusion_filter"),Om.forEach(t),H_=r(Sc," ("),Br=c(Sc,"CODE",{});var Dm=i(Br);z_=r(Dm,"str"),Dm.forEach(t),B_=r(Sc,") \u2014 DeepSpeed exclusion filter string when using mutli-node setup."),Sc.forEach(t),K_=f(_),Xe=c(_,"LI",{});var bc=i(Xe);Kr=c(bc,"CODE",{});var wm=i(Kr);j_=r(wm,"--deepspeed_inclusion_filter"),wm.forEach(t),Y_=r(bc," ("),jr=c(bc,"CODE",{});var Sm=i(jr);Z_=r(Sm,"str"),Sm.forEach(t),q_=r(bc,") \u2014 DeepSpeed inclusion filter string when using mutli-node setup."),bc.forEach(t),X_=f(_),Je=c(_,"LI",{});var yc=i(Je);Yr=c(yc,"CODE",{});var bm=i(Yr);J_=r(bm,"--deepspeed_multinode_launcher"),bm.forEach(t),Q_=r(yc," ("),Zr=c(yc,"CODE",{});var ym=i(Zr);V_=r(ym,"str"),ym.forEach(t),eu=r(yc,") \u2014 DeepSpeed multi-node launcher to use."),yc.forEach(t),_.forEach(t),Li=f(o),Gt=c(o,"P",{});var L1=i(Gt);qr=c(L1,"STRONG",{});var Pm=i(qr);tu=r(Pm,"Fully Sharded Data Parallelism Arguments"),Pm.forEach(t),ou=r(L1,":"),L1.forEach(t),Ti=f(o),H=c(o,"P",{});var Eo=i(H);lu=r(Eo,"The following arguments are only useful when "),Xr=c(Eo,"CODE",{});var Lm=i(Xr);ru=r(Lm,"use_fdsp"),Lm.forEach(t),au=r(Eo," is passed or Fully Sharded Data Parallelism is configured through "),Jr=c(Eo,"CODE",{});var Tm=i(Jr);cu=r(Tm,"accelerate config"),Tm.forEach(t),iu=r(Eo,":"),Eo.forEach(t),Ii=f(o),E=c(o,"UL",{});var S=i(E);Qe=c(S,"LI",{});var Pc=i(Qe);Qr=c(Pc,"CODE",{});var Im=i(Qr);su=r(Im,"--fsdp_offload_params"),Im.forEach(t),nu=r(Pc," ("),Vr=c(Pc,"CODE",{});var Am=i(Vr);du=r(Am,"str"),Am.forEach(t),fu=r(Pc,") \u2014 Decides Whether (true|false) to offload parameters and gradients to CPU."),Pc.forEach(t),hu=f(S),Ve=c(S,"LI",{});var Lc=i(Ve);ea=c(Lc,"CODE",{});var $m=i(ea);_u=r($m,"--fsdp_min_num_params"),$m.forEach(t),uu=r(Lc," ("),ta=c(Lc,"CODE",{});var Mm=i(ta);pu=r(Mm,"int"),Mm.forEach(t),Eu=r(Lc,") \u2014 FSDP\u2019s minimum number of parameters for Default Auto Wrapping."),Lc.forEach(t),vu=f(S),et=c(S,"LI",{});var Tc=i(et);oa=c(Tc,"CODE",{});var Nm=i(oa);mu=r(Nm,"--fsdp_sharding_strategy"),Nm.forEach(t),gu=r(Tc," ("),la=c(Tc,"CODE",{});var km=i(la);Cu=r(km,"int"),km.forEach(t),Ou=r(Tc,") \u2014 FSDP\u2019s Sharding Strategy."),Tc.forEach(t),Du=f(S),tt=c(S,"LI",{});var Ic=i(tt);ra=c(Ic,"CODE",{});var Um=i(ra);wu=r(Um,"--fsdp_auto_wrap_policy"),Um.forEach(t),Su=r(Ic," ("),aa=c(Ic,"CODE",{});var Rm=i(aa);bu=r(Rm,"str"),Rm.forEach(t),yu=r(Ic,") \u2014 FSDP\u2019s auto wrap policy."),Ic.forEach(t),Pu=f(S),w=c(S,"LI",{});var $=i(w);ca=c($,"CODE",{});var Gm=i(ca);Lu=r(Gm,"--fsdp_transformer_layer_cls_to_wrap"),Gm.forEach(t),Tu=r($," ("),ia=c($,"CODE",{});var Wm=i(ia);Iu=r(Wm,"str"),Wm.forEach(t),Au=r($,") \u2014 Transformer layer class name (case-sensitive) to wrap, e.g, "),sa=c($,"CODE",{});var xm=i(sa);$u=r(xm,"BertLayer"),xm.forEach(t),Mu=r($,", "),na=c($,"CODE",{});var Fm=i(na);Nu=r(Fm,"GPTJBlock"),Fm.forEach(t),ku=r($,", "),da=c($,"CODE",{});var Hm=i(da);Uu=r(Hm,"T5Block"),Hm.forEach(t),Ru=r($," \u2026"),$.forEach(t),Gu=f(S),ot=c(S,"LI",{});var Ac=i(ot);fa=c(Ac,"CODE",{});var zm=i(fa);Wu=r(zm,"--fsdp_backward_prefetch_policy"),zm.forEach(t),xu=r(Ac," ("),ha=c(Ac,"CODE",{});var Bm=i(ha);Fu=r(Bm,"str"),Bm.forEach(t),Hu=r(Ac,") \u2014 FSDP\u2019s backward prefetch policy."),Ac.forEach(t),zu=f(S),lt=c(S,"LI",{});var $c=i(lt);_a=c($c,"CODE",{});var Km=i(_a);Bu=r(Km,"--fsdp_state_dict_type"),Km.forEach(t),Ku=r($c," ("),ua=c($c,"CODE",{});var jm=i(ua);ju=r(jm,"str"),jm.forEach(t),Yu=r($c,") \u2014 FSDP\u2019s state dict type."),$c.forEach(t),S.forEach(t),Ai=f(o),Wt=c(o,"P",{});var T1=i(Wt);pa=c(T1,"STRONG",{});var Ym=i(pa);Zu=r(Ym,"Megatron-LM Arguments"),Ym.forEach(t),qu=r(T1,":"),T1.forEach(t),$i=f(o),z=c(o,"P",{});var vo=i(z);Xu=r(vo,"The following arguments are only useful when "),Ea=c(vo,"CODE",{});var Zm=i(Ea);Ju=r(Zm,"use_megatron_lm"),Zm.forEach(t),Qu=r(vo," is passed or Megatron-LM is configured through "),va=c(vo,"CODE",{});var qm=i(va);Vu=r(qm,"accelerate config"),qm.forEach(t),ep=r(vo,":"),vo.forEach(t),Mi=f(o),v=c(o,"UL",{});var b=i(v);lo=c(b,"LI",{});var I1=i(lo);ma=c(I1,"CODE",{});var Xm=i(ma);tp=r(Xm,"--megatron_lm_tp_degree"),Xm.forEach(t),op=r(I1," (\u201C) \u2014 Megatron-LM\u2019s Tensor Parallelism (TP) degree."),I1.forEach(t),lp=f(b),ro=c(b,"LI",{});var A1=i(ro);ga=c(A1,"CODE",{});var Jm=i(ga);rp=r(Jm,"--megatron_lm_pp_degree"),Jm.forEach(t),ap=r(A1," (\u201C) \u2014 Megatron-LM\u2019s Pipeline Parallelism (PP) degree."),A1.forEach(t),cp=f(b),ao=c(b,"LI",{});var $1=i(ao);Ca=c($1,"CODE",{});var Qm=i(Ca);ip=r(Qm,"--megatron_lm_num_micro_batches"),Qm.forEach(t),sp=r($1," (\u201C) \u2014 Megatron-LM\u2019s number of micro batches when PP degree > 1."),$1.forEach(t),np=f(b),co=c(b,"LI",{});var M1=i(co);Oa=c(M1,"CODE",{});var Vm=i(Oa);dp=r(Vm,"--megatron_lm_sequence_parallelism"),Vm.forEach(t),fp=r(M1," (\u201C) \u2014 Decides Whether (true|false) to enable Sequence Parallelism when TP degree > 1."),M1.forEach(t),hp=f(b),io=c(b,"LI",{});var N1=i(io);Da=c(N1,"CODE",{});var e2=i(Da);_p=r(e2,"--megatron_lm_recompute_activations"),e2.forEach(t),up=r(N1," (\u201C) \u2014 Decides Whether (true|false) to enable Selective Activation Recomputation."),N1.forEach(t),pp=f(b),so=c(b,"LI",{});var k1=i(so);wa=c(k1,"CODE",{});var t2=i(wa);Ep=r(t2,"--megatron_lm_use_distributed_optimizer"),t2.forEach(t),vp=r(k1," (\u201C) \u2014 Decides Whether (true|false) to use distributed optimizer which shards optimizer state and gradients across Data Pralellel (DP) ranks."),k1.forEach(t),mp=f(b),no=c(b,"LI",{});var U1=i(no);Sa=c(U1,"CODE",{});var o2=i(Sa);gp=r(o2,"--megatron_lm_gradient_clipping"),o2.forEach(t),Cp=r(U1," (\u201C) \u2014 Megatron-LM\u2019s gradient clipping value based on global L2 Norm (0 to disable)."),U1.forEach(t),b.forEach(t),Ni=f(o),xt=c(o,"P",{});var R1=i(xt);ba=c(R1,"STRONG",{});var l2=i(ba);Op=r(l2,"AWS SageMaker Arguments"),l2.forEach(t),Dp=r(R1,":"),R1.forEach(t),ki=f(o),fo=c(o,"P",{});var r2=i(fo);wp=r(r2,"The following arguments are only useful when training in SageMaker"),r2.forEach(t),Ui=f(o),rt=c(o,"UL",{});var rs=i(rt);at=c(rs,"LI",{});var Mc=i(at);ya=c(Mc,"CODE",{});var a2=i(ya);Sp=r(a2,"--aws_access_key_id AWS_ACCESS_KEY_ID"),a2.forEach(t),bp=r(Mc," ("),Pa=c(Mc,"CODE",{});var c2=i(Pa);yp=r(c2,"str"),c2.forEach(t),Pp=r(Mc,") \u2014 The AWS_ACCESS_KEY_ID used to launch the Amazon SageMaker training job"),Mc.forEach(t),Lp=f(rs),ct=c(rs,"LI",{});var Nc=i(ct);La=c(Nc,"CODE",{});var i2=i(La);Tp=r(i2,"--aws_secret_access_key AWS_SECRET_ACCESS_KEY"),i2.forEach(t),Ip=r(Nc," ("),Ta=c(Nc,"CODE",{});var s2=i(Ta);Ap=r(s2,"str"),s2.forEach(t),$p=r(Nc,") \u2014 The AWS_SECRET_ACCESS_KEY used to launch the Amazon SageMaker training job"),Nc.forEach(t),rs.forEach(t),Ri=f(o),J=c(o,"H2",{class:!0});var as=i(J);it=c(as,"A",{id:!0,class:!0,href:!0});var n2=i(it);Ia=c(n2,"SPAN",{});var d2=i(Ia);V(Ft.$$.fragment,d2),d2.forEach(t),n2.forEach(t),Mp=f(as),Aa=c(as,"SPAN",{});var f2=i(Aa);Np=r(f2,"accelerate test"),f2.forEach(t),as.forEach(t),Gi=f(o),st=c(o,"P",{});var cs=i(st);$a=c(cs,"CODE",{});var h2=i($a);kp=r(h2,"accelerate test"),h2.forEach(t),Up=r(cs," or "),Ma=c(cs,"CODE",{});var _2=i(Ma);Rp=r(_2,"accelerate-test"),_2.forEach(t),cs.forEach(t),Wi=f(o),nt=c(o,"P",{});var is=i(nt);Gp=r(is,"Runs "),Na=c(is,"CODE",{});var u2=i(Na);Wp=r(u2,"accelerate/test_utils/test_script.py"),u2.forEach(t),xp=r(is," to verify that \u{1F917} Accelerate has been properly configured on your system and runs."),is.forEach(t),xi=f(o),Ht=c(o,"P",{});var G1=i(Ht);ka=c(G1,"STRONG",{});var p2=i(ka);Fp=r(p2,"Usage"),p2.forEach(t),Hp=r(G1,":"),G1.forEach(t),Fi=f(o),V(zt.$$.fragment,o),Hi=f(o),Bt=c(o,"P",{});var W1=i(Bt);Ua=c(W1,"STRONG",{});var E2=i(Ua);zp=r(E2,"Optional Arguments"),E2.forEach(t),Bp=r(W1,":"),W1.forEach(t),zi=f(o),dt=c(o,"UL",{});var ss=i(dt);C=c(ss,"LI",{});var L=i(C);Ra=c(L,"CODE",{});var v2=i(Ra);Kp=r(v2,"--config_file CONFIG_FILE"),v2.forEach(t),jp=r(L," ("),Ga=c(L,"CODE",{});var m2=i(Ga);Yp=r(m2,"str"),m2.forEach(t),Zp=r(L,`) \u2014 The path to use to store the config file. Will default to a file named default_config.yaml in the cache location, which is the content
of the environment `),Wa=c(L,"CODE",{});var g2=i(Wa);qp=r(g2,"HF_HOME"),g2.forEach(t),Xp=r(L,` suffixed with \u2018accelerate\u2019, or if you don\u2019t have such an environment variable, your cache directory
(`),xa=c(L,"CODE",{});var C2=i(xa);Jp=r(C2,"~/.cache"),C2.forEach(t),Qp=r(L," or the content of "),Fa=c(L,"CODE",{});var O2=i(Fa);Vp=r(O2,"XDG_CACHE_HOME"),O2.forEach(t),e1=r(L,") suffixed with "),Ha=c(L,"CODE",{});var D2=i(Ha);t1=r(D2,"huggingface"),D2.forEach(t),o1=r(L,"."),L.forEach(t),l1=f(ss),B=c(ss,"LI",{});var Xt=i(B);za=c(Xt,"CODE",{});var w2=i(za);r1=r(w2,"-h"),w2.forEach(t),a1=r(Xt,", "),Ba=c(Xt,"CODE",{});var S2=i(Ba);c1=r(S2,"--help"),S2.forEach(t),i1=r(Xt," ("),Ka=c(Xt,"CODE",{});var b2=i(Ka);s1=r(b2,"bool"),b2.forEach(t),n1=r(Xt,") \u2014 Show a help message and exit"),Xt.forEach(t),ss.forEach(t),this.h()},h(){u(j,"name","hf:doc:metadata"),u(j,"content",JSON.stringify(M2)),u(re,"id","the-command-line"),u(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(re,"href","#the-command-line"),u(Y,"class","relative group"),u(ae,"id","accelerate-config"),u(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ae,"href","#accelerate-config"),u(Z,"class","relative group"),u(ne,"id","accelerate-env"),u(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ne,"href","#accelerate-env"),u(q,"class","relative group"),u(Dt,"href","https://github.com/huggingface/accelerate"),u(Dt,"rel","nofollow"),u(_e,"id","accelerate-launch"),u(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(_e,"href","#accelerate-launch"),u(X,"class","relative group"),u(it,"id","accelerate-test"),u(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(it,"href","#accelerate-test"),u(J,"class","relative group")},m(o,s){e(document.head,j),n(o,Uc,s),n(o,Y,s),e(Y,re),e(re,mo),ee(ut,mo,null),e(Y,ds),e(Y,go),e(go,fs),n(o,Rc,s),n(o,Jt,s),e(Jt,hs),n(o,Gc,s),n(o,Z,s),e(Z,ae),e(ae,Co),ee(pt,Co,null),e(Z,_s),e(Z,Oo),e(Oo,us),n(o,Wc,s),n(o,Et,s),e(Et,Do),e(Do,ps),e(Et,Es),n(o,xc,s),n(o,ce,s),e(ce,wo),e(wo,vs),e(ce,ms),e(ce,So),e(So,gs),n(o,Fc,s),n(o,ie,s),e(ie,Cs),e(ie,bo),e(bo,Os),e(ie,Ds),n(o,Hc,s),n(o,vt,s),e(vt,yo),e(yo,ws),e(vt,Ss),n(o,zc,s),ee(mt,o,s),n(o,Bc,s),n(o,gt,s),e(gt,Po),e(Po,bs),e(gt,ys),n(o,Kc,s),n(o,se,s),e(se,m),e(m,Lo),e(Lo,Ps),e(m,Ls),e(m,To),e(To,Ts),e(m,Is),e(m,Io),e(Io,As),e(m,$s),e(m,Ao),e(Ao,Ms),e(m,Ns),e(m,$o),e($o,ks),e(m,Us),e(m,Mo),e(Mo,Rs),e(m,Gs),e(se,Ws),e(se,M),e(M,No),e(No,xs),e(M,Fs),e(M,ko),e(ko,Hs),e(M,zs),e(M,Uo),e(Uo,Bs),e(M,Ks),n(o,jc,s),n(o,q,s),e(q,ne),e(ne,Ro),ee(Ct,Ro,null),e(q,js),e(q,Go),e(Go,Ys),n(o,Yc,s),n(o,Ot,s),e(Ot,Wo),e(Wo,Zs),e(Ot,qs),n(o,Zc,s),n(o,de,s),e(de,xo),e(xo,Xs),e(de,Js),e(de,Fo),e(Fo,Qs),n(o,qc,s),n(o,fe,s),e(fe,Vs),e(fe,Dt),e(Dt,en),e(fe,tn),n(o,Xc,s),n(o,wt,s),e(wt,Ho),e(Ho,on),e(wt,ln),n(o,Jc,s),ee(St,o,s),n(o,Qc,s),n(o,bt,s),e(bt,zo),e(zo,rn),e(bt,an),n(o,Vc,s),n(o,he,s),e(he,g),e(g,Bo),e(Bo,cn),e(g,sn),e(g,Ko),e(Ko,nn),e(g,dn),e(g,jo),e(jo,fn),e(g,hn),e(g,Yo),e(Yo,_n),e(g,un),e(g,Zo),e(Zo,pn),e(g,En),e(g,qo),e(qo,vn),e(g,mn),e(he,gn),e(he,N),e(N,Xo),e(Xo,Cn),e(N,On),e(N,Jo),e(Jo,Dn),e(N,wn),e(N,Qo),e(Qo,Sn),e(N,bn),n(o,ei,s),n(o,X,s),e(X,_e),e(_e,Vo),ee(yt,Vo,null),e(X,yn),e(X,el),e(el,Pn),n(o,ti,s),n(o,Pt,s),e(Pt,tl),e(tl,Ln),e(Pt,Tn),n(o,oi,s),n(o,ue,s),e(ue,ol),e(ol,In),e(ue,An),e(ue,ll),e(ll,$n),n(o,li,s),n(o,Qt,s),e(Qt,Mn),n(o,ri,s),n(o,Lt,s),e(Lt,rl),e(rl,Nn),e(Lt,kn),n(o,ai,s),ee(Tt,o,s),n(o,ci,s),n(o,It,s),e(It,al),e(al,Un),e(It,Rn),n(o,ii,s),n(o,pe,s),e(pe,Vt),e(Vt,cl),e(cl,Gn),e(Vt,Wn),e(pe,xn),e(pe,eo),e(eo,il),e(il,Fn),e(eo,Hn),n(o,si,s),n(o,At,s),e(At,sl),e(sl,zn),e(At,Bn),n(o,ni,s),n(o,D,s),e(D,k),e(k,nl),e(nl,Kn),e(k,jn),e(k,dl),e(dl,Yn),e(k,Zn),e(k,fl),e(fl,qn),e(k,Xn),e(D,Jn),e(D,Ee),e(Ee,hl),e(hl,Qn),e(Ee,Vn),e(Ee,_l),e(_l,ed),e(Ee,td),e(D,od),e(D,U),e(U,ul),e(ul,ld),e(U,rd),e(U,pl),e(pl,ad),e(U,cd),e(U,El),e(El,id),e(U,sd),e(D,nd),e(D,ve),e(ve,vl),e(vl,dd),e(ve,fd),e(ve,ml),e(ml,hd),e(ve,_d),e(D,ud),e(D,me),e(me,gl),e(gl,pd),e(me,Ed),e(me,Cl),e(Cl,vd),e(me,md),n(o,di,s),n(o,R,s),e(R,gd),e(R,Ol),e(Ol,Cd),e(R,Od),e(R,Dl),e(Dl,Dd),e(R,wd),n(o,fi,s),n(o,$t,s),e($t,wl),e(wl,Sd),e($t,bd),n(o,hi,s),n(o,T,s),e(T,ge),e(ge,Sl),e(Sl,yd),e(ge,Pd),e(ge,bl),e(bl,Ld),e(ge,Td),e(T,Id),e(T,Ce),e(Ce,yl),e(yl,Ad),e(Ce,$d),e(Ce,Pl),e(Pl,Md),e(Ce,Nd),e(T,kd),e(T,Oe),e(Oe,Ll),e(Ll,Ud),e(Oe,Rd),e(Oe,Tl),e(Tl,Gd),e(Oe,Wd),e(T,xd),e(T,De),e(De,Il),e(Il,Fd),e(De,Hd),e(De,Al),e(Al,zd),e(De,Bd),n(o,_i,s),n(o,Mt,s),e(Mt,$l),e($l,Kd),e(Mt,jd),n(o,ui,s),n(o,to,s),e(to,Yd),n(o,pi,s),n(o,I,s),e(I,we),e(we,Ml),e(Ml,Zd),e(we,qd),e(we,Nl),e(Nl,Xd),e(we,Jd),e(I,Qd),e(I,Se),e(Se,kl),e(kl,Vd),e(Se,ef),e(Se,Ul),e(Ul,tf),e(Se,of),e(I,lf),e(I,be),e(be,Rl),e(Rl,rf),e(be,af),e(be,Gl),e(Gl,cf),e(be,sf),e(I,nf),e(I,ye),e(ye,Wl),e(Wl,df),e(ye,ff),e(ye,xl),e(xl,hf),e(ye,_f),n(o,Ei,s),n(o,Nt,s),e(Nt,Fl),e(Fl,uf),e(Nt,pf),n(o,vi,s),n(o,oo,s),e(oo,Ef),n(o,mi,s),n(o,G,s),e(G,Pe),e(Pe,Hl),e(Hl,vf),e(Pe,mf),e(Pe,zl),e(zl,gf),e(Pe,Cf),e(G,Of),e(G,Le),e(Le,Bl),e(Bl,Df),e(Le,wf),e(Le,Kl),e(Kl,Sf),e(Le,bf),e(G,yf),e(G,Te),e(Te,jl),e(jl,Pf),e(Te,Lf),e(Te,Yl),e(Yl,Tf),e(Te,If),n(o,gi,s),n(o,kt,s),e(kt,Zl),e(Zl,Af),e(kt,$f),n(o,Ci,s),n(o,W,s),e(W,Mf),e(W,ql),e(ql,Nf),e(W,kf),e(W,Xl),e(Xl,Uf),e(W,Rf),n(o,Oi,s),n(o,p,s),e(p,Ie),e(Ie,Jl),e(Jl,Gf),e(Ie,Wf),e(Ie,Ql),e(Ql,xf),e(Ie,Ff),e(p,Hf),e(p,Ae),e(Ae,Vl),e(Vl,zf),e(Ae,Bf),e(Ae,er),e(er,Kf),e(Ae,jf),e(p,Yf),e(p,$e),e($e,tr),e(tr,Zf),e($e,qf),e($e,or),e(or,Xf),e($e,Jf),e(p,Qf),e(p,Me),e(Me,lr),e(lr,Vf),e(Me,eh),e(Me,rr),e(rr,th),e(Me,oh),e(p,lh),e(p,Ne),e(Ne,ar),e(ar,rh),e(Ne,ah),e(Ne,cr),e(cr,ch),e(Ne,ih),e(p,sh),e(p,ke),e(ke,ir),e(ir,nh),e(ke,dh),e(ke,sr),e(sr,fh),e(ke,hh),e(p,_h),e(p,Ue),e(Ue,nr),e(nr,uh),e(Ue,ph),e(Ue,dr),e(dr,Eh),e(Ue,vh),e(p,mh),e(p,Re),e(Re,fr),e(fr,gh),e(Re,Ch),e(Re,hr),e(hr,Oh),e(Re,Dh),n(o,Di,s),n(o,Ut,s),e(Ut,_r),e(_r,wh),e(Ut,Sh),n(o,wi,s),n(o,x,s),e(x,bh),e(x,ur),e(ur,yh),e(x,Ph),e(x,pr),e(pr,Lh),e(x,Th),n(o,Si,s),n(o,Ge,s),e(Ge,We),e(We,Er),e(Er,Ih),e(We,Ah),e(We,vr),e(vr,$h),e(We,Mh),e(Ge,Nh),e(Ge,xe),e(xe,mr),e(mr,kh),e(xe,Uh),e(xe,gr),e(gr,Rh),e(xe,Gh),n(o,bi,s),n(o,Rt,s),e(Rt,Cr),e(Cr,Wh),e(Rt,xh),n(o,yi,s),n(o,A,s),e(A,Fh),e(A,Or),e(Or,Hh),e(A,zh),e(A,Dr),e(Dr,Bh),e(A,Kh),e(A,wr),e(wr,jh),e(A,Yh),n(o,Pi,s),n(o,h,s),e(h,Fe),e(Fe,Sr),e(Sr,Zh),e(Fe,qh),e(Fe,br),e(br,Xh),e(Fe,Jh),e(h,Qh),e(h,He),e(He,yr),e(yr,Vh),e(He,e_),e(He,Pr),e(Pr,t_),e(He,o_),e(h,l_),e(h,ze),e(ze,Lr),e(Lr,r_),e(ze,a_),e(ze,Tr),e(Tr,c_),e(ze,i_),e(h,s_),e(h,Be),e(Be,Ir),e(Ir,n_),e(Be,d_),e(Be,Ar),e(Ar,f_),e(Be,h_),e(h,__),e(h,Ke),e(Ke,$r),e($r,u_),e(Ke,p_),e(Ke,Mr),e(Mr,E_),e(Ke,v_),e(h,m_),e(h,je),e(je,Nr),e(Nr,g_),e(je,C_),e(je,kr),e(kr,O_),e(je,D_),e(h,w_),e(h,F),e(F,Ur),e(Ur,S_),e(F,b_),e(F,Rr),e(Rr,y_),e(F,P_),e(F,Gr),e(Gr,L_),e(F,T_),e(h,I_),e(h,Ye),e(Ye,Wr),e(Wr,A_),e(Ye,$_),e(Ye,xr),e(xr,M_),e(Ye,N_),e(h,k_),e(h,Ze),e(Ze,Fr),e(Fr,U_),e(Ze,R_),e(Ze,Hr),e(Hr,G_),e(Ze,W_),e(h,x_),e(h,qe),e(qe,zr),e(zr,F_),e(qe,H_),e(qe,Br),e(Br,z_),e(qe,B_),e(h,K_),e(h,Xe),e(Xe,Kr),e(Kr,j_),e(Xe,Y_),e(Xe,jr),e(jr,Z_),e(Xe,q_),e(h,X_),e(h,Je),e(Je,Yr),e(Yr,J_),e(Je,Q_),e(Je,Zr),e(Zr,V_),e(Je,eu),n(o,Li,s),n(o,Gt,s),e(Gt,qr),e(qr,tu),e(Gt,ou),n(o,Ti,s),n(o,H,s),e(H,lu),e(H,Xr),e(Xr,ru),e(H,au),e(H,Jr),e(Jr,cu),e(H,iu),n(o,Ii,s),n(o,E,s),e(E,Qe),e(Qe,Qr),e(Qr,su),e(Qe,nu),e(Qe,Vr),e(Vr,du),e(Qe,fu),e(E,hu),e(E,Ve),e(Ve,ea),e(ea,_u),e(Ve,uu),e(Ve,ta),e(ta,pu),e(Ve,Eu),e(E,vu),e(E,et),e(et,oa),e(oa,mu),e(et,gu),e(et,la),e(la,Cu),e(et,Ou),e(E,Du),e(E,tt),e(tt,ra),e(ra,wu),e(tt,Su),e(tt,aa),e(aa,bu),e(tt,yu),e(E,Pu),e(E,w),e(w,ca),e(ca,Lu),e(w,Tu),e(w,ia),e(ia,Iu),e(w,Au),e(w,sa),e(sa,$u),e(w,Mu),e(w,na),e(na,Nu),e(w,ku),e(w,da),e(da,Uu),e(w,Ru),e(E,Gu),e(E,ot),e(ot,fa),e(fa,Wu),e(ot,xu),e(ot,ha),e(ha,Fu),e(ot,Hu),e(E,zu),e(E,lt),e(lt,_a),e(_a,Bu),e(lt,Ku),e(lt,ua),e(ua,ju),e(lt,Yu),n(o,Ai,s),n(o,Wt,s),e(Wt,pa),e(pa,Zu),e(Wt,qu),n(o,$i,s),n(o,z,s),e(z,Xu),e(z,Ea),e(Ea,Ju),e(z,Qu),e(z,va),e(va,Vu),e(z,ep),n(o,Mi,s),n(o,v,s),e(v,lo),e(lo,ma),e(ma,tp),e(lo,op),e(v,lp),e(v,ro),e(ro,ga),e(ga,rp),e(ro,ap),e(v,cp),e(v,ao),e(ao,Ca),e(Ca,ip),e(ao,sp),e(v,np),e(v,co),e(co,Oa),e(Oa,dp),e(co,fp),e(v,hp),e(v,io),e(io,Da),e(Da,_p),e(io,up),e(v,pp),e(v,so),e(so,wa),e(wa,Ep),e(so,vp),e(v,mp),e(v,no),e(no,Sa),e(Sa,gp),e(no,Cp),n(o,Ni,s),n(o,xt,s),e(xt,ba),e(ba,Op),e(xt,Dp),n(o,ki,s),n(o,fo,s),e(fo,wp),n(o,Ui,s),n(o,rt,s),e(rt,at),e(at,ya),e(ya,Sp),e(at,bp),e(at,Pa),e(Pa,yp),e(at,Pp),e(rt,Lp),e(rt,ct),e(ct,La),e(La,Tp),e(ct,Ip),e(ct,Ta),e(Ta,Ap),e(ct,$p),n(o,Ri,s),n(o,J,s),e(J,it),e(it,Ia),ee(Ft,Ia,null),e(J,Mp),e(J,Aa),e(Aa,Np),n(o,Gi,s),n(o,st,s),e(st,$a),e($a,kp),e(st,Up),e(st,Ma),e(Ma,Rp),n(o,Wi,s),n(o,nt,s),e(nt,Gp),e(nt,Na),e(Na,Wp),e(nt,xp),n(o,xi,s),n(o,Ht,s),e(Ht,ka),e(ka,Fp),e(Ht,Hp),n(o,Fi,s),ee(zt,o,s),n(o,Hi,s),n(o,Bt,s),e(Bt,Ua),e(Ua,zp),e(Bt,Bp),n(o,zi,s),n(o,dt,s),e(dt,C),e(C,Ra),e(Ra,Kp),e(C,jp),e(C,Ga),e(Ga,Yp),e(C,Zp),e(C,Wa),e(Wa,qp),e(C,Xp),e(C,xa),e(xa,Jp),e(C,Qp),e(C,Fa),e(Fa,Vp),e(C,e1),e(C,Ha),e(Ha,t1),e(C,o1),e(dt,l1),e(dt,B),e(B,za),e(za,r1),e(B,a1),e(B,Ba),e(Ba,c1),e(B,i1),e(B,Ka),e(Ka,s1),e(B,n1),Bi=!0},p:I2,i(o){Bi||(te(ut.$$.fragment,o),te(pt.$$.fragment,o),te(mt.$$.fragment,o),te(Ct.$$.fragment,o),te(St.$$.fragment,o),te(yt.$$.fragment,o),te(Tt.$$.fragment,o),te(Ft.$$.fragment,o),te(zt.$$.fragment,o),Bi=!0)},o(o){oe(ut.$$.fragment,o),oe(pt.$$.fragment,o),oe(mt.$$.fragment,o),oe(Ct.$$.fragment,o),oe(St.$$.fragment,o),oe(yt.$$.fragment,o),oe(Tt.$$.fragment,o),oe(Ft.$$.fragment,o),oe(zt.$$.fragment,o),Bi=!1},d(o){t(j),o&&t(Uc),o&&t(Y),le(ut),o&&t(Rc),o&&t(Jt),o&&t(Gc),o&&t(Z),le(pt),o&&t(Wc),o&&t(Et),o&&t(xc),o&&t(ce),o&&t(Fc),o&&t(ie),o&&t(Hc),o&&t(vt),o&&t(zc),le(mt,o),o&&t(Bc),o&&t(gt),o&&t(Kc),o&&t(se),o&&t(jc),o&&t(q),le(Ct),o&&t(Yc),o&&t(Ot),o&&t(Zc),o&&t(de),o&&t(qc),o&&t(fe),o&&t(Xc),o&&t(wt),o&&t(Jc),le(St,o),o&&t(Qc),o&&t(bt),o&&t(Vc),o&&t(he),o&&t(ei),o&&t(X),le(yt),o&&t(ti),o&&t(Pt),o&&t(oi),o&&t(ue),o&&t(li),o&&t(Qt),o&&t(ri),o&&t(Lt),o&&t(ai),le(Tt,o),o&&t(ci),o&&t(It),o&&t(ii),o&&t(pe),o&&t(si),o&&t(At),o&&t(ni),o&&t(D),o&&t(di),o&&t(R),o&&t(fi),o&&t($t),o&&t(hi),o&&t(T),o&&t(_i),o&&t(Mt),o&&t(ui),o&&t(to),o&&t(pi),o&&t(I),o&&t(Ei),o&&t(Nt),o&&t(vi),o&&t(oo),o&&t(mi),o&&t(G),o&&t(gi),o&&t(kt),o&&t(Ci),o&&t(W),o&&t(Oi),o&&t(p),o&&t(Di),o&&t(Ut),o&&t(wi),o&&t(x),o&&t(Si),o&&t(Ge),o&&t(bi),o&&t(Rt),o&&t(yi),o&&t(A),o&&t(Pi),o&&t(h),o&&t(Li),o&&t(Gt),o&&t(Ti),o&&t(H),o&&t(Ii),o&&t(E),o&&t(Ai),o&&t(Wt),o&&t($i),o&&t(z),o&&t(Mi),o&&t(v),o&&t(Ni),o&&t(xt),o&&t(ki),o&&t(fo),o&&t(Ui),o&&t(rt),o&&t(Ri),o&&t(J),le(Ft),o&&t(Gi),o&&t(st),o&&t(Wi),o&&t(nt),o&&t(xi),o&&t(Ht),o&&t(Fi),le(zt,o),o&&t(Hi),o&&t(Bt),o&&t(zi),o&&t(dt)}}}const M2={local:"the-command-line",sections:[{local:"accelerate-config",title:"accelerate config"},{local:"accelerate-env",title:"accelerate env"},{local:"accelerate-launch",title:"accelerate launch"},{local:"accelerate-test",title:"accelerate test"}],title:"The Command Line "};function N2(x1){return A2(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class G2 extends y2{constructor(j){super();P2(this,j,N2,$2,L2,{})}}export{G2 as default,M2 as metadata};
