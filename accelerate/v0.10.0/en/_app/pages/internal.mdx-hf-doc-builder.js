import{S as mc,i as fc,s as gc,e as r,k as c,w as u,t as n,M as vc,c as o,d as a,m as i,a as s,x as m,h as l,b as p,G as t,g as h,y as f,q as g,o as v,B as _,v as _c}from"../chunks/vendor-hf-doc-builder.js";import{T as Wn}from"../chunks/Tip-hf-doc-builder.js";import{D as y}from"../chunks/Docstring-hf-doc-builder.js";import{I as L}from"../chunks/IconCopyLink-hf-doc-builder.js";function bc(me){let b,D,$,w,P;return{c(){b=r("p"),D=n("This does not support "),$=r("code"),w=n("BatchSampler"),P=n(" with varying batch size yet.")},l(E){b=o(E,"P",{});var x=s(b);D=l(x,"This does not support "),$=o(x,"CODE",{});var N=s($);w=l(N,"BatchSampler"),N.forEach(a),P=l(x," with varying batch size yet."),x.forEach(a)},m(E,x){h(E,b,x),t(b,D),t(b,$),t($,w),t(b,P)},d(E){E&&a(b)}}}function $c(me){let b,D,$,w,P;return{c(){b=r("p"),D=n("This does not support "),$=r("code"),w=n("BatchSampler"),P=n(" with varying batch size yet.")},l(E){b=o(E,"P",{});var x=s(b);D=l(x,"This does not support "),$=o(x,"CODE",{});var N=s($);w=l(N,"BatchSampler"),N.forEach(a),P=l(x," with varying batch size yet."),x.forEach(a)},m(E,x){h(E,b,x),t(b,D),t(b,$),t($,w),t(b,P)},d(E){E&&a(b)}}}function yc(me){let b,D;return{c(){b=r("p"),D=n("Make sure all processes will reach this instruction otherwise one of your processes will hang forever.")},l($){b=o($,"P",{});var w=s(b);D=l(w,"Make sure all processes will reach this instruction otherwise one of your processes will hang forever."),w.forEach(a)},m($,w){h($,b,w),t(b,D)},d($){$&&a(b)}}}function wc(me){let b,D,$,w,P,E,x,N,so,Xa,F,fe,Ut,Ue,no,Gt,lo,Ja,W,Ge,co,Vt,io,Ka,R,ge,Bt,Ve,po,qt,ho,Qa,ve,uo,Ft,mo,fo,Ya,k,Be,go,qe,vo,Wt,_o,bo,$o,H,yo,Rt,wo,Eo,Ht,xo,Do,So,_e,Za,M,be,Mt,Fe,To,jt,Po,er,j,We,ko,Re,Ao,Xt,Io,zo,tr,X,$e,Jt,He,Lo,Kt,No,ar,O,Me,Oo,C,Co,Qt,Uo,Go,Yt,Vo,Bo,Zt,qo,Fo,Wo,ye,rr,J,we,ea,je,Ro,ta,Ho,or,K,Xe,Mo,S,jo,aa,Xo,Jo,ra,Ko,Qo,oa,Yo,Zo,sa,es,ts,na,as,rs,sr,Q,Ee,la,Je,os,ca,ss,nr,U,Ke,ns,ia,ls,cs,Qe,da,is,ds,pa,ps,lr,Y,xe,ha,Ye,hs,ua,us,cr,Z,De,ma,Ze,ms,fa,fs,ir,ee,et,gs,te,vs,tt,_s,bs,ga,$s,ys,dr,ae,Se,va,at,ws,_a,Es,pr,A,rt,xs,ba,Ds,Ss,$a,Ts,Ps,I,St,ya,ks,As,Is,Tt,wa,zs,Ls,Ns,Pt,Ea,Os,Cs,Us,kt,xa,Gs,Vs,Bs,At,Da,qs,Fs,hr,re,Te,Sa,ot,Ws,Ta,Rs,ur,T,st,Hs,Pa,Ms,js,Pe,nt,Xs,ka,Js,Ks,ke,lt,Qs,oe,Ys,Aa,Zs,en,Ia,tn,an,rn,Ae,ct,on,it,sn,za,nn,ln,mr,se,Ie,La,dt,cn,Na,dn,fr,ne,pt,pn,Oa,hn,gr,le,ht,un,Ca,mn,vr,ce,ut,fn,Ua,gn,_r,ie,mt,vn,ft,_n,Ga,bn,$n,br,de,gt,yn,Va,wn,$r,pe,vt,En,Ba,xn,yr,he,_t,Dn,G,Sn,qa,Tn,Pn,Fa,kn,An,Wa,In,zn,wr,bt,$t,Er,yt,wt,xr,V,Et,Ln,Ra,Nn,On,ze,Dr,ue,xt,Cn,Ha,Un,Sr;return E=new L({}),Ue=new L({}),Ge=new y({props:{name:"class accelerate.optimizer.AcceleratedOptimizer",anchor:"accelerate.optimizer.AcceleratedOptimizer",parameters:[{name:"optimizer",val:""},{name:"device_placement",val:" = True"},{name:"scaler",val:" = None"}],parametersDescription:[{anchor:"accelerate.optimizer.AcceleratedOptimizer.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.optimizer.Optimizer</code>) &#x2014;
The optimizer to wrap.`,name:"optimizer"},{anchor:"accelerate.optimizer.AcceleratedOptimizer.device_placement",description:`<strong>device_placement</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the optimizer should handle device placement. If so, it will place the state dictionary of
<code>optimizer</code> on the right device.`,name:"device_placement"},{anchor:"accelerate.optimizer.AcceleratedOptimizer.scaler",description:`<strong>scaler</strong> (<code>torch.cuda.amp.grad_scaler.GradScaler</code>, <em>optional</em>) &#x2014;
The scaler to use in the step function if training with mixed precision.`,name:"scaler"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/optimizer.py#L38"}}),Ve=new L({}),Be=new y({props:{name:"accelerate.data_loader.prepare_data_loader",anchor:"accelerate.data_loader.prepare_data_loader",parameters:[{name:"dataloader",val:": DataLoader"},{name:"device",val:": typing.Optional[torch.device] = None"},{name:"num_processes",val:": typing.Optional[int] = None"},{name:"process_index",val:": typing.Optional[int] = None"},{name:"split_batches",val:": bool = False"},{name:"put_on_device",val:": bool = False"},{name:"rng_types",val:": typing.Union[typing.List[typing.Union[str, accelerate.utils.dataclasses.RNGType]], NoneType] = None"},{name:"dispatch_batches",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"accelerate.data_loader.prepare_data_loader.dataloader",description:`<strong>dataloader</strong> (<code>torch.utils.data.dataloader.DataLoader</code>) &#x2014;
The data loader to split across several devices.`,name:"dataloader"},{anchor:"accelerate.data_loader.prepare_data_loader.device",description:`<strong>device</strong> (<code>torch.device</code>) &#x2014;
The target device for the returned <code>DataLoader</code>.`,name:"device"},{anchor:"accelerate.data_loader.prepare_data_loader.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of processes running concurrently. Will default to the value given by
<a href="/docs/accelerate/v0.10.0/en/internal#accelerate.state.AcceleratorState">AcceleratorState</a>.`,name:"num_processes"},{anchor:"accelerate.data_loader.prepare_data_loader.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The index of the current process. Will default to the value given by <a href="/docs/accelerate/v0.10.0/en/internal#accelerate.state.AcceleratorState">AcceleratorState</a>.`,name:"process_index"},{anchor:"accelerate.data_loader.prepare_data_loader.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the resulting <code>DataLoader</code> should split the batches of the original data loader across devices or
yield full batches (in which case it will yield batches starting at the <code>process_index</code>-th and advancing of
<code>num_processes</code> batches at each iteration).</p>
<p>Another way to see this is that the observed batch size will be the same as the initial <code>dataloader</code> if
this option is set to <code>True</code>, the batch size of the initial <code>dataloader</code> multiplied by <code>num_processes</code>
otherwise.</p>
<p>Setting this option to <code>True</code> requires that the batch size of the <code>dataloader</code> is a round multiple of
<code>batch_size</code>.`,name:"split_batches"},{anchor:"accelerate.data_loader.prepare_data_loader.put_on_device",description:`<strong>put_on_device</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to put the batches on <code>device</code> (only works if the batches are nested list, tuples or
dictionaries of tensors).`,name:"put_on_device"},{anchor:"accelerate.data_loader.prepare_data_loader.rng_types",description:`<strong>rng_types</strong> (list of <code>str</code> or <code>RNGType</code>) &#x2014;
The list of random number generators to synchronize at the beginning of each iteration. Should be one or
several of:</p>
<ul>
<li><code>&quot;torch&quot;</code>: the base torch random number generator</li>
<li><code>&quot;cuda&quot;</code>: the CUDA random number generator (GPU only)</li>
<li><code>&quot;xla&quot;</code>: the XLA random number generator (TPU only)</li>
<li><code>&quot;generator&quot;</code>: the <code>torch.Generator</code> of the sampler (or batch sampler if there is no sampler in your
dataloader) or of the iterable dataset (if it exists) if the underlying dataset is of that type.</li>
</ul>`,name:"rng_types"},{anchor:"accelerate.data_loader.prepare_data_loader.dispatch_batches",description:`<strong>dispatch_batches</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the datalaoder prepared is only iterated through on the main process and then the batches
are split and broadcast to each process. Will default to <code>True</code> when the underlying dataset is an
<code>IterableDataset</code>, <code>False</code> otherwise.`,name:"dispatch_batches"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/data_loader.py#L419",returnDescription:`
<p>A new data loader that will yield the portion of the batches</p>
`,returnType:`
<p><code>torch.utils.data.dataloader.DataLoader</code></p>
`}}),_e=new Wn({props:{warning:!0,$$slots:{default:[bc]},$$scope:{ctx:me}}}),Fe=new L({}),We=new y({props:{name:"class accelerate.data_loader.DataLoaderShard",anchor:"accelerate.data_loader.DataLoaderShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],parametersDescription:[{anchor:"accelerate.data_loader.DataLoaderShard.dataset",description:`<strong>dataset</strong> (<code>torch.utils.data.dataset.Dataset</code>) &#x2014;
The dataset to use to build this datalaoder.`,name:"dataset"},{anchor:"accelerate.data_loader.DataLoaderShard.device",description:`<strong>device</strong> (<code>torch.device</code>, <em>optional</em>) &#x2014;
If passed, the device to put all batches on.`,name:"device"},{anchor:"accelerate.data_loader.DataLoaderShard.rng_types",description:`<strong>rng_types</strong> (list of <code>str</code> or <code>RNGType</code>) &#x2014;
The list of random number generators to synchronize at the beginning of each iteration. Should be one or
several of:</p>
<ul>
<li><code>&quot;torch&quot;</code>: the base torch random number generator</li>
<li><code>&quot;cuda&quot;</code>: the CUDA random number generator (GPU only)</li>
<li><code>&quot;xla&quot;</code>: the XLA random number generator (TPU only)</li>
<li><code>&quot;generator&quot;</code>: an optional <code>torch.Generator</code></li>
</ul>`,name:"rng_types"},{anchor:"accelerate.data_loader.DataLoaderShard.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A random number generator to keep synchronized across processes.
kwargs &#x2014;
All other keyword arguments to pass to the regular <code>DataLoader</code> initialization.`,name:"generator"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/data_loader.py#L269"}}),He=new L({}),Me=new y({props:{name:"class accelerate.data_loader.BatchSamplerShard",anchor:"accelerate.data_loader.BatchSamplerShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],parametersDescription:[{anchor:"accelerate.data_loader.BatchSamplerShard.batch_sampler",description:`<strong>batch_sampler</strong> (<code>torch.utils.data.sampler.BatchSampler</code>) &#x2014;
The batch sampler to split in several shards.`,name:"batch_sampler"},{anchor:"accelerate.data_loader.BatchSamplerShard.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of processes running concurrently.`,name:"num_processes"},{anchor:"accelerate.data_loader.BatchSamplerShard.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The index of the current process.`,name:"process_index"},{anchor:"accelerate.data_loader.BatchSamplerShard.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
yielding different full batches on each process.</p>
<p>On two processes with a sampler of <code>[[0, 1, 2, 3], [4, 5, 6, 7]]</code>, this will result in:</p>
<ul>
<li>the sampler on process 0 to yield <code>[0, 1, 2, 3]</code> and the sampler on process 1 to yield <code>[4, 5, 6, 7]</code> if
this argument is set to <code>False</code>.</li>
<li>the sampler on process 0 to yield <code>[0, 1]</code> then <code>[4, 5]</code> and the sampler on process 1 to yield <code>[2, 3]</code>
then <code>[6, 7]</code> if this argument is set to <code>True</code>.</li>
</ul>`,name:"split_batches"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/data_loader.py#L67"}}),ye=new Wn({props:{warning:!0,$$slots:{default:[$c]},$$scope:{ctx:me}}}),je=new L({}),Xe=new y({props:{name:"class accelerate.data_loader.IterableDatasetShard",anchor:"accelerate.data_loader.IterableDatasetShard",parameters:[{name:"*args",val:""},{name:"**kwds",val:""}],parametersDescription:[{anchor:"accelerate.data_loader.IterableDatasetShard.dataset",description:`<strong>dataset</strong> (<code>torch.utils.data.dataset.IterableDataset</code>) &#x2014;
The batch sampler to split in several shards.`,name:"dataset"},{anchor:"accelerate.data_loader.IterableDatasetShard.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The size of the batches per shard (if <code>split_batches=False</code>) or the size of the batches (if
<code>split_batches=True</code>).`,name:"batch_size"},{anchor:"accelerate.data_loader.IterableDatasetShard.drop_last",description:`<strong>drop_last</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to drop the last incomplete batch or complete the last batches by using the samples from the
beginning.`,name:"drop_last"},{anchor:"accelerate.data_loader.IterableDatasetShard.num_processes",description:`<strong>num_processes</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of processes running concurrently.`,name:"num_processes"},{anchor:"accelerate.data_loader.IterableDatasetShard.process_index",description:`<strong>process_index</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The index of the current process.`,name:"process_index"},{anchor:"accelerate.data_loader.IterableDatasetShard.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the shards should be created by splitting a batch to give a piece of it on each process, or by
yielding different full batches on each process.</p>
<p>On two processes with an iterable dataset yielding of <code>[0, 1, 2, 3, 4, 5, 6, 7]</code>, this will result in:</p>
<ul>
<li>the shard on process 0 to yield <code>[0, 1, 2, 3]</code> and the shard on process 1 to yield <code>[4, 5, 6, 7]</code> if this
argument is set to <code>False</code>.</li>
<li>the shard on process 0 to yield <code>[0, 1, 4, 5]</code> and the sampler on process 1 to yield <code>[2, 3, 6, 7]</code> if
this argument is set to <code>True</code>.</li>
</ul>`,name:"split_batches"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/data_loader.py#L188"}}),Je=new L({}),Ke=new y({props:{name:"class accelerate.scheduler.AcceleratedScheduler",anchor:"accelerate.scheduler.AcceleratedScheduler",parameters:[{name:"scheduler",val:""},{name:"optimizers",val:""},{name:"step_with_optimizer",val:": bool = True"},{name:"split_batches",val:": bool = False"}],parametersDescription:[{anchor:"accelerate.scheduler.AcceleratedScheduler.scheduler",description:`<strong>scheduler</strong> (<code>torch.optim.lr_scheduler._LRScheduler</code>) &#x2014;
The scheduler to wrap.`,name:"scheduler"},{anchor:"accelerate.scheduler.AcceleratedScheduler.optimizers",description:`<strong>optimizers</strong> (one or a list of <code>torch.optim.Optimizer</code>) &#x2014;
The optimizers used.`,name:"optimizers"},{anchor:"accelerate.scheduler.AcceleratedScheduler.step_with_optimizer",description:`<strong>step_with_optimizer</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the scheduler should be stepped at each optimizer step.`,name:"step_with_optimizer"},{anchor:"accelerate.scheduler.AcceleratedScheduler.split_batches",description:`<strong>split_batches</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the dataloaders split one batch across the different processes (so batch size is the same
regardless of the number of processes) or create batches on each process (so batch size is the original
batch size multiplied by the number of processes).`,name:"split_batches"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/scheduler.py#L18"}}),Ye=new L({}),Ze=new L({}),et=new y({props:{name:"class accelerate.state.AcceleratorState",anchor:"accelerate.state.AcceleratorState",parameters:[{name:"mixed_precision",val:": str = None"},{name:"cpu",val:": bool = False"},{name:"deepspeed_plugin",val:" = None"},{name:"fsdp_plugin",val:" = None"},{name:"_from_accelerator",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>device</strong> (<code>torch.device</code>) &#x2014; The device to use. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:`<strong>-</strong> <strong>distributed_type</strong> (<code>~accelerate.state.DistributedType</code>) &#x2014; The type of distributed environment currently &#x2014;
in use.`,name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>num_processes</strong> (<code>int</code>) &#x2014; The number of processes currently launched in parallel. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>process_index</strong> (<code>int</code>) &#x2014; The index of the current process. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:"<strong>-</strong> <strong>local_process_index</strong> (<code>int</code>) &#x2014; The index of the current process on the current server. &#x2014;",name:"-"},{anchor:"accelerate.state.AcceleratorState.-",description:`<strong>-</strong> <strong>mixed_precision</strong> (<code>str</code>) &#x2014; Whether or not the current script will use mixed precision. If you are using &#x2014;
mixed precision, define if you want to use FP16 or BF16 (bfloat16) as the floating point.`,name:"-"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/state.py#L47"}}),at=new L({}),rt=new y({props:{name:"class accelerate.DistributedType",anchor:"accelerate.DistributedType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/dataclasses.py#L104"}}),ot=new L({}),st=new y({props:{name:"class accelerate.tracking.GeneralTracker",anchor:"accelerate.tracking.GeneralTracker",parameters:[],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/tracking.py#L52"}}),nt=new y({props:{name:"finish",anchor:"accelerate.tracking.GeneralTracker.finish",parameters:[],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/tracking.py#L91"}}),lt=new y({props:{name:"log",anchor:"accelerate.tracking.GeneralTracker.log",parameters:[{name:"values",val:": dict"},{name:"step",val:": typing.Optional[int]"}],parametersDescription:[{anchor:"accelerate.tracking.GeneralTracker.log.values",description:`<strong>values</strong> (Dictionary <code>str</code> to <code>str</code>, <code>float</code>, or <code>int</code>) &#x2014;
Values to be logged as key-value pairs. The values need to have type <code>str</code>, <code>float</code>, or <code>int</code>.`,name:"values"},{anchor:"accelerate.tracking.GeneralTracker.log.step",description:`<strong>step</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The run step. If included, the log will be affiliated with this step.`,name:"step"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/tracking.py#L77"}}),ct=new y({props:{name:"store_init_configuration",anchor:"accelerate.tracking.GeneralTracker.store_init_configuration",parameters:[{name:"values",val:": dict"}],parametersDescription:[{anchor:"accelerate.tracking.GeneralTracker.store_init_configuration.values",description:`<strong>values</strong> (Dictionary <code>str</code> to <code>bool</code>, <code>str</code>, <code>float</code> or <code>int</code>) &#x2014;
Values to be stored as initial hyperparameters as key-value pairs. The values need to have type <code>bool</code>,
<code>str</code>, <code>float</code>, <code>int</code>, or <code>None</code>.`,name:"values"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/tracking.py#L64"}}),dt=new L({}),pt=new y({props:{name:"accelerate.utils.extract_model_from_parallel",anchor:"accelerate.utils.extract_model_from_parallel",parameters:[{name:"model",val:""}],parametersDescription:[{anchor:"accelerate.utils.extract_model_from_parallel.model",description:"<strong>model</strong> (<code>torch.nn.Module</code>) &#x2014; The model to extract.",name:"model"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/other.py#L35",returnDescription:`
<p>The extracted model.</p>
`,returnType:`
<p><code>torch.nn.Module</code></p>
`}}),ht=new y({props:{name:"accelerate.utils.is_bf16_available",anchor:"accelerate.utils.is_bf16_available",parameters:[{name:"ignore_tpu",val:" = False"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/imports.py#L76"}}),ut=new y({props:{name:"accelerate.utils.is_torch_version",anchor:"accelerate.utils.is_torch_version",parameters:[{name:"operation",val:": str"},{name:"version",val:": str"}],parametersDescription:[{anchor:"accelerate.utils.is_torch_version.operation",description:`<strong>operation</strong> (<code>str</code>) &#x2014;
A string representation of an operator, such as <code>&quot;&gt;&quot;</code> or <code>&quot;&lt;=&quot;</code>`,name:"operation"},{anchor:"accelerate.utils.is_torch_version.version",description:`<strong>version</strong> (<code>str</code>) &#x2014;
A string version of PyTorch`,name:"version"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/versions.py#L51"}}),mt=new y({props:{name:"accelerate.utils.is_tpu_available",anchor:"accelerate.utils.is_tpu_available",parameters:[],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/imports.py#L59"}}),gt=new y({props:{name:"accelerate.utils.gather",anchor:"accelerate.utils.gather",parameters:[{name:"tensor",val:""}],parametersDescription:[{anchor:"accelerate.utils.gather.tensor",description:`<strong>tensor</strong> (nested list/tuple/dictionary of <code>torch.Tensor</code>) &#x2014;
The data to gather.`,name:"tensor"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/operations.py#L207",returnDescription:`
<p>The same data structure as <code>tensor</code> with all tensors sent to the proper device.</p>
`}}),vt=new y({props:{name:"accelerate.utils.send_to_device",anchor:"accelerate.utils.send_to_device",parameters:[{name:"tensor",val:""},{name:"device",val:""}],parametersDescription:[{anchor:"accelerate.utils.send_to_device.tensor",description:`<strong>tensor</strong> (nested list/tuple/dictionary of <code>torch.Tensor</code>) &#x2014;
The data to send to a given device.`,name:"tensor"},{anchor:"accelerate.utils.send_to_device.device",description:`<strong>device</strong> (<code>torch.device</code>) &#x2014;
The device to send the data to.`,name:"device"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/operations.py#L106",returnDescription:`
<p>The same data structure as <code>tensor</code> with all tensors sent to the proper device.</p>
`}}),_t=new y({props:{name:"accelerate.utils.set_seed",anchor:"accelerate.utils.set_seed",parameters:[{name:"seed",val:": int"},{name:"device_specific",val:": bool = False"}],parametersDescription:[{anchor:"accelerate.utils.set_seed.seed",description:"<strong>seed</strong> (<code>int</code>) &#x2014; The seed to set.",name:"seed"},{anchor:"accelerate.utils.set_seed.device_specific",description:`<strong>device_specific</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to differ the seed on each device slightly with <code>self.process_index</code>.`,name:"device_specific"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/random.py#L30"}}),$t=new y({props:{name:"accelerate.utils.synchronize_rng_state",anchor:"accelerate.utils.synchronize_rng_state",parameters:[{name:"rng_type",val:": typing.Optional[accelerate.utils.dataclasses.RNGType] = None"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/random.py#L50"}}),wt=new y({props:{name:"accelerate.synchronize_rng_states",anchor:"accelerate.synchronize_rng_states",parameters:[{name:"rng_types",val:": typing.List[typing.Union[str, accelerate.utils.dataclasses.RNGType]]"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/random.py#L85"}}),Et=new y({props:{name:"accelerate.utils.wait_for_everyone",anchor:"accelerate.utils.wait_for_everyone",parameters:[],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/other.py#L54"}}),ze=new Wn({props:{warning:!0,$$slots:{default:[yc]},$$scope:{ctx:me}}}),xt=new y({props:{name:"accelerate.utils.write_basic_config",anchor:"accelerate.utils.write_basic_config",parameters:[{name:"mixed_precision",val:" = 'no'"},{name:"save_location",val:": str = '/github/home/.cache/huggingface/accelerate/default_config.yaml'"}],parametersDescription:[{anchor:"accelerate.utils.write_basic_config.mixed_precision",description:`<strong>mixed_precision</strong> (<code>str</code>, <em>optional</em>, defaults to &#x201C;no&#x201D;) &#x2014;
Mixed Precision to use. Should be one of &#x201C;no&#x201D;, &#x201C;fp16&#x201D;, or &#x201C;bf16&#x201D;`,name:"mixed_precision"},{anchor:"accelerate.utils.write_basic_config.save_location",description:`<strong>save_location</strong> (<code>str</code>, <em>optional</em>, defaults to <code>default_json_config_file</code>) &#x2014;
Optional custom save location. Should be passed to <code>--config_file</code> when using <code>accelerate launch</code>. Default
location is inside the huggingface cache folder (<code>~/.cache/huggingface</code>) but can be overriden by setting
the <code>HF_HOME</code> environmental variable, followed by <code>accelerate/default_config.yaml</code>.`,name:"save_location"}],source:"https://github.com/huggingface/accelerate/blob/v0.10.0/src/accelerate/utils/other.py#L117"}}),{c(){b=r("meta"),D=c(),$=r("h1"),w=r("a"),P=r("span"),u(E.$$.fragment),x=c(),N=r("span"),so=n("Internals"),Xa=c(),F=r("h2"),fe=r("a"),Ut=r("span"),u(Ue.$$.fragment),no=c(),Gt=r("span"),lo=n("Optimizer"),Ja=c(),W=r("div"),u(Ge.$$.fragment),co=c(),Vt=r("p"),io=n("Internal wrapper around a torch optimizer."),Ka=c(),R=r("h2"),ge=r("a"),Bt=r("span"),u(Ve.$$.fragment),po=c(),qt=r("span"),ho=n("DataLoader"),Qa=c(),ve=r("p"),uo=n("The main work on your PyTorch "),Ft=r("code"),mo=n("DataLoader"),fo=n(" is done by the following function:"),Ya=c(),k=r("div"),u(Be.$$.fragment),go=c(),qe=r("p"),vo=n("Wraps a PyTorch "),Wt=r("code"),_o=n("DataLoader"),bo=n(" to generate batches for one of the processes only."),$o=c(),H=r("p"),yo=n("Depending on the value of the "),Rt=r("code"),wo=n("drop_last"),Eo=n(" attribute of the "),Ht=r("code"),xo=n("dataloader"),Do=n(` passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),So=c(),u(_e.$$.fragment),Za=c(),M=r("h3"),be=r("a"),Mt=r("span"),u(Fe.$$.fragment),To=c(),jt=r("span"),Po=n("BatchSamplerShard"),er=c(),j=r("div"),u(We.$$.fragment),ko=c(),Re=r("p"),Ao=n("Subclass of a PyTorch "),Xt=r("code"),Io=n("DataLoader"),zo=n(" that will deal with device placement and current distributed setup."),tr=c(),X=r("h3"),$e=r("a"),Jt=r("span"),u(He.$$.fragment),Lo=c(),Kt=r("span"),No=n("BatchSamplerShard"),ar=c(),O=r("div"),u(Me.$$.fragment),Oo=c(),C=r("p"),Co=n("Wraps a PyTorch "),Qt=r("code"),Uo=n("BatchSampler"),Go=n(` to generate batches for one of the processes only. Instances of this class will
always yield a number of batches that is a round multiple of `),Yt=r("code"),Vo=n("num_processes"),Bo=n(` and that all have the same size.
Depending on the value of the `),Zt=r("code"),qo=n("drop_last"),Fo=n(` attribute of the batch sampler passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),Wo=c(),u(ye.$$.fragment),rr=c(),J=r("h3"),we=r("a"),ea=r("span"),u(je.$$.fragment),Ro=c(),ta=r("span"),Ho=n("IterableDatasetShard"),or=c(),K=r("div"),u(Xe.$$.fragment),Mo=c(),S=r("p"),jo=n("Wraps a PyTorch "),aa=r("code"),Xo=n("IterableDataset"),Jo=n(` to generate samples for one of the processes only. Instances of this class will
always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
`),ra=r("code"),Ko=n("split_batches"),Qo=n(", this is either "),oa=r("code"),Yo=n("batch_size"),Zo=n(" or "),sa=r("code"),es=n("batch_size x num_processes"),ts=n(`). Depending on the value of the
`),na=r("code"),as=n("drop_last"),rs=n(` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
be too small or loop with indices from the beginning.`),sr=c(),Q=r("h2"),Ee=r("a"),la=r("span"),u(Je.$$.fragment),os=c(),ca=r("span"),ss=n("Scheduler"),nr=c(),U=r("div"),u(Ke.$$.fragment),ns=c(),ia=r("p"),ls=n(`A wrapper around a learning rate scheduler that will only step when the optimizer(s) have a training step. Useful
to avoid making a scheduler step too fast when:`),cs=c(),Qe=r("ul"),da=r("li"),is=n("gradients went overflow and there was no training step (in mixed precision training)"),ds=c(),pa=r("li"),ps=n("step was skipped because of gradient accumulation"),lr=c(),Y=r("h2"),xe=r("a"),ha=r("span"),u(Ye.$$.fragment),hs=c(),ua=r("span"),us=n("Distributed Config"),cr=c(),Z=r("h3"),De=r("a"),ma=r("span"),u(Ze.$$.fragment),ms=c(),fa=r("span"),fs=n("AcceleratorState"),ir=c(),ee=r("div"),u(et.$$.fragment),gs=c(),te=r("p"),vs=n("This is a variation of a "),tt=r("a"),_s=n("singleton class"),bs=n(` in the sense that all
instance of `),ga=r("code"),$s=n("AcceleratorState"),ys=n(" share the same state, which is initialized on the first instantiation."),dr=c(),ae=r("h3"),Se=r("a"),va=r("span"),u(at.$$.fragment),ws=c(),_a=r("span"),Es=n("DistributedType"),pr=c(),A=r("div"),u(rt.$$.fragment),xs=c(),ba=r("p"),Ds=n("Represents a type of distributed environment."),Ss=c(),$a=r("p"),Ts=n("Values:"),Ps=c(),I=r("ul"),St=r("li"),ya=r("strong"),ks=n("NO"),As=n(" \u2014 Not a distributed environment, just a single process."),Is=c(),Tt=r("li"),wa=r("strong"),zs=n("MULTI_CPU"),Ls=n(" \u2014 Distributed on multiple CPU nodes."),Ns=c(),Pt=r("li"),Ea=r("strong"),Os=n("MULTI_GPU"),Cs=n(" \u2014 Distributed on multiple GPUs."),Us=c(),kt=r("li"),xa=r("strong"),Gs=n("DEEPSPEED"),Vs=n(" \u2014 Using DeepSpeed."),Bs=c(),At=r("li"),Da=r("strong"),qs=n("TPU"),Fs=n(" \u2014 Distributed on TPUs."),hr=c(),re=r("h2"),Te=r("a"),Sa=r("span"),u(ot.$$.fragment),Ws=c(),Ta=r("span"),Rs=n("Tracking"),ur=c(),T=r("div"),u(st.$$.fragment),Hs=c(),Pa=r("p"),Ms=n("A base Tracker class to be used for all logging integration implementations."),js=c(),Pe=r("div"),u(nt.$$.fragment),Xs=c(),ka=r("p"),Js=n(`Should run any finalizing functions within the tracking API. If the API should not have one, just don\u2019t
overwrite that method.`),Ks=c(),ke=r("div"),u(lt.$$.fragment),Qs=c(),oe=r("p"),Ys=n("Logs "),Aa=r("code"),Zs=n("values"),en=n(" to the current run. Base "),Ia=r("code"),tn=n("log"),an=n(" implementations of a tracking API should go in here, along with\nspecial behavior for the `step parameter."),rn=c(),Ae=r("div"),u(ct.$$.fragment),on=c(),it=r("p"),sn=n("Logs "),za=r("code"),nn=n("values"),ln=n(` as hyperparameters for the run. Implementations should use the experiment configuration
functionality of a tracking API.`),mr=c(),se=r("h2"),Ie=r("a"),La=r("span"),u(dt.$$.fragment),cn=c(),Na=r("span"),dn=n("Utilities"),fr=c(),ne=r("div"),u(pt.$$.fragment),pn=c(),Oa=r("p"),hn=n("Extract a model from its distributed containers."),gr=c(),le=r("div"),u(ht.$$.fragment),un=c(),Ca=r("p"),mn=n("Checks if bf16 is supported, optionally ignoring the TPU"),vr=c(),ce=r("div"),u(ut.$$.fragment),fn=c(),Ua=r("p"),gn=n("Compares the current PyTorch version to a given reference with an operation."),_r=c(),ie=r("div"),u(mt.$$.fragment),vn=c(),ft=r("p"),_n=n("Checks if "),Ga=r("code"),bn=n("torch_xla"),$n=n(" is installed and if a TPU is in the environment"),br=c(),de=r("div"),u(gt.$$.fragment),yn=c(),Va=r("p"),wn=n("Recursively gather tensor in a nested list/tuple/dictionary of tensors from all devices."),$r=c(),pe=r("div"),u(vt.$$.fragment),En=c(),Ba=r("p"),xn=n("Recursively sends the elements in a nested list/tuple/dictionary of tensors to a given device."),yr=c(),he=r("div"),u(_t.$$.fragment),Dn=c(),G=r("p"),Sn=n("Helper function for reproducible behavior to set the seed in "),qa=r("code"),Tn=n("random"),Pn=n(", "),Fa=r("code"),kn=n("numpy"),An=n(", "),Wa=r("code"),In=n("torch"),zn=n("."),wr=c(),bt=r("div"),u($t.$$.fragment),Er=c(),yt=r("div"),u(wt.$$.fragment),xr=c(),V=r("div"),u(Et.$$.fragment),Ln=c(),Ra=r("p"),Nn=n("Introduces a blocking point in the script, making sure all processes have reached this point before continuing."),On=c(),u(ze.$$.fragment),Dr=c(),ue=r("div"),u(xt.$$.fragment),Cn=c(),Ha=r("p"),Un=n(`Creates and saves a basic cluster config to be used on a local machine with potentially multiple GPUs. Will also
set CPU if it is a CPU-only machine.`),this.h()},l(e){const d=vc('[data-svelte="svelte-1phssyn"]',document.head);b=o(d,"META",{name:!0,content:!0}),d.forEach(a),D=i(e),$=o(e,"H1",{class:!0});var Dt=s($);w=o(Dt,"A",{id:!0,class:!0,href:!0});var Ma=s(w);P=o(Ma,"SPAN",{});var ja=s(P);m(E.$$.fragment,ja),ja.forEach(a),Ma.forEach(a),x=i(Dt),N=o(Dt,"SPAN",{});var Rn=s(N);so=l(Rn,"Internals"),Rn.forEach(a),Dt.forEach(a),Xa=i(e),F=o(e,"H2",{class:!0});var Tr=s(F);fe=o(Tr,"A",{id:!0,class:!0,href:!0});var Hn=s(fe);Ut=o(Hn,"SPAN",{});var Mn=s(Ut);m(Ue.$$.fragment,Mn),Mn.forEach(a),Hn.forEach(a),no=i(Tr),Gt=o(Tr,"SPAN",{});var jn=s(Gt);lo=l(jn,"Optimizer"),jn.forEach(a),Tr.forEach(a),Ja=i(e),W=o(e,"DIV",{class:!0});var Pr=s(W);m(Ge.$$.fragment,Pr),co=i(Pr),Vt=o(Pr,"P",{});var Xn=s(Vt);io=l(Xn,"Internal wrapper around a torch optimizer."),Xn.forEach(a),Pr.forEach(a),Ka=i(e),R=o(e,"H2",{class:!0});var kr=s(R);ge=o(kr,"A",{id:!0,class:!0,href:!0});var Jn=s(ge);Bt=o(Jn,"SPAN",{});var Kn=s(Bt);m(Ve.$$.fragment,Kn),Kn.forEach(a),Jn.forEach(a),po=i(kr),qt=o(kr,"SPAN",{});var Qn=s(qt);ho=l(Qn,"DataLoader"),Qn.forEach(a),kr.forEach(a),Qa=i(e),ve=o(e,"P",{});var Ar=s(ve);uo=l(Ar,"The main work on your PyTorch "),Ft=o(Ar,"CODE",{});var Yn=s(Ft);mo=l(Yn,"DataLoader"),Yn.forEach(a),fo=l(Ar," is done by the following function:"),Ar.forEach(a),Ya=i(e),k=o(e,"DIV",{class:!0});var Le=s(k);m(Be.$$.fragment,Le),go=i(Le),qe=o(Le,"P",{});var Ir=s(qe);vo=l(Ir,"Wraps a PyTorch "),Wt=o(Ir,"CODE",{});var Zn=s(Wt);_o=l(Zn,"DataLoader"),Zn.forEach(a),bo=l(Ir," to generate batches for one of the processes only."),Ir.forEach(a),$o=i(Le),H=o(Le,"P",{});var It=s(H);yo=l(It,"Depending on the value of the "),Rt=o(It,"CODE",{});var el=s(Rt);wo=l(el,"drop_last"),el.forEach(a),Eo=l(It," attribute of the "),Ht=o(It,"CODE",{});var tl=s(Ht);xo=l(tl,"dataloader"),tl.forEach(a),Do=l(It,` passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),It.forEach(a),So=i(Le),m(_e.$$.fragment,Le),Le.forEach(a),Za=i(e),M=o(e,"H3",{class:!0});var zr=s(M);be=o(zr,"A",{id:!0,class:!0,href:!0});var al=s(be);Mt=o(al,"SPAN",{});var rl=s(Mt);m(Fe.$$.fragment,rl),rl.forEach(a),al.forEach(a),To=i(zr),jt=o(zr,"SPAN",{});var ol=s(jt);Po=l(ol,"BatchSamplerShard"),ol.forEach(a),zr.forEach(a),er=i(e),j=o(e,"DIV",{class:!0});var Lr=s(j);m(We.$$.fragment,Lr),ko=i(Lr),Re=o(Lr,"P",{});var Nr=s(Re);Ao=l(Nr,"Subclass of a PyTorch "),Xt=o(Nr,"CODE",{});var sl=s(Xt);Io=l(sl,"DataLoader"),sl.forEach(a),zo=l(Nr," that will deal with device placement and current distributed setup."),Nr.forEach(a),Lr.forEach(a),tr=i(e),X=o(e,"H3",{class:!0});var Or=s(X);$e=o(Or,"A",{id:!0,class:!0,href:!0});var nl=s($e);Jt=o(nl,"SPAN",{});var ll=s(Jt);m(He.$$.fragment,ll),ll.forEach(a),nl.forEach(a),Lo=i(Or),Kt=o(Or,"SPAN",{});var cl=s(Kt);No=l(cl,"BatchSamplerShard"),cl.forEach(a),Or.forEach(a),ar=i(e),O=o(e,"DIV",{class:!0});var zt=s(O);m(Me.$$.fragment,zt),Oo=i(zt),C=o(zt,"P",{});var Ne=s(C);Co=l(Ne,"Wraps a PyTorch "),Qt=o(Ne,"CODE",{});var il=s(Qt);Uo=l(il,"BatchSampler"),il.forEach(a),Go=l(Ne,` to generate batches for one of the processes only. Instances of this class will
always yield a number of batches that is a round multiple of `),Yt=o(Ne,"CODE",{});var dl=s(Yt);Vo=l(dl,"num_processes"),dl.forEach(a),Bo=l(Ne,` and that all have the same size.
Depending on the value of the `),Zt=o(Ne,"CODE",{});var pl=s(Zt);qo=l(pl,"drop_last"),pl.forEach(a),Fo=l(Ne,` attribute of the batch sampler passed, it will either stop the iteration
at the first batch that would be too small / not present on all processes or loop with indices from the beginning.`),Ne.forEach(a),Wo=i(zt),m(ye.$$.fragment,zt),zt.forEach(a),rr=i(e),J=o(e,"H3",{class:!0});var Cr=s(J);we=o(Cr,"A",{id:!0,class:!0,href:!0});var hl=s(we);ea=o(hl,"SPAN",{});var ul=s(ea);m(je.$$.fragment,ul),ul.forEach(a),hl.forEach(a),Ro=i(Cr),ta=o(Cr,"SPAN",{});var ml=s(ta);Ho=l(ml,"IterableDatasetShard"),ml.forEach(a),Cr.forEach(a),or=i(e),K=o(e,"DIV",{class:!0});var Ur=s(K);m(Xe.$$.fragment,Ur),Mo=i(Ur),S=o(Ur,"P",{});var z=s(S);jo=l(z,"Wraps a PyTorch "),aa=o(z,"CODE",{});var fl=s(aa);Xo=l(fl,"IterableDataset"),fl.forEach(a),Jo=l(z,` to generate samples for one of the processes only. Instances of this class will
always yield a number of samples that is a round multiple of the actual batch size (depending of the value of
`),ra=o(z,"CODE",{});var gl=s(ra);Ko=l(gl,"split_batches"),gl.forEach(a),Qo=l(z,", this is either "),oa=o(z,"CODE",{});var vl=s(oa);Yo=l(vl,"batch_size"),vl.forEach(a),Zo=l(z," or "),sa=o(z,"CODE",{});var _l=s(sa);es=l(_l,"batch_size x num_processes"),_l.forEach(a),ts=l(z,`). Depending on the value of the
`),na=o(z,"CODE",{});var bl=s(na);as=l(bl,"drop_last"),bl.forEach(a),rs=l(z,` attribute of the batch sampler passed, it will either stop the iteration at the first batch that would
be too small or loop with indices from the beginning.`),z.forEach(a),Ur.forEach(a),sr=i(e),Q=o(e,"H2",{class:!0});var Gr=s(Q);Ee=o(Gr,"A",{id:!0,class:!0,href:!0});var $l=s(Ee);la=o($l,"SPAN",{});var yl=s(la);m(Je.$$.fragment,yl),yl.forEach(a),$l.forEach(a),os=i(Gr),ca=o(Gr,"SPAN",{});var wl=s(ca);ss=l(wl,"Scheduler"),wl.forEach(a),Gr.forEach(a),nr=i(e),U=o(e,"DIV",{class:!0});var Lt=s(U);m(Ke.$$.fragment,Lt),ns=i(Lt),ia=o(Lt,"P",{});var El=s(ia);ls=l(El,`A wrapper around a learning rate scheduler that will only step when the optimizer(s) have a training step. Useful
to avoid making a scheduler step too fast when:`),El.forEach(a),cs=i(Lt),Qe=o(Lt,"UL",{});var Vr=s(Qe);da=o(Vr,"LI",{});var xl=s(da);is=l(xl,"gradients went overflow and there was no training step (in mixed precision training)"),xl.forEach(a),ds=i(Vr),pa=o(Vr,"LI",{});var Dl=s(pa);ps=l(Dl,"step was skipped because of gradient accumulation"),Dl.forEach(a),Vr.forEach(a),Lt.forEach(a),lr=i(e),Y=o(e,"H2",{class:!0});var Br=s(Y);xe=o(Br,"A",{id:!0,class:!0,href:!0});var Sl=s(xe);ha=o(Sl,"SPAN",{});var Tl=s(ha);m(Ye.$$.fragment,Tl),Tl.forEach(a),Sl.forEach(a),hs=i(Br),ua=o(Br,"SPAN",{});var Pl=s(ua);us=l(Pl,"Distributed Config"),Pl.forEach(a),Br.forEach(a),cr=i(e),Z=o(e,"H3",{class:!0});var qr=s(Z);De=o(qr,"A",{id:!0,class:!0,href:!0});var kl=s(De);ma=o(kl,"SPAN",{});var Al=s(ma);m(Ze.$$.fragment,Al),Al.forEach(a),kl.forEach(a),ms=i(qr),fa=o(qr,"SPAN",{});var Il=s(fa);fs=l(Il,"AcceleratorState"),Il.forEach(a),qr.forEach(a),ir=i(e),ee=o(e,"DIV",{class:!0});var Fr=s(ee);m(et.$$.fragment,Fr),gs=i(Fr),te=o(Fr,"P",{});var Nt=s(te);vs=l(Nt,"This is a variation of a "),tt=o(Nt,"A",{href:!0,rel:!0});var zl=s(tt);_s=l(zl,"singleton class"),zl.forEach(a),bs=l(Nt,` in the sense that all
instance of `),ga=o(Nt,"CODE",{});var Ll=s(ga);$s=l(Ll,"AcceleratorState"),Ll.forEach(a),ys=l(Nt," share the same state, which is initialized on the first instantiation."),Nt.forEach(a),Fr.forEach(a),dr=i(e),ae=o(e,"H3",{class:!0});var Wr=s(ae);Se=o(Wr,"A",{id:!0,class:!0,href:!0});var Nl=s(Se);va=o(Nl,"SPAN",{});var Ol=s(va);m(at.$$.fragment,Ol),Ol.forEach(a),Nl.forEach(a),ws=i(Wr),_a=o(Wr,"SPAN",{});var Cl=s(_a);Es=l(Cl,"DistributedType"),Cl.forEach(a),Wr.forEach(a),pr=i(e),A=o(e,"DIV",{class:!0});var Oe=s(A);m(rt.$$.fragment,Oe),xs=i(Oe),ba=o(Oe,"P",{});var Ul=s(ba);Ds=l(Ul,"Represents a type of distributed environment."),Ul.forEach(a),Ss=i(Oe),$a=o(Oe,"P",{});var Gl=s($a);Ts=l(Gl,"Values:"),Gl.forEach(a),Ps=i(Oe),I=o(Oe,"UL",{});var B=s(I);St=o(B,"LI",{});var Gn=s(St);ya=o(Gn,"STRONG",{});var Vl=s(ya);ks=l(Vl,"NO"),Vl.forEach(a),As=l(Gn," \u2014 Not a distributed environment, just a single process."),Gn.forEach(a),Is=i(B),Tt=o(B,"LI",{});var Vn=s(Tt);wa=o(Vn,"STRONG",{});var Bl=s(wa);zs=l(Bl,"MULTI_CPU"),Bl.forEach(a),Ls=l(Vn," \u2014 Distributed on multiple CPU nodes."),Vn.forEach(a),Ns=i(B),Pt=o(B,"LI",{});var Bn=s(Pt);Ea=o(Bn,"STRONG",{});var ql=s(Ea);Os=l(ql,"MULTI_GPU"),ql.forEach(a),Cs=l(Bn," \u2014 Distributed on multiple GPUs."),Bn.forEach(a),Us=i(B),kt=o(B,"LI",{});var qn=s(kt);xa=o(qn,"STRONG",{});var Fl=s(xa);Gs=l(Fl,"DEEPSPEED"),Fl.forEach(a),Vs=l(qn," \u2014 Using DeepSpeed."),qn.forEach(a),Bs=i(B),At=o(B,"LI",{});var Fn=s(At);Da=o(Fn,"STRONG",{});var Wl=s(Da);qs=l(Wl,"TPU"),Wl.forEach(a),Fs=l(Fn," \u2014 Distributed on TPUs."),Fn.forEach(a),B.forEach(a),Oe.forEach(a),hr=i(e),re=o(e,"H2",{class:!0});var Rr=s(re);Te=o(Rr,"A",{id:!0,class:!0,href:!0});var Rl=s(Te);Sa=o(Rl,"SPAN",{});var Hl=s(Sa);m(ot.$$.fragment,Hl),Hl.forEach(a),Rl.forEach(a),Ws=i(Rr),Ta=o(Rr,"SPAN",{});var Ml=s(Ta);Rs=l(Ml,"Tracking"),Ml.forEach(a),Rr.forEach(a),ur=i(e),T=o(e,"DIV",{class:!0});var q=s(T);m(st.$$.fragment,q),Hs=i(q),Pa=o(q,"P",{});var jl=s(Pa);Ms=l(jl,"A base Tracker class to be used for all logging integration implementations."),jl.forEach(a),js=i(q),Pe=o(q,"DIV",{class:!0});var Hr=s(Pe);m(nt.$$.fragment,Hr),Xs=i(Hr),ka=o(Hr,"P",{});var Xl=s(ka);Js=l(Xl,`Should run any finalizing functions within the tracking API. If the API should not have one, just don\u2019t
overwrite that method.`),Xl.forEach(a),Hr.forEach(a),Ks=i(q),ke=o(q,"DIV",{class:!0});var Mr=s(ke);m(lt.$$.fragment,Mr),Qs=i(Mr),oe=o(Mr,"P",{});var Ot=s(oe);Ys=l(Ot,"Logs "),Aa=o(Ot,"CODE",{});var Jl=s(Aa);Zs=l(Jl,"values"),Jl.forEach(a),en=l(Ot," to the current run. Base "),Ia=o(Ot,"CODE",{});var Kl=s(Ia);tn=l(Kl,"log"),Kl.forEach(a),an=l(Ot," implementations of a tracking API should go in here, along with\nspecial behavior for the `step parameter."),Ot.forEach(a),Mr.forEach(a),rn=i(q),Ae=o(q,"DIV",{class:!0});var jr=s(Ae);m(ct.$$.fragment,jr),on=i(jr),it=o(jr,"P",{});var Xr=s(it);sn=l(Xr,"Logs "),za=o(Xr,"CODE",{});var Ql=s(za);nn=l(Ql,"values"),Ql.forEach(a),ln=l(Xr,` as hyperparameters for the run. Implementations should use the experiment configuration
functionality of a tracking API.`),Xr.forEach(a),jr.forEach(a),q.forEach(a),mr=i(e),se=o(e,"H2",{class:!0});var Jr=s(se);Ie=o(Jr,"A",{id:!0,class:!0,href:!0});var Yl=s(Ie);La=o(Yl,"SPAN",{});var Zl=s(La);m(dt.$$.fragment,Zl),Zl.forEach(a),Yl.forEach(a),cn=i(Jr),Na=o(Jr,"SPAN",{});var ec=s(Na);dn=l(ec,"Utilities"),ec.forEach(a),Jr.forEach(a),fr=i(e),ne=o(e,"DIV",{class:!0});var Kr=s(ne);m(pt.$$.fragment,Kr),pn=i(Kr),Oa=o(Kr,"P",{});var tc=s(Oa);hn=l(tc,"Extract a model from its distributed containers."),tc.forEach(a),Kr.forEach(a),gr=i(e),le=o(e,"DIV",{class:!0});var Qr=s(le);m(ht.$$.fragment,Qr),un=i(Qr),Ca=o(Qr,"P",{});var ac=s(Ca);mn=l(ac,"Checks if bf16 is supported, optionally ignoring the TPU"),ac.forEach(a),Qr.forEach(a),vr=i(e),ce=o(e,"DIV",{class:!0});var Yr=s(ce);m(ut.$$.fragment,Yr),fn=i(Yr),Ua=o(Yr,"P",{});var rc=s(Ua);gn=l(rc,"Compares the current PyTorch version to a given reference with an operation."),rc.forEach(a),Yr.forEach(a),_r=i(e),ie=o(e,"DIV",{class:!0});var Zr=s(ie);m(mt.$$.fragment,Zr),vn=i(Zr),ft=o(Zr,"P",{});var eo=s(ft);_n=l(eo,"Checks if "),Ga=o(eo,"CODE",{});var oc=s(Ga);bn=l(oc,"torch_xla"),oc.forEach(a),$n=l(eo," is installed and if a TPU is in the environment"),eo.forEach(a),Zr.forEach(a),br=i(e),de=o(e,"DIV",{class:!0});var to=s(de);m(gt.$$.fragment,to),yn=i(to),Va=o(to,"P",{});var sc=s(Va);wn=l(sc,"Recursively gather tensor in a nested list/tuple/dictionary of tensors from all devices."),sc.forEach(a),to.forEach(a),$r=i(e),pe=o(e,"DIV",{class:!0});var ao=s(pe);m(vt.$$.fragment,ao),En=i(ao),Ba=o(ao,"P",{});var nc=s(Ba);xn=l(nc,"Recursively sends the elements in a nested list/tuple/dictionary of tensors to a given device."),nc.forEach(a),ao.forEach(a),yr=i(e),he=o(e,"DIV",{class:!0});var ro=s(he);m(_t.$$.fragment,ro),Dn=i(ro),G=o(ro,"P",{});var Ce=s(G);Sn=l(Ce,"Helper function for reproducible behavior to set the seed in "),qa=o(Ce,"CODE",{});var lc=s(qa);Tn=l(lc,"random"),lc.forEach(a),Pn=l(Ce,", "),Fa=o(Ce,"CODE",{});var cc=s(Fa);kn=l(cc,"numpy"),cc.forEach(a),An=l(Ce,", "),Wa=o(Ce,"CODE",{});var ic=s(Wa);In=l(ic,"torch"),ic.forEach(a),zn=l(Ce,"."),Ce.forEach(a),ro.forEach(a),wr=i(e),bt=o(e,"DIV",{class:!0});var dc=s(bt);m($t.$$.fragment,dc),dc.forEach(a),Er=i(e),yt=o(e,"DIV",{class:!0});var pc=s(yt);m(wt.$$.fragment,pc),pc.forEach(a),xr=i(e),V=o(e,"DIV",{class:!0});var Ct=s(V);m(Et.$$.fragment,Ct),Ln=i(Ct),Ra=o(Ct,"P",{});var hc=s(Ra);Nn=l(hc,"Introduces a blocking point in the script, making sure all processes have reached this point before continuing."),hc.forEach(a),On=i(Ct),m(ze.$$.fragment,Ct),Ct.forEach(a),Dr=i(e),ue=o(e,"DIV",{class:!0});var oo=s(ue);m(xt.$$.fragment,oo),Cn=i(oo),Ha=o(oo,"P",{});var uc=s(Ha);Un=l(uc,`Creates and saves a basic cluster config to be used on a local machine with potentially multiple GPUs. Will also
set CPU if it is a CPU-only machine.`),uc.forEach(a),oo.forEach(a),this.h()},h(){p(b,"name","hf:doc:metadata"),p(b,"content",JSON.stringify(Ec)),p(w,"id","internals"),p(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(w,"href","#internals"),p($,"class","relative group"),p(fe,"id","accelerate.optimizer.AcceleratedOptimizer"),p(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(fe,"href","#accelerate.optimizer.AcceleratedOptimizer"),p(F,"class","relative group"),p(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ge,"id","accelerate.data_loader.prepare_data_loader"),p(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ge,"href","#accelerate.data_loader.prepare_data_loader"),p(R,"class","relative group"),p(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(be,"id","accelerate.data_loader.DataLoaderShard"),p(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(be,"href","#accelerate.data_loader.DataLoaderShard"),p(M,"class","relative group"),p(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p($e,"id","accelerate.data_loader.BatchSamplerShard"),p($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p($e,"href","#accelerate.data_loader.BatchSamplerShard"),p(X,"class","relative group"),p(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(we,"id","accelerate.data_loader.IterableDatasetShard"),p(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(we,"href","#accelerate.data_loader.IterableDatasetShard"),p(J,"class","relative group"),p(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ee,"id","accelerate.scheduler.AcceleratedScheduler"),p(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ee,"href","#accelerate.scheduler.AcceleratedScheduler"),p(Q,"class","relative group"),p(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(xe,"id","distributed-config"),p(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(xe,"href","#distributed-config"),p(Y,"class","relative group"),p(De,"id","accelerate.state.AcceleratorState"),p(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(De,"href","#accelerate.state.AcceleratorState"),p(Z,"class","relative group"),p(tt,"href","https://en.wikipedia.org/wiki/Singleton_pattern"),p(tt,"rel","nofollow"),p(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Se,"id","accelerate.DistributedType"),p(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Se,"href","#accelerate.DistributedType"),p(ae,"class","relative group"),p(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Te,"id","accelerate.tracking.GeneralTracker"),p(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Te,"href","#accelerate.tracking.GeneralTracker"),p(re,"class","relative group"),p(Pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ie,"id","accelerate.utils.extract_model_from_parallel"),p(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ie,"href","#accelerate.utils.extract_model_from_parallel"),p(se,"class","relative group"),p(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,d){t(document.head,b),h(e,D,d),h(e,$,d),t($,w),t(w,P),f(E,P,null),t($,x),t($,N),t(N,so),h(e,Xa,d),h(e,F,d),t(F,fe),t(fe,Ut),f(Ue,Ut,null),t(F,no),t(F,Gt),t(Gt,lo),h(e,Ja,d),h(e,W,d),f(Ge,W,null),t(W,co),t(W,Vt),t(Vt,io),h(e,Ka,d),h(e,R,d),t(R,ge),t(ge,Bt),f(Ve,Bt,null),t(R,po),t(R,qt),t(qt,ho),h(e,Qa,d),h(e,ve,d),t(ve,uo),t(ve,Ft),t(Ft,mo),t(ve,fo),h(e,Ya,d),h(e,k,d),f(Be,k,null),t(k,go),t(k,qe),t(qe,vo),t(qe,Wt),t(Wt,_o),t(qe,bo),t(k,$o),t(k,H),t(H,yo),t(H,Rt),t(Rt,wo),t(H,Eo),t(H,Ht),t(Ht,xo),t(H,Do),t(k,So),f(_e,k,null),h(e,Za,d),h(e,M,d),t(M,be),t(be,Mt),f(Fe,Mt,null),t(M,To),t(M,jt),t(jt,Po),h(e,er,d),h(e,j,d),f(We,j,null),t(j,ko),t(j,Re),t(Re,Ao),t(Re,Xt),t(Xt,Io),t(Re,zo),h(e,tr,d),h(e,X,d),t(X,$e),t($e,Jt),f(He,Jt,null),t(X,Lo),t(X,Kt),t(Kt,No),h(e,ar,d),h(e,O,d),f(Me,O,null),t(O,Oo),t(O,C),t(C,Co),t(C,Qt),t(Qt,Uo),t(C,Go),t(C,Yt),t(Yt,Vo),t(C,Bo),t(C,Zt),t(Zt,qo),t(C,Fo),t(O,Wo),f(ye,O,null),h(e,rr,d),h(e,J,d),t(J,we),t(we,ea),f(je,ea,null),t(J,Ro),t(J,ta),t(ta,Ho),h(e,or,d),h(e,K,d),f(Xe,K,null),t(K,Mo),t(K,S),t(S,jo),t(S,aa),t(aa,Xo),t(S,Jo),t(S,ra),t(ra,Ko),t(S,Qo),t(S,oa),t(oa,Yo),t(S,Zo),t(S,sa),t(sa,es),t(S,ts),t(S,na),t(na,as),t(S,rs),h(e,sr,d),h(e,Q,d),t(Q,Ee),t(Ee,la),f(Je,la,null),t(Q,os),t(Q,ca),t(ca,ss),h(e,nr,d),h(e,U,d),f(Ke,U,null),t(U,ns),t(U,ia),t(ia,ls),t(U,cs),t(U,Qe),t(Qe,da),t(da,is),t(Qe,ds),t(Qe,pa),t(pa,ps),h(e,lr,d),h(e,Y,d),t(Y,xe),t(xe,ha),f(Ye,ha,null),t(Y,hs),t(Y,ua),t(ua,us),h(e,cr,d),h(e,Z,d),t(Z,De),t(De,ma),f(Ze,ma,null),t(Z,ms),t(Z,fa),t(fa,fs),h(e,ir,d),h(e,ee,d),f(et,ee,null),t(ee,gs),t(ee,te),t(te,vs),t(te,tt),t(tt,_s),t(te,bs),t(te,ga),t(ga,$s),t(te,ys),h(e,dr,d),h(e,ae,d),t(ae,Se),t(Se,va),f(at,va,null),t(ae,ws),t(ae,_a),t(_a,Es),h(e,pr,d),h(e,A,d),f(rt,A,null),t(A,xs),t(A,ba),t(ba,Ds),t(A,Ss),t(A,$a),t($a,Ts),t(A,Ps),t(A,I),t(I,St),t(St,ya),t(ya,ks),t(St,As),t(I,Is),t(I,Tt),t(Tt,wa),t(wa,zs),t(Tt,Ls),t(I,Ns),t(I,Pt),t(Pt,Ea),t(Ea,Os),t(Pt,Cs),t(I,Us),t(I,kt),t(kt,xa),t(xa,Gs),t(kt,Vs),t(I,Bs),t(I,At),t(At,Da),t(Da,qs),t(At,Fs),h(e,hr,d),h(e,re,d),t(re,Te),t(Te,Sa),f(ot,Sa,null),t(re,Ws),t(re,Ta),t(Ta,Rs),h(e,ur,d),h(e,T,d),f(st,T,null),t(T,Hs),t(T,Pa),t(Pa,Ms),t(T,js),t(T,Pe),f(nt,Pe,null),t(Pe,Xs),t(Pe,ka),t(ka,Js),t(T,Ks),t(T,ke),f(lt,ke,null),t(ke,Qs),t(ke,oe),t(oe,Ys),t(oe,Aa),t(Aa,Zs),t(oe,en),t(oe,Ia),t(Ia,tn),t(oe,an),t(T,rn),t(T,Ae),f(ct,Ae,null),t(Ae,on),t(Ae,it),t(it,sn),t(it,za),t(za,nn),t(it,ln),h(e,mr,d),h(e,se,d),t(se,Ie),t(Ie,La),f(dt,La,null),t(se,cn),t(se,Na),t(Na,dn),h(e,fr,d),h(e,ne,d),f(pt,ne,null),t(ne,pn),t(ne,Oa),t(Oa,hn),h(e,gr,d),h(e,le,d),f(ht,le,null),t(le,un),t(le,Ca),t(Ca,mn),h(e,vr,d),h(e,ce,d),f(ut,ce,null),t(ce,fn),t(ce,Ua),t(Ua,gn),h(e,_r,d),h(e,ie,d),f(mt,ie,null),t(ie,vn),t(ie,ft),t(ft,_n),t(ft,Ga),t(Ga,bn),t(ft,$n),h(e,br,d),h(e,de,d),f(gt,de,null),t(de,yn),t(de,Va),t(Va,wn),h(e,$r,d),h(e,pe,d),f(vt,pe,null),t(pe,En),t(pe,Ba),t(Ba,xn),h(e,yr,d),h(e,he,d),f(_t,he,null),t(he,Dn),t(he,G),t(G,Sn),t(G,qa),t(qa,Tn),t(G,Pn),t(G,Fa),t(Fa,kn),t(G,An),t(G,Wa),t(Wa,In),t(G,zn),h(e,wr,d),h(e,bt,d),f($t,bt,null),h(e,Er,d),h(e,yt,d),f(wt,yt,null),h(e,xr,d),h(e,V,d),f(Et,V,null),t(V,Ln),t(V,Ra),t(Ra,Nn),t(V,On),f(ze,V,null),h(e,Dr,d),h(e,ue,d),f(xt,ue,null),t(ue,Cn),t(ue,Ha),t(Ha,Un),Sr=!0},p(e,[d]){const Dt={};d&2&&(Dt.$$scope={dirty:d,ctx:e}),_e.$set(Dt);const Ma={};d&2&&(Ma.$$scope={dirty:d,ctx:e}),ye.$set(Ma);const ja={};d&2&&(ja.$$scope={dirty:d,ctx:e}),ze.$set(ja)},i(e){Sr||(g(E.$$.fragment,e),g(Ue.$$.fragment,e),g(Ge.$$.fragment,e),g(Ve.$$.fragment,e),g(Be.$$.fragment,e),g(_e.$$.fragment,e),g(Fe.$$.fragment,e),g(We.$$.fragment,e),g(He.$$.fragment,e),g(Me.$$.fragment,e),g(ye.$$.fragment,e),g(je.$$.fragment,e),g(Xe.$$.fragment,e),g(Je.$$.fragment,e),g(Ke.$$.fragment,e),g(Ye.$$.fragment,e),g(Ze.$$.fragment,e),g(et.$$.fragment,e),g(at.$$.fragment,e),g(rt.$$.fragment,e),g(ot.$$.fragment,e),g(st.$$.fragment,e),g(nt.$$.fragment,e),g(lt.$$.fragment,e),g(ct.$$.fragment,e),g(dt.$$.fragment,e),g(pt.$$.fragment,e),g(ht.$$.fragment,e),g(ut.$$.fragment,e),g(mt.$$.fragment,e),g(gt.$$.fragment,e),g(vt.$$.fragment,e),g(_t.$$.fragment,e),g($t.$$.fragment,e),g(wt.$$.fragment,e),g(Et.$$.fragment,e),g(ze.$$.fragment,e),g(xt.$$.fragment,e),Sr=!0)},o(e){v(E.$$.fragment,e),v(Ue.$$.fragment,e),v(Ge.$$.fragment,e),v(Ve.$$.fragment,e),v(Be.$$.fragment,e),v(_e.$$.fragment,e),v(Fe.$$.fragment,e),v(We.$$.fragment,e),v(He.$$.fragment,e),v(Me.$$.fragment,e),v(ye.$$.fragment,e),v(je.$$.fragment,e),v(Xe.$$.fragment,e),v(Je.$$.fragment,e),v(Ke.$$.fragment,e),v(Ye.$$.fragment,e),v(Ze.$$.fragment,e),v(et.$$.fragment,e),v(at.$$.fragment,e),v(rt.$$.fragment,e),v(ot.$$.fragment,e),v(st.$$.fragment,e),v(nt.$$.fragment,e),v(lt.$$.fragment,e),v(ct.$$.fragment,e),v(dt.$$.fragment,e),v(pt.$$.fragment,e),v(ht.$$.fragment,e),v(ut.$$.fragment,e),v(mt.$$.fragment,e),v(gt.$$.fragment,e),v(vt.$$.fragment,e),v(_t.$$.fragment,e),v($t.$$.fragment,e),v(wt.$$.fragment,e),v(Et.$$.fragment,e),v(ze.$$.fragment,e),v(xt.$$.fragment,e),Sr=!1},d(e){a(b),e&&a(D),e&&a($),_(E),e&&a(Xa),e&&a(F),_(Ue),e&&a(Ja),e&&a(W),_(Ge),e&&a(Ka),e&&a(R),_(Ve),e&&a(Qa),e&&a(ve),e&&a(Ya),e&&a(k),_(Be),_(_e),e&&a(Za),e&&a(M),_(Fe),e&&a(er),e&&a(j),_(We),e&&a(tr),e&&a(X),_(He),e&&a(ar),e&&a(O),_(Me),_(ye),e&&a(rr),e&&a(J),_(je),e&&a(or),e&&a(K),_(Xe),e&&a(sr),e&&a(Q),_(Je),e&&a(nr),e&&a(U),_(Ke),e&&a(lr),e&&a(Y),_(Ye),e&&a(cr),e&&a(Z),_(Ze),e&&a(ir),e&&a(ee),_(et),e&&a(dr),e&&a(ae),_(at),e&&a(pr),e&&a(A),_(rt),e&&a(hr),e&&a(re),_(ot),e&&a(ur),e&&a(T),_(st),_(nt),_(lt),_(ct),e&&a(mr),e&&a(se),_(dt),e&&a(fr),e&&a(ne),_(pt),e&&a(gr),e&&a(le),_(ht),e&&a(vr),e&&a(ce),_(ut),e&&a(_r),e&&a(ie),_(mt),e&&a(br),e&&a(de),_(gt),e&&a($r),e&&a(pe),_(vt),e&&a(yr),e&&a(he),_(_t),e&&a(wr),e&&a(bt),_($t),e&&a(Er),e&&a(yt),_(wt),e&&a(xr),e&&a(V),_(Et),_(ze),e&&a(Dr),e&&a(ue),_(xt)}}}const Ec={local:"internals",sections:[{local:"accelerate.optimizer.AcceleratedOptimizer",title:"Optimizer"},{local:"accelerate.data_loader.prepare_data_loader",sections:[{local:"accelerate.data_loader.DataLoaderShard",title:"BatchSamplerShard"},{local:"accelerate.data_loader.BatchSamplerShard",title:"BatchSamplerShard"},{local:"accelerate.data_loader.IterableDatasetShard",title:"IterableDatasetShard"}],title:"DataLoader"},{local:"accelerate.scheduler.AcceleratedScheduler",title:"Scheduler"},{local:"distributed-config",sections:[{local:"accelerate.state.AcceleratorState",title:"AcceleratorState"},{local:"accelerate.DistributedType",title:"DistributedType"}],title:"Distributed Config"},{local:"accelerate.tracking.GeneralTracker",title:"Tracking"},{local:"accelerate.utils.extract_model_from_parallel",title:"Utilities"}],title:"Internals"};function xc(me){return _c(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kc extends mc{constructor(b){super();fc(this,b,xc,wc,gc,{})}}export{kc as default,Ec as metadata};
