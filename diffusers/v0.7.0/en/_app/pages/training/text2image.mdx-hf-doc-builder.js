import{S as mn,i as _n,s as gn,e as o,k as u,w as _,t as r,M as vn,c as s,d as t,m as h,a as i,x as g,h as l,b as f,G as a,g as p,y as v,q as $,o as b,B as w,v as $n}from"../../chunks/vendor-hf-doc-builder.js";import{T as bn}from"../../chunks/Tip-hf-doc-builder.js";import{I as ye}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as he}from"../../chunks/CodeBlock-hf-doc-builder.js";function wn(Le){let c,N;return{c(){c=o("p"),N=r("The text-to-image fine-tuning script is experimental. It\u2019s easy to overfit and run into issues like catastrophic forgetting. We recommend to explore different hyperparameters to get the best results on your dataset.")},l(d){c=s(d,"P",{});var y=i(c);N=l(y,"The text-to-image fine-tuning script is experimental. It\u2019s easy to overfit and run into issues like catastrophic forgetting. We recommend to explore different hyperparameters to get the best results on your dataset."),y.forEach(t)},m(d,y){p(d,c,y),a(c,N)},d(d){d&&t(c)}}}function yn(Le){let c,N,d,y,xe,Y,Pt,Ee,At,Ye,O,qt,z,ke,jt,Dt,ze,I,Be,q,M,Pe,B,Tt,Ae,Ut,Je,j,S,qe,J,Nt,je,Ot,Ve,ce,It,Xe,V,We,F,Mt,X,St,Ft,Ke,W,Qe,x,Rt,De,Ht,Ct,K,Gt,Lt,Ze,R,Yt,Q,zt,Bt,et,de,Jt,tt,Z,at,me,Vt,nt,D,H,Te,ee,Xt,Ue,Wt,ot,m,Kt,Ne,Qt,Zt,Oe,ea,ta,Ie,aa,na,_e,oa,sa,st,T,C,Me,te,ia,Se,ra,it,G,la,ae,pa,fa,rt,ne,lt,E,ua,Fe,ha,ca,oe,da,ma,pt,ge,_a,ft,se,ut,k,ga,Re,va,$a,He,ba,wa,ht,ie,ct,U,L,Ce,re,ya,Ge,xa,dt,P,Ea,le,ka,Pa,pe,Aa,qa,mt,fe,_t;return Y=new ye({}),I=new bn({props:{warning:!0,$$slots:{default:[wn]},$$scope:{ctx:Le}}}),B=new ye({}),J=new ye({}),V=new he({props:{code:`pip install git+https://github.com/huggingface/diffusers.git
pip install -U -r requirements.txt`,highlighted:`pip install git+https://github.com/huggingface/diffusers.git
pip install -U -r requirements.txt`}}),W=new he({props:{code:"accelerate config",highlighted:"accelerate config"}}),Z=new he({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),ee=new ye({}),te=new ye({}),ne=new he({props:{code:`export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export dataset_name="lambdalabs/pokemon-blip-captions"

accelerate launch train_text_to_image.py \\
  --pretrained_model_name_or_path=$MODEL_NAME \\
  --dataset_name=$dataset_name \\
  --use_ema \\
  --resolution=512 --center_crop --random_flip \\
  --train_batch_size=1 \\
  --gradient_accumulation_steps=4 \\
  --gradient_checkpointing \\
  --mixed_precision="fp16" \\
  --max_train_steps=15000 \\
  --learning_rate=1e-05 \\
  --max_grad_norm=1 \\
  --lr_scheduler="constant" --lr_warmup_steps=0 \\
  --output_dir="sd-pokemon-model" `,highlighted:`<span class="hljs-built_in">export</span> MODEL_NAME=<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>
<span class="hljs-built_in">export</span> dataset_name=<span class="hljs-string">&quot;lambdalabs/pokemon-blip-captions&quot;</span>

accelerate launch train_text_to_image.py \\
  --pretrained_model_name_or_path=<span class="hljs-variable">$MODEL_NAME</span> \\
  --dataset_name=<span class="hljs-variable">$dataset_name</span> \\
  --use_ema \\
  --resolution=512 --center_crop --random_flip \\
  --train_batch_size=1 \\
  --gradient_accumulation_steps=4 \\
  --gradient_checkpointing \\
  --mixed_precision=<span class="hljs-string">&quot;fp16&quot;</span> \\
  --max_train_steps=15000 \\
  --learning_rate=1e-05 \\
  --max_grad_norm=1 \\
  --lr_scheduler=<span class="hljs-string">&quot;constant&quot;</span> --lr_warmup_steps=0 \\
  --output_dir=<span class="hljs-string">&quot;sd-pokemon-model&quot;</span> `}}),se=new he({props:{code:`export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export TRAIN_DIR="path_to_your_dataset"
export OUTPUT_DIR="path_to_save_model"

accelerate launch train_text_to_image.py \\
  --pretrained_model_name_or_path=$MODEL_NAME \\
  --train_data_dir=$TRAIN_DIR \\
  --use_ema \\
  --resolution=512 --center_crop --random_flip \\
  --train_batch_size=1 \\
  --gradient_accumulation_steps=4 \\
  --gradient_checkpointing \\
  --mixed_precision="fp16" \\
  --max_train_steps=15000 \\
  --learning_rate=1e-05 \\
  --max_grad_norm=1 \\
  --lr_scheduler="constant" --lr_warmup_steps=0 \\
  --output_dir=\${OUTPUT_DIR}`,highlighted:`<span class="hljs-built_in">export</span> MODEL_NAME=<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>
<span class="hljs-built_in">export</span> TRAIN_DIR=<span class="hljs-string">&quot;path_to_your_dataset&quot;</span>
<span class="hljs-built_in">export</span> OUTPUT_DIR=<span class="hljs-string">&quot;path_to_save_model&quot;</span>

accelerate launch train_text_to_image.py \\
  --pretrained_model_name_or_path=<span class="hljs-variable">$MODEL_NAME</span> \\
  --train_data_dir=<span class="hljs-variable">$TRAIN_DIR</span> \\
  --use_ema \\
  --resolution=512 --center_crop --random_flip \\
  --train_batch_size=1 \\
  --gradient_accumulation_steps=4 \\
  --gradient_checkpointing \\
  --mixed_precision=<span class="hljs-string">&quot;fp16&quot;</span> \\
  --max_train_steps=15000 \\
  --learning_rate=1e-05 \\
  --max_grad_norm=1 \\
  --lr_scheduler=<span class="hljs-string">&quot;constant&quot;</span> --lr_warmup_steps=0 \\
  --output_dir=<span class="hljs-variable">\${OUTPUT_DIR}</span>`}}),ie=new he({props:{code:`from diffusers import StableDiffusionPipeline

model_path = "path_to_saved_model"
pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)
pipe.to("cuda")

image = pipe(prompt="yoda").images[0]
image.save("yoda-pokemon.png")`,highlighted:`<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline

model_path = <span class="hljs-string">&quot;path_to_saved_model&quot;</span>
pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16)
pipe.to(<span class="hljs-string">&quot;cuda&quot;</span>)

image = pipe(prompt=<span class="hljs-string">&quot;yoda&quot;</span>).images[<span class="hljs-number">0</span>]
image.save(<span class="hljs-string">&quot;yoda-pokemon.png&quot;</span>)`}}),re=new ye({}),fe=new he({props:{code:`export MODEL_NAME="runwayml/stable-diffusion-v1-5"
export dataset_name="lambdalabs/pokemon-blip-captions"

python train_text_to_image_flax.py \\
  --pretrained_model_name_or_path=$MODEL_NAME \\
  --dataset_name=$dataset_name \\
  --resolution=512 --center_crop --random_flip \\
  --train_batch_size=1 \\
  --max_train_steps=15000 \\
  --learning_rate=1e-05 \\
  --max_grad_norm=1 \\
  --output_dir="sd-pokemon-model" `,highlighted:`export MODEL_NAME=<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>
export dataset_name=<span class="hljs-string">&quot;lambdalabs/pokemon-blip-captions&quot;</span>

python train_text_to_image_flax.py \\
  --pretrained_model_name_or_path=$MODEL_NAME \\
  --dataset_name=$dataset_name \\
  --resolution=<span class="hljs-number">512</span> --center_crop --random_flip \\
  --train_batch_size=<span class="hljs-number">1</span> \\
  --max_train_steps=<span class="hljs-number">15000</span> \\
  --learning_rate=<span class="hljs-number">1e-05</span> \\
  --max_grad_norm=<span class="hljs-number">1</span> \\
  --output_dir=<span class="hljs-string">&quot;sd-pokemon-model&quot;</span> `}}),{c(){c=o("meta"),N=u(),d=o("h1"),y=o("a"),xe=o("span"),_(Y.$$.fragment),Pt=u(),Ee=o("span"),At=r("Stable Diffusion text-to-image fine-tuning"),Ye=u(),O=o("p"),qt=r("The "),z=o("a"),ke=o("code"),jt=r("train_text_to_image.py"),Dt=r(" script shows how to fine-tune the stable diffusion model on your own dataset."),ze=u(),_(I.$$.fragment),Be=u(),q=o("h2"),M=o("a"),Pe=o("span"),_(B.$$.fragment),Tt=u(),Ae=o("span"),Ut=r("Running locally"),Je=u(),j=o("h3"),S=o("a"),qe=o("span"),_(J.$$.fragment),Nt=u(),je=o("span"),Ot=r("Installing the dependencies"),Ve=u(),ce=o("p"),It=r("Before running the scripts, make sure to install the library\u2019s training dependencies:"),Xe=u(),_(V.$$.fragment),We=u(),F=o("p"),Mt=r("And initialize an "),X=o("a"),St=r("\u{1F917}Accelerate"),Ft=r(" environment with:"),Ke=u(),_(W.$$.fragment),Qe=u(),x=o("p"),Rt=r("You need to accept the model license before downloading or using the weights. In this example we\u2019ll use model version "),De=o("code"),Ht=r("v1-4"),Ct=r(", so you\u2019ll need to visit "),K=o("a"),Gt=r("its card"),Lt=r(", read the license and tick the checkbox if you agree."),Ze=u(),R=o("p"),Yt=r("You have to be a registered user in \u{1F917} Hugging Face Hub, and you\u2019ll also need to use an access token for the code to work. For more information on access tokens, please refer to "),Q=o("a"),zt=r("this section of the documentation"),Bt=r("."),et=u(),de=o("p"),Jt=r("Run the following command to authenticate your token"),tt=u(),_(Z.$$.fragment),at=u(),me=o("p"),Vt=r("If you have already cloned the repo, then you won\u2019t need to go through these steps. Instead, you can pass the path to your local checkout to the training script and it will be loaded from there."),nt=u(),D=o("h3"),H=o("a"),Te=o("span"),_(ee.$$.fragment),Xt=u(),Ue=o("span"),Wt=r("Hardware Requirements for Fine-tuning"),ot=u(),m=o("p"),Kt=r("Using "),Ne=o("code"),Qt=r("gradient_checkpointing"),Zt=r(" and "),Oe=o("code"),ea=r("mixed_precision"),ta=r(" it should be possible to fine tune the model on a single 24GB GPU. For higher "),Ie=o("code"),aa=r("batch_size"),na=r(" and faster training it\u2019s better to use GPUs with more than 30GB of GPU memory. You can also use JAX / Flax for fine-tuning on TPUs or GPUs, see "),_e=o("a"),oa=r("below"),sa=r(" for details."),st=u(),T=o("h3"),C=o("a"),Me=o("span"),_(te.$$.fragment),ia=u(),Se=o("span"),ra=r("Fine-tuning Example"),it=u(),G=o("p"),la=r("The following script will launch a fine-tuning run using "),ae=o("a"),pa=r("Justin Pinkneys\u2019 captioned Pokemon dataset"),fa=r(", available in Hugging Face Hub."),rt=u(),_(ne.$$.fragment),lt=u(),E=o("p"),ua=r("To run on your own training files you need to prepare the dataset according to the format required by "),Fe=o("code"),ha=r("datasets"),ca=r(". You can upload your dataset to the Hub, or you can prepare a local folder with your files. "),oe=o("a"),da=r("This documentation"),ma=r(" explains how to do it."),pt=u(),ge=o("p"),_a=r("You should modify the script if you wish to use custom loading logic. We have left pointers in the code in the appropriate places :)"),ft=u(),_(se.$$.fragment),ut=u(),k=o("p"),ga=r("Once training is finished the model will be saved to the "),Re=o("code"),va=r("OUTPUT_DIR"),$a=r(" specified in the command. To load the fine-tuned model for inference, just pass that path to "),He=o("code"),ba=r("StableDiffusionPipeline"),wa=r(":"),ht=u(),_(ie.$$.fragment),ct=u(),U=o("h3"),L=o("a"),Ce=o("span"),_(re.$$.fragment),ya=u(),Ge=o("span"),xa=r("Flax / JAX fine-tuning"),dt=u(),P=o("p"),Ea=r("Thanks to "),le=o("a"),ka=r("@duongna211"),Pa=r(" it\u2019s possible to fine-tune Stable Diffusion using Flax! This is very efficient on TPU hardware but works great on GPUs too. You can use the "),pe=o("a"),Aa=r("Flax training script"),qa=r(" like this:"),mt=u(),_(fe.$$.fragment),this.h()},l(e){const n=vn('[data-svelte="svelte-1phssyn"]',document.head);c=s(n,"META",{name:!0,content:!0}),n.forEach(t),N=h(e),d=s(e,"H1",{class:!0});var ue=i(d);y=s(ue,"A",{id:!0,class:!0,href:!0});var ja=i(y);xe=s(ja,"SPAN",{});var Da=i(xe);g(Y.$$.fragment,Da),Da.forEach(t),ja.forEach(t),Pt=h(ue),Ee=s(ue,"SPAN",{});var Ta=i(Ee);At=l(Ta,"Stable Diffusion text-to-image fine-tuning"),Ta.forEach(t),ue.forEach(t),Ye=h(e),O=s(e,"P",{});var gt=i(O);qt=l(gt,"The "),z=s(gt,"A",{href:!0,rel:!0});var Ua=i(z);ke=s(Ua,"CODE",{});var Na=i(ke);jt=l(Na,"train_text_to_image.py"),Na.forEach(t),Ua.forEach(t),Dt=l(gt," script shows how to fine-tune the stable diffusion model on your own dataset."),gt.forEach(t),ze=h(e),g(I.$$.fragment,e),Be=h(e),q=s(e,"H2",{class:!0});var vt=i(q);M=s(vt,"A",{id:!0,class:!0,href:!0});var Oa=i(M);Pe=s(Oa,"SPAN",{});var Ia=i(Pe);g(B.$$.fragment,Ia),Ia.forEach(t),Oa.forEach(t),Tt=h(vt),Ae=s(vt,"SPAN",{});var Ma=i(Ae);Ut=l(Ma,"Running locally"),Ma.forEach(t),vt.forEach(t),Je=h(e),j=s(e,"H3",{class:!0});var $t=i(j);S=s($t,"A",{id:!0,class:!0,href:!0});var Sa=i(S);qe=s(Sa,"SPAN",{});var Fa=i(qe);g(J.$$.fragment,Fa),Fa.forEach(t),Sa.forEach(t),Nt=h($t),je=s($t,"SPAN",{});var Ra=i(je);Ot=l(Ra,"Installing the dependencies"),Ra.forEach(t),$t.forEach(t),Ve=h(e),ce=s(e,"P",{});var Ha=i(ce);It=l(Ha,"Before running the scripts, make sure to install the library\u2019s training dependencies:"),Ha.forEach(t),Xe=h(e),g(V.$$.fragment,e),We=h(e),F=s(e,"P",{});var bt=i(F);Mt=l(bt,"And initialize an "),X=s(bt,"A",{href:!0,rel:!0});var Ca=i(X);St=l(Ca,"\u{1F917}Accelerate"),Ca.forEach(t),Ft=l(bt," environment with:"),bt.forEach(t),Ke=h(e),g(W.$$.fragment,e),Qe=h(e),x=s(e,"P",{});var ve=i(x);Rt=l(ve,"You need to accept the model license before downloading or using the weights. In this example we\u2019ll use model version "),De=s(ve,"CODE",{});var Ga=i(De);Ht=l(Ga,"v1-4"),Ga.forEach(t),Ct=l(ve,", so you\u2019ll need to visit "),K=s(ve,"A",{href:!0,rel:!0});var La=i(K);Gt=l(La,"its card"),La.forEach(t),Lt=l(ve,", read the license and tick the checkbox if you agree."),ve.forEach(t),Ze=h(e),R=s(e,"P",{});var wt=i(R);Yt=l(wt,"You have to be a registered user in \u{1F917} Hugging Face Hub, and you\u2019ll also need to use an access token for the code to work. For more information on access tokens, please refer to "),Q=s(wt,"A",{href:!0,rel:!0});var Ya=i(Q);zt=l(Ya,"this section of the documentation"),Ya.forEach(t),Bt=l(wt,"."),wt.forEach(t),et=h(e),de=s(e,"P",{});var za=i(de);Jt=l(za,"Run the following command to authenticate your token"),za.forEach(t),tt=h(e),g(Z.$$.fragment,e),at=h(e),me=s(e,"P",{});var Ba=i(me);Vt=l(Ba,"If you have already cloned the repo, then you won\u2019t need to go through these steps. Instead, you can pass the path to your local checkout to the training script and it will be loaded from there."),Ba.forEach(t),nt=h(e),D=s(e,"H3",{class:!0});var yt=i(D);H=s(yt,"A",{id:!0,class:!0,href:!0});var Ja=i(H);Te=s(Ja,"SPAN",{});var Va=i(Te);g(ee.$$.fragment,Va),Va.forEach(t),Ja.forEach(t),Xt=h(yt),Ue=s(yt,"SPAN",{});var Xa=i(Ue);Wt=l(Xa,"Hardware Requirements for Fine-tuning"),Xa.forEach(t),yt.forEach(t),ot=h(e),m=s(e,"P",{});var A=i(m);Kt=l(A,"Using "),Ne=s(A,"CODE",{});var Wa=i(Ne);Qt=l(Wa,"gradient_checkpointing"),Wa.forEach(t),Zt=l(A," and "),Oe=s(A,"CODE",{});var Ka=i(Oe);ea=l(Ka,"mixed_precision"),Ka.forEach(t),ta=l(A," it should be possible to fine tune the model on a single 24GB GPU. For higher "),Ie=s(A,"CODE",{});var Qa=i(Ie);aa=l(Qa,"batch_size"),Qa.forEach(t),na=l(A," and faster training it\u2019s better to use GPUs with more than 30GB of GPU memory. You can also use JAX / Flax for fine-tuning on TPUs or GPUs, see "),_e=s(A,"A",{href:!0});var Za=i(_e);oa=l(Za,"below"),Za.forEach(t),sa=l(A," for details."),A.forEach(t),st=h(e),T=s(e,"H3",{class:!0});var xt=i(T);C=s(xt,"A",{id:!0,class:!0,href:!0});var en=i(C);Me=s(en,"SPAN",{});var tn=i(Me);g(te.$$.fragment,tn),tn.forEach(t),en.forEach(t),ia=h(xt),Se=s(xt,"SPAN",{});var an=i(Se);ra=l(an,"Fine-tuning Example"),an.forEach(t),xt.forEach(t),it=h(e),G=s(e,"P",{});var Et=i(G);la=l(Et,"The following script will launch a fine-tuning run using "),ae=s(Et,"A",{href:!0,rel:!0});var nn=i(ae);pa=l(nn,"Justin Pinkneys\u2019 captioned Pokemon dataset"),nn.forEach(t),fa=l(Et,", available in Hugging Face Hub."),Et.forEach(t),rt=h(e),g(ne.$$.fragment,e),lt=h(e),E=s(e,"P",{});var $e=i(E);ua=l($e,"To run on your own training files you need to prepare the dataset according to the format required by "),Fe=s($e,"CODE",{});var on=i(Fe);ha=l(on,"datasets"),on.forEach(t),ca=l($e,". You can upload your dataset to the Hub, or you can prepare a local folder with your files. "),oe=s($e,"A",{href:!0,rel:!0});var sn=i(oe);da=l(sn,"This documentation"),sn.forEach(t),ma=l($e," explains how to do it."),$e.forEach(t),pt=h(e),ge=s(e,"P",{});var rn=i(ge);_a=l(rn,"You should modify the script if you wish to use custom loading logic. We have left pointers in the code in the appropriate places :)"),rn.forEach(t),ft=h(e),g(se.$$.fragment,e),ut=h(e),k=s(e,"P",{});var be=i(k);ga=l(be,"Once training is finished the model will be saved to the "),Re=s(be,"CODE",{});var ln=i(Re);va=l(ln,"OUTPUT_DIR"),ln.forEach(t),$a=l(be," specified in the command. To load the fine-tuned model for inference, just pass that path to "),He=s(be,"CODE",{});var pn=i(He);ba=l(pn,"StableDiffusionPipeline"),pn.forEach(t),wa=l(be,":"),be.forEach(t),ht=h(e),g(ie.$$.fragment,e),ct=h(e),U=s(e,"H3",{class:!0});var kt=i(U);L=s(kt,"A",{id:!0,class:!0,href:!0});var fn=i(L);Ce=s(fn,"SPAN",{});var un=i(Ce);g(re.$$.fragment,un),un.forEach(t),fn.forEach(t),ya=h(kt),Ge=s(kt,"SPAN",{});var hn=i(Ge);xa=l(hn,"Flax / JAX fine-tuning"),hn.forEach(t),kt.forEach(t),dt=h(e),P=s(e,"P",{});var we=i(P);Ea=l(we,"Thanks to "),le=s(we,"A",{href:!0,rel:!0});var cn=i(le);ka=l(cn,"@duongna211"),cn.forEach(t),Pa=l(we," it\u2019s possible to fine-tune Stable Diffusion using Flax! This is very efficient on TPU hardware but works great on GPUs too. You can use the "),pe=s(we,"A",{href:!0,rel:!0});var dn=i(pe);Aa=l(dn,"Flax training script"),dn.forEach(t),qa=l(we," like this:"),we.forEach(t),mt=h(e),g(fe.$$.fragment,e),this.h()},h(){f(c,"name","hf:doc:metadata"),f(c,"content",JSON.stringify(xn)),f(y,"id","stable-diffusion-texttoimage-finetuning"),f(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(y,"href","#stable-diffusion-texttoimage-finetuning"),f(d,"class","relative group"),f(z,"href","https://github.com/huggingface/diffusers/tree/main/examples/textual_inversion"),f(z,"rel","nofollow"),f(M,"id","running-locally"),f(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(M,"href","#running-locally"),f(q,"class","relative group"),f(S,"id","installing-the-dependencies"),f(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(S,"href","#installing-the-dependencies"),f(j,"class","relative group"),f(X,"href","https://github.com/huggingface/accelerate/"),f(X,"rel","nofollow"),f(K,"href","https://huggingface.co/CompVis/stable-diffusion-v1-4"),f(K,"rel","nofollow"),f(Q,"href","https://huggingface.co/docs/hub/security-tokens"),f(Q,"rel","nofollow"),f(H,"id","hardware-requirements-for-finetuning"),f(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(H,"href","#hardware-requirements-for-finetuning"),f(D,"class","relative group"),f(_e,"href","#flax-jax-finetuning"),f(C,"id","finetuning-example"),f(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(C,"href","#finetuning-example"),f(T,"class","relative group"),f(ae,"href","https://huggingface.co/datasets/lambdalabs/pokemon-blip-captions"),f(ae,"rel","nofollow"),f(oe,"href","https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder-with-metadata"),f(oe,"rel","nofollow"),f(L,"id","flax-jax-finetuning"),f(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(L,"href","#flax-jax-finetuning"),f(U,"class","relative group"),f(le,"href","https://github.com/duongna21"),f(le,"rel","nofollow"),f(pe,"href","https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_flax.py"),f(pe,"rel","nofollow")},m(e,n){a(document.head,c),p(e,N,n),p(e,d,n),a(d,y),a(y,xe),v(Y,xe,null),a(d,Pt),a(d,Ee),a(Ee,At),p(e,Ye,n),p(e,O,n),a(O,qt),a(O,z),a(z,ke),a(ke,jt),a(O,Dt),p(e,ze,n),v(I,e,n),p(e,Be,n),p(e,q,n),a(q,M),a(M,Pe),v(B,Pe,null),a(q,Tt),a(q,Ae),a(Ae,Ut),p(e,Je,n),p(e,j,n),a(j,S),a(S,qe),v(J,qe,null),a(j,Nt),a(j,je),a(je,Ot),p(e,Ve,n),p(e,ce,n),a(ce,It),p(e,Xe,n),v(V,e,n),p(e,We,n),p(e,F,n),a(F,Mt),a(F,X),a(X,St),a(F,Ft),p(e,Ke,n),v(W,e,n),p(e,Qe,n),p(e,x,n),a(x,Rt),a(x,De),a(De,Ht),a(x,Ct),a(x,K),a(K,Gt),a(x,Lt),p(e,Ze,n),p(e,R,n),a(R,Yt),a(R,Q),a(Q,zt),a(R,Bt),p(e,et,n),p(e,de,n),a(de,Jt),p(e,tt,n),v(Z,e,n),p(e,at,n),p(e,me,n),a(me,Vt),p(e,nt,n),p(e,D,n),a(D,H),a(H,Te),v(ee,Te,null),a(D,Xt),a(D,Ue),a(Ue,Wt),p(e,ot,n),p(e,m,n),a(m,Kt),a(m,Ne),a(Ne,Qt),a(m,Zt),a(m,Oe),a(Oe,ea),a(m,ta),a(m,Ie),a(Ie,aa),a(m,na),a(m,_e),a(_e,oa),a(m,sa),p(e,st,n),p(e,T,n),a(T,C),a(C,Me),v(te,Me,null),a(T,ia),a(T,Se),a(Se,ra),p(e,it,n),p(e,G,n),a(G,la),a(G,ae),a(ae,pa),a(G,fa),p(e,rt,n),v(ne,e,n),p(e,lt,n),p(e,E,n),a(E,ua),a(E,Fe),a(Fe,ha),a(E,ca),a(E,oe),a(oe,da),a(E,ma),p(e,pt,n),p(e,ge,n),a(ge,_a),p(e,ft,n),v(se,e,n),p(e,ut,n),p(e,k,n),a(k,ga),a(k,Re),a(Re,va),a(k,$a),a(k,He),a(He,ba),a(k,wa),p(e,ht,n),v(ie,e,n),p(e,ct,n),p(e,U,n),a(U,L),a(L,Ce),v(re,Ce,null),a(U,ya),a(U,Ge),a(Ge,xa),p(e,dt,n),p(e,P,n),a(P,Ea),a(P,le),a(le,ka),a(P,Pa),a(P,pe),a(pe,Aa),a(P,qa),p(e,mt,n),v(fe,e,n),_t=!0},p(e,[n]){const ue={};n&2&&(ue.$$scope={dirty:n,ctx:e}),I.$set(ue)},i(e){_t||($(Y.$$.fragment,e),$(I.$$.fragment,e),$(B.$$.fragment,e),$(J.$$.fragment,e),$(V.$$.fragment,e),$(W.$$.fragment,e),$(Z.$$.fragment,e),$(ee.$$.fragment,e),$(te.$$.fragment,e),$(ne.$$.fragment,e),$(se.$$.fragment,e),$(ie.$$.fragment,e),$(re.$$.fragment,e),$(fe.$$.fragment,e),_t=!0)},o(e){b(Y.$$.fragment,e),b(I.$$.fragment,e),b(B.$$.fragment,e),b(J.$$.fragment,e),b(V.$$.fragment,e),b(W.$$.fragment,e),b(Z.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(ne.$$.fragment,e),b(se.$$.fragment,e),b(ie.$$.fragment,e),b(re.$$.fragment,e),b(fe.$$.fragment,e),_t=!1},d(e){t(c),e&&t(N),e&&t(d),w(Y),e&&t(Ye),e&&t(O),e&&t(ze),w(I,e),e&&t(Be),e&&t(q),w(B),e&&t(Je),e&&t(j),w(J),e&&t(Ve),e&&t(ce),e&&t(Xe),w(V,e),e&&t(We),e&&t(F),e&&t(Ke),w(W,e),e&&t(Qe),e&&t(x),e&&t(Ze),e&&t(R),e&&t(et),e&&t(de),e&&t(tt),w(Z,e),e&&t(at),e&&t(me),e&&t(nt),e&&t(D),w(ee),e&&t(ot),e&&t(m),e&&t(st),e&&t(T),w(te),e&&t(it),e&&t(G),e&&t(rt),w(ne,e),e&&t(lt),e&&t(E),e&&t(pt),e&&t(ge),e&&t(ft),w(se,e),e&&t(ut),e&&t(k),e&&t(ht),w(ie,e),e&&t(ct),e&&t(U),w(re),e&&t(dt),e&&t(P),e&&t(mt),w(fe,e)}}}const xn={local:"stable-diffusion-texttoimage-finetuning",sections:[{local:"running-locally",sections:[{local:"installing-the-dependencies",title:"Installing the dependencies"},{local:"hardware-requirements-for-finetuning",title:"Hardware Requirements for Fine-tuning"},{local:"finetuning-example",title:"Fine-tuning Example"},{local:"flax-jax-finetuning",title:"Flax / JAX fine-tuning"}],title:"Running locally "}],title:"Stable Diffusion text-to-image fine-tuning"};function En(Le){return $n(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jn extends mn{constructor(c){super();_n(this,c,En,yn,gn,{})}}export{jn as default,xn as metadata};
