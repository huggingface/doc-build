import{S as Er,i as xr,s as kr,e as n,k as l,w as v,t as r,M as $r,c as o,d as i,m as f,a as s,x as b,h as a,b as d,G as e,g as u,y as D,L as Tr,q as y,o as w,B as A,v as Sr}from"../../../chunks/vendor-hf-doc-builder.js";import{D as j}from"../../../chunks/Docstring-hf-doc-builder.js";import{C as Pr}from"../../../chunks/CodeBlock-hf-doc-builder.js";import{I as Vt}from"../../../chunks/IconCopyLink-hf-doc-builder.js";function Lr(es){let C,Xt,M,F,tt,ie,Ni,it,Oi,Kt,W,zi,ne,qi,Gi,Yt,ke,ji,Zt,$e,nt,Fi,Qt,oe,ot,Wi,Ui,Jt,U,st,x,rt,Ri,Bi,at,Hi,Vi,Te,Xi,Ki,Se,Yi,Zi,se,k,lt,re,Qi,Ji,ft,dt,en,tn,Le,nn,on,Ce,sn,rn,$,ct,ae,an,ln,ut,pt,fn,dn,Me,cn,un,Ne,pn,ei,N,R,ht,le,hn,mt,mn,ti,B,gt,fe,gn,Oe,_n,vn,bn,_t,vt,bt,Dn,ii,_,yn,ze,wn,An,qe,In,Pn,Dt,En,xn,Ge,kn,$n,je,Tn,Sn,ni,Fe,yt,wt,Ln,oi,p,Cn,We,Mn,Nn,At,On,zn,Ue,qn,Gn,Re,jn,Fn,Be,Wn,Un,He,Rn,Bn,Ve,Hn,Vn,It,Xn,Kn,Pt,Yn,Zn,Xe,Qn,Jn,si,de,ri,Ke,Et,xt,eo,ai,T,to,kt,io,no,$t,oo,so,li,ce,fi,O,H,Tt,ue,ro,St,ao,di,z,pe,lo,Lt,fo,ci,q,V,Ct,he,co,Mt,uo,ui,m,me,po,Nt,ho,mo,ge,go,Ye,_o,vo,bo,X,_e,Do,Ot,yo,wo,S,ve,Ao,zt,Io,Po,qt,Eo,xo,K,be,ko,De,$o,Gt,To,So,pi,G,Y,jt,ye,Lo,Ft,Co,hi,g,we,Mo,Wt,No,Oo,Ae,zo,Ze,qo,Go,jo,Z,Ie,Fo,Ut,Wo,Uo,L,Pe,Ro,Rt,Bo,Ho,Bt,Vo,Xo,Q,Ee,Ko,xe,Yo,Ht,Zo,Qo,mi;return ie=new Vt({}),le=new Vt({}),de=new Pr({props:{code:`from diffusers import AltDiffusionPipeline, EulerDiscreteScheduler

pipeline = AltDiffusionPipeline.from_pretrained("BAAI/AltDiffusion")
pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)

# or
euler_scheduler = EulerDiscreteScheduler.from_pretrained("BAAI/AltDiffusion", subfolder="scheduler")
pipeline = AltDiffusionPipeline.from_pretrained("BAAI/AltDiffusion", scheduler=euler_scheduler)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> AltDiffusionPipeline, EulerDiscreteScheduler

<span class="hljs-meta">&gt;&gt;&gt; </span>pipeline = AltDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;BAAI/AltDiffusion&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># or</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>euler_scheduler = EulerDiscreteScheduler.from_pretrained(<span class="hljs-string">&quot;BAAI/AltDiffusion&quot;</span>, subfolder=<span class="hljs-string">&quot;scheduler&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipeline = AltDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;BAAI/AltDiffusion&quot;</span>, scheduler=euler_scheduler)`}}),ce=new Pr({props:{code:`from diffusers import (
    AltDiffusionPipeline,
    AltDiffusionImg2ImgPipeline,
)

img2text = AltDiffusionPipeline.from_pretrained("BAAI/AltDiffusion")
img2img = AltDiffusionImg2ImgPipeline(**img2text.components)

# now you can use img2text(...) and img2img(...) just like the call methods of each respective pipeline`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AltDiffusionPipeline,
<span class="hljs-meta">... </span>    AltDiffusionImg2ImgPipeline,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>img2text = AltDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;BAAI/AltDiffusion&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>img2img = AltDiffusionImg2ImgPipeline(**img2text.components)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># now you can use img2text(...) and img2img(...) just like the call methods of each respective pipeline</span>`}}),ue=new Vt({}),pe=new j({props:{name:"class diffusers.pipelines.alt_diffusion.AltDiffusionPipelineOutput",anchor:"diffusers.pipelines.alt_diffusion.AltDiffusionPipelineOutput",parameters:[{name:"images",val:": typing.Union[typing.List[PIL.Image.Image], numpy.ndarray]"},{name:"nsfw_content_detected",val:": typing.Optional[typing.List[bool]]"}],parametersDescription:[{anchor:"diffusers.pipelines.alt_diffusion.AltDiffusionPipelineOutput.images",description:`<strong>images</strong> (<code>List[PIL.Image.Image]</code> or <code>np.ndarray</code>) &#x2014;
List of denoised PIL images of length <code>batch_size</code> or numpy array of shape <code>(batch_size, height, width, num_channels)</code>. PIL images or numpy array present the denoised images of the diffusion pipeline.`,name:"images"},{anchor:"diffusers.pipelines.alt_diffusion.AltDiffusionPipelineOutput.nsfw_content_detected",description:`<strong>nsfw_content_detected</strong> (<code>List[bool]</code>) &#x2014;
List of flags denoting whether the corresponding generated image likely represents &#x201C;not-safe-for-work&#x201D;
(nsfw) content, or <code>None</code> if safety checking could not be performed.`,name:"nsfw_content_detected"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/__init__.py#L14"}}),he=new Vt({}),me=new j({props:{name:"class diffusers.AltDiffusionPipeline",anchor:"diffusers.AltDiffusionPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": RobertaSeriesModelWithTransformation"},{name:"tokenizer",val:": XLMRobertaTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler, diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler, diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler, diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPImageProcessor"}],parametersDescription:[{anchor:"diffusers.AltDiffusionPipeline.vae",description:`<strong>vae</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.AutoencoderKL">AutoencoderKL</a>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.AltDiffusionPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>RobertaSeriesModelWithTransformation</code>) &#x2014;
Frozen text-encoder. Alt Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.RobertaSeriesModelWithTransformation" rel="nofollow">CLIP</a>,
specifically the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.AltDiffusionPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>XLMRobertaTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.XLMRobertaTokenizer" rel="nofollow">XLMRobertaTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.AltDiffusionPipeline.unet",description:'<strong>unet</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.UNet2DConditionModel">UNet2DConditionModel</a>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.',name:"unet"},{anchor:"diffusers.AltDiffusionPipeline.scheduler",description:`<strong>scheduler</strong> (<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin">SchedulerMixin</a>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latents. Can be one of
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">DDIMScheduler</a>, <a href="/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler">LMSDiscreteScheduler</a>, or <a href="/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler">PNDMScheduler</a>.`,name:"scheduler"},{anchor:"diffusers.AltDiffusionPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offensive or harmful.
Please, refer to the <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.AltDiffusionPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py#L43"}}),_e=new j({props:{name:"__call__",anchor:"diffusers.AltDiffusionPipeline.__call__",parameters:[{name:"prompt",val:": typing.Union[str, typing.List[str]]"},{name:"height",val:": int = 512"},{name:"width",val:": int = 512"},{name:"num_inference_steps",val:": int = 50"},{name:"guidance_scale",val:": float = 7.5"},{name:"negative_prompt",val:": typing.Union[str, typing.List[str], NoneType] = None"},{name:"num_images_per_prompt",val:": typing.Optional[int] = 1"},{name:"eta",val:": float = 0.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"latents",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_type",val:": typing.Optional[str] = 'pil'"},{name:"return_dict",val:": bool = True"},{name:"callback",val:": typing.Union[typing.Callable[[int, int, torch.FloatTensor], NoneType], NoneType] = None"},{name:"callback_steps",val:": typing.Optional[int] = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.AltDiffusionPipeline.__call__.prompt",description:`<strong>prompt</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The prompt or prompts to guide the image generation.`,name:"prompt"},{anchor:"diffusers.AltDiffusionPipeline.__call__.height",description:`<strong>height</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The height in pixels of the generated image.`,name:"height"},{anchor:"diffusers.AltDiffusionPipeline.__call__.width",description:`<strong>width</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The width in pixels of the generated image.`,name:"width"},{anchor:"diffusers.AltDiffusionPipeline.__call__.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of denoising steps. More denoising steps usually lead to a higher quality image at the
expense of slower inference.`,name:"num_inference_steps"},{anchor:"diffusers.AltDiffusionPipeline.__call__.guidance_scale",description:`<strong>guidance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 7.5) &#x2014;
Guidance scale as defined in <a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance</a>.
<code>guidance_scale</code> is defined as <code>w</code> of equation 2. of <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow">Imagen
Paper</a>. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>,
usually at the expense of lower image quality.`,name:"guidance_scale"},{anchor:"diffusers.AltDiffusionPipeline.__call__.negative_prompt",description:`<strong>negative_prompt</strong> (<code>str</code> or <code>List[str]</code>, <em>optional</em>) &#x2014;
The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored
if <code>guidance_scale</code> is less than <code>1</code>).`,name:"negative_prompt"},{anchor:"diffusers.AltDiffusionPipeline.__call__.num_images_per_prompt",description:`<strong>num_images_per_prompt</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of images to generate per prompt.`,name:"num_images_per_prompt"},{anchor:"diffusers.AltDiffusionPipeline.__call__.eta",description:`<strong>eta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Corresponds to parameter eta (&#x3B7;) in the DDIM paper: <a href="https://arxiv.org/abs/2010.02502" rel="nofollow">https://arxiv.org/abs/2010.02502</a>. Only applies to
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">schedulers.DDIMScheduler</a>, will be ignored for others.`,name:"eta"},{anchor:"diffusers.AltDiffusionPipeline.__call__.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.`,name:"generator"},{anchor:"diffusers.AltDiffusionPipeline.__call__.latents",description:`<strong>latents</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
tensor will ge generated by sampling using the supplied random <code>generator</code>.`,name:"latents"},{anchor:"diffusers.AltDiffusionPipeline.__call__.output_type",description:`<strong>output_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pil&quot;</code>) &#x2014;
The output format of the generate image. Choose between
<a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">PIL</a>: <code>PIL.Image.Image</code> or <code>np.array</code>.`,name:"output_type"},{anchor:"diffusers.AltDiffusionPipeline.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>~pipelines.stable_diffusion.AltDiffusionPipelineOutput</code> instead of a
plain tuple.`,name:"return_dict"},{anchor:"diffusers.AltDiffusionPipeline.__call__.callback",description:`<strong>callback</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
A function that will be called every <code>callback_steps</code> steps during inference. The function will be
called with the following arguments: <code>callback(step: int, timestep: int, latents: torch.FloatTensor)</code>.`,name:"callback"},{anchor:"diffusers.AltDiffusionPipeline.__call__.callback_steps",description:`<strong>callback_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The frequency at which the <code>callback</code> function will be called. If not specified, the callback will be
called at every step.`,name:"callback_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py#L389",returnDescription:`
<p><code>~pipelines.stable_diffusion.AltDiffusionPipelineOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple. When returning a tuple, the first element is a list with the generated images, and the second element is a list of </code>bool<code>s denoting whether the corresponding generated image likely represents "not-safe-for-work" (nsfw) content, according to the </code>safety_checker\`.</p>
`,returnType:`
<p><code>~pipelines.stable_diffusion.AltDiffusionPipelineOutput</code> or <code>tuple</code></p>
`}}),ve=new j({props:{name:"enable_attention_slicing",anchor:"diffusers.AltDiffusionPipeline.enable_attention_slicing",parameters:[{name:"slice_size",val:": typing.Union[str, int, NoneType] = 'auto'"}],parametersDescription:[{anchor:"diffusers.AltDiffusionPipeline.enable_attention_slicing.slice_size",description:`<strong>slice_size</strong> (<code>str</code> or <code>int</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
When <code>&quot;auto&quot;</code>, halves the input to the attention heads, so attention will be computed in two steps. If
a number is provided, uses as many slices as <code>attention_head_dim // slice_size</code>. In this case,
<code>attention_head_dim</code> must be a multiple of <code>slice_size</code>.`,name:"slice_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py#L155"}}),be=new j({props:{name:"disable_attention_slicing",anchor:"diffusers.AltDiffusionPipeline.disable_attention_slicing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py#L174"}}),ye=new Vt({}),we=new j({props:{name:"class diffusers.AltDiffusionImg2ImgPipeline",anchor:"diffusers.AltDiffusionImg2ImgPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": RobertaSeriesModelWithTransformation"},{name:"tokenizer",val:": XLMRobertaTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler, diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler, diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler, diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPImageProcessor"}],parametersDescription:[{anchor:"diffusers.AltDiffusionImg2ImgPipeline.vae",description:`<strong>vae</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.AutoencoderKL">AutoencoderKL</a>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>RobertaSeriesModelWithTransformation</code>) &#x2014;
Frozen text-encoder. Alt Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.RobertaSeriesModelWithTransformation" rel="nofollow">CLIP</a>,
specifically the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>XLMRobertaTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.XLMRobertaTokenizer" rel="nofollow">XLMRobertaTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.unet",description:'<strong>unet</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.UNet2DConditionModel">UNet2DConditionModel</a>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.',name:"unet"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.scheduler",description:`<strong>scheduler</strong> (<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin">SchedulerMixin</a>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latents. Can be one of
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">DDIMScheduler</a>, <a href="/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler">LMSDiscreteScheduler</a>, or <a href="/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler">PNDMScheduler</a>.`,name:"scheduler"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offensive or harmful.
Please, refer to the <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py#L55"}}),Ie=new j({props:{name:"__call__",anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__",parameters:[{name:"prompt",val:": typing.Union[str, typing.List[str]]"},{name:"init_image",val:": typing.Union[torch.FloatTensor, PIL.Image.Image]"},{name:"strength",val:": float = 0.8"},{name:"num_inference_steps",val:": typing.Optional[int] = 50"},{name:"guidance_scale",val:": typing.Optional[float] = 7.5"},{name:"negative_prompt",val:": typing.Union[str, typing.List[str], NoneType] = None"},{name:"num_images_per_prompt",val:": typing.Optional[int] = 1"},{name:"eta",val:": typing.Optional[float] = 0.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"output_type",val:": typing.Optional[str] = 'pil'"},{name:"return_dict",val:": bool = True"},{name:"callback",val:": typing.Union[typing.Callable[[int, int, torch.FloatTensor], NoneType], NoneType] = None"},{name:"callback_steps",val:": typing.Optional[int] = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.prompt",description:`<strong>prompt</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The prompt or prompts to guide the image generation.`,name:"prompt"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.init_image",description:`<strong>init_image</strong> (<code>torch.FloatTensor</code> or <code>PIL.Image.Image</code>) &#x2014;
<code>Image</code>, or tensor representing an image batch, that will be used as the starting point for the
process.`,name:"init_image"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.strength",description:`<strong>strength</strong> (<code>float</code>, <em>optional</em>, defaults to 0.8) &#x2014;
Conceptually, indicates how much to transform the reference <code>init_image</code>. Must be between 0 and 1.
<code>init_image</code> will be used as a starting point, adding more noise to it the larger the <code>strength</code>. The
number of denoising steps depends on the amount of noise initially added. When <code>strength</code> is 1, added
noise will be maximum and the denoising process will run for the full number of iterations specified in
<code>num_inference_steps</code>. A value of 1, therefore, essentially ignores <code>init_image</code>.`,name:"strength"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of denoising steps. More denoising steps usually lead to a higher quality image at the
expense of slower inference. This parameter will be modulated by <code>strength</code>.`,name:"num_inference_steps"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.guidance_scale",description:`<strong>guidance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 7.5) &#x2014;
Guidance scale as defined in <a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance</a>.
<code>guidance_scale</code> is defined as <code>w</code> of equation 2. of <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow">Imagen
Paper</a>. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>,
usually at the expense of lower image quality.`,name:"guidance_scale"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.negative_prompt",description:`<strong>negative_prompt</strong> (<code>str</code> or <code>List[str]</code>, <em>optional</em>) &#x2014;
The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored
if <code>guidance_scale</code> is less than <code>1</code>).`,name:"negative_prompt"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.num_images_per_prompt",description:`<strong>num_images_per_prompt</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of images to generate per prompt.`,name:"num_images_per_prompt"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.eta",description:`<strong>eta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Corresponds to parameter eta (&#x3B7;) in the DDIM paper: <a href="https://arxiv.org/abs/2010.02502" rel="nofollow">https://arxiv.org/abs/2010.02502</a>. Only applies to
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">schedulers.DDIMScheduler</a>, will be ignored for others.`,name:"eta"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.`,name:"generator"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.output_type",description:`<strong>output_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pil&quot;</code>) &#x2014;
The output format of the generate image. Choose between
<a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">PIL</a>: <code>PIL.Image.Image</code> or <code>np.array</code>.`,name:"output_type"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>~pipelines.stable_diffusion.AltDiffusionPipelineOutput</code> instead of a
plain tuple.`,name:"return_dict"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.callback",description:`<strong>callback</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
A function that will be called every <code>callback_steps</code> steps during inference. The function will be
called with the following arguments: <code>callback(step: int, timestep: int, latents: torch.FloatTensor)</code>.`,name:"callback"},{anchor:"diffusers.AltDiffusionImg2ImgPipeline.__call__.callback_steps",description:`<strong>callback_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The frequency at which the <code>callback</code> function will be called. If not specified, the callback will be
called at every step.`,name:"callback_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py#L439",returnDescription:`
<p><code>~pipelines.stable_diffusion.AltDiffusionPipelineOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple. When returning a tuple, the first element is a list with the generated images, and the second element is a list of </code>bool<code>s denoting whether the corresponding generated image likely represents "not-safe-for-work" (nsfw) content, according to the </code>safety_checker\`.</p>
`,returnType:`
<p><code>~pipelines.stable_diffusion.AltDiffusionPipelineOutput</code> or <code>tuple</code></p>
`}}),Pe=new j({props:{name:"enable_attention_slicing",anchor:"diffusers.AltDiffusionImg2ImgPipeline.enable_attention_slicing",parameters:[{name:"slice_size",val:": typing.Union[str, int, NoneType] = 'auto'"}],parametersDescription:[{anchor:"diffusers.AltDiffusionImg2ImgPipeline.enable_attention_slicing.slice_size",description:`<strong>slice_size</strong> (<code>str</code> or <code>int</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
When <code>&quot;auto&quot;</code>, halves the input to the attention heads, so attention will be computed in two steps. If
a number is provided, uses as many slices as <code>attention_head_dim // slice_size</code>. In this case,
<code>attention_head_dim</code> must be a multiple of <code>slice_size</code>.`,name:"slice_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py#L151"}}),Ee=new j({props:{name:"disable_attention_slicing",anchor:"diffusers.AltDiffusionImg2ImgPipeline.disable_attention_slicing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py#L171"}}),{c(){C=n("meta"),Xt=l(),M=n("h1"),F=n("a"),tt=n("span"),v(ie.$$.fragment),Ni=l(),it=n("span"),Oi=r("AltDiffusion"),Kt=l(),W=n("p"),zi=r("AltDiffusion was proposed in "),ne=n("a"),qi=r("AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities"),Gi=r(" by Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu"),Yt=l(),ke=n("p"),ji=r("The abstract of the paper is the following:"),Zt=l(),$e=n("p"),nt=n("em"),Fi=r("In this work, we present a conceptually simple and effective method to train a strong bilingual multimodal representation model. Starting from the pretrained multimodal representation model CLIP released by OpenAI, we switched its text encoder with a pretrained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding."),Qt=l(),oe=n("p"),ot=n("em"),Wi=r("Overview"),Ui=r(":"),Jt=l(),U=n("table"),st=n("thead"),x=n("tr"),rt=n("th"),Ri=r("Pipeline"),Bi=l(),at=n("th"),Hi=r("Tasks"),Vi=l(),Te=n("th"),Xi=r("Colab"),Ki=l(),Se=n("th"),Yi=r("Demo"),Zi=l(),se=n("tbody"),k=n("tr"),lt=n("td"),re=n("a"),Qi=r("pipeline_alt_diffusion.py"),Ji=l(),ft=n("td"),dt=n("em"),en=r("Text-to-Image Generation"),tn=l(),Le=n("td"),nn=r("-"),on=l(),Ce=n("td"),sn=r("-"),rn=l(),$=n("tr"),ct=n("td"),ae=n("a"),an=r("pipeline_alt_diffusion_img2img.py"),ln=l(),ut=n("td"),pt=n("em"),fn=r("Image-to-Image Text-Guided Generation"),dn=l(),Me=n("td"),cn=r("-"),un=l(),Ne=n("td"),pn=r("-"),ei=l(),N=n("h2"),R=n("a"),ht=n("span"),v(le.$$.fragment),hn=l(),mt=n("span"),mn=r("Tips"),ti=l(),B=n("ul"),gt=n("li"),fe=n("p"),gn=r("AltDiffusion is conceptually exaclty the same as "),Oe=n("a"),_n=r("Stable Diffusion"),vn=r("."),bn=l(),_t=n("li"),vt=n("p"),bt=n("em"),Dn=r("Run AltDiffusion"),ii=l(),_=n("p"),yn=r("AltDiffusion can be tested very easily with the "),ze=n("a"),wn=r("AltDiffusionPipeline"),An=r(", "),qe=n("a"),In=r("AltDiffusionImg2ImgPipeline"),Pn=r(" and the "),Dt=n("code"),En=r('"BAAI/AltDiffusion"'),xn=r(" checkpoint exactly in the same way it is shown in the "),Ge=n("a"),kn=r("Conditional Image Generation Guide"),$n=r(" and the "),je=n("a"),Tn=r("Image-to-Image Generation Guide"),Sn=r("."),ni=l(),Fe=n("ul"),yt=n("li"),wt=n("em"),Ln=r("How to load and use different schedulers."),oi=l(),p=n("p"),Cn=r("The alt diffusion pipeline uses "),We=n("a"),Mn=r("DDIMScheduler"),Nn=r(" scheduler by default. But "),At=n("code"),On=r("diffusers"),zn=r(" provides many other schedulers that can be used with the alt diffusion pipeline such as "),Ue=n("a"),qn=r("PNDMScheduler"),Gn=r(", "),Re=n("a"),jn=r("LMSDiscreteScheduler"),Fn=r(", "),Be=n("a"),Wn=r("EulerDiscreteScheduler"),Un=r(", "),He=n("a"),Rn=r("EulerAncestralDiscreteScheduler"),Bn=r(` etc.
To use a different scheduler, you can either change it via the `),Ve=n("a"),Hn=r("ConfigMixin.from_config()"),Vn=r(" method or pass the "),It=n("code"),Xn=r("scheduler"),Kn=r(" argument to the "),Pt=n("code"),Yn=r("from_pretrained"),Zn=r(" method of the pipeline. For example, to use the "),Xe=n("a"),Qn=r("EulerDiscreteScheduler"),Jn=r(", you can do the following:"),si=l(),v(de.$$.fragment),ri=l(),Ke=n("ul"),Et=n("li"),xt=n("em"),eo=r("How to conver all use cases with multiple or single pipeline"),ai=l(),T=n("p"),to=r("If you want to use all possible use cases in a single "),kt=n("code"),io=r("DiffusionPipeline"),no=r(" we recommend using the "),$t=n("code"),oo=r("components"),so=r(" functionality to instantiate all components in the most memory-efficient way:"),li=l(),v(ce.$$.fragment),fi=l(),O=n("h2"),H=n("a"),Tt=n("span"),v(ue.$$.fragment),ro=l(),St=n("span"),ao=r("AltDiffusionPipelineOutput"),di=l(),z=n("div"),v(pe.$$.fragment),lo=l(),Lt=n("p"),fo=r("Output class for Alt Diffusion pipelines."),ci=l(),q=n("h2"),V=n("a"),Ct=n("span"),v(he.$$.fragment),co=l(),Mt=n("span"),uo=r("AltDiffusionPipeline"),ui=l(),m=n("div"),v(me.$$.fragment),po=l(),Nt=n("p"),ho=r("Pipeline for text-to-image generation using Alt Diffusion."),mo=l(),ge=n("p"),go=r("This model inherits from "),Ye=n("a"),_o=r("DiffusionPipeline"),vo=r(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),bo=l(),X=n("div"),v(_e.$$.fragment),Do=l(),Ot=n("p"),yo=r("Function invoked when calling the pipeline for generation."),wo=l(),S=n("div"),v(ve.$$.fragment),Ao=l(),zt=n("p"),Io=r("Enable sliced attention computation."),Po=l(),qt=n("p"),Eo=r(`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),xo=l(),K=n("div"),v(be.$$.fragment),ko=l(),De=n("p"),$o=r("Disable sliced attention computation. If "),Gt=n("code"),To=r("enable_attention_slicing"),So=r(` was previously invoked, this method will go
back to computing attention in one step.`),pi=l(),G=n("h2"),Y=n("a"),jt=n("span"),v(ye.$$.fragment),Lo=l(),Ft=n("span"),Co=r("AltDiffusionImg2ImgPipeline"),hi=l(),g=n("div"),v(we.$$.fragment),Mo=l(),Wt=n("p"),No=r("Pipeline for text-guided image to image generation using Alt Diffusion."),Oo=l(),Ae=n("p"),zo=r("This model inherits from "),Ze=n("a"),qo=r("DiffusionPipeline"),Go=r(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),jo=l(),Z=n("div"),v(Ie.$$.fragment),Fo=l(),Ut=n("p"),Wo=r("Function invoked when calling the pipeline for generation."),Uo=l(),L=n("div"),v(Pe.$$.fragment),Ro=l(),Rt=n("p"),Bo=r("Enable sliced attention computation."),Ho=l(),Bt=n("p"),Vo=r(`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),Xo=l(),Q=n("div"),v(Ee.$$.fragment),Ko=l(),xe=n("p"),Yo=r("Disable sliced attention computation. If "),Ht=n("code"),Zo=r("enable_attention_slicing"),Qo=r(` was previously invoked, this method will go
back to computing attention in one step.`),this.h()},l(t){const c=$r('[data-svelte="svelte-1phssyn"]',document.head);C=o(c,"META",{name:!0,content:!0}),c.forEach(i),Xt=f(t),M=o(t,"H1",{class:!0});var gi=s(M);F=o(gi,"A",{id:!0,class:!0,href:!0});var ts=s(F);tt=o(ts,"SPAN",{});var is=s(tt);b(ie.$$.fragment,is),is.forEach(i),ts.forEach(i),Ni=f(gi),it=o(gi,"SPAN",{});var ns=s(it);Oi=a(ns,"AltDiffusion"),ns.forEach(i),gi.forEach(i),Kt=f(t),W=o(t,"P",{});var _i=s(W);zi=a(_i,"AltDiffusion was proposed in "),ne=o(_i,"A",{href:!0,rel:!0});var os=s(ne);qi=a(os,"AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities"),os.forEach(i),Gi=a(_i," by Zhongzhi Chen, Guang Liu, Bo-Wen Zhang, Fulong Ye, Qinghong Yang, Ledell Wu"),_i.forEach(i),Yt=f(t),ke=o(t,"P",{});var ss=s(ke);ji=a(ss,"The abstract of the paper is the following:"),ss.forEach(i),Zt=f(t),$e=o(t,"P",{});var rs=s($e);nt=o(rs,"EM",{});var as=s(nt);Fi=a(as,"In this work, we present a conceptually simple and effective method to train a strong bilingual multimodal representation model. Starting from the pretrained multimodal representation model CLIP released by OpenAI, we switched its text encoder with a pretrained multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art performances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding."),as.forEach(i),rs.forEach(i),Qt=f(t),oe=o(t,"P",{});var Jo=s(oe);ot=o(Jo,"EM",{});var ls=s(ot);Wi=a(ls,"Overview"),ls.forEach(i),Ui=a(Jo,":"),Jo.forEach(i),Jt=f(t),U=o(t,"TABLE",{});var vi=s(U);st=o(vi,"THEAD",{});var fs=s(st);x=o(fs,"TR",{});var J=s(x);rt=o(J,"TH",{});var ds=s(rt);Ri=a(ds,"Pipeline"),ds.forEach(i),Bi=f(J),at=o(J,"TH",{});var cs=s(at);Hi=a(cs,"Tasks"),cs.forEach(i),Vi=f(J),Te=o(J,"TH",{align:!0});var us=s(Te);Xi=a(us,"Colab"),us.forEach(i),Ki=f(J),Se=o(J,"TH",{align:!0});var ps=s(Se);Yi=a(ps,"Demo"),ps.forEach(i),J.forEach(i),fs.forEach(i),Zi=f(vi),se=o(vi,"TBODY",{});var bi=s(se);k=o(bi,"TR",{});var ee=s(k);lt=o(ee,"TD",{});var hs=s(lt);re=o(hs,"A",{href:!0,rel:!0});var ms=s(re);Qi=a(ms,"pipeline_alt_diffusion.py"),ms.forEach(i),hs.forEach(i),Ji=f(ee),ft=o(ee,"TD",{});var gs=s(ft);dt=o(gs,"EM",{});var _s=s(dt);en=a(_s,"Text-to-Image Generation"),_s.forEach(i),gs.forEach(i),tn=f(ee),Le=o(ee,"TD",{align:!0});var vs=s(Le);nn=a(vs,"-"),vs.forEach(i),on=f(ee),Ce=o(ee,"TD",{align:!0});var bs=s(Ce);sn=a(bs,"-"),bs.forEach(i),ee.forEach(i),rn=f(bi),$=o(bi,"TR",{});var te=s($);ct=o(te,"TD",{});var Ds=s(ct);ae=o(Ds,"A",{href:!0,rel:!0});var ys=s(ae);an=a(ys,"pipeline_alt_diffusion_img2img.py"),ys.forEach(i),Ds.forEach(i),ln=f(te),ut=o(te,"TD",{});var ws=s(ut);pt=o(ws,"EM",{});var As=s(pt);fn=a(As,"Image-to-Image Text-Guided Generation"),As.forEach(i),ws.forEach(i),dn=f(te),Me=o(te,"TD",{align:!0});var Is=s(Me);cn=a(Is,"-"),Is.forEach(i),un=f(te),Ne=o(te,"TD",{align:!0});var Ps=s(Ne);pn=a(Ps,"-"),Ps.forEach(i),te.forEach(i),bi.forEach(i),vi.forEach(i),ei=f(t),N=o(t,"H2",{class:!0});var Di=s(N);R=o(Di,"A",{id:!0,class:!0,href:!0});var Es=s(R);ht=o(Es,"SPAN",{});var xs=s(ht);b(le.$$.fragment,xs),xs.forEach(i),Es.forEach(i),hn=f(Di),mt=o(Di,"SPAN",{});var ks=s(mt);mn=a(ks,"Tips"),ks.forEach(i),Di.forEach(i),ti=f(t),B=o(t,"UL",{});var yi=s(B);gt=o(yi,"LI",{});var $s=s(gt);fe=o($s,"P",{});var wi=s(fe);gn=a(wi,"AltDiffusion is conceptually exaclty the same as "),Oe=o(wi,"A",{href:!0});var Ts=s(Oe);_n=a(Ts,"Stable Diffusion"),Ts.forEach(i),vn=a(wi,"."),wi.forEach(i),$s.forEach(i),bn=f(yi),_t=o(yi,"LI",{});var Ss=s(_t);vt=o(Ss,"P",{});var Ls=s(vt);bt=o(Ls,"EM",{});var Cs=s(bt);Dn=a(Cs,"Run AltDiffusion"),Cs.forEach(i),Ls.forEach(i),Ss.forEach(i),yi.forEach(i),ii=f(t),_=o(t,"P",{});var I=s(_);yn=a(I,"AltDiffusion can be tested very easily with the "),ze=o(I,"A",{href:!0});var Ms=s(ze);wn=a(Ms,"AltDiffusionPipeline"),Ms.forEach(i),An=a(I,", "),qe=o(I,"A",{href:!0});var Ns=s(qe);In=a(Ns,"AltDiffusionImg2ImgPipeline"),Ns.forEach(i),Pn=a(I," and the "),Dt=o(I,"CODE",{});var Os=s(Dt);En=a(Os,'"BAAI/AltDiffusion"'),Os.forEach(i),xn=a(I," checkpoint exactly in the same way it is shown in the "),Ge=o(I,"A",{href:!0});var zs=s(Ge);kn=a(zs,"Conditional Image Generation Guide"),zs.forEach(i),$n=a(I," and the "),je=o(I,"A",{href:!0});var qs=s(je);Tn=a(qs,"Image-to-Image Generation Guide"),qs.forEach(i),Sn=a(I,"."),I.forEach(i),ni=f(t),Fe=o(t,"UL",{});var Gs=s(Fe);yt=o(Gs,"LI",{});var js=s(yt);wt=o(js,"EM",{});var Fs=s(wt);Ln=a(Fs,"How to load and use different schedulers."),Fs.forEach(i),js.forEach(i),Gs.forEach(i),oi=f(t),p=o(t,"P",{});var h=s(p);Cn=a(h,"The alt diffusion pipeline uses "),We=o(h,"A",{href:!0});var Ws=s(We);Mn=a(Ws,"DDIMScheduler"),Ws.forEach(i),Nn=a(h," scheduler by default. But "),At=o(h,"CODE",{});var Us=s(At);On=a(Us,"diffusers"),Us.forEach(i),zn=a(h," provides many other schedulers that can be used with the alt diffusion pipeline such as "),Ue=o(h,"A",{href:!0});var Rs=s(Ue);qn=a(Rs,"PNDMScheduler"),Rs.forEach(i),Gn=a(h,", "),Re=o(h,"A",{href:!0});var Bs=s(Re);jn=a(Bs,"LMSDiscreteScheduler"),Bs.forEach(i),Fn=a(h,", "),Be=o(h,"A",{href:!0});var Hs=s(Be);Wn=a(Hs,"EulerDiscreteScheduler"),Hs.forEach(i),Un=a(h,", "),He=o(h,"A",{href:!0});var Vs=s(He);Rn=a(Vs,"EulerAncestralDiscreteScheduler"),Vs.forEach(i),Bn=a(h,` etc.
To use a different scheduler, you can either change it via the `),Ve=o(h,"A",{href:!0});var Xs=s(Ve);Hn=a(Xs,"ConfigMixin.from_config()"),Xs.forEach(i),Vn=a(h," method or pass the "),It=o(h,"CODE",{});var Ks=s(It);Xn=a(Ks,"scheduler"),Ks.forEach(i),Kn=a(h," argument to the "),Pt=o(h,"CODE",{});var Ys=s(Pt);Yn=a(Ys,"from_pretrained"),Ys.forEach(i),Zn=a(h," method of the pipeline. For example, to use the "),Xe=o(h,"A",{href:!0});var Zs=s(Xe);Qn=a(Zs,"EulerDiscreteScheduler"),Zs.forEach(i),Jn=a(h,", you can do the following:"),h.forEach(i),si=f(t),b(de.$$.fragment,t),ri=f(t),Ke=o(t,"UL",{});var Qs=s(Ke);Et=o(Qs,"LI",{});var Js=s(Et);xt=o(Js,"EM",{});var er=s(xt);eo=a(er,"How to conver all use cases with multiple or single pipeline"),er.forEach(i),Js.forEach(i),Qs.forEach(i),ai=f(t),T=o(t,"P",{});var Qe=s(T);to=a(Qe,"If you want to use all possible use cases in a single "),kt=o(Qe,"CODE",{});var tr=s(kt);io=a(tr,"DiffusionPipeline"),tr.forEach(i),no=a(Qe," we recommend using the "),$t=o(Qe,"CODE",{});var ir=s($t);oo=a(ir,"components"),ir.forEach(i),so=a(Qe," functionality to instantiate all components in the most memory-efficient way:"),Qe.forEach(i),li=f(t),b(ce.$$.fragment,t),fi=f(t),O=o(t,"H2",{class:!0});var Ai=s(O);H=o(Ai,"A",{id:!0,class:!0,href:!0});var nr=s(H);Tt=o(nr,"SPAN",{});var or=s(Tt);b(ue.$$.fragment,or),or.forEach(i),nr.forEach(i),ro=f(Ai),St=o(Ai,"SPAN",{});var sr=s(St);ao=a(sr,"AltDiffusionPipelineOutput"),sr.forEach(i),Ai.forEach(i),di=f(t),z=o(t,"DIV",{class:!0});var Ii=s(z);b(pe.$$.fragment,Ii),lo=f(Ii),Lt=o(Ii,"P",{});var rr=s(Lt);fo=a(rr,"Output class for Alt Diffusion pipelines."),rr.forEach(i),Ii.forEach(i),ci=f(t),q=o(t,"H2",{class:!0});var Pi=s(q);V=o(Pi,"A",{id:!0,class:!0,href:!0});var ar=s(V);Ct=o(ar,"SPAN",{});var lr=s(Ct);b(he.$$.fragment,lr),lr.forEach(i),ar.forEach(i),co=f(Pi),Mt=o(Pi,"SPAN",{});var fr=s(Mt);uo=a(fr,"AltDiffusionPipeline"),fr.forEach(i),Pi.forEach(i),ui=f(t),m=o(t,"DIV",{class:!0});var P=s(m);b(me.$$.fragment,P),po=f(P),Nt=o(P,"P",{});var dr=s(Nt);ho=a(dr,"Pipeline for text-to-image generation using Alt Diffusion."),dr.forEach(i),mo=f(P),ge=o(P,"P",{});var Ei=s(ge);go=a(Ei,"This model inherits from "),Ye=o(Ei,"A",{href:!0});var cr=s(Ye);_o=a(cr,"DiffusionPipeline"),cr.forEach(i),vo=a(Ei,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Ei.forEach(i),bo=f(P),X=o(P,"DIV",{class:!0});var xi=s(X);b(_e.$$.fragment,xi),Do=f(xi),Ot=o(xi,"P",{});var ur=s(Ot);yo=a(ur,"Function invoked when calling the pipeline for generation."),ur.forEach(i),xi.forEach(i),wo=f(P),S=o(P,"DIV",{class:!0});var Je=s(S);b(ve.$$.fragment,Je),Ao=f(Je),zt=o(Je,"P",{});var pr=s(zt);Io=a(pr,"Enable sliced attention computation."),pr.forEach(i),Po=f(Je),qt=o(Je,"P",{});var hr=s(qt);Eo=a(hr,`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),hr.forEach(i),Je.forEach(i),xo=f(P),K=o(P,"DIV",{class:!0});var ki=s(K);b(be.$$.fragment,ki),ko=f(ki),De=o(ki,"P",{});var $i=s(De);$o=a($i,"Disable sliced attention computation. If "),Gt=o($i,"CODE",{});var mr=s(Gt);To=a(mr,"enable_attention_slicing"),mr.forEach(i),So=a($i,` was previously invoked, this method will go
back to computing attention in one step.`),$i.forEach(i),ki.forEach(i),P.forEach(i),pi=f(t),G=o(t,"H2",{class:!0});var Ti=s(G);Y=o(Ti,"A",{id:!0,class:!0,href:!0});var gr=s(Y);jt=o(gr,"SPAN",{});var _r=s(jt);b(ye.$$.fragment,_r),_r.forEach(i),gr.forEach(i),Lo=f(Ti),Ft=o(Ti,"SPAN",{});var vr=s(Ft);Co=a(vr,"AltDiffusionImg2ImgPipeline"),vr.forEach(i),Ti.forEach(i),hi=f(t),g=o(t,"DIV",{class:!0});var E=s(g);b(we.$$.fragment,E),Mo=f(E),Wt=o(E,"P",{});var br=s(Wt);No=a(br,"Pipeline for text-guided image to image generation using Alt Diffusion."),br.forEach(i),Oo=f(E),Ae=o(E,"P",{});var Si=s(Ae);zo=a(Si,"This model inherits from "),Ze=o(Si,"A",{href:!0});var Dr=s(Ze);qo=a(Dr,"DiffusionPipeline"),Dr.forEach(i),Go=a(Si,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Si.forEach(i),jo=f(E),Z=o(E,"DIV",{class:!0});var Li=s(Z);b(Ie.$$.fragment,Li),Fo=f(Li),Ut=o(Li,"P",{});var yr=s(Ut);Wo=a(yr,"Function invoked when calling the pipeline for generation."),yr.forEach(i),Li.forEach(i),Uo=f(E),L=o(E,"DIV",{class:!0});var et=s(L);b(Pe.$$.fragment,et),Ro=f(et),Rt=o(et,"P",{});var wr=s(Rt);Bo=a(wr,"Enable sliced attention computation."),wr.forEach(i),Ho=f(et),Bt=o(et,"P",{});var Ar=s(Bt);Vo=a(Ar,`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),Ar.forEach(i),et.forEach(i),Xo=f(E),Q=o(E,"DIV",{class:!0});var Ci=s(Q);b(Ee.$$.fragment,Ci),Ko=f(Ci),xe=o(Ci,"P",{});var Mi=s(xe);Yo=a(Mi,"Disable sliced attention computation. If "),Ht=o(Mi,"CODE",{});var Ir=s(Ht);Zo=a(Ir,"enable_attention_slicing"),Ir.forEach(i),Qo=a(Mi,` was previously invoked, this method will go
back to computing attention in one step.`),Mi.forEach(i),Ci.forEach(i),E.forEach(i),this.h()},h(){d(C,"name","hf:doc:metadata"),d(C,"content",JSON.stringify(Cr)),d(F,"id","altdiffusion"),d(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(F,"href","#altdiffusion"),d(M,"class","relative group"),d(ne,"href","https://arxiv.org/abs/2211.06679"),d(ne,"rel","nofollow"),d(Te,"align","center"),d(Se,"align","center"),d(re,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py"),d(re,"rel","nofollow"),d(Le,"align","center"),d(Ce,"align","center"),d(ae,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py"),d(ae,"rel","nofollow"),d(Me,"align","center"),d(Ne,"align","center"),d(R,"id","tips"),d(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(R,"href","#tips"),d(N,"class","relative group"),d(Oe,"href","./api/pipelines/stable_diffusion"),d(ze,"href","/docs/diffusers/main/en/api/pipelines/alt_diffusion#diffusers.AltDiffusionPipeline"),d(qe,"href","/docs/diffusers/main/en/api/pipelines/alt_diffusion#diffusers.AltDiffusionImg2ImgPipeline"),d(Ge,"href","./using-diffusers/conditional_image_generation"),d(je,"href","./using-diffusers/img2img"),d(We,"href","/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler"),d(Ue,"href","/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler"),d(Re,"href","/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler"),d(Be,"href","/docs/diffusers/main/en/api/schedulers#diffusers.EulerDiscreteScheduler"),d(He,"href","/docs/diffusers/main/en/api/schedulers#diffusers.EulerAncestralDiscreteScheduler"),d(Ve,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Xe,"href","/docs/diffusers/main/en/api/schedulers#diffusers.EulerDiscreteScheduler"),d(H,"id","diffusers.pipelines.alt_diffusion.AltDiffusionPipelineOutput"),d(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(H,"href","#diffusers.pipelines.alt_diffusion.AltDiffusionPipelineOutput"),d(O,"class","relative group"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(V,"id","diffusers.AltDiffusionPipeline"),d(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(V,"href","#diffusers.AltDiffusionPipeline"),d(q,"class","relative group"),d(Ye,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.DiffusionPipeline"),d(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Y,"id","diffusers.AltDiffusionImg2ImgPipeline"),d(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y,"href","#diffusers.AltDiffusionImg2ImgPipeline"),d(G,"class","relative group"),d(Ze,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.DiffusionPipeline"),d(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,c){e(document.head,C),u(t,Xt,c),u(t,M,c),e(M,F),e(F,tt),D(ie,tt,null),e(M,Ni),e(M,it),e(it,Oi),u(t,Kt,c),u(t,W,c),e(W,zi),e(W,ne),e(ne,qi),e(W,Gi),u(t,Yt,c),u(t,ke,c),e(ke,ji),u(t,Zt,c),u(t,$e,c),e($e,nt),e(nt,Fi),u(t,Qt,c),u(t,oe,c),e(oe,ot),e(ot,Wi),e(oe,Ui),u(t,Jt,c),u(t,U,c),e(U,st),e(st,x),e(x,rt),e(rt,Ri),e(x,Bi),e(x,at),e(at,Hi),e(x,Vi),e(x,Te),e(Te,Xi),e(x,Ki),e(x,Se),e(Se,Yi),e(U,Zi),e(U,se),e(se,k),e(k,lt),e(lt,re),e(re,Qi),e(k,Ji),e(k,ft),e(ft,dt),e(dt,en),e(k,tn),e(k,Le),e(Le,nn),e(k,on),e(k,Ce),e(Ce,sn),e(se,rn),e(se,$),e($,ct),e(ct,ae),e(ae,an),e($,ln),e($,ut),e(ut,pt),e(pt,fn),e($,dn),e($,Me),e(Me,cn),e($,un),e($,Ne),e(Ne,pn),u(t,ei,c),u(t,N,c),e(N,R),e(R,ht),D(le,ht,null),e(N,hn),e(N,mt),e(mt,mn),u(t,ti,c),u(t,B,c),e(B,gt),e(gt,fe),e(fe,gn),e(fe,Oe),e(Oe,_n),e(fe,vn),e(B,bn),e(B,_t),e(_t,vt),e(vt,bt),e(bt,Dn),u(t,ii,c),u(t,_,c),e(_,yn),e(_,ze),e(ze,wn),e(_,An),e(_,qe),e(qe,In),e(_,Pn),e(_,Dt),e(Dt,En),e(_,xn),e(_,Ge),e(Ge,kn),e(_,$n),e(_,je),e(je,Tn),e(_,Sn),u(t,ni,c),u(t,Fe,c),e(Fe,yt),e(yt,wt),e(wt,Ln),u(t,oi,c),u(t,p,c),e(p,Cn),e(p,We),e(We,Mn),e(p,Nn),e(p,At),e(At,On),e(p,zn),e(p,Ue),e(Ue,qn),e(p,Gn),e(p,Re),e(Re,jn),e(p,Fn),e(p,Be),e(Be,Wn),e(p,Un),e(p,He),e(He,Rn),e(p,Bn),e(p,Ve),e(Ve,Hn),e(p,Vn),e(p,It),e(It,Xn),e(p,Kn),e(p,Pt),e(Pt,Yn),e(p,Zn),e(p,Xe),e(Xe,Qn),e(p,Jn),u(t,si,c),D(de,t,c),u(t,ri,c),u(t,Ke,c),e(Ke,Et),e(Et,xt),e(xt,eo),u(t,ai,c),u(t,T,c),e(T,to),e(T,kt),e(kt,io),e(T,no),e(T,$t),e($t,oo),e(T,so),u(t,li,c),D(ce,t,c),u(t,fi,c),u(t,O,c),e(O,H),e(H,Tt),D(ue,Tt,null),e(O,ro),e(O,St),e(St,ao),u(t,di,c),u(t,z,c),D(pe,z,null),e(z,lo),e(z,Lt),e(Lt,fo),u(t,ci,c),u(t,q,c),e(q,V),e(V,Ct),D(he,Ct,null),e(q,co),e(q,Mt),e(Mt,uo),u(t,ui,c),u(t,m,c),D(me,m,null),e(m,po),e(m,Nt),e(Nt,ho),e(m,mo),e(m,ge),e(ge,go),e(ge,Ye),e(Ye,_o),e(ge,vo),e(m,bo),e(m,X),D(_e,X,null),e(X,Do),e(X,Ot),e(Ot,yo),e(m,wo),e(m,S),D(ve,S,null),e(S,Ao),e(S,zt),e(zt,Io),e(S,Po),e(S,qt),e(qt,Eo),e(m,xo),e(m,K),D(be,K,null),e(K,ko),e(K,De),e(De,$o),e(De,Gt),e(Gt,To),e(De,So),u(t,pi,c),u(t,G,c),e(G,Y),e(Y,jt),D(ye,jt,null),e(G,Lo),e(G,Ft),e(Ft,Co),u(t,hi,c),u(t,g,c),D(we,g,null),e(g,Mo),e(g,Wt),e(Wt,No),e(g,Oo),e(g,Ae),e(Ae,zo),e(Ae,Ze),e(Ze,qo),e(Ae,Go),e(g,jo),e(g,Z),D(Ie,Z,null),e(Z,Fo),e(Z,Ut),e(Ut,Wo),e(g,Uo),e(g,L),D(Pe,L,null),e(L,Ro),e(L,Rt),e(Rt,Bo),e(L,Ho),e(L,Bt),e(Bt,Vo),e(g,Xo),e(g,Q),D(Ee,Q,null),e(Q,Ko),e(Q,xe),e(xe,Yo),e(xe,Ht),e(Ht,Zo),e(xe,Qo),mi=!0},p:Tr,i(t){mi||(y(ie.$$.fragment,t),y(le.$$.fragment,t),y(de.$$.fragment,t),y(ce.$$.fragment,t),y(ue.$$.fragment,t),y(pe.$$.fragment,t),y(he.$$.fragment,t),y(me.$$.fragment,t),y(_e.$$.fragment,t),y(ve.$$.fragment,t),y(be.$$.fragment,t),y(ye.$$.fragment,t),y(we.$$.fragment,t),y(Ie.$$.fragment,t),y(Pe.$$.fragment,t),y(Ee.$$.fragment,t),mi=!0)},o(t){w(ie.$$.fragment,t),w(le.$$.fragment,t),w(de.$$.fragment,t),w(ce.$$.fragment,t),w(ue.$$.fragment,t),w(pe.$$.fragment,t),w(he.$$.fragment,t),w(me.$$.fragment,t),w(_e.$$.fragment,t),w(ve.$$.fragment,t),w(be.$$.fragment,t),w(ye.$$.fragment,t),w(we.$$.fragment,t),w(Ie.$$.fragment,t),w(Pe.$$.fragment,t),w(Ee.$$.fragment,t),mi=!1},d(t){i(C),t&&i(Xt),t&&i(M),A(ie),t&&i(Kt),t&&i(W),t&&i(Yt),t&&i(ke),t&&i(Zt),t&&i($e),t&&i(Qt),t&&i(oe),t&&i(Jt),t&&i(U),t&&i(ei),t&&i(N),A(le),t&&i(ti),t&&i(B),t&&i(ii),t&&i(_),t&&i(ni),t&&i(Fe),t&&i(oi),t&&i(p),t&&i(si),A(de,t),t&&i(ri),t&&i(Ke),t&&i(ai),t&&i(T),t&&i(li),A(ce,t),t&&i(fi),t&&i(O),A(ue),t&&i(di),t&&i(z),A(pe),t&&i(ci),t&&i(q),A(he),t&&i(ui),t&&i(m),A(me),A(_e),A(ve),A(be),t&&i(pi),t&&i(G),A(ye),t&&i(hi),t&&i(g),A(we),A(Ie),A(Pe),A(Ee)}}}const Cr={local:"altdiffusion",sections:[{local:"tips",title:"Tips"},{local:"diffusers.pipelines.alt_diffusion.AltDiffusionPipelineOutput",title:"AltDiffusionPipelineOutput"},{local:"diffusers.AltDiffusionPipeline",title:"AltDiffusionPipeline"},{local:"diffusers.AltDiffusionImg2ImgPipeline",title:"AltDiffusionImg2ImgPipeline"}],title:"AltDiffusion"};function Mr(es){return Sr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Gr extends Er{constructor(C){super();xr(this,C,Mr,Lr,kr,{})}}export{Gr as default,Cr as metadata};
