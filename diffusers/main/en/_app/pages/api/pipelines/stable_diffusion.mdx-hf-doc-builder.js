import{S as Rl,i as Kl,s as Yl,e as n,k as f,w as h,t as a,M as Jl,c as o,d as i,m as d,a as s,x as m,h as r,b as l,N as Wn,G as e,g as p,y as g,L as Ql,q as _,o as b,B as v,v as Xl}from"../../../chunks/vendor-hf-doc-builder.js";import{D as I}from"../../../chunks/Docstring-hf-doc-builder.js";import{C as Bl}from"../../../chunks/CodeBlock-hf-doc-builder.js";import{I as _e}from"../../../chunks/IconCopyLink-hf-doc-builder.js";function Zl(Xa){let U,Wi,G,J,Rt,be,Bn,Kt,Rn,Bi,P,Kn,Yt,Yn,Jn,ve,Qn,Xn,De,Zn,eo,we,to,io,ye,no,oo,Ri,A,so,Se,ao,ro,gt,lo,fo,Ki,O,co,Pe,po,uo,Ie,ho,mo,Yi,xe,Jt,go,_o,Ji,_t,bt,bo,Ee,vt,Za,Qi,ke,Qt,vo,Do,Xi,Q,Xt,T,Zt,wo,yo,ei,So,Po,Dt,Io,xo,wt,Eo,ko,F,L,ti,$e,$o,To,ii,ni,Lo,Co,yt,Te,St,er,Mo,Pt,Le,Ao,Oo,C,oi,Ce,No,zo,si,ai,qo,Uo,It,Me,xt,tr,Go,Et,Ae,Fo,Vo,M,ri,Oe,jo,Ho,Ne,li,Wo,Bo,fi,Ro,Ko,kt,ze,$t,ir,Yo,Tt,Jo,Zi,V,X,di,qe,Qo,ci,Xo,en,j,Z,pi,Ue,Zo,ui,es,tn,u,ts,Lt,is,ns,hi,os,ss,Ct,as,rs,Mt,ls,fs,At,ds,cs,Ot,ps,us,Nt,hs,ms,mi,gs,_s,gi,bs,vs,zt,Ds,ws,nn,Ge,on,H,ee,_i,Fe,ys,bi,Ss,sn,te,Ps,vi,Is,xs,an,ie,Ve,Es,je,ks,$s,Ts,He,Ls,Di,Cs,Ms,rn,We,ln,W,ne,wi,Be,As,yi,Os,fn,B,Re,Ns,Si,zs,dn,R,oe,Pi,Ke,qs,Ii,Us,cn,w,Ye,Gs,xi,Fs,Vs,Je,js,qt,Hs,Ws,Bs,se,Qe,Rs,Ei,Ks,Ys,N,Xe,Js,ki,Qs,Xs,$i,Zs,ea,ae,Ze,ta,et,ia,Ti,na,oa,pn,K,re,Li,tt,sa,Ci,aa,un,y,it,ra,Mi,la,fa,nt,da,Ut,ca,pa,ua,le,ot,ha,Ai,ma,ga,z,st,_a,Oi,ba,va,Ni,Da,wa,fe,at,ya,rt,Sa,zi,Pa,Ia,hn,Y,de,qi,lt,xa,Ui,Ea,mn,S,ft,ka,dt,$a,Gi,Ta,La,Ca,ct,Ma,Gt,Aa,Oa,Na,ce,pt,za,Fi,qa,Ua,q,ut,Ga,Vi,Fa,Va,ji,ja,Ha,pe,ht,Wa,mt,Ba,Hi,Ra,Ka,gn;return be=new _e({}),qe=new _e({}),Ue=new _e({}),Ge=new Bl({props:{code:`from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler

pipeline = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)

# or
euler_scheduler = EulerDiscreteScheduler.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="scheduler")
pipeline = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", scheduler=euler_scheduler)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline, EulerDiscreteScheduler

<span class="hljs-meta">&gt;&gt;&gt; </span>pipeline = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># or</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>euler_scheduler = EulerDiscreteScheduler.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, subfolder=<span class="hljs-string">&quot;scheduler&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipeline = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>, scheduler=euler_scheduler)`}}),Fe=new _e({}),We=new Bl({props:{code:`from diffusers import (
    StableDiffusionPipeline,
    StableDiffusionImg2ImgPipeline,
    StableDiffusionInpaintPipeline,
)

img2text = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4")
img2img = StableDiffusionImg2ImgPipeline(**img2text.components)
inpaint = StableDiffusionInpaintPipeline(**img2text.components)

# now you can use img2text(...), img2img(...), inpaint(...) just like the call methods of each respective pipeline`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    StableDiffusionPipeline,
<span class="hljs-meta">... </span>    StableDiffusionImg2ImgPipeline,
<span class="hljs-meta">... </span>    StableDiffusionInpaintPipeline,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>img2text = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;CompVis/stable-diffusion-v1-4&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>img2img = StableDiffusionImg2ImgPipeline(**img2text.components)
<span class="hljs-meta">&gt;&gt;&gt; </span>inpaint = StableDiffusionInpaintPipeline(**img2text.components)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># now you can use img2text(...), img2img(...), inpaint(...) just like the call methods of each respective pipeline</span>`}}),Be=new _e({}),Re=new I({props:{name:"class diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput",anchor:"diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput",parameters:[{name:"images",val:": typing.Union[typing.List[PIL.Image.Image], numpy.ndarray]"},{name:"nsfw_content_detected",val:": typing.Optional[typing.List[bool]]"}],parametersDescription:[{anchor:"diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput.images",description:`<strong>images</strong> (<code>List[PIL.Image.Image]</code> or <code>np.ndarray</code>) &#x2014;
List of denoised PIL images of length <code>batch_size</code> or numpy array of shape <code>(batch_size, height, width, num_channels)</code>. PIL images or numpy array present the denoised images of the diffusion pipeline.`,name:"images"},{anchor:"diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput.nsfw_content_detected",description:`<strong>nsfw_content_detected</strong> (<code>List[bool]</code>) &#x2014;
List of flags denoting whether the corresponding generated image likely represents &#x201C;not-safe-for-work&#x201D;
(nsfw) content, or <code>None</code> if safety checking could not be performed.`,name:"nsfw_content_detected"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/__init__.py#L13"}}),Ke=new _e({}),Ye=new I({props:{name:"class diffusers.StableDiffusionPipeline",anchor:"diffusers.StableDiffusionPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler, diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler, diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler, diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPImageProcessor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionPipeline.vae",description:`<strong>vae</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.AutoencoderKL">AutoencoderKL</a>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionPipeline.unet",description:'<strong>unet</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.UNet2DConditionModel">UNet2DConditionModel</a>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.',name:"unet"},{anchor:"diffusers.StableDiffusionPipeline.scheduler",description:`<strong>scheduler</strong> (<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin">SchedulerMixin</a>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latents. Can be one of
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">DDIMScheduler</a>, <a href="/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler">LMSDiscreteScheduler</a>, or <a href="/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler">PNDMScheduler</a>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offensive or harmful.
Please, refer to the <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L42"}}),Qe=new I({props:{name:"__call__",anchor:"diffusers.StableDiffusionPipeline.__call__",parameters:[{name:"prompt",val:": typing.Union[str, typing.List[str]]"},{name:"height",val:": int = 512"},{name:"width",val:": int = 512"},{name:"num_inference_steps",val:": int = 50"},{name:"guidance_scale",val:": float = 7.5"},{name:"negative_prompt",val:": typing.Union[str, typing.List[str], NoneType] = None"},{name:"num_images_per_prompt",val:": typing.Optional[int] = 1"},{name:"eta",val:": float = 0.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"latents",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_type",val:": typing.Optional[str] = 'pil'"},{name:"return_dict",val:": bool = True"},{name:"callback",val:": typing.Union[typing.Callable[[int, int, torch.FloatTensor], NoneType], NoneType] = None"},{name:"callback_steps",val:": typing.Optional[int] = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.StableDiffusionPipeline.__call__.prompt",description:`<strong>prompt</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The prompt or prompts to guide the image generation.`,name:"prompt"},{anchor:"diffusers.StableDiffusionPipeline.__call__.height",description:`<strong>height</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The height in pixels of the generated image.`,name:"height"},{anchor:"diffusers.StableDiffusionPipeline.__call__.width",description:`<strong>width</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The width in pixels of the generated image.`,name:"width"},{anchor:"diffusers.StableDiffusionPipeline.__call__.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of denoising steps. More denoising steps usually lead to a higher quality image at the
expense of slower inference.`,name:"num_inference_steps"},{anchor:"diffusers.StableDiffusionPipeline.__call__.guidance_scale",description:`<strong>guidance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 7.5) &#x2014;
Guidance scale as defined in <a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance</a>.
<code>guidance_scale</code> is defined as <code>w</code> of equation 2. of <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow">Imagen
Paper</a>. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>,
usually at the expense of lower image quality.`,name:"guidance_scale"},{anchor:"diffusers.StableDiffusionPipeline.__call__.negative_prompt",description:`<strong>negative_prompt</strong> (<code>str</code> or <code>List[str]</code>, <em>optional</em>) &#x2014;
The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored
if <code>guidance_scale</code> is less than <code>1</code>).`,name:"negative_prompt"},{anchor:"diffusers.StableDiffusionPipeline.__call__.num_images_per_prompt",description:`<strong>num_images_per_prompt</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of images to generate per prompt.`,name:"num_images_per_prompt"},{anchor:"diffusers.StableDiffusionPipeline.__call__.eta",description:`<strong>eta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Corresponds to parameter eta (&#x3B7;) in the DDIM paper: <a href="https://arxiv.org/abs/2010.02502" rel="nofollow">https://arxiv.org/abs/2010.02502</a>. Only applies to
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">schedulers.DDIMScheduler</a>, will be ignored for others.`,name:"eta"},{anchor:"diffusers.StableDiffusionPipeline.__call__.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.`,name:"generator"},{anchor:"diffusers.StableDiffusionPipeline.__call__.latents",description:`<strong>latents</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
tensor will ge generated by sampling using the supplied random <code>generator</code>.`,name:"latents"},{anchor:"diffusers.StableDiffusionPipeline.__call__.output_type",description:`<strong>output_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pil&quot;</code>) &#x2014;
The output format of the generate image. Choose between
<a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">PIL</a>: <code>PIL.Image.Image</code> or <code>np.array</code>.`,name:"output_type"},{anchor:"diffusers.StableDiffusionPipeline.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput">StableDiffusionPipelineOutput</a> instead of a
plain tuple.`,name:"return_dict"},{anchor:"diffusers.StableDiffusionPipeline.__call__.callback",description:`<strong>callback</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
A function that will be called every <code>callback_steps</code> steps during inference. The function will be
called with the following arguments: <code>callback(step: int, timestep: int, latents: torch.FloatTensor)</code>.`,name:"callback"},{anchor:"diffusers.StableDiffusionPipeline.__call__.callback_steps",description:`<strong>callback_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The frequency at which the <code>callback</code> function will be called. If not specified, the callback will be
called at every step.`,name:"callback_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L368",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput"
>StableDiffusionPipelineOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple. When returning a tuple, the first element is a list with the generated images, and the second element is a list of </code>bool<code>s denoting whether the corresponding generated image likely represents "not-safe-for-work" (nsfw) content, according to the </code>safety_checker\`.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput"
>StableDiffusionPipelineOutput</a> or <code>tuple</code></p>
`}}),Xe=new I({props:{name:"enable_attention_slicing",anchor:"diffusers.StableDiffusionPipeline.enable_attention_slicing",parameters:[{name:"slice_size",val:": typing.Union[str, int, NoneType] = 'auto'"}],parametersDescription:[{anchor:"diffusers.StableDiffusionPipeline.enable_attention_slicing.slice_size",description:`<strong>slice_size</strong> (<code>str</code> or <code>int</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
When <code>&quot;auto&quot;</code>, halves the input to the attention heads, so attention will be computed in two steps. If
a number is provided, uses as many slices as <code>attention_head_dim // slice_size</code>. In this case,
<code>attention_head_dim</code> must be a multiple of <code>slice_size</code>.`,name:"slice_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L154"}}),Ze=new I({props:{name:"disable_attention_slicing",anchor:"diffusers.StableDiffusionPipeline.disable_attention_slicing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py#L173"}}),tt=new _e({}),it=new I({props:{name:"class diffusers.StableDiffusionImg2ImgPipeline",anchor:"diffusers.StableDiffusionImg2ImgPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler, diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler, diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler, diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPImageProcessor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionImg2ImgPipeline.vae",description:`<strong>vae</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.AutoencoderKL">AutoencoderKL</a>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.unet",description:'<strong>unet</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.UNet2DConditionModel">UNet2DConditionModel</a>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.',name:"unet"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.scheduler",description:`<strong>scheduler</strong> (<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin">SchedulerMixin</a>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latents. Can be one of
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">DDIMScheduler</a>, <a href="/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler">LMSDiscreteScheduler</a>, or <a href="/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler">PNDMScheduler</a>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offensive or harmful.
Please, refer to the <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L54"}}),ot=new I({props:{name:"__call__",anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__",parameters:[{name:"prompt",val:": typing.Union[str, typing.List[str]]"},{name:"init_image",val:": typing.Union[torch.FloatTensor, PIL.Image.Image]"},{name:"strength",val:": float = 0.8"},{name:"num_inference_steps",val:": typing.Optional[int] = 50"},{name:"guidance_scale",val:": typing.Optional[float] = 7.5"},{name:"negative_prompt",val:": typing.Union[str, typing.List[str], NoneType] = None"},{name:"num_images_per_prompt",val:": typing.Optional[int] = 1"},{name:"eta",val:": typing.Optional[float] = 0.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"output_type",val:": typing.Optional[str] = 'pil'"},{name:"return_dict",val:": bool = True"},{name:"callback",val:": typing.Union[typing.Callable[[int, int, torch.FloatTensor], NoneType], NoneType] = None"},{name:"callback_steps",val:": typing.Optional[int] = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.prompt",description:`<strong>prompt</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The prompt or prompts to guide the image generation.`,name:"prompt"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.init_image",description:`<strong>init_image</strong> (<code>torch.FloatTensor</code> or <code>PIL.Image.Image</code>) &#x2014;
<code>Image</code>, or tensor representing an image batch, that will be used as the starting point for the
process.`,name:"init_image"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.strength",description:`<strong>strength</strong> (<code>float</code>, <em>optional</em>, defaults to 0.8) &#x2014;
Conceptually, indicates how much to transform the reference <code>init_image</code>. Must be between 0 and 1.
<code>init_image</code> will be used as a starting point, adding more noise to it the larger the <code>strength</code>. The
number of denoising steps depends on the amount of noise initially added. When <code>strength</code> is 1, added
noise will be maximum and the denoising process will run for the full number of iterations specified in
<code>num_inference_steps</code>. A value of 1, therefore, essentially ignores <code>init_image</code>.`,name:"strength"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of denoising steps. More denoising steps usually lead to a higher quality image at the
expense of slower inference. This parameter will be modulated by <code>strength</code>.`,name:"num_inference_steps"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.guidance_scale",description:`<strong>guidance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 7.5) &#x2014;
Guidance scale as defined in <a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance</a>.
<code>guidance_scale</code> is defined as <code>w</code> of equation 2. of <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow">Imagen
Paper</a>. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>,
usually at the expense of lower image quality.`,name:"guidance_scale"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.negative_prompt",description:`<strong>negative_prompt</strong> (<code>str</code> or <code>List[str]</code>, <em>optional</em>) &#x2014;
The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored
if <code>guidance_scale</code> is less than <code>1</code>).`,name:"negative_prompt"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.num_images_per_prompt",description:`<strong>num_images_per_prompt</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of images to generate per prompt.`,name:"num_images_per_prompt"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.eta",description:`<strong>eta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Corresponds to parameter eta (&#x3B7;) in the DDIM paper: <a href="https://arxiv.org/abs/2010.02502" rel="nofollow">https://arxiv.org/abs/2010.02502</a>. Only applies to
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">schedulers.DDIMScheduler</a>, will be ignored for others.`,name:"eta"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.`,name:"generator"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.output_type",description:`<strong>output_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pil&quot;</code>) &#x2014;
The output format of the generate image. Choose between
<a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">PIL</a>: <code>PIL.Image.Image</code> or <code>np.array</code>.`,name:"output_type"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput">StableDiffusionPipelineOutput</a> instead of a
plain tuple.`,name:"return_dict"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.callback",description:`<strong>callback</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
A function that will be called every <code>callback_steps</code> steps during inference. The function will be
called with the following arguments: <code>callback(step: int, timestep: int, latents: torch.FloatTensor)</code>.`,name:"callback"},{anchor:"diffusers.StableDiffusionImg2ImgPipeline.__call__.callback_steps",description:`<strong>callback_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The frequency at which the <code>callback</code> function will be called. If not specified, the callback will be
called at every step.`,name:"callback_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L418",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput"
>StableDiffusionPipelineOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple. When returning a tuple, the first element is a list with the generated images, and the second element is a list of </code>bool<code>s denoting whether the corresponding generated image likely represents "not-safe-for-work" (nsfw) content, according to the </code>safety_checker\`.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput"
>StableDiffusionPipelineOutput</a> or <code>tuple</code></p>
`}}),st=new I({props:{name:"enable_attention_slicing",anchor:"diffusers.StableDiffusionImg2ImgPipeline.enable_attention_slicing",parameters:[{name:"slice_size",val:": typing.Union[str, int, NoneType] = 'auto'"}],parametersDescription:[{anchor:"diffusers.StableDiffusionImg2ImgPipeline.enable_attention_slicing.slice_size",description:`<strong>slice_size</strong> (<code>str</code> or <code>int</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
When <code>&quot;auto&quot;</code>, halves the input to the attention heads, so attention will be computed in two steps. If
a number is provided, uses as many slices as <code>attention_head_dim // slice_size</code>. In this case,
<code>attention_head_dim</code> must be a multiple of <code>slice_size</code>.`,name:"slice_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L150"}}),at=new I({props:{name:"disable_attention_slicing",anchor:"diffusers.StableDiffusionImg2ImgPipeline.disable_attention_slicing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py#L170"}}),lt=new _e({}),ft=new I({props:{name:"class diffusers.StableDiffusionInpaintPipeline",anchor:"diffusers.StableDiffusionInpaintPipeline",parameters:[{name:"vae",val:": AutoencoderKL"},{name:"text_encoder",val:": CLIPTextModel"},{name:"tokenizer",val:": CLIPTokenizer"},{name:"unet",val:": UNet2DConditionModel"},{name:"scheduler",val:": typing.Union[diffusers.schedulers.scheduling_ddim.DDIMScheduler, diffusers.schedulers.scheduling_pndm.PNDMScheduler, diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler]"},{name:"safety_checker",val:": StableDiffusionSafetyChecker"},{name:"feature_extractor",val:": CLIPImageProcessor"}],parametersDescription:[{anchor:"diffusers.StableDiffusionInpaintPipeline.vae",description:`<strong>vae</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.AutoencoderKL">AutoencoderKL</a>) &#x2014;
Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.`,name:"vae"},{anchor:"diffusers.StableDiffusionInpaintPipeline.text_encoder",description:`<strong>text_encoder</strong> (<code>CLIPTextModel</code>) &#x2014;
Frozen text-encoder. Stable Diffusion uses the text portion of
<a href="https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel" rel="nofollow">CLIP</a>, specifically
the <a href="https://huggingface.co/openai/clip-vit-large-patch14" rel="nofollow">clip-vit-large-patch14</a> variant.`,name:"text_encoder"},{anchor:"diffusers.StableDiffusionInpaintPipeline.tokenizer",description:`<strong>tokenizer</strong> (<code>CLIPTokenizer</code>) &#x2014;
Tokenizer of class
<a href="https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer" rel="nofollow">CLIPTokenizer</a>.`,name:"tokenizer"},{anchor:"diffusers.StableDiffusionInpaintPipeline.unet",description:'<strong>unet</strong> (<a href="/docs/diffusers/main/en/api/models#diffusers.UNet2DConditionModel">UNet2DConditionModel</a>) &#x2014; Conditional U-Net architecture to denoise the encoded image latents.',name:"unet"},{anchor:"diffusers.StableDiffusionInpaintPipeline.scheduler",description:`<strong>scheduler</strong> (<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin">SchedulerMixin</a>) &#x2014;
A scheduler to be used in combination with <code>unet</code> to denoise the encoded image latents. Can be one of
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">DDIMScheduler</a>, <a href="/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler">LMSDiscreteScheduler</a>, or <a href="/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler">PNDMScheduler</a>.`,name:"scheduler"},{anchor:"diffusers.StableDiffusionInpaintPipeline.safety_checker",description:`<strong>safety_checker</strong> (<code>StableDiffusionSafetyChecker</code>) &#x2014;
Classification module that estimates whether generated images could be considered offensive or harmful.
Please, refer to the <a href="https://huggingface.co/runwayml/stable-diffusion-v1-5" rel="nofollow">model card</a> for details.`,name:"safety_checker"},{anchor:"diffusers.StableDiffusionInpaintPipeline.feature_extractor",description:`<strong>feature_extractor</strong> (<code>CLIPFeatureExtractor</code>) &#x2014;
Model that extracts features from generated images to be used as inputs for the <code>safety_checker</code>.`,name:"feature_extractor"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L54"}}),pt=new I({props:{name:"__call__",anchor:"diffusers.StableDiffusionInpaintPipeline.__call__",parameters:[{name:"prompt",val:": typing.Union[str, typing.List[str]]"},{name:"image",val:": typing.Union[torch.FloatTensor, PIL.Image.Image]"},{name:"mask_image",val:": typing.Union[torch.FloatTensor, PIL.Image.Image]"},{name:"height",val:": int = 512"},{name:"width",val:": int = 512"},{name:"num_inference_steps",val:": int = 50"},{name:"guidance_scale",val:": float = 7.5"},{name:"negative_prompt",val:": typing.Union[str, typing.List[str], NoneType] = None"},{name:"num_images_per_prompt",val:": typing.Optional[int] = 1"},{name:"eta",val:": float = 0.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"latents",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_type",val:": typing.Optional[str] = 'pil'"},{name:"return_dict",val:": bool = True"},{name:"callback",val:": typing.Union[typing.Callable[[int, int, torch.FloatTensor], NoneType], NoneType] = None"},{name:"callback_steps",val:": typing.Optional[int] = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.prompt",description:`<strong>prompt</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The prompt or prompts to guide the image generation.`,name:"prompt"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code>) &#x2014;
<code>Image</code>, or tensor representing an image batch which will be inpainted, <em>i.e.</em> parts of the image will
be masked out with <code>mask_image</code> and repainted according to <code>prompt</code>.`,name:"image"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.mask_image",description:`<strong>mask_image</strong> (<code>PIL.Image.Image</code>) &#x2014;
<code>Image</code>, or tensor representing an image batch, to mask <code>image</code>. White pixels in the mask will be
repainted, while black pixels will be preserved. If <code>mask_image</code> is a PIL image, it will be converted
to a single channel (luminance) before use. If it&#x2019;s a tensor, it should contain one color channel (L)
instead of 3, so the expected shape would be <code>(B, H, W, 1)</code>.`,name:"mask_image"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.height",description:`<strong>height</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The height in pixels of the generated image.`,name:"height"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.width",description:`<strong>width</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The width in pixels of the generated image.`,name:"width"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of denoising steps. More denoising steps usually lead to a higher quality image at the
expense of slower inference.`,name:"num_inference_steps"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.guidance_scale",description:`<strong>guidance_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 7.5) &#x2014;
Guidance scale as defined in <a href="https://arxiv.org/abs/2207.12598" rel="nofollow">Classifier-Free Diffusion Guidance</a>.
<code>guidance_scale</code> is defined as <code>w</code> of equation 2. of <a href="https://arxiv.org/pdf/2205.11487.pdf" rel="nofollow">Imagen
Paper</a>. Guidance scale is enabled by setting <code>guidance_scale &gt; 1</code>. Higher guidance scale encourages to generate images that are closely linked to the text <code>prompt</code>,
usually at the expense of lower image quality.`,name:"guidance_scale"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.negative_prompt",description:`<strong>negative_prompt</strong> (<code>str</code> or <code>List[str]</code>, <em>optional</em>) &#x2014;
The prompt or prompts not to guide the image generation. Ignored when not using guidance (i.e., ignored
if <code>guidance_scale</code> is less than <code>1</code>).`,name:"negative_prompt"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.num_images_per_prompt",description:`<strong>num_images_per_prompt</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of images to generate per prompt.`,name:"num_images_per_prompt"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.eta",description:`<strong>eta</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Corresponds to parameter eta (&#x3B7;) in the DDIM paper: <a href="https://arxiv.org/abs/2010.02502" rel="nofollow">https://arxiv.org/abs/2010.02502</a>. Only applies to
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler">schedulers.DDIMScheduler</a>, will be ignored for others.`,name:"eta"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.generator",description:`<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014;
A <a href="https://pytorch.org/docs/stable/generated/torch.Generator.html" rel="nofollow">torch generator</a> to make generation
deterministic.`,name:"generator"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.latents",description:`<strong>latents</strong> (<code>torch.FloatTensor</code>, <em>optional</em>) &#x2014;
Pre-generated noisy latents, sampled from a Gaussian distribution, to be used as inputs for image
generation. Can be used to tweak the same generation with different prompts. If not provided, a latents
tensor will ge generated by sampling using the supplied random <code>generator</code>.`,name:"latents"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.output_type",description:`<strong>output_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pil&quot;</code>) &#x2014;
The output format of the generate image. Choose between
<a href="https://pillow.readthedocs.io/en/stable/" rel="nofollow">PIL</a>: <code>PIL.Image.Image</code> or <code>np.array</code>.`,name:"output_type"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput">StableDiffusionPipelineOutput</a> instead of a
plain tuple.`,name:"return_dict"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.callback",description:`<strong>callback</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
A function that will be called every <code>callback_steps</code> steps during inference. The function will be
called with the following arguments: <code>callback(step: int, timestep: int, latents: torch.FloatTensor)</code>.`,name:"callback"},{anchor:"diffusers.StableDiffusionInpaintPipeline.__call__.callback_steps",description:`<strong>callback_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The frequency at which the <code>callback</code> function will be called. If not specified, the callback will be
called at every step.`,name:"callback_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L414",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput"
>StableDiffusionPipelineOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple. When returning a tuple, the first element is a list with the generated images, and the second element is a list of </code>bool<code>s denoting whether the corresponding generated image likely represents "not-safe-for-work" (nsfw) content, according to the </code>safety_checker\`.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/pipelines/stable_diffusion#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput"
>StableDiffusionPipelineOutput</a> or <code>tuple</code></p>
`}}),ut=new I({props:{name:"enable_attention_slicing",anchor:"diffusers.StableDiffusionInpaintPipeline.enable_attention_slicing",parameters:[{name:"slice_size",val:": typing.Union[str, int, NoneType] = 'auto'"}],parametersDescription:[{anchor:"diffusers.StableDiffusionInpaintPipeline.enable_attention_slicing.slice_size",description:`<strong>slice_size</strong> (<code>str</code> or <code>int</code>, <em>optional</em>, defaults to <code>&quot;auto&quot;</code>) &#x2014;
When <code>&quot;auto&quot;</code>, halves the input to the attention heads, so attention will be computed in two steps. If
a number is provided, uses as many slices as <code>attention_head_dim // slice_size</code>. In this case,
<code>attention_head_dim</code> must be a multiple of <code>slice_size</code>.`,name:"slice_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L143"}}),ht=new I({props:{name:"disable_attention_slicing",anchor:"diffusers.StableDiffusionInpaintPipeline.disable_attention_slicing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py#L163"}}),{c(){U=n("meta"),Wi=f(),G=n("h1"),J=n("a"),Rt=n("span"),h(be.$$.fragment),Bn=f(),Kt=n("span"),Rn=a("Stable diffusion pipelines"),Bi=f(),P=n("p"),Kn=a("Stable Diffusion is a text-to-image "),Yt=n("em"),Yn=a("latent diffusion"),Jn=a(" model created by the researchers and engineers from "),ve=n("a"),Qn=a("CompVis"),Xn=a(", "),De=n("a"),Zn=a("Stability AI"),eo=a(" and "),we=n("a"),to=a("LAION"),io=a(". It\u2019s trained on 512x512 images from a subset of the "),ye=n("a"),no=a("LAION-5B"),oo=a(" dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs."),Ri=f(),A=n("p"),so=a("Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in "),Se=n("a"),ao=a("High-Resolution Image Synthesis with Latent Diffusion Models"),ro=a(" by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\xF6rn Ommer. You can learn more details about it in the "),gt=n("a"),lo=a("specific pipeline for latent diffusion"),fo=a(" that is part of \u{1F917} Diffusers."),Ki=f(),O=n("p"),co=a("For more details about how Stable Diffusion works and how it differs from the base latent diffusion model, please refer to the official "),Pe=n("a"),po=a("launch announcement post"),uo=a(" and "),Ie=n("a"),ho=a("this section of our own blog post"),mo=a("."),Yi=f(),xe=n("p"),Jt=n("em"),go=a("Tips"),_o=a(":"),Ji=f(),_t=n("ul"),bt=n("li"),bo=a("To tweak your prompts on a specific result you liked, you can generate your own latents, as demonstrated in the following notebook: "),Ee=n("a"),vt=n("img"),Qi=f(),ke=n("p"),Qt=n("em"),vo=a("Overview"),Do=a(":"),Xi=f(),Q=n("table"),Xt=n("thead"),T=n("tr"),Zt=n("th"),wo=a("Pipeline"),yo=f(),ei=n("th"),So=a("Tasks"),Po=f(),Dt=n("th"),Io=a("Colab"),xo=f(),wt=n("th"),Eo=a("Demo"),ko=f(),F=n("tbody"),L=n("tr"),ti=n("td"),$e=n("a"),$o=a("pipeline_stable_diffusion.py"),To=f(),ii=n("td"),ni=n("em"),Lo=a("Text-to-Image Generation"),Co=f(),yt=n("td"),Te=n("a"),St=n("img"),Mo=f(),Pt=n("td"),Le=n("a"),Ao=a("\u{1F917} Stable Diffusion"),Oo=f(),C=n("tr"),oi=n("td"),Ce=n("a"),No=a("pipeline_stable_diffusion_img2img.py"),zo=f(),si=n("td"),ai=n("em"),qo=a("Image-to-Image Text-Guided Generation"),Uo=f(),It=n("td"),Me=n("a"),xt=n("img"),Go=f(),Et=n("td"),Ae=n("a"),Fo=a("\u{1F917} Diffuse the Rest"),Vo=f(),M=n("tr"),ri=n("td"),Oe=n("a"),jo=a("pipeline_stable_diffusion_inpaint.py"),Ho=f(),Ne=n("td"),li=n("strong"),Wo=a("Experimental"),Bo=a(" \u2013 "),fi=n("em"),Ro=a("Text-Guided Image Inpainting"),Ko=f(),kt=n("td"),ze=n("a"),$t=n("img"),Yo=f(),Tt=n("td"),Jo=a("Coming soon"),Zi=f(),V=n("h2"),X=n("a"),di=n("span"),h(qe.$$.fragment),Qo=f(),ci=n("span"),Xo=a("Tips"),en=f(),j=n("h3"),Z=n("a"),pi=n("span"),h(Ue.$$.fragment),Zo=f(),ui=n("span"),es=a("How to load and use different schedulers."),tn=f(),u=n("p"),ts=a("The stable diffusion pipeline uses "),Lt=n("a"),is=a("PNDMScheduler"),ns=a(" scheduler by default. But "),hi=n("code"),os=a("diffusers"),ss=a(" provides many other schedulers that can be used with the stable diffusion pipeline such as "),Ct=n("a"),as=a("DDIMScheduler"),rs=a(", "),Mt=n("a"),ls=a("LMSDiscreteScheduler"),fs=a(", "),At=n("a"),ds=a("EulerDiscreteScheduler"),cs=a(", "),Ot=n("a"),ps=a("EulerAncestralDiscreteScheduler"),us=a(` etc.
To use a different scheduler, you can either change it via the `),Nt=n("a"),hs=a("ConfigMixin.from_config()"),ms=a(" method or pass the "),mi=n("code"),gs=a("scheduler"),_s=a(" argument to the "),gi=n("code"),bs=a("from_pretrained"),vs=a(" method of the pipeline. For example, to use the "),zt=n("a"),Ds=a("EulerDiscreteScheduler"),ws=a(", you can do the following:"),nn=f(),h(Ge.$$.fragment),on=f(),H=n("h3"),ee=n("a"),_i=n("span"),h(Fe.$$.fragment),ys=f(),bi=n("span"),Ss=a("How to conver all use cases with multiple or single pipeline"),sn=f(),te=n("p"),Ps=a("If you want to use all possible use cases in a single "),vi=n("code"),Is=a("DiffusionPipeline"),xs=a(" you can either:"),an=f(),ie=n("ul"),Ve=n("li"),Es=a("Make use of the "),je=n("a"),ks=a("Stable Diffusion Mega Pipeline"),$s=a(" or"),Ts=f(),He=n("li"),Ls=a("Make use of the "),Di=n("code"),Cs=a("components"),Ms=a(" functionality to instantiate all components in the most memory-efficient way:"),rn=f(),h(We.$$.fragment),ln=f(),W=n("h2"),ne=n("a"),wi=n("span"),h(Be.$$.fragment),As=f(),yi=n("span"),Os=a("StableDiffusionPipelineOutput"),fn=f(),B=n("div"),h(Re.$$.fragment),Ns=f(),Si=n("p"),zs=a("Output class for Stable Diffusion pipelines."),dn=f(),R=n("h2"),oe=n("a"),Pi=n("span"),h(Ke.$$.fragment),qs=f(),Ii=n("span"),Us=a("StableDiffusionPipeline"),cn=f(),w=n("div"),h(Ye.$$.fragment),Gs=f(),xi=n("p"),Fs=a("Pipeline for text-to-image generation using Stable Diffusion."),Vs=f(),Je=n("p"),js=a("This model inherits from "),qt=n("a"),Hs=a("DiffusionPipeline"),Ws=a(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Bs=f(),se=n("div"),h(Qe.$$.fragment),Rs=f(),Ei=n("p"),Ks=a("Function invoked when calling the pipeline for generation."),Ys=f(),N=n("div"),h(Xe.$$.fragment),Js=f(),ki=n("p"),Qs=a("Enable sliced attention computation."),Xs=f(),$i=n("p"),Zs=a(`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),ea=f(),ae=n("div"),h(Ze.$$.fragment),ta=f(),et=n("p"),ia=a("Disable sliced attention computation. If "),Ti=n("code"),na=a("enable_attention_slicing"),oa=a(` was previously invoked, this method will go
back to computing attention in one step.`),pn=f(),K=n("h2"),re=n("a"),Li=n("span"),h(tt.$$.fragment),sa=f(),Ci=n("span"),aa=a("StableDiffusionImg2ImgPipeline"),un=f(),y=n("div"),h(it.$$.fragment),ra=f(),Mi=n("p"),la=a("Pipeline for text-guided image to image generation using Stable Diffusion."),fa=f(),nt=n("p"),da=a("This model inherits from "),Ut=n("a"),ca=a("DiffusionPipeline"),pa=a(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),ua=f(),le=n("div"),h(ot.$$.fragment),ha=f(),Ai=n("p"),ma=a("Function invoked when calling the pipeline for generation."),ga=f(),z=n("div"),h(st.$$.fragment),_a=f(),Oi=n("p"),ba=a("Enable sliced attention computation."),va=f(),Ni=n("p"),Da=a(`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),wa=f(),fe=n("div"),h(at.$$.fragment),ya=f(),rt=n("p"),Sa=a("Disable sliced attention computation. If "),zi=n("code"),Pa=a("enable_attention_slicing"),Ia=a(` was previously invoked, this method will go
back to computing attention in one step.`),hn=f(),Y=n("h2"),de=n("a"),qi=n("span"),h(lt.$$.fragment),xa=f(),Ui=n("span"),Ea=a("StableDiffusionInpaintPipeline"),mn=f(),S=n("div"),h(ft.$$.fragment),ka=f(),dt=n("p"),$a=a("Pipeline for text-guided image inpainting using Stable Diffusion. "),Gi=n("em"),Ta=a("This is an experimental feature"),La=a("."),Ca=f(),ct=n("p"),Ma=a("This model inherits from "),Gt=n("a"),Aa=a("DiffusionPipeline"),Oa=a(`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Na=f(),ce=n("div"),h(pt.$$.fragment),za=f(),Fi=n("p"),qa=a("Function invoked when calling the pipeline for generation."),Ua=f(),q=n("div"),h(ut.$$.fragment),Ga=f(),Vi=n("p"),Fa=a("Enable sliced attention computation."),Va=f(),ji=n("p"),ja=a(`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),Ha=f(),pe=n("div"),h(ht.$$.fragment),Wa=f(),mt=n("p"),Ba=a("Disable sliced attention computation. If "),Hi=n("code"),Ra=a("enable_attention_slicing"),Ka=a(` was previously invoked, this method will go
back to computing attention in one step.`),this.h()},l(t){const c=Jl('[data-svelte="svelte-1phssyn"]',document.head);U=o(c,"META",{name:!0,content:!0}),c.forEach(i),Wi=d(t),G=o(t,"H1",{class:!0});var _n=s(G);J=o(_n,"A",{id:!0,class:!0,href:!0});var nr=s(J);Rt=o(nr,"SPAN",{});var or=s(Rt);m(be.$$.fragment,or),or.forEach(i),nr.forEach(i),Bn=d(_n),Kt=o(_n,"SPAN",{});var sr=s(Kt);Rn=r(sr,"Stable diffusion pipelines"),sr.forEach(i),_n.forEach(i),Bi=d(t),P=o(t,"P",{});var x=s(P);Kn=r(x,"Stable Diffusion is a text-to-image "),Yt=o(x,"EM",{});var ar=s(Yt);Yn=r(ar,"latent diffusion"),ar.forEach(i),Jn=r(x," model created by the researchers and engineers from "),ve=o(x,"A",{href:!0,rel:!0});var rr=s(ve);Qn=r(rr,"CompVis"),rr.forEach(i),Xn=r(x,", "),De=o(x,"A",{href:!0,rel:!0});var lr=s(De);Zn=r(lr,"Stability AI"),lr.forEach(i),eo=r(x," and "),we=o(x,"A",{href:!0,rel:!0});var fr=s(we);to=r(fr,"LAION"),fr.forEach(i),io=r(x,". It\u2019s trained on 512x512 images from a subset of the "),ye=o(x,"A",{href:!0,rel:!0});var dr=s(ye);no=r(dr,"LAION-5B"),dr.forEach(i),oo=r(x," dataset. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on consumer GPUs."),x.forEach(i),Ri=d(t),A=o(t,"P",{});var Ft=s(A);so=r(Ft,"Latent diffusion is the research on top of which Stable Diffusion was built. It was proposed in "),Se=o(Ft,"A",{href:!0,rel:!0});var cr=s(Se);ao=r(cr,"High-Resolution Image Synthesis with Latent Diffusion Models"),cr.forEach(i),ro=r(Ft," by Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\xF6rn Ommer. You can learn more details about it in the "),gt=o(Ft,"A",{href:!0});var pr=s(gt);lo=r(pr,"specific pipeline for latent diffusion"),pr.forEach(i),fo=r(Ft," that is part of \u{1F917} Diffusers."),Ft.forEach(i),Ki=d(t),O=o(t,"P",{});var Vt=s(O);co=r(Vt,"For more details about how Stable Diffusion works and how it differs from the base latent diffusion model, please refer to the official "),Pe=o(Vt,"A",{href:!0,rel:!0});var ur=s(Pe);po=r(ur,"launch announcement post"),ur.forEach(i),uo=r(Vt," and "),Ie=o(Vt,"A",{href:!0,rel:!0});var hr=s(Ie);ho=r(hr,"this section of our own blog post"),hr.forEach(i),mo=r(Vt,"."),Vt.forEach(i),Yi=d(t),xe=o(t,"P",{});var Ya=s(xe);Jt=o(Ya,"EM",{});var mr=s(Jt);go=r(mr,"Tips"),mr.forEach(i),_o=r(Ya,":"),Ya.forEach(i),Ji=d(t),_t=o(t,"UL",{});var gr=s(_t);bt=o(gr,"LI",{});var Ja=s(bt);bo=r(Ja,"To tweak your prompts on a specific result you liked, you can generate your own latents, as demonstrated in the following notebook: "),Ee=o(Ja,"A",{href:!0,rel:!0});var _r=s(Ee);vt=o(_r,"IMG",{src:!0,alt:!0}),_r.forEach(i),Ja.forEach(i),gr.forEach(i),Qi=d(t),ke=o(t,"P",{});var Qa=s(ke);Qt=o(Qa,"EM",{});var br=s(Qt);vo=r(br,"Overview"),br.forEach(i),Do=r(Qa,":"),Qa.forEach(i),Xi=d(t),Q=o(t,"TABLE",{});var bn=s(Q);Xt=o(bn,"THEAD",{});var vr=s(Xt);T=o(vr,"TR",{});var ue=s(T);Zt=o(ue,"TH",{});var Dr=s(Zt);wo=r(Dr,"Pipeline"),Dr.forEach(i),yo=d(ue),ei=o(ue,"TH",{});var wr=s(ei);So=r(wr,"Tasks"),wr.forEach(i),Po=d(ue),Dt=o(ue,"TH",{align:!0});var yr=s(Dt);Io=r(yr,"Colab"),yr.forEach(i),xo=d(ue),wt=o(ue,"TH",{align:!0});var Sr=s(wt);Eo=r(Sr,"Demo"),Sr.forEach(i),ue.forEach(i),vr.forEach(i),ko=d(bn),F=o(bn,"TBODY",{});var jt=s(F);L=o(jt,"TR",{});var he=s(L);ti=o(he,"TD",{});var Pr=s(ti);$e=o(Pr,"A",{href:!0,rel:!0});var Ir=s($e);$o=r(Ir,"pipeline_stable_diffusion.py"),Ir.forEach(i),Pr.forEach(i),To=d(he),ii=o(he,"TD",{});var xr=s(ii);ni=o(xr,"EM",{});var Er=s(ni);Lo=r(Er,"Text-to-Image Generation"),Er.forEach(i),xr.forEach(i),Co=d(he),yt=o(he,"TD",{align:!0});var kr=s(yt);Te=o(kr,"A",{href:!0,rel:!0});var $r=s(Te);St=o($r,"IMG",{src:!0,alt:!0}),$r.forEach(i),kr.forEach(i),Mo=d(he),Pt=o(he,"TD",{align:!0});var Tr=s(Pt);Le=o(Tr,"A",{href:!0,rel:!0});var Lr=s(Le);Ao=r(Lr,"\u{1F917} Stable Diffusion"),Lr.forEach(i),Tr.forEach(i),he.forEach(i),Oo=d(jt),C=o(jt,"TR",{});var me=s(C);oi=o(me,"TD",{});var Cr=s(oi);Ce=o(Cr,"A",{href:!0,rel:!0});var Mr=s(Ce);No=r(Mr,"pipeline_stable_diffusion_img2img.py"),Mr.forEach(i),Cr.forEach(i),zo=d(me),si=o(me,"TD",{});var Ar=s(si);ai=o(Ar,"EM",{});var Or=s(ai);qo=r(Or,"Image-to-Image Text-Guided Generation"),Or.forEach(i),Ar.forEach(i),Uo=d(me),It=o(me,"TD",{align:!0});var Nr=s(It);Me=o(Nr,"A",{href:!0,rel:!0});var zr=s(Me);xt=o(zr,"IMG",{src:!0,alt:!0}),zr.forEach(i),Nr.forEach(i),Go=d(me),Et=o(me,"TD",{align:!0});var qr=s(Et);Ae=o(qr,"A",{href:!0,rel:!0});var Ur=s(Ae);Fo=r(Ur,"\u{1F917} Diffuse the Rest"),Ur.forEach(i),qr.forEach(i),me.forEach(i),Vo=d(jt),M=o(jt,"TR",{});var ge=s(M);ri=o(ge,"TD",{});var Gr=s(ri);Oe=o(Gr,"A",{href:!0,rel:!0});var Fr=s(Oe);jo=r(Fr,"pipeline_stable_diffusion_inpaint.py"),Fr.forEach(i),Gr.forEach(i),Ho=d(ge),Ne=o(ge,"TD",{});var vn=s(Ne);li=o(vn,"STRONG",{});var Vr=s(li);Wo=r(Vr,"Experimental"),Vr.forEach(i),Bo=r(vn," \u2013 "),fi=o(vn,"EM",{});var jr=s(fi);Ro=r(jr,"Text-Guided Image Inpainting"),jr.forEach(i),vn.forEach(i),Ko=d(ge),kt=o(ge,"TD",{align:!0});var Hr=s(kt);ze=o(Hr,"A",{href:!0,rel:!0});var Wr=s(ze);$t=o(Wr,"IMG",{src:!0,alt:!0}),Wr.forEach(i),Hr.forEach(i),Yo=d(ge),Tt=o(ge,"TD",{align:!0});var Br=s(Tt);Jo=r(Br,"Coming soon"),Br.forEach(i),ge.forEach(i),jt.forEach(i),bn.forEach(i),Zi=d(t),V=o(t,"H2",{class:!0});var Dn=s(V);X=o(Dn,"A",{id:!0,class:!0,href:!0});var Rr=s(X);di=o(Rr,"SPAN",{});var Kr=s(di);m(qe.$$.fragment,Kr),Kr.forEach(i),Rr.forEach(i),Qo=d(Dn),ci=o(Dn,"SPAN",{});var Yr=s(ci);Xo=r(Yr,"Tips"),Yr.forEach(i),Dn.forEach(i),en=d(t),j=o(t,"H3",{class:!0});var wn=s(j);Z=o(wn,"A",{id:!0,class:!0,href:!0});var Jr=s(Z);pi=o(Jr,"SPAN",{});var Qr=s(pi);m(Ue.$$.fragment,Qr),Qr.forEach(i),Jr.forEach(i),Zo=d(wn),ui=o(wn,"SPAN",{});var Xr=s(ui);es=r(Xr,"How to load and use different schedulers."),Xr.forEach(i),wn.forEach(i),tn=d(t),u=o(t,"P",{});var D=s(u);ts=r(D,"The stable diffusion pipeline uses "),Lt=o(D,"A",{href:!0});var Zr=s(Lt);is=r(Zr,"PNDMScheduler"),Zr.forEach(i),ns=r(D," scheduler by default. But "),hi=o(D,"CODE",{});var el=s(hi);os=r(el,"diffusers"),el.forEach(i),ss=r(D," provides many other schedulers that can be used with the stable diffusion pipeline such as "),Ct=o(D,"A",{href:!0});var tl=s(Ct);as=r(tl,"DDIMScheduler"),tl.forEach(i),rs=r(D,", "),Mt=o(D,"A",{href:!0});var il=s(Mt);ls=r(il,"LMSDiscreteScheduler"),il.forEach(i),fs=r(D,", "),At=o(D,"A",{href:!0});var nl=s(At);ds=r(nl,"EulerDiscreteScheduler"),nl.forEach(i),cs=r(D,", "),Ot=o(D,"A",{href:!0});var ol=s(Ot);ps=r(ol,"EulerAncestralDiscreteScheduler"),ol.forEach(i),us=r(D,` etc.
To use a different scheduler, you can either change it via the `),Nt=o(D,"A",{href:!0});var sl=s(Nt);hs=r(sl,"ConfigMixin.from_config()"),sl.forEach(i),ms=r(D," method or pass the "),mi=o(D,"CODE",{});var al=s(mi);gs=r(al,"scheduler"),al.forEach(i),_s=r(D," argument to the "),gi=o(D,"CODE",{});var rl=s(gi);bs=r(rl,"from_pretrained"),rl.forEach(i),vs=r(D," method of the pipeline. For example, to use the "),zt=o(D,"A",{href:!0});var ll=s(zt);Ds=r(ll,"EulerDiscreteScheduler"),ll.forEach(i),ws=r(D,", you can do the following:"),D.forEach(i),nn=d(t),m(Ge.$$.fragment,t),on=d(t),H=o(t,"H3",{class:!0});var yn=s(H);ee=o(yn,"A",{id:!0,class:!0,href:!0});var fl=s(ee);_i=o(fl,"SPAN",{});var dl=s(_i);m(Fe.$$.fragment,dl),dl.forEach(i),fl.forEach(i),ys=d(yn),bi=o(yn,"SPAN",{});var cl=s(bi);Ss=r(cl,"How to conver all use cases with multiple or single pipeline"),cl.forEach(i),yn.forEach(i),sn=d(t),te=o(t,"P",{});var Sn=s(te);Ps=r(Sn,"If you want to use all possible use cases in a single "),vi=o(Sn,"CODE",{});var pl=s(vi);Is=r(pl,"DiffusionPipeline"),pl.forEach(i),xs=r(Sn," you can either:"),Sn.forEach(i),an=d(t),ie=o(t,"UL",{});var Pn=s(ie);Ve=o(Pn,"LI",{});var In=s(Ve);Es=r(In,"Make use of the "),je=o(In,"A",{href:!0,rel:!0});var ul=s(je);ks=r(ul,"Stable Diffusion Mega Pipeline"),ul.forEach(i),$s=r(In," or"),In.forEach(i),Ts=d(Pn),He=o(Pn,"LI",{});var xn=s(He);Ls=r(xn,"Make use of the "),Di=o(xn,"CODE",{});var hl=s(Di);Cs=r(hl,"components"),hl.forEach(i),Ms=r(xn," functionality to instantiate all components in the most memory-efficient way:"),xn.forEach(i),Pn.forEach(i),rn=d(t),m(We.$$.fragment,t),ln=d(t),W=o(t,"H2",{class:!0});var En=s(W);ne=o(En,"A",{id:!0,class:!0,href:!0});var ml=s(ne);wi=o(ml,"SPAN",{});var gl=s(wi);m(Be.$$.fragment,gl),gl.forEach(i),ml.forEach(i),As=d(En),yi=o(En,"SPAN",{});var _l=s(yi);Os=r(_l,"StableDiffusionPipelineOutput"),_l.forEach(i),En.forEach(i),fn=d(t),B=o(t,"DIV",{class:!0});var kn=s(B);m(Re.$$.fragment,kn),Ns=d(kn),Si=o(kn,"P",{});var bl=s(Si);zs=r(bl,"Output class for Stable Diffusion pipelines."),bl.forEach(i),kn.forEach(i),dn=d(t),R=o(t,"H2",{class:!0});var $n=s(R);oe=o($n,"A",{id:!0,class:!0,href:!0});var vl=s(oe);Pi=o(vl,"SPAN",{});var Dl=s(Pi);m(Ke.$$.fragment,Dl),Dl.forEach(i),vl.forEach(i),qs=d($n),Ii=o($n,"SPAN",{});var wl=s(Ii);Us=r(wl,"StableDiffusionPipeline"),wl.forEach(i),$n.forEach(i),cn=d(t),w=o(t,"DIV",{class:!0});var E=s(w);m(Ye.$$.fragment,E),Gs=d(E),xi=o(E,"P",{});var yl=s(xi);Fs=r(yl,"Pipeline for text-to-image generation using Stable Diffusion."),yl.forEach(i),Vs=d(E),Je=o(E,"P",{});var Tn=s(Je);js=r(Tn,"This model inherits from "),qt=o(Tn,"A",{href:!0});var Sl=s(qt);Hs=r(Sl,"DiffusionPipeline"),Sl.forEach(i),Ws=r(Tn,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Tn.forEach(i),Bs=d(E),se=o(E,"DIV",{class:!0});var Ln=s(se);m(Qe.$$.fragment,Ln),Rs=d(Ln),Ei=o(Ln,"P",{});var Pl=s(Ei);Ks=r(Pl,"Function invoked when calling the pipeline for generation."),Pl.forEach(i),Ln.forEach(i),Ys=d(E),N=o(E,"DIV",{class:!0});var Ht=s(N);m(Xe.$$.fragment,Ht),Js=d(Ht),ki=o(Ht,"P",{});var Il=s(ki);Qs=r(Il,"Enable sliced attention computation."),Il.forEach(i),Xs=d(Ht),$i=o(Ht,"P",{});var xl=s($i);Zs=r(xl,`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),xl.forEach(i),Ht.forEach(i),ea=d(E),ae=o(E,"DIV",{class:!0});var Cn=s(ae);m(Ze.$$.fragment,Cn),ta=d(Cn),et=o(Cn,"P",{});var Mn=s(et);ia=r(Mn,"Disable sliced attention computation. If "),Ti=o(Mn,"CODE",{});var El=s(Ti);na=r(El,"enable_attention_slicing"),El.forEach(i),oa=r(Mn,` was previously invoked, this method will go
back to computing attention in one step.`),Mn.forEach(i),Cn.forEach(i),E.forEach(i),pn=d(t),K=o(t,"H2",{class:!0});var An=s(K);re=o(An,"A",{id:!0,class:!0,href:!0});var kl=s(re);Li=o(kl,"SPAN",{});var $l=s(Li);m(tt.$$.fragment,$l),$l.forEach(i),kl.forEach(i),sa=d(An),Ci=o(An,"SPAN",{});var Tl=s(Ci);aa=r(Tl,"StableDiffusionImg2ImgPipeline"),Tl.forEach(i),An.forEach(i),un=d(t),y=o(t,"DIV",{class:!0});var k=s(y);m(it.$$.fragment,k),ra=d(k),Mi=o(k,"P",{});var Ll=s(Mi);la=r(Ll,"Pipeline for text-guided image to image generation using Stable Diffusion."),Ll.forEach(i),fa=d(k),nt=o(k,"P",{});var On=s(nt);da=r(On,"This model inherits from "),Ut=o(On,"A",{href:!0});var Cl=s(Ut);ca=r(Cl,"DiffusionPipeline"),Cl.forEach(i),pa=r(On,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),On.forEach(i),ua=d(k),le=o(k,"DIV",{class:!0});var Nn=s(le);m(ot.$$.fragment,Nn),ha=d(Nn),Ai=o(Nn,"P",{});var Ml=s(Ai);ma=r(Ml,"Function invoked when calling the pipeline for generation."),Ml.forEach(i),Nn.forEach(i),ga=d(k),z=o(k,"DIV",{class:!0});var Wt=s(z);m(st.$$.fragment,Wt),_a=d(Wt),Oi=o(Wt,"P",{});var Al=s(Oi);ba=r(Al,"Enable sliced attention computation."),Al.forEach(i),va=d(Wt),Ni=o(Wt,"P",{});var Ol=s(Ni);Da=r(Ol,`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),Ol.forEach(i),Wt.forEach(i),wa=d(k),fe=o(k,"DIV",{class:!0});var zn=s(fe);m(at.$$.fragment,zn),ya=d(zn),rt=o(zn,"P",{});var qn=s(rt);Sa=r(qn,"Disable sliced attention computation. If "),zi=o(qn,"CODE",{});var Nl=s(zi);Pa=r(Nl,"enable_attention_slicing"),Nl.forEach(i),Ia=r(qn,` was previously invoked, this method will go
back to computing attention in one step.`),qn.forEach(i),zn.forEach(i),k.forEach(i),hn=d(t),Y=o(t,"H2",{class:!0});var Un=s(Y);de=o(Un,"A",{id:!0,class:!0,href:!0});var zl=s(de);qi=o(zl,"SPAN",{});var ql=s(qi);m(lt.$$.fragment,ql),ql.forEach(i),zl.forEach(i),xa=d(Un),Ui=o(Un,"SPAN",{});var Ul=s(Ui);Ea=r(Ul,"StableDiffusionInpaintPipeline"),Ul.forEach(i),Un.forEach(i),mn=d(t),S=o(t,"DIV",{class:!0});var $=s(S);m(ft.$$.fragment,$),ka=d($),dt=o($,"P",{});var Gn=s(dt);$a=r(Gn,"Pipeline for text-guided image inpainting using Stable Diffusion. "),Gi=o(Gn,"EM",{});var Gl=s(Gi);Ta=r(Gl,"This is an experimental feature"),Gl.forEach(i),La=r(Gn,"."),Gn.forEach(i),Ca=d($),ct=o($,"P",{});var Fn=s(ct);Ma=r(Fn,"This model inherits from "),Gt=o(Fn,"A",{href:!0});var Fl=s(Gt);Aa=r(Fl,"DiffusionPipeline"),Fl.forEach(i),Oa=r(Fn,`. Check the superclass documentation for the generic methods the
library implements for all the pipelines (such as downloading or saving, running on a particular device, etc.)`),Fn.forEach(i),Na=d($),ce=o($,"DIV",{class:!0});var Vn=s(ce);m(pt.$$.fragment,Vn),za=d(Vn),Fi=o(Vn,"P",{});var Vl=s(Fi);qa=r(Vl,"Function invoked when calling the pipeline for generation."),Vl.forEach(i),Vn.forEach(i),Ua=d($),q=o($,"DIV",{class:!0});var Bt=s(q);m(ut.$$.fragment,Bt),Ga=d(Bt),Vi=o(Bt,"P",{});var jl=s(Vi);Fa=r(jl,"Enable sliced attention computation."),jl.forEach(i),Va=d(Bt),ji=o(Bt,"P",{});var Hl=s(ji);ja=r(Hl,`When this option is enabled, the attention module will split the input tensor in slices, to compute attention
in several steps. This is useful to save some memory in exchange for a small speed decrease.`),Hl.forEach(i),Bt.forEach(i),Ha=d($),pe=o($,"DIV",{class:!0});var jn=s(pe);m(ht.$$.fragment,jn),Wa=d(jn),mt=o(jn,"P",{});var Hn=s(mt);Ba=r(Hn,"Disable sliced attention computation. If "),Hi=o(Hn,"CODE",{});var Wl=s(Hi);Ra=r(Wl,"enable_attention_slicing"),Wl.forEach(i),Ka=r(Hn,` was previously invoked, this method will go
back to computing attention in one step.`),Hn.forEach(i),jn.forEach(i),$.forEach(i),this.h()},h(){l(U,"name","hf:doc:metadata"),l(U,"content",JSON.stringify(ef)),l(J,"id","stable-diffusion-pipelines"),l(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(J,"href","#stable-diffusion-pipelines"),l(G,"class","relative group"),l(ve,"href","https://github.com/CompVis"),l(ve,"rel","nofollow"),l(De,"href","https://stability.ai/"),l(De,"rel","nofollow"),l(we,"href","https://laion.ai/"),l(we,"rel","nofollow"),l(ye,"href","https://laion.ai/blog/laion-5b/"),l(ye,"rel","nofollow"),l(Se,"href","https://arxiv.org/abs/2112.10752"),l(Se,"rel","nofollow"),l(gt,"href","pipelines/latent_diffusion"),l(Pe,"href","https://stability.ai/blog/stable-diffusion-announcement"),l(Pe,"rel","nofollow"),l(Ie,"href","https://huggingface.co/blog/stable_diffusion#how-does-stable-diffusion-work"),l(Ie,"rel","nofollow"),Wn(vt.src,Za="https://colab.research.google.com/assets/colab-badge.svg")||l(vt,"src",Za),l(vt,"alt","Open In Colab"),l(Ee,"href","https://colab.research.google.com/github/pcuenca/diffusers-examples/blob/main/notebooks/stable-diffusion-seeds.ipynb"),l(Ee,"rel","nofollow"),l(Dt,"align","center"),l(wt,"align","center"),l($e,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py"),l($e,"rel","nofollow"),Wn(St.src,er="https://colab.research.google.com/assets/colab-badge.svg")||l(St,"src",er),l(St,"alt","Open In Colab"),l(Te,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb"),l(Te,"rel","nofollow"),l(yt,"align","center"),l(Le,"href","https://huggingface.co/spaces/stabilityai/stable-diffusion"),l(Le,"rel","nofollow"),l(Pt,"align","center"),l(Ce,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py"),l(Ce,"rel","nofollow"),Wn(xt.src,tr="https://colab.research.google.com/assets/colab-badge.svg")||l(xt,"src",tr),l(xt,"alt","Open In Colab"),l(Me,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/image_2_image_using_diffusers.ipynb"),l(Me,"rel","nofollow"),l(It,"align","center"),l(Ae,"href","https://huggingface.co/spaces/huggingface/diffuse-the-rest"),l(Ae,"rel","nofollow"),l(Et,"align","center"),l(Oe,"href","https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py"),l(Oe,"rel","nofollow"),Wn($t.src,ir="https://colab.research.google.com/assets/colab-badge.svg")||l($t,"src",ir),l($t,"alt","Open In Colab"),l(ze,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/in_painting_with_stable_diffusion_using_diffusers.ipynb"),l(ze,"rel","nofollow"),l(kt,"align","center"),l(Tt,"align","center"),l(X,"id","tips"),l(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(X,"href","#tips"),l(V,"class","relative group"),l(Z,"id","how-to-load-and-use-different-schedulers"),l(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Z,"href","#how-to-load-and-use-different-schedulers"),l(j,"class","relative group"),l(Lt,"href","/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler"),l(Ct,"href","/docs/diffusers/main/en/api/schedulers#diffusers.DDIMScheduler"),l(Mt,"href","/docs/diffusers/main/en/api/schedulers#diffusers.LMSDiscreteScheduler"),l(At,"href","/docs/diffusers/main/en/api/schedulers#diffusers.EulerDiscreteScheduler"),l(Ot,"href","/docs/diffusers/main/en/api/schedulers#diffusers.EulerAncestralDiscreteScheduler"),l(Nt,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),l(zt,"href","/docs/diffusers/main/en/api/schedulers#diffusers.EulerDiscreteScheduler"),l(ee,"id","how-to-conver-all-use-cases-with-multiple-or-single-pipeline"),l(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ee,"href","#how-to-conver-all-use-cases-with-multiple-or-single-pipeline"),l(H,"class","relative group"),l(je,"href","https://github.com/huggingface/diffusers/tree/main/examples/community#stable-diffusion-mega"),l(je,"rel","nofollow"),l(ne,"id","diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput"),l(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ne,"href","#diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput"),l(W,"class","relative group"),l(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(oe,"id","diffusers.StableDiffusionPipeline"),l(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(oe,"href","#diffusers.StableDiffusionPipeline"),l(R,"class","relative group"),l(qt,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.DiffusionPipeline"),l(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(re,"id","diffusers.StableDiffusionImg2ImgPipeline"),l(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(re,"href","#diffusers.StableDiffusionImg2ImgPipeline"),l(K,"class","relative group"),l(Ut,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.DiffusionPipeline"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(de,"id","diffusers.StableDiffusionInpaintPipeline"),l(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(de,"href","#diffusers.StableDiffusionInpaintPipeline"),l(Y,"class","relative group"),l(Gt,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.DiffusionPipeline"),l(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,c){e(document.head,U),p(t,Wi,c),p(t,G,c),e(G,J),e(J,Rt),g(be,Rt,null),e(G,Bn),e(G,Kt),e(Kt,Rn),p(t,Bi,c),p(t,P,c),e(P,Kn),e(P,Yt),e(Yt,Yn),e(P,Jn),e(P,ve),e(ve,Qn),e(P,Xn),e(P,De),e(De,Zn),e(P,eo),e(P,we),e(we,to),e(P,io),e(P,ye),e(ye,no),e(P,oo),p(t,Ri,c),p(t,A,c),e(A,so),e(A,Se),e(Se,ao),e(A,ro),e(A,gt),e(gt,lo),e(A,fo),p(t,Ki,c),p(t,O,c),e(O,co),e(O,Pe),e(Pe,po),e(O,uo),e(O,Ie),e(Ie,ho),e(O,mo),p(t,Yi,c),p(t,xe,c),e(xe,Jt),e(Jt,go),e(xe,_o),p(t,Ji,c),p(t,_t,c),e(_t,bt),e(bt,bo),e(bt,Ee),e(Ee,vt),p(t,Qi,c),p(t,ke,c),e(ke,Qt),e(Qt,vo),e(ke,Do),p(t,Xi,c),p(t,Q,c),e(Q,Xt),e(Xt,T),e(T,Zt),e(Zt,wo),e(T,yo),e(T,ei),e(ei,So),e(T,Po),e(T,Dt),e(Dt,Io),e(T,xo),e(T,wt),e(wt,Eo),e(Q,ko),e(Q,F),e(F,L),e(L,ti),e(ti,$e),e($e,$o),e(L,To),e(L,ii),e(ii,ni),e(ni,Lo),e(L,Co),e(L,yt),e(yt,Te),e(Te,St),e(L,Mo),e(L,Pt),e(Pt,Le),e(Le,Ao),e(F,Oo),e(F,C),e(C,oi),e(oi,Ce),e(Ce,No),e(C,zo),e(C,si),e(si,ai),e(ai,qo),e(C,Uo),e(C,It),e(It,Me),e(Me,xt),e(C,Go),e(C,Et),e(Et,Ae),e(Ae,Fo),e(F,Vo),e(F,M),e(M,ri),e(ri,Oe),e(Oe,jo),e(M,Ho),e(M,Ne),e(Ne,li),e(li,Wo),e(Ne,Bo),e(Ne,fi),e(fi,Ro),e(M,Ko),e(M,kt),e(kt,ze),e(ze,$t),e(M,Yo),e(M,Tt),e(Tt,Jo),p(t,Zi,c),p(t,V,c),e(V,X),e(X,di),g(qe,di,null),e(V,Qo),e(V,ci),e(ci,Xo),p(t,en,c),p(t,j,c),e(j,Z),e(Z,pi),g(Ue,pi,null),e(j,Zo),e(j,ui),e(ui,es),p(t,tn,c),p(t,u,c),e(u,ts),e(u,Lt),e(Lt,is),e(u,ns),e(u,hi),e(hi,os),e(u,ss),e(u,Ct),e(Ct,as),e(u,rs),e(u,Mt),e(Mt,ls),e(u,fs),e(u,At),e(At,ds),e(u,cs),e(u,Ot),e(Ot,ps),e(u,us),e(u,Nt),e(Nt,hs),e(u,ms),e(u,mi),e(mi,gs),e(u,_s),e(u,gi),e(gi,bs),e(u,vs),e(u,zt),e(zt,Ds),e(u,ws),p(t,nn,c),g(Ge,t,c),p(t,on,c),p(t,H,c),e(H,ee),e(ee,_i),g(Fe,_i,null),e(H,ys),e(H,bi),e(bi,Ss),p(t,sn,c),p(t,te,c),e(te,Ps),e(te,vi),e(vi,Is),e(te,xs),p(t,an,c),p(t,ie,c),e(ie,Ve),e(Ve,Es),e(Ve,je),e(je,ks),e(Ve,$s),e(ie,Ts),e(ie,He),e(He,Ls),e(He,Di),e(Di,Cs),e(He,Ms),p(t,rn,c),g(We,t,c),p(t,ln,c),p(t,W,c),e(W,ne),e(ne,wi),g(Be,wi,null),e(W,As),e(W,yi),e(yi,Os),p(t,fn,c),p(t,B,c),g(Re,B,null),e(B,Ns),e(B,Si),e(Si,zs),p(t,dn,c),p(t,R,c),e(R,oe),e(oe,Pi),g(Ke,Pi,null),e(R,qs),e(R,Ii),e(Ii,Us),p(t,cn,c),p(t,w,c),g(Ye,w,null),e(w,Gs),e(w,xi),e(xi,Fs),e(w,Vs),e(w,Je),e(Je,js),e(Je,qt),e(qt,Hs),e(Je,Ws),e(w,Bs),e(w,se),g(Qe,se,null),e(se,Rs),e(se,Ei),e(Ei,Ks),e(w,Ys),e(w,N),g(Xe,N,null),e(N,Js),e(N,ki),e(ki,Qs),e(N,Xs),e(N,$i),e($i,Zs),e(w,ea),e(w,ae),g(Ze,ae,null),e(ae,ta),e(ae,et),e(et,ia),e(et,Ti),e(Ti,na),e(et,oa),p(t,pn,c),p(t,K,c),e(K,re),e(re,Li),g(tt,Li,null),e(K,sa),e(K,Ci),e(Ci,aa),p(t,un,c),p(t,y,c),g(it,y,null),e(y,ra),e(y,Mi),e(Mi,la),e(y,fa),e(y,nt),e(nt,da),e(nt,Ut),e(Ut,ca),e(nt,pa),e(y,ua),e(y,le),g(ot,le,null),e(le,ha),e(le,Ai),e(Ai,ma),e(y,ga),e(y,z),g(st,z,null),e(z,_a),e(z,Oi),e(Oi,ba),e(z,va),e(z,Ni),e(Ni,Da),e(y,wa),e(y,fe),g(at,fe,null),e(fe,ya),e(fe,rt),e(rt,Sa),e(rt,zi),e(zi,Pa),e(rt,Ia),p(t,hn,c),p(t,Y,c),e(Y,de),e(de,qi),g(lt,qi,null),e(Y,xa),e(Y,Ui),e(Ui,Ea),p(t,mn,c),p(t,S,c),g(ft,S,null),e(S,ka),e(S,dt),e(dt,$a),e(dt,Gi),e(Gi,Ta),e(dt,La),e(S,Ca),e(S,ct),e(ct,Ma),e(ct,Gt),e(Gt,Aa),e(ct,Oa),e(S,Na),e(S,ce),g(pt,ce,null),e(ce,za),e(ce,Fi),e(Fi,qa),e(S,Ua),e(S,q),g(ut,q,null),e(q,Ga),e(q,Vi),e(Vi,Fa),e(q,Va),e(q,ji),e(ji,ja),e(S,Ha),e(S,pe),g(ht,pe,null),e(pe,Wa),e(pe,mt),e(mt,Ba),e(mt,Hi),e(Hi,Ra),e(mt,Ka),gn=!0},p:Ql,i(t){gn||(_(be.$$.fragment,t),_(qe.$$.fragment,t),_(Ue.$$.fragment,t),_(Ge.$$.fragment,t),_(Fe.$$.fragment,t),_(We.$$.fragment,t),_(Be.$$.fragment,t),_(Re.$$.fragment,t),_(Ke.$$.fragment,t),_(Ye.$$.fragment,t),_(Qe.$$.fragment,t),_(Xe.$$.fragment,t),_(Ze.$$.fragment,t),_(tt.$$.fragment,t),_(it.$$.fragment,t),_(ot.$$.fragment,t),_(st.$$.fragment,t),_(at.$$.fragment,t),_(lt.$$.fragment,t),_(ft.$$.fragment,t),_(pt.$$.fragment,t),_(ut.$$.fragment,t),_(ht.$$.fragment,t),gn=!0)},o(t){b(be.$$.fragment,t),b(qe.$$.fragment,t),b(Ue.$$.fragment,t),b(Ge.$$.fragment,t),b(Fe.$$.fragment,t),b(We.$$.fragment,t),b(Be.$$.fragment,t),b(Re.$$.fragment,t),b(Ke.$$.fragment,t),b(Ye.$$.fragment,t),b(Qe.$$.fragment,t),b(Xe.$$.fragment,t),b(Ze.$$.fragment,t),b(tt.$$.fragment,t),b(it.$$.fragment,t),b(ot.$$.fragment,t),b(st.$$.fragment,t),b(at.$$.fragment,t),b(lt.$$.fragment,t),b(ft.$$.fragment,t),b(pt.$$.fragment,t),b(ut.$$.fragment,t),b(ht.$$.fragment,t),gn=!1},d(t){i(U),t&&i(Wi),t&&i(G),v(be),t&&i(Bi),t&&i(P),t&&i(Ri),t&&i(A),t&&i(Ki),t&&i(O),t&&i(Yi),t&&i(xe),t&&i(Ji),t&&i(_t),t&&i(Qi),t&&i(ke),t&&i(Xi),t&&i(Q),t&&i(Zi),t&&i(V),v(qe),t&&i(en),t&&i(j),v(Ue),t&&i(tn),t&&i(u),t&&i(nn),v(Ge,t),t&&i(on),t&&i(H),v(Fe),t&&i(sn),t&&i(te),t&&i(an),t&&i(ie),t&&i(rn),v(We,t),t&&i(ln),t&&i(W),v(Be),t&&i(fn),t&&i(B),v(Re),t&&i(dn),t&&i(R),v(Ke),t&&i(cn),t&&i(w),v(Ye),v(Qe),v(Xe),v(Ze),t&&i(pn),t&&i(K),v(tt),t&&i(un),t&&i(y),v(it),v(ot),v(st),v(at),t&&i(hn),t&&i(Y),v(lt),t&&i(mn),t&&i(S),v(ft),v(pt),v(ut),v(ht)}}}const ef={local:"stable-diffusion-pipelines",sections:[{local:"tips",sections:[{local:"how-to-load-and-use-different-schedulers",title:"How to load and use different schedulers."},{local:"how-to-conver-all-use-cases-with-multiple-or-single-pipeline",title:"How to conver all use cases with multiple or single pipeline"}],title:"Tips"},{local:"diffusers.pipelines.stable_diffusion.StableDiffusionPipelineOutput",title:"StableDiffusionPipelineOutput"},{local:"diffusers.StableDiffusionPipeline",title:"StableDiffusionPipeline"},{local:"diffusers.StableDiffusionImg2ImgPipeline",title:"StableDiffusionImg2ImgPipeline"},{local:"diffusers.StableDiffusionInpaintPipeline",title:"StableDiffusionInpaintPipeline"}],title:"Stable diffusion pipelines"};function tf(Xa){return Xl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class rf extends Rl{constructor(U){super();Kl(this,U,tf,Zl,Yl,{})}}export{rf as default,ef as metadata};
