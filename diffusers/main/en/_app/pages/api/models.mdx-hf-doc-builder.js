import{S as Ru,i as Zu,s as eh,e as n,k as d,w as u,t as r,M as th,c as a,d as o,m as c,a as s,x as h,h as i,b as l,G as e,g as m,y as g,q as _,o as v,B as b,v as oh,L as hi}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ju}from"../../chunks/Tip-hf-doc-builder.js";import{D}from"../../chunks/Docstring-hf-doc-builder.js";import{C as gi}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as N}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as ui}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function nh(q){let x,M,$,y,k,f,w,Y;return{c(){x=n("p"),M=r("It is required to be logged in ("),$=n("code"),y=r("huggingface-cli login"),k=r(") when you want to use private or "),f=n("a"),w=r(`gated
models`),Y=r("."),this.h()},l(fe){x=a(fe,"P",{});var B=s(x);M=i(B,"It is required to be logged in ("),$=a(B,"CODE",{});var I=s($);y=i(I,"huggingface-cli login"),I.forEach(o),k=i(B,") when you want to use private or "),f=a(B,"A",{href:!0,rel:!0});var pn=s(f);w=i(pn,`gated
models`),pn.forEach(o),Y=i(B,"."),B.forEach(o),this.h()},h(){l(f,"href","https://huggingface.co/docs/hub/models-gated#gated-models"),l(f,"rel","nofollow")},m(fe,B){m(fe,x,B),e(x,M),e(x,$),e($,y),e(x,k),e(x,f),e(f,w),e(x,Y)},d(fe){fe&&o(x)}}}function ah(q){let x,M,$,y,k;return{c(){x=n("p"),M=r("Activate the special "),$=n("a"),y=r("\u201Coffline-mode\u201D"),k=r(` to use
this method in a firewalled environment.`),this.h()},l(f){x=a(f,"P",{});var w=s(x);M=i(w,"Activate the special "),$=a(w,"A",{href:!0,rel:!0});var Y=s($);y=i(Y,"\u201Coffline-mode\u201D"),Y.forEach(o),k=i(w,` to use
this method in a firewalled environment.`),w.forEach(o),this.h()},h(){l($,"href","https://huggingface.co/diffusers/installation.html#offline-mode"),l($,"rel","nofollow")},m(f,w){m(f,x,w),e(x,M),e(x,$),e($,y),e(x,k)},d(f){f&&o(x)}}}function sh(q){let x,M,$,y,k;return y=new gi({props:{code:`from diffusers import FlaxUNet2DConditionModel

# Download model and configuration from huggingface.co and cache.
model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
# Model was saved using *save_pretrained('./test/saved_model/')* (for example purposes, not runnable).
model, params = FlaxUNet2DConditionModel.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> FlaxUNet2DConditionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model, params = FlaxUNet2DConditionModel.from_pretrained(<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)* (for example purposes, not runnable).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model, params = FlaxUNet2DConditionModel.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),{c(){x=n("p"),M=r("Examples:"),$=d(),u(y.$$.fragment)},l(f){x=a(f,"P",{});var w=s(x);M=i(w,"Examples:"),w.forEach(o),$=c(f),h(y.$$.fragment,f)},m(f,w){m(f,x,w),e(x,M),m(f,$,w),g(y,f,w),k=!0},p:hi,i(f){k||(_(y.$$.fragment,f),k=!0)},o(f){v(y.$$.fragment,f),k=!1},d(f){f&&o(x),f&&o($),b(y,f)}}}function rh(q){let x,M,$,y,k;return y=new gi({props:{code:`from diffusers import FlaxUNet2DConditionModel

# load model
model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
# By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision
params = model.to_bf16(params)
# If you don't want to cast certain parameters (for example layer norm bias and scale)
# then pass the mask as follows
from flax import traverse_util

model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
flat_params = traverse_util.flatten_dict(params)
mask = {
    path: (path[-2] != ("LayerNorm", "bias") and path[-2:] != ("LayerNorm", "scale"))
    for path in flat_params
}
mask = traverse_util.unflatten_dict(mask)
params = model.to_bf16(params, mask)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> FlaxUNet2DConditionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model, params = FlaxUNet2DConditionModel.from_pretrained(<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># By default, the model parameters will be in fp32 precision, to cast these to bfloat16 precision</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>params = model.to_bf16(params)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If you don&#x27;t want to cast certain parameters (for example layer norm bias and scale)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># then pass the mask as follows</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> flax <span class="hljs-keyword">import</span> traverse_util

<span class="hljs-meta">&gt;&gt;&gt; </span>model, params = FlaxUNet2DConditionModel.from_pretrained(<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>flat_params = traverse_util.flatten_dict(params)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask = {
<span class="hljs-meta">... </span>    path: (path[-<span class="hljs-number">2</span>] != (<span class="hljs-string">&quot;LayerNorm&quot;</span>, <span class="hljs-string">&quot;bias&quot;</span>) <span class="hljs-keyword">and</span> path[-<span class="hljs-number">2</span>:] != (<span class="hljs-string">&quot;LayerNorm&quot;</span>, <span class="hljs-string">&quot;scale&quot;</span>))
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> path <span class="hljs-keyword">in</span> flat_params
<span class="hljs-meta">... </span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>mask = traverse_util.unflatten_dict(mask)
<span class="hljs-meta">&gt;&gt;&gt; </span>params = model.to_bf16(params, mask)`}}),{c(){x=n("p"),M=r("Examples:"),$=d(),u(y.$$.fragment)},l(f){x=a(f,"P",{});var w=s(x);M=i(w,"Examples:"),w.forEach(o),$=c(f),h(y.$$.fragment,f)},m(f,w){m(f,x,w),e(x,M),m(f,$,w),g(y,f,w),k=!0},p:hi,i(f){k||(_(y.$$.fragment,f),k=!0)},o(f){v(y.$$.fragment,f),k=!1},d(f){f&&o(x),f&&o($),b(y,f)}}}function ih(q){let x,M,$,y,k;return y=new gi({props:{code:`from diffusers import FlaxUNet2DConditionModel

# load model
model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
# By default, the model params will be in fp32, to cast these to float16
params = model.to_fp16(params)
# If you want don't want to cast certain parameters (for example layer norm bias and scale)
# then pass the mask as follows
from flax import traverse_util

model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
flat_params = traverse_util.flatten_dict(params)
mask = {
    path: (path[-2] != ("LayerNorm", "bias") and path[-2:] != ("LayerNorm", "scale"))
    for path in flat_params
}
mask = traverse_util.unflatten_dict(mask)
params = model.to_fp16(params, mask)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> FlaxUNet2DConditionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model, params = FlaxUNet2DConditionModel.from_pretrained(<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># By default, the model params will be in fp32, to cast these to float16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>params = model.to_fp16(params)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If you want don&#x27;t want to cast certain parameters (for example layer norm bias and scale)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># then pass the mask as follows</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> flax <span class="hljs-keyword">import</span> traverse_util

<span class="hljs-meta">&gt;&gt;&gt; </span>model, params = FlaxUNet2DConditionModel.from_pretrained(<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>flat_params = traverse_util.flatten_dict(params)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask = {
<span class="hljs-meta">... </span>    path: (path[-<span class="hljs-number">2</span>] != (<span class="hljs-string">&quot;LayerNorm&quot;</span>, <span class="hljs-string">&quot;bias&quot;</span>) <span class="hljs-keyword">and</span> path[-<span class="hljs-number">2</span>:] != (<span class="hljs-string">&quot;LayerNorm&quot;</span>, <span class="hljs-string">&quot;scale&quot;</span>))
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> path <span class="hljs-keyword">in</span> flat_params
<span class="hljs-meta">... </span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>mask = traverse_util.unflatten_dict(mask)
<span class="hljs-meta">&gt;&gt;&gt; </span>params = model.to_fp16(params, mask)`}}),{c(){x=n("p"),M=r("Examples:"),$=d(),u(y.$$.fragment)},l(f){x=a(f,"P",{});var w=s(x);M=i(w,"Examples:"),w.forEach(o),$=c(f),h(y.$$.fragment,f)},m(f,w){m(f,x,w),e(x,M),m(f,$,w),g(y,f,w),k=!0},p:hi,i(f){k||(_(y.$$.fragment,f),k=!0)},o(f){v(y.$$.fragment,f),k=!1},d(f){f&&o(x),f&&o($),b(y,f)}}}function lh(q){let x,M,$,y,k;return y=new gi({props:{code:`from diffusers import FlaxUNet2DConditionModel

# Download model and configuration from huggingface.co
model, params = FlaxUNet2DConditionModel.from_pretrained("runwayml/stable-diffusion-v1-5")
# By default, the model params will be in fp32, to illustrate the use of this method,
# we'll first cast to fp16 and back to fp32
params = model.to_f16(params)
# now cast back to fp32
params = model.to_fp32(params)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> FlaxUNet2DConditionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model, params = FlaxUNet2DConditionModel.from_pretrained(<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># By default, the model params will be in fp32, to illustrate the use of this method,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># we&#x27;ll first cast to fp16 and back to fp32</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>params = model.to_f16(params)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># now cast back to fp32</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>params = model.to_fp32(params)`}}),{c(){x=n("p"),M=r("Examples:"),$=d(),u(y.$$.fragment)},l(f){x=a(f,"P",{});var w=s(x);M=i(w,"Examples:"),w.forEach(o),$=c(f),h(y.$$.fragment,f)},m(f,w){m(f,x,w),e(x,M),m(f,$,w),g(y,f,w),k=!0},p:hi,i(f){k||(_(y.$$.fragment,f),k=!0)},o(f){v(y.$$.fragment,f),k=!1},d(f){f&&o(x),f&&o($),b(y,f)}}}function dh(q){let x,M,$,y,k,f,w,Y,fe,B,I,pn,jn,_i,vi,qn,bi,xi,qs,ue,Ie,Pn,Tt,yi,Ln,$i,Ps,T,Et,wi,Bn,Di,ki,mn,fn,Mi,Ti,Ei,In,ae,Vn,Ni,Ui,zn,Ai,Fi,un,Ci,Oi,ji,se,Nt,qi,Kn,Pi,Li,Sn,Bi,Ii,re,Ut,Vi,Xn,zi,Ki,Wn,Si,Xi,U,At,Wi,Qn,Qi,Hi,he,Yi,Hn,Gi,Ji,Yn,Ri,Zi,el,Ft,tl,Gn,ol,nl,al,Ct,sl,Jn,rl,il,ll,Ve,dl,ze,cl,Ke,Ot,pl,Rn,ml,fl,Se,jt,ul,qt,hl,Zn,gl,_l,Ls,ge,Xe,ea,Pt,vl,ta,bl,Bs,Lt,Bt,Is,_e,We,oa,It,xl,na,yl,Vs,V,Vt,$l,aa,wl,Dl,zt,kl,hn,Ml,Tl,El,gn,Kt,zs,ve,Qe,sa,St,Nl,ra,Ul,Ks,Xt,Wt,Ss,be,He,ia,Qt,Al,la,Fl,Xs,z,Ht,Cl,da,Ol,jl,Yt,ql,_n,Pl,Ll,Bl,vn,Gt,Ws,xe,Ye,ca,Jt,Il,pa,Vl,Qs,Rt,Zt,Hs,ye,Ge,ma,eo,zl,fa,Kl,Ys,K,to,Sl,ua,Xl,Wl,oo,Ql,bn,Hl,Yl,Gl,xn,no,Gs,$e,Je,ha,ao,Jl,ga,Rl,Js,we,so,Zl,_a,ed,Rs,De,Re,va,ro,td,ba,od,Zs,ke,io,nd,xa,ad,er,Me,Ze,ya,lo,sd,$a,rd,tr,S,co,id,wa,ld,dd,po,cd,yn,pd,md,fd,$n,mo,or,Te,et,Da,fo,ud,ka,hd,nr,Ee,uo,gd,Ma,_d,ar,Ne,tt,Ta,ho,vd,Ea,bd,sr,X,go,xd,Na,yd,$d,_o,wd,wn,Dd,kd,Md,Dn,vo,rr,Ue,ot,Ua,bo,Td,Aa,Ed,ir,F,xo,Nd,Fa,Ud,Ad,Ca,Fd,Cd,yo,Od,Oa,jd,qd,Pd,ja,Ld,Bd,kn,$o,lr,Ae,nt,qa,wo,Id,Pa,Vd,dr,Do,ko,cr,Fe,at,La,Mo,zd,Ba,Kd,pr,E,To,Sd,Ia,Xd,Wd,Mn,Tn,Qd,Hd,Yd,P,Eo,Gd,Va,Jd,Rd,No,Zd,za,ec,tc,oc,Uo,nc,Ka,ac,sc,rc,st,ic,rt,Ao,lc,Fo,dc,Sa,cc,pc,mc,G,Co,fc,W,uc,Xa,hc,gc,Wa,_c,vc,Qa,bc,xc,Ha,yc,$c,wc,Ya,Dc,kc,it,Mc,J,Oo,Tc,Q,Ec,Ga,Nc,Uc,Ja,Ac,Fc,Ra,Cc,Oc,Za,jc,qc,Pc,es,Lc,Bc,lt,Ic,ie,jo,Vc,H,zc,ts,Kc,Sc,os,Xc,Wc,ns,Qc,Hc,as,Yc,Gc,Jc,dt,mr,Ce,ct,ss,qo,Rc,rs,Zc,fr,Oe,Po,ep,pt,Lo,tp,is,op,ur,je,mt,ls,Bo,np,ds,ap,hr,C,Io,sp,cs,rp,ip,Vo,lp,En,dp,cp,pp,zo,mp,Ko,fp,up,hp,ps,gp,_p,ee,ms,So,vp,bp,fs,Xo,xp,yp,us,Wo,$p,wp,hs,Qo,Dp,gr,qe,ft,gs,Ho,kp,_s,Mp,_r,te,Yo,Tp,vs,Ep,Np,ut,Go,Up,bs,Ap,vr,Pe,ht,xs,Jo,Fp,ys,Cp,br,oe,Ro,Op,$s,jp,qp,gt,Zo,Pp,ws,Lp,xr,Le,_t,Ds,en,Bp,ks,Ip,yr,j,tn,Vp,Ms,zp,Kp,on,Sp,nn,Xp,Wp,Qp,Ts,Hp,Yp,ne,Es,an,Gp,Jp,Ns,sn,Rp,Zp,Us,rn,em,tm,As,ln,om,$r;return f=new N({}),Tt=new N({}),Et=new D({props:{name:"class diffusers.ModelMixin",anchor:"diffusers.ModelMixin",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L134"}}),Nt=new D({props:{name:"disable_gradient_checkpointing",anchor:"diffusers.ModelMixin.disable_gradient_checkpointing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L172"}}),Ut=new D({props:{name:"enable_gradient_checkpointing",anchor:"diffusers.ModelMixin.enable_gradient_checkpointing",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L161"}}),At=new D({props:{name:"from_pretrained",anchor:"diffusers.ModelMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.ModelMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids should have an organization name, like <code>google/ddpm-celebahq-256</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using <code>~ModelMixin.save_config</code>, e.g.,
<code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"diffusers.ModelMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>Union[str, os.PathLike]</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"diffusers.ModelMixin.from_pretrained.torch_dtype",description:`<strong>torch_dtype</strong> (<code>str</code> or <code>torch.dtype</code>, <em>optional</em>) &#x2014;
Override the default <code>torch.dtype</code> and load the model under this dtype. If <code>&quot;auto&quot;</code> is passed the dtype
will be automatically derived from the model&#x2019;s weights.`,name:"torch_dtype"},{anchor:"diffusers.ModelMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"diffusers.ModelMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"diffusers.ModelMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"diffusers.ModelMixin.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"diffusers.ModelMixin.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (i.e., do not try to download the model).`,name:"local_files_only(bool,"},{anchor:"diffusers.ModelMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>diffusers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"diffusers.ModelMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"diffusers.ModelMixin.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&quot;</code>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo (either remote in
huggingface.co or downloaded locally), you can specify the folder name here.`,name:"subfolder"},{anchor:"diffusers.ModelMixin.from_pretrained.mirror",description:`<strong>mirror</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Mirror source to accelerate downloads in China. If you are from China and have an accessibility
problem, you can set this option to resolve it. Note that we do not guarantee the timeliness or safety.
Please refer to the mirror site for more information.`,name:"mirror"},{anchor:"diffusers.ModelMixin.from_pretrained.device_map",description:`<strong>device_map</strong> (<code>str</code> or <code>Dict[str, Union[int, str, torch.device]]</code>, <em>optional</em>) &#x2014;
A map that specifies where each submodule should go. It doesn&#x2019;t need to be refined to each
parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the
same device.</p>
<p>To have Accelerate compute the most optimized <code>device_map</code> automatically, set <code>device_map=&quot;auto&quot;</code>. For
more information about each option see <a href="https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map" rel="nofollow">designing a device
map</a>.`,name:"device_map"},{anchor:"diffusers.ModelMixin.from_pretrained.low_cpu_mem_usage",description:`<strong>low_cpu_mem_usage</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code> if torch version &gt;= 1.9.0 else <code>False</code>) &#x2014;
Speed up model loading by not initializing the weights and only loading the pre-trained weights. This
also tries to not use more than 1x model size in CPU memory (including peak memory) while loading the
model. This is only supported when torch version &gt;= 1.9.0. If you are using an older version of torch,
setting this argument to <code>True</code> will raise an error.`,name:"low_cpu_mem_usage"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L232"}}),Ve=new Ju({props:{$$slots:{default:[nh]},$$scope:{ctx:q}}}),ze=new Ju({props:{$$slots:{default:[ah]},$$scope:{ctx:q}}}),Ot=new D({props:{name:"num_parameters",anchor:"diffusers.ModelMixin.num_parameters",parameters:[{name:"only_trainable",val:": bool = False"},{name:"exclude_embeddings",val:": bool = False"}],parametersDescription:[{anchor:"diffusers.ModelMixin.num_parameters.only_trainable",description:`<strong>only_trainable</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return only the number of trainable parameters`,name:"only_trainable"},{anchor:"diffusers.ModelMixin.num_parameters.exclude_embeddings",description:`<strong>exclude_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return only the number of non-embeddings parameters`,name:"exclude_embeddings"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L651",returnDescription:`
<p>The number of parameters.</p>
`,returnType:`
<p><code>int</code></p>
`}}),jt=new D({props:{name:"save_pretrained",anchor:"diffusers.ModelMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"is_main_process",val:": bool = True"},{name:"save_function",val:": typing.Callable = <function save at 0x7f1ede98a670>"}],parametersDescription:[{anchor:"diffusers.ModelMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory to which to save. Will be created if it doesn&#x2019;t exist.`,name:"save_directory"},{anchor:"diffusers.ModelMixin.save_pretrained.is_main_process",description:`<strong>is_main_process</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the process calling this is the main process or not. Useful when in distributed training like
TPUs and need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on
the main process to avoid race conditions.`,name:"is_main_process"},{anchor:"diffusers.ModelMixin.save_pretrained.save_function",description:`<strong>save_function</strong> (<code>Callable</code>) &#x2014;
The function to use to save the state dictionary. Useful on distributed training like TPUs when one
need to replace <code>torch.save</code> by another method.`,name:"save_function"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_utils.py#L182"}}),Pt=new N({}),Bt=new D({props:{name:"class diffusers.models.unet_2d.UNet2DOutput",anchor:"diffusers.models.unet_2d.UNet2DOutput",parameters:[{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.unet_2d.UNet2DOutput.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Hidden states output. Output of last layer of model.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d.py#L28"}}),It=new N({}),Vt=new D({props:{name:"class diffusers.UNet2DModel",anchor:"diffusers.UNet2DModel",parameters:[{name:"sample_size",val:": typing.Optional[int] = None"},{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 3"},{name:"center_input_sample",val:": bool = False"},{name:"time_embedding_type",val:": str = 'positional'"},{name:"freq_shift",val:": int = 0"},{name:"flip_sin_to_cos",val:": bool = True"},{name:"down_block_types",val:": typing.Tuple[str] = ('DownBlock2D', 'AttnDownBlock2D', 'AttnDownBlock2D', 'AttnDownBlock2D')"},{name:"up_block_types",val:": typing.Tuple[str] = ('AttnUpBlock2D', 'AttnUpBlock2D', 'AttnUpBlock2D', 'UpBlock2D')"},{name:"block_out_channels",val:": typing.Tuple[int] = (224, 448, 672, 896)"},{name:"layers_per_block",val:": int = 2"},{name:"mid_block_scale_factor",val:": float = 1"},{name:"downsample_padding",val:": int = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"attention_head_dim",val:": int = 8"},{name:"norm_num_groups",val:": int = 32"},{name:"norm_eps",val:": float = 1e-05"}],parametersDescription:[{anchor:"diffusers.UNet2DModel.sample_size",description:`<strong>sample_size</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>, <em>optional</em>) &#x2014;
Input sample size.`,name:"sample_size"},{anchor:"diffusers.UNet2DModel.in_channels",description:"<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014; Number of channels in the input image.",name:"in_channels"},{anchor:"diffusers.UNet2DModel.out_channels",description:"<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014; Number of channels in the output.",name:"out_channels"},{anchor:"diffusers.UNet2DModel.center_input_sample",description:"<strong>center_input_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014; Whether to center the input sample.",name:"center_input_sample"},{anchor:"diffusers.UNet2DModel.time_embedding_type",description:"<strong>time_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;positional&quot;</code>) &#x2014; Type of time embedding to use.",name:"time_embedding_type"},{anchor:"diffusers.UNet2DModel.freq_shift",description:"<strong>freq_shift</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014; Frequency shift for fourier time embedding.",name:"freq_shift"},{anchor:"diffusers.UNet2DModel.flip_sin_to_cos",description:`<strong>flip_sin_to_cos</strong> (<code>bool</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>True</code>): Whether to flip sin to cos for fourier time embedding.`,name:"flip_sin_to_cos"},{anchor:"diffusers.UNet2DModel.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;DownBlock2D&quot;, &quot;AttnDownBlock2D&quot;, &quot;AttnDownBlock2D&quot;, &quot;AttnDownBlock2D&quot;)</code>): Tuple of downsample block
types.`,name:"down_block_types"},{anchor:"diffusers.UNet2DModel.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;AttnUpBlock2D&quot;, &quot;AttnUpBlock2D&quot;, &quot;AttnUpBlock2D&quot;, &quot;UpBlock2D&quot;)</code>): Tuple of upsample block types.`,name:"up_block_types"},{anchor:"diffusers.UNet2DModel.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(224, 448, 672, 896)</code>): Tuple of block output channels.`,name:"block_out_channels"},{anchor:"diffusers.UNet2DModel.layers_per_block",description:"<strong>layers_per_block</strong> (<code>int</code>, <em>optional</em>, defaults to <code>2</code>) &#x2014; The number of layers per block.",name:"layers_per_block"},{anchor:"diffusers.UNet2DModel.mid_block_scale_factor",description:"<strong>mid_block_scale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>1</code>) &#x2014; The scale factor for the mid block.",name:"mid_block_scale_factor"},{anchor:"diffusers.UNet2DModel.downsample_padding",description:"<strong>downsample_padding</strong> (<code>int</code>, <em>optional</em>, defaults to <code>1</code>) &#x2014; The padding for the downsample convolution.",name:"downsample_padding"},{anchor:"diffusers.UNet2DModel.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014; The activation function to use.",name:"act_fn"},{anchor:"diffusers.UNet2DModel.attention_head_dim",description:"<strong>attention_head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to <code>8</code>) &#x2014; The attention head dimension.",name:"attention_head_dim"},{anchor:"diffusers.UNet2DModel.norm_num_groups",description:"<strong>norm_num_groups</strong> (<code>int</code>, <em>optional</em>, defaults to <code>32</code>) &#x2014; The number of groups for the normalization.",name:"norm_num_groups"},{anchor:"diffusers.UNet2DModel.norm_eps",description:"<strong>norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to <code>1e-5</code>) &#x2014; The epsilon for the normalization.",name:"norm_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d.py#L38"}}),Kt=new D({props:{name:"forward",anchor:"diffusers.UNet2DModel.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[torch.Tensor, float, int]"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.UNet2DModel.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; (batch, channel, height, width) noisy inputs tensor",name:"sample"},{anchor:"diffusers.UNet2DModel.forward.timestep",description:"<strong>timestep</strong> (<code>torch.FloatTensor</code> or <code>float</code> or `int) &#x2014; (batch) timesteps",name:"timestep"},{anchor:"diffusers.UNet2DModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d.UNet2DOutput">UNet2DOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d.py#L180",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d.UNet2DOutput"
>UNet2DOutput</a> if <code>return_dict</code> is True,
otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d.UNet2DOutput"
>UNet2DOutput</a> or <code>tuple</code></p>
`}}),St=new N({}),Wt=new D({props:{name:"class diffusers.models.unet_1d.UNet1DOutput",anchor:"diffusers.models.unet_1d.UNet1DOutput",parameters:[{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.unet_1d.UNet1DOutput.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, sample_size)</code>) &#x2014;
Hidden states output. Output of last layer of model.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_1d.py#L29"}}),Qt=new N({}),Ht=new D({props:{name:"class diffusers.UNet1DModel",anchor:"diffusers.UNet1DModel",parameters:[{name:"sample_size",val:": int = 65536"},{name:"sample_rate",val:": typing.Optional[int] = None"},{name:"in_channels",val:": int = 2"},{name:"out_channels",val:": int = 2"},{name:"extra_in_channels",val:": int = 0"},{name:"time_embedding_type",val:": str = 'fourier'"},{name:"flip_sin_to_cos",val:": bool = True"},{name:"use_timestep_embedding",val:": bool = False"},{name:"freq_shift",val:": float = 0.0"},{name:"down_block_types",val:": typing.Tuple[str] = ('DownBlock1DNoSkip', 'DownBlock1D', 'AttnDownBlock1D')"},{name:"up_block_types",val:": typing.Tuple[str] = ('AttnUpBlock1D', 'UpBlock1D', 'UpBlock1DNoSkip')"},{name:"mid_block_type",val:": typing.Tuple[str] = 'UNetMidBlock1D'"},{name:"out_block_type",val:": str = None"},{name:"block_out_channels",val:": typing.Tuple[int] = (32, 32, 64)"},{name:"act_fn",val:": str = None"},{name:"norm_num_groups",val:": int = 8"},{name:"layers_per_block",val:": int = 1"},{name:"downsample_each_block",val:": bool = False"}],parametersDescription:[{anchor:"diffusers.UNet1DModel.sample_size",description:"<strong>sample_size</strong> (<code>int</code>, <em>optional</em>) &#x2014; Default length of sample. Should be adaptable at runtime.",name:"sample_size"},{anchor:"diffusers.UNet1DModel.in_channels",description:"<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014; Number of channels in the input sample.",name:"in_channels"},{anchor:"diffusers.UNet1DModel.out_channels",description:"<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014; Number of channels in the output.",name:"out_channels"},{anchor:"diffusers.UNet1DModel.time_embedding_type",description:"<strong>time_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;fourier&quot;</code>) &#x2014; Type of time embedding to use.",name:"time_embedding_type"},{anchor:"diffusers.UNet1DModel.freq_shift",description:"<strong>freq_shift</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014; Frequency shift for fourier time embedding.",name:"freq_shift"},{anchor:"diffusers.UNet1DModel.flip_sin_to_cos",description:`<strong>flip_sin_to_cos</strong> (<code>bool</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>False</code>): Whether to flip sin to cos for fourier time embedding.`,name:"flip_sin_to_cos"},{anchor:"diffusers.UNet1DModel.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;DownBlock1D&quot;, &quot;DownBlock1DNoSkip&quot;, &quot;AttnDownBlock1D&quot;)</code>): Tuple of downsample block types.`,name:"down_block_types"},{anchor:"diffusers.UNet1DModel.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;UpBlock1D&quot;, &quot;UpBlock1DNoSkip&quot;, &quot;AttnUpBlock1D&quot;)</code>): Tuple of upsample block types.`,name:"up_block_types"},{anchor:"diffusers.UNet1DModel.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(32, 32, 64)</code>): Tuple of block output channels.`,name:"block_out_channels"},{anchor:"diffusers.UNet1DModel.mid_block_type",description:"<strong>mid_block_type</strong> (<code>str</code>, <em>optional</em>, defaults to &#x201C;UNetMidBlock1D&#x201D;) &#x2014; block type for middle of UNet.",name:"mid_block_type"},{anchor:"diffusers.UNet1DModel.out_block_type",description:"<strong>out_block_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014; optional output processing of UNet.",name:"out_block_type"},{anchor:"diffusers.UNet1DModel.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to None) &#x2014; optional activitation function in UNet blocks.",name:"act_fn"},{anchor:"diffusers.UNet1DModel.norm_num_groups",description:"<strong>norm_num_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014; group norm member count in UNet blocks.",name:"norm_num_groups"},{anchor:"diffusers.UNet1DModel.layers_per_block",description:"<strong>layers_per_block</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014; added number of layers in a UNet block.",name:"layers_per_block"},{anchor:"diffusers.UNet1DModel.downsample_each_block",description:`<strong>downsample_each_block</strong> (<code>int</code>, <em>optional</em>, defaults to False &#x2014;
experimental feature for using a UNet without upsampling.`,name:"downsample_each_block"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_1d.py#L39"}}),Gt=new D({props:{name:"forward",anchor:"diffusers.UNet1DModel.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[torch.Tensor, float, int]"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.UNet1DModel.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; <code>(batch_size, sample_size, num_channels)</code> noisy inputs tensor",name:"sample"},{anchor:"diffusers.UNet1DModel.forward.timestep",description:"<strong>timestep</strong> (<code>torch.FloatTensor</code> or <code>float</code> or `int) &#x2014; (batch) timesteps",name:"timestep"},{anchor:"diffusers.UNet1DModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/models#diffusers.models.unet_1d.UNet1DOutput">UNet1DOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_1d.py#L190",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_1d.UNet1DOutput"
>UNet1DOutput</a> if <code>return_dict</code> is True,
otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_1d.UNet1DOutput"
>UNet1DOutput</a> or <code>tuple</code></p>
`}}),Jt=new N({}),Zt=new D({props:{name:"class diffusers.models.unet_2d_condition.UNet2DConditionOutput",anchor:"diffusers.models.unet_2d_condition.UNet2DConditionOutput",parameters:[{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.unet_2d_condition.UNet2DConditionOutput.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Hidden states conditioned on <code>encoder_hidden_states</code> input. Output of last layer of model.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py#L40"}}),eo=new N({}),to=new D({props:{name:"class diffusers.UNet2DConditionModel",anchor:"diffusers.UNet2DConditionModel",parameters:[{name:"sample_size",val:": typing.Optional[int] = None"},{name:"in_channels",val:": int = 4"},{name:"out_channels",val:": int = 4"},{name:"center_input_sample",val:": bool = False"},{name:"flip_sin_to_cos",val:": bool = True"},{name:"freq_shift",val:": int = 0"},{name:"down_block_types",val:": typing.Tuple[str] = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D')"},{name:"up_block_types",val:": typing.Tuple[str] = ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D')"},{name:"block_out_channels",val:": typing.Tuple[int] = (320, 640, 1280, 1280)"},{name:"layers_per_block",val:": int = 2"},{name:"downsample_padding",val:": int = 1"},{name:"mid_block_scale_factor",val:": float = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"norm_num_groups",val:": int = 32"},{name:"norm_eps",val:": float = 1e-05"},{name:"cross_attention_dim",val:": int = 1280"},{name:"attention_head_dim",val:": int = 8"}],parametersDescription:[{anchor:"diffusers.UNet2DConditionModel.sample_size",description:"<strong>sample_size</strong> (<code>int</code>, <em>optional</em>) &#x2014; The size of the input sample.",name:"sample_size"},{anchor:"diffusers.UNet2DConditionModel.in_channels",description:"<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014; The number of channels in the input sample.",name:"in_channels"},{anchor:"diffusers.UNet2DConditionModel.out_channels",description:"<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014; The number of channels in the output.",name:"out_channels"},{anchor:"diffusers.UNet2DConditionModel.center_input_sample",description:"<strong>center_input_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014; Whether to center the input sample.",name:"center_input_sample"},{anchor:"diffusers.UNet2DConditionModel.flip_sin_to_cos",description:`<strong>flip_sin_to_cos</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to flip the sin to cos in the time embedding.`,name:"flip_sin_to_cos"},{anchor:"diffusers.UNet2DConditionModel.freq_shift",description:"<strong>freq_shift</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014; The frequency shift to apply to the time embedding.",name:"freq_shift"},{anchor:"diffusers.UNet2DConditionModel.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <code>(&quot;CrossAttnDownBlock2D&quot;, &quot;CrossAttnDownBlock2D&quot;, &quot;CrossAttnDownBlock2D&quot;, &quot;DownBlock2D&quot;)</code>) &#x2014;
The tuple of downsample blocks to use.`,name:"down_block_types"},{anchor:"diffusers.UNet2DConditionModel.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <code>(&quot;UpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;,)</code>) &#x2014;
The tuple of upsample blocks to use.`,name:"up_block_types"},{anchor:"diffusers.UNet2DConditionModel.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(320, 640, 1280, 1280)</code>) &#x2014;
The tuple of output channels for each block.`,name:"block_out_channels"},{anchor:"diffusers.UNet2DConditionModel.layers_per_block",description:"<strong>layers_per_block</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014; The number of layers per block.",name:"layers_per_block"},{anchor:"diffusers.UNet2DConditionModel.downsample_padding",description:"<strong>downsample_padding</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014; The padding to use for the downsampling convolution.",name:"downsample_padding"},{anchor:"diffusers.UNet2DConditionModel.mid_block_scale_factor",description:"<strong>mid_block_scale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014; The scale factor to use for the mid block.",name:"mid_block_scale_factor"},{anchor:"diffusers.UNet2DConditionModel.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014; The activation function to use.",name:"act_fn"},{anchor:"diffusers.UNet2DConditionModel.norm_num_groups",description:"<strong>norm_num_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014; The number of groups to use for the normalization.",name:"norm_num_groups"},{anchor:"diffusers.UNet2DConditionModel.norm_eps",description:"<strong>norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014; The epsilon to use for the normalization.",name:"norm_eps"},{anchor:"diffusers.UNet2DConditionModel.cross_attention_dim",description:"<strong>cross_attention_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014; The dimension of the cross attention features.",name:"cross_attention_dim"},{anchor:"diffusers.UNet2DConditionModel.attention_head_dim",description:"<strong>attention_head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014; The dimension of the attention heads.",name:"attention_head_dim"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py#L50"}}),no=new D({props:{name:"forward",anchor:"diffusers.UNet2DConditionModel.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[torch.Tensor, float, int]"},{name:"encoder_hidden_states",val:": Tensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.UNet2DConditionModel.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; (batch, channel, height, width) noisy inputs tensor",name:"sample"},{anchor:"diffusers.UNet2DConditionModel.forward.timestep",description:"<strong>timestep</strong> (<code>torch.FloatTensor</code> or <code>float</code> or <code>int</code>) &#x2014; (batch) timesteps",name:"timestep"},{anchor:"diffusers.UNet2DConditionModel.forward.encoder_hidden_states",description:"<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code>) &#x2014; (batch, channel, height, width) encoder hidden states",name:"encoder_hidden_states"},{anchor:"diffusers.UNet2DConditionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d_condition.UNet2DConditionOutput">models.unet_2d_condition.UNet2DConditionOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition.py#L243",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d_condition.UNet2DConditionOutput"
>UNet2DConditionOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d_condition.UNet2DConditionOutput"
>UNet2DConditionOutput</a> or <code>tuple</code></p>
`}}),ao=new N({}),so=new D({props:{name:"class diffusers.models.vae.DecoderOutput",anchor:"diffusers.models.vae.DecoderOutput",parameters:[{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.vae.DecoderOutput.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Decoded output sample of the model. Output of the last layer of the model.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L28"}}),ro=new N({}),io=new D({props:{name:"class diffusers.models.vae.VQEncoderOutput",anchor:"diffusers.models.vae.VQEncoderOutput",parameters:[{name:"latents",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.vae.VQEncoderOutput.latents",description:`<strong>latents</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Encoded output sample of the model. Output of the last layer of the model.`,name:"latents"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L41"}}),lo=new N({}),co=new D({props:{name:"class diffusers.VQModel",anchor:"diffusers.VQModel",parameters:[{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 3"},{name:"down_block_types",val:": typing.Tuple[str] = ('DownEncoderBlock2D',)"},{name:"up_block_types",val:": typing.Tuple[str] = ('UpDecoderBlock2D',)"},{name:"block_out_channels",val:": typing.Tuple[int] = (64,)"},{name:"layers_per_block",val:": int = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"latent_channels",val:": int = 3"},{name:"sample_size",val:": int = 32"},{name:"num_vq_embeddings",val:": int = 256"},{name:"norm_num_groups",val:": int = 32"},{name:"vq_embed_dim",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.VQModel.in_channels",description:"<strong>in_channels</strong> (int, <em>optional</em>, defaults to 3) &#x2014; Number of channels in the input image.",name:"in_channels"},{anchor:"diffusers.VQModel.out_channels",description:"<strong>out_channels</strong> (int,  <em>optional</em>, defaults to 3) &#x2014; Number of channels in the output.",name:"out_channels"},{anchor:"diffusers.VQModel.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;DownEncoderBlock2D&quot;,)</code>): Tuple of downsample block types.`,name:"down_block_types"},{anchor:"diffusers.VQModel.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;UpDecoderBlock2D&quot;,)</code>): Tuple of upsample block types.`,name:"up_block_types"},{anchor:"diffusers.VQModel.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(64,)</code>): Tuple of block output channels.`,name:"block_out_channels"},{anchor:"diffusers.VQModel.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014; The activation function to use.",name:"act_fn"},{anchor:"diffusers.VQModel.latent_channels",description:"<strong>latent_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>3</code>) &#x2014; Number of channels in the latent space.",name:"latent_channels"},{anchor:"diffusers.VQModel.sample_size",description:"<strong>sample_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>32</code>) &#x2014; TODO",name:"sample_size"},{anchor:"diffusers.VQModel.num_vq_embeddings",description:"<strong>num_vq_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to <code>256</code>) &#x2014; Number of codebook vectors in the VQ-VAE.",name:"num_vq_embeddings"},{anchor:"diffusers.VQModel.vq_embed_dim",description:"<strong>vq_embed_dim</strong> (<code>int</code>, <em>optional</em>) &#x2014; Hidden dim of codebook vectors in the VQ-VAE.",name:"vq_embed_dim"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L394"}}),mo=new D({props:{name:"forward",anchor:"diffusers.VQModel.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.VQModel.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; Input sample.",name:"sample"},{anchor:"diffusers.VQModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>DecoderOutput</code> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L489"}}),fo=new N({}),uo=new D({props:{name:"class diffusers.models.vae.AutoencoderKLOutput",anchor:"diffusers.models.vae.AutoencoderKLOutput",parameters:[{name:"latent_dist",val:": DiagonalGaussianDistribution"}],parametersDescription:[{anchor:"diffusers.models.vae.AutoencoderKLOutput.latent_dist",description:`<strong>latent_dist</strong> (<code>DiagonalGaussianDistribution</code>) &#x2014;
Encoded outputs of <code>Encoder</code> represented as the mean and logvar of <code>DiagonalGaussianDistribution</code>.
<code>DiagonalGaussianDistribution</code> allows for sampling latents from the distribution.`,name:"latent_dist"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L54"}}),ho=new N({}),go=new D({props:{name:"class diffusers.AutoencoderKL",anchor:"diffusers.AutoencoderKL",parameters:[{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 3"},{name:"down_block_types",val:": typing.Tuple[str] = ('DownEncoderBlock2D',)"},{name:"up_block_types",val:": typing.Tuple[str] = ('UpDecoderBlock2D',)"},{name:"block_out_channels",val:": typing.Tuple[int] = (64,)"},{name:"layers_per_block",val:": int = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"latent_channels",val:": int = 4"},{name:"norm_num_groups",val:": int = 32"},{name:"sample_size",val:": int = 32"}],parametersDescription:[{anchor:"diffusers.AutoencoderKL.in_channels",description:"<strong>in_channels</strong> (int, <em>optional</em>, defaults to 3) &#x2014; Number of channels in the input image.",name:"in_channels"},{anchor:"diffusers.AutoencoderKL.out_channels",description:"<strong>out_channels</strong> (int,  <em>optional</em>, defaults to 3) &#x2014; Number of channels in the output.",name:"out_channels"},{anchor:"diffusers.AutoencoderKL.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;DownEncoderBlock2D&quot;,)</code>): Tuple of downsample block types.`,name:"down_block_types"},{anchor:"diffusers.AutoencoderKL.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(&quot;UpDecoderBlock2D&quot;,)</code>): Tuple of upsample block types.`,name:"up_block_types"},{anchor:"diffusers.AutoencoderKL.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to  &#x2014;
obj:<code>(64,)</code>): Tuple of block output channels.`,name:"block_out_channels"},{anchor:"diffusers.AutoencoderKL.act_fn",description:"<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014; The activation function to use.",name:"act_fn"},{anchor:"diffusers.AutoencoderKL.latent_channels",description:"<strong>latent_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <code>4</code>) &#x2014; Number of channels in the latent space.",name:"latent_channels"},{anchor:"diffusers.AutoencoderKL.sample_size",description:"<strong>sample_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>32</code>) &#x2014; TODO",name:"sample_size"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L506"}}),vo=new D({props:{name:"forward",anchor:"diffusers.AutoencoderKL.forward",parameters:[{name:"sample",val:": FloatTensor"},{name:"sample_posterior",val:": bool = False"},{name:"return_dict",val:": bool = True"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],parametersDescription:[{anchor:"diffusers.AutoencoderKL.forward.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; Input sample.",name:"sample"},{anchor:"diffusers.AutoencoderKL.forward.sample_posterior",description:`<strong>sample_posterior</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to sample from the posterior.`,name:"sample_posterior"},{anchor:"diffusers.AutoencoderKL.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <code>DecoderOutput</code> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae.py#L588"}}),bo=new N({}),xo=new D({props:{name:"class diffusers.Transformer2DModel",anchor:"diffusers.Transformer2DModel",parameters:[{name:"num_attention_heads",val:": int = 16"},{name:"attention_head_dim",val:": int = 88"},{name:"in_channels",val:": typing.Optional[int] = None"},{name:"num_layers",val:": int = 1"},{name:"dropout",val:": float = 0.0"},{name:"norm_num_groups",val:": int = 32"},{name:"cross_attention_dim",val:": typing.Optional[int] = None"},{name:"attention_bias",val:": bool = False"},{name:"sample_size",val:": typing.Optional[int] = None"},{name:"num_vector_embeds",val:": typing.Optional[int] = None"},{name:"activation_fn",val:": str = 'geglu'"},{name:"num_embeds_ada_norm",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.Transformer2DModel.num_attention_heads",description:"<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014; The number of heads to use for multi-head attention.",name:"num_attention_heads"},{anchor:"diffusers.Transformer2DModel.attention_head_dim",description:"<strong>attention_head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 88) &#x2014; The number of channels in each head.",name:"attention_head_dim"},{anchor:"diffusers.Transformer2DModel.in_channels",description:`<strong>in_channels</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Pass if the input is continuous. The number of channels in the input and output.`,name:"in_channels"},{anchor:"diffusers.Transformer2DModel.num_layers",description:"<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014; The number of layers of Transformer blocks to use.",name:"num_layers"},{anchor:"diffusers.Transformer2DModel.dropout",description:"<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014; The dropout probability to use.",name:"dropout"},{anchor:"diffusers.Transformer2DModel.cross_attention_dim",description:"<strong>cross_attention_dim</strong> (<code>int</code>, <em>optional</em>) &#x2014; The number of context dimensions to use.",name:"cross_attention_dim"},{anchor:"diffusers.Transformer2DModel.sample_size",description:`<strong>sample_size</strong> (<code>int</code>, <em>optional</em>) &#x2014; Pass if the input is discrete. The width of the latent images.
Note that this is fixed at training time as it is used for learning a number of position embeddings. See
<code>ImagePositionalEmbeddings</code>.`,name:"sample_size"},{anchor:"diffusers.Transformer2DModel.num_vector_embeds",description:`<strong>num_vector_embeds</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Pass if the input is discrete. The number of classes of the vector embeddings of the latent pixels.
Includes the class for the masked latent pixel.`,name:"num_vector_embeds"},{anchor:"diffusers.Transformer2DModel.activation_fn",description:"<strong>activation_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;geglu&quot;</code>) &#x2014; Activation function to be used in feed-forward.",name:"activation_fn"},{anchor:"diffusers.Transformer2DModel.num_embeds_ada_norm",description:`<strong>num_embeds_ada_norm</strong> ( <code>int</code>, <em>optional</em>) &#x2014; Pass if at least one of the norm_layers is <code>AdaLayerNorm</code>.
The number of diffusion steps used during training. Note that this is fixed at training time as it is used
to learn a number of embeddings that are added to the hidden states. During inference, you can denoise for
up to but not more than steps than <code>num_embeds_ada_norm</code>.`,name:"num_embeds_ada_norm"},{anchor:"diffusers.Transformer2DModel.attention_bias",description:`<strong>attention_bias</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Configure if the TransformerBlocks&#x2019; attention should contain a bias parameter.`,name:"attention_bias"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py#L48"}}),$o=new D({props:{name:"forward",anchor:"diffusers.Transformer2DModel.forward",parameters:[{name:"hidden_states",val:""},{name:"encoder_hidden_states",val:" = None"},{name:"timestep",val:" = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.Transformer2DModel.forward.hidden_states",description:`<strong>hidden_states</strong> ( When discrete, <code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>. &#x2014;
When continous, <code>torch.FloatTensor</code> of shape <code>(batch size, channel, height, width)</code>): Input
hidden_states`,name:"hidden_states"},{anchor:"diffusers.Transformer2DModel.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> ( <code>torch.LongTensor</code> of shape <code>(batch size, context dim)</code>, <em>optional</em>) &#x2014;
Conditional embeddings for cross attention layer. If not given, cross-attention defaults to
self-attention.`,name:"encoder_hidden_states"},{anchor:"diffusers.Transformer2DModel.forward.timestep",description:`<strong>timestep</strong> ( <code>torch.long</code>, <em>optional</em>) &#x2014;
Optional timestep to be applied as an embedding in AdaLayerNorm&#x2019;s. Used to indicate denoising step.`,name:"timestep"},{anchor:"diffusers.Transformer2DModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return a <a href="/docs/diffusers/main/en/api/models#diffusers.models.unet_2d_condition.UNet2DConditionOutput">models.unet_2d_condition.UNet2DConditionOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py#L170",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.attention.Transformer2DModelOutput"
>Transformer2DModelOutput</a>
if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample
tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/models#diffusers.models.attention.Transformer2DModelOutput"
>Transformer2DModelOutput</a> or <code>tuple</code></p>
`}}),wo=new N({}),ko=new D({props:{name:"class diffusers.models.attention.Transformer2DModelOutput",anchor:"diffusers.models.attention.Transformer2DModelOutput",parameters:[{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.models.attention.Transformer2DModelOutput.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code> or <code>(batch size, num_vector_embeds - 1, num_latent_pixels)</code> if <a href="/docs/diffusers/main/en/api/models#diffusers.Transformer2DModel">Transformer2DModel</a> is discrete) &#x2014;
Hidden states conditioned on <code>encoder_hidden_states</code> input. If discrete, returns probability distributions
for the unnoised latent pixels.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/attention.py#L30"}}),Mo=new N({}),To=new D({props:{name:"class diffusers.FlaxModelMixin",anchor:"diffusers.FlaxModelMixin",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_flax_utils.py#L45"}}),Eo=new D({props:{name:"from_pretrained",anchor:"diffusers.FlaxModelMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.FlaxModelMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids are namespaced under a user or organization name, like
<code>runwayml/stable-diffusion-v1-5</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using <a href="/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin.save_pretrained">save_pretrained()</a>,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <code>~ModelMixin.to_fp16</code> and
<code>~ModelMixin.to_bf16</code>.`,name:"dtype"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.model_args",description:`<strong>model_args</strong> (sequence of positional arguments, <em>optional</em>) &#x2014;
All remaining positional arguments will be passed to the underlying model&#x2019;s <code>__init__</code> method.`,name:"model_args"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>Union[str, os.PathLike]</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (i.e., do not try to download the model).`,name:"local_files_only(bool,"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file.`,name:"from_pt"},{anchor:"diffusers.FlaxModelMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (remaining dictionary of keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config">from_config()</a>). Each key of <code>kwargs</code> that corresponds to
a configuration attribute will be used to override said attribute with the supplied <code>kwargs</code>
value. Remaining keys that do not correspond to any configuration attribute will be passed to the
underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_flax_utils.py#L195"}}),st=new ui({props:{anchor:"diffusers.FlaxModelMixin.from_pretrained.example",$$slots:{default:[sh]},$$scope:{ctx:q}}}),Ao=new D({props:{name:"save_pretrained",anchor:"diffusers.FlaxModelMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"params",val:": typing.Union[typing.Dict, flax.core.frozen_dict.FrozenDict]"},{name:"is_main_process",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.FlaxModelMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory to which to save. Will be created if it doesn&#x2019;t exist.`,name:"save_directory"},{anchor:"diffusers.FlaxModelMixin.save_pretrained.params",description:`<strong>params</strong> (<code>Union[Dict, FrozenDict]</code>) &#x2014;
A <code>PyTree</code> of model parameters.`,name:"params"},{anchor:"diffusers.FlaxModelMixin.save_pretrained.is_main_process",description:`<strong>is_main_process</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the process calling this is the main process or not. Useful when in distributed training like
TPUs and need to call this function on all processes. In this case, set <code>is_main_process=True</code> only on
the main process to avoid race conditions.`,name:"is_main_process"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_flax_utils.py#L487"}}),Co=new D({props:{name:"to_bf16",anchor:"diffusers.FlaxModelMixin.to_bf16",parameters:[{name:"params",val:": typing.Union[typing.Dict, flax.core.frozen_dict.FrozenDict]"},{name:"mask",val:": typing.Any = None"}],parametersDescription:[{anchor:"diffusers.FlaxModelMixin.to_bf16.params",description:`<strong>params</strong> (<code>Union[Dict, FrozenDict]</code>) &#x2014;
A <code>PyTree</code> of model parameters.`,name:"params"},{anchor:"diffusers.FlaxModelMixin.to_bf16.mask",description:`<strong>mask</strong> (<code>Union[Dict, FrozenDict]</code>) &#x2014;
A <code>PyTree</code> with same structure as the <code>params</code> tree. The leaves should be booleans, <code>True</code> for params
you want to cast, and should be <code>False</code> for those you want to skip.`,name:"mask"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_flax_utils.py#L87"}}),it=new ui({props:{anchor:"diffusers.FlaxModelMixin.to_bf16.example",$$slots:{default:[rh]},$$scope:{ctx:q}}}),Oo=new D({props:{name:"to_fp16",anchor:"diffusers.FlaxModelMixin.to_fp16",parameters:[{name:"params",val:": typing.Union[typing.Dict, flax.core.frozen_dict.FrozenDict]"},{name:"mask",val:": typing.Any = None"}],parametersDescription:[{anchor:"diffusers.FlaxModelMixin.to_fp16.params",description:`<strong>params</strong> (<code>Union[Dict, FrozenDict]</code>) &#x2014;
A <code>PyTree</code> of model parameters.`,name:"params"},{anchor:"diffusers.FlaxModelMixin.to_fp16.mask",description:`<strong>mask</strong> (<code>Union[Dict, FrozenDict]</code>) &#x2014;
A <code>PyTree</code> with same structure as the <code>params</code> tree. The leaves should be booleans, <code>True</code> for params
you want to cast, and should be <code>False</code> for those you want to skip`,name:"mask"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_flax_utils.py#L153"}}),lt=new ui({props:{anchor:"diffusers.FlaxModelMixin.to_fp16.example",$$slots:{default:[ih]},$$scope:{ctx:q}}}),jo=new D({props:{name:"to_fp32",anchor:"diffusers.FlaxModelMixin.to_fp32",parameters:[{name:"params",val:": typing.Union[typing.Dict, flax.core.frozen_dict.FrozenDict]"},{name:"mask",val:": typing.Any = None"}],parametersDescription:[{anchor:"diffusers.FlaxModelMixin.to_fp32.params",description:`<strong>params</strong> (<code>Union[Dict, FrozenDict]</code>) &#x2014;
A <code>PyTree</code> of model parameters.`,name:"params"},{anchor:"diffusers.FlaxModelMixin.to_fp32.mask",description:`<strong>mask</strong> (<code>Union[Dict, FrozenDict]</code>) &#x2014;
A <code>PyTree</code> with same structure as the <code>params</code> tree. The leaves should be booleans, <code>True</code> for params
you want to cast, and should be <code>False</code> for those you want to skip`,name:"mask"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/modeling_flax_utils.py#L126"}}),dt=new ui({props:{anchor:"diffusers.FlaxModelMixin.to_fp32.example",$$slots:{default:[lh]},$$scope:{ctx:q}}}),qo=new N({}),Po=new D({props:{name:"class diffusers.models.unet_2d_condition_flax.FlaxUNet2DConditionOutput",anchor:"diffusers.models.unet_2d_condition_flax.FlaxUNet2DConditionOutput",parameters:[{name:"sample",val:": ndarray"}],parametersDescription:[{anchor:"diffusers.models.unet_2d_condition_flax.FlaxUNet2DConditionOutput.sample",description:`<strong>sample</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Hidden states conditioned on <code>encoder_hidden_states</code> input. Output of last layer of model.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition_flax.py#L36"}}),Lo=new D({props:{name:"replace",anchor:"diffusers.models.unet_2d_condition_flax.FlaxUNet2DConditionOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/diffusers/blob/main/src/flax/struct.py#L108"}}),Bo=new N({}),Io=new D({props:{name:"class diffusers.FlaxUNet2DConditionModel",anchor:"diffusers.FlaxUNet2DConditionModel",parameters:[{name:"sample_size",val:": int = 32"},{name:"in_channels",val:": int = 4"},{name:"out_channels",val:": int = 4"},{name:"down_block_types",val:": typing.Tuple[str] = ('CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D')"},{name:"up_block_types",val:": typing.Tuple[str] = ('UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D')"},{name:"block_out_channels",val:": typing.Tuple[int] = (320, 640, 1280, 1280)"},{name:"layers_per_block",val:": int = 2"},{name:"attention_head_dim",val:": int = 8"},{name:"cross_attention_dim",val:": int = 1280"},{name:"dropout",val:": float = 0.0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"freq_shift",val:": int = 0"},{name:"parent",val:": typing.Union[typing.Type[flax.linen.module.Module], typing.Type[flax.core.scope.Scope], typing.Type[flax.linen.module._Sentinel], NoneType] = <flax.linen.module._Sentinel object at 0x7f1e7b300460>"},{name:"name",val:": str = None"}],parametersDescription:[{anchor:"diffusers.FlaxUNet2DConditionModel.sample_size",description:`<strong>sample_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The size of the input sample.`,name:"sample_size"},{anchor:"diffusers.FlaxUNet2DConditionModel.in_channels",description:`<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of channels in the input sample.`,name:"in_channels"},{anchor:"diffusers.FlaxUNet2DConditionModel.out_channels",description:`<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of channels in the output.`,name:"out_channels"},{anchor:"diffusers.FlaxUNet2DConditionModel.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <code>(&quot;CrossAttnDownBlock2D&quot;, &quot;CrossAttnDownBlock2D&quot;, &quot;CrossAttnDownBlock2D&quot;, &quot;DownBlock2D&quot;)</code>) &#x2014;
The tuple of downsample blocks to use. The corresponding class names will be: &#x201C;FlaxCrossAttnDownBlock2D&#x201D;,
&#x201C;FlaxCrossAttnDownBlock2D&#x201D;, &#x201C;FlaxCrossAttnDownBlock2D&#x201D;, &#x201C;FlaxDownBlock2D&#x201D;`,name:"down_block_types"},{anchor:"diffusers.FlaxUNet2DConditionModel.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <code>(&quot;UpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;, &quot;CrossAttnUpBlock2D&quot;,)</code>) &#x2014;
The tuple of upsample blocks to use. The corresponding class names will be: &#x201C;FlaxUpBlock2D&#x201D;,
&#x201C;FlaxCrossAttnUpBlock2D&#x201D;, &#x201C;FlaxCrossAttnUpBlock2D&#x201D;, &#x201C;FlaxCrossAttnUpBlock2D&#x201D;`,name:"up_block_types"},{anchor:"diffusers.FlaxUNet2DConditionModel.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(320, 640, 1280, 1280)</code>) &#x2014;
The tuple of output channels for each block.`,name:"block_out_channels"},{anchor:"diffusers.FlaxUNet2DConditionModel.layers_per_block",description:`<strong>layers_per_block</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers per block.`,name:"layers_per_block"},{anchor:"diffusers.FlaxUNet2DConditionModel.attention_head_dim",description:`<strong>attention_head_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The dimension of the attention heads.`,name:"attention_head_dim"},{anchor:"diffusers.FlaxUNet2DConditionModel.cross_attention_dim",description:`<strong>cross_attention_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The dimension of the cross attention features.`,name:"cross_attention_dim"},{anchor:"diffusers.FlaxUNet2DConditionModel.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Dropout probability for down, up and bottleneck blocks.`,name:"dropout"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/unet_2d_condition_flax.py#L47"}}),Ho=new N({}),Yo=new D({props:{name:"class diffusers.models.vae_flax.FlaxDecoderOutput",anchor:"diffusers.models.vae_flax.FlaxDecoderOutput",parameters:[{name:"sample",val:": ndarray"}],parametersDescription:[{anchor:"diffusers.models.vae_flax.FlaxDecoderOutput.sample",description:`<strong>sample</strong> (<em>jnp.ndarray</em> of shape <em>(batch_size, num_channels, height, width)</em>) &#x2014;
Decoded output sample of the model. Output of the last layer of the model.`,name:"sample"},{anchor:"diffusers.models.vae_flax.FlaxDecoderOutput.dtype",description:`<strong>dtype</strong> (<code>jnp.dtype</code>, <em>optional</em>, defaults to jnp.float32) &#x2014;
Parameters <em>dtype</em>`,name:"dtype"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae_flax.py#L33"}}),Go=new D({props:{name:"replace",anchor:"diffusers.models.vae_flax.FlaxDecoderOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/diffusers/blob/main/src/flax/struct.py#L108"}}),Jo=new N({}),Ro=new D({props:{name:"class diffusers.models.vae_flax.FlaxAutoencoderKLOutput",anchor:"diffusers.models.vae_flax.FlaxAutoencoderKLOutput",parameters:[{name:"latent_dist",val:": FlaxDiagonalGaussianDistribution"}],parametersDescription:[{anchor:"diffusers.models.vae_flax.FlaxAutoencoderKLOutput.latent_dist",description:`<strong>latent_dist</strong> (<code>FlaxDiagonalGaussianDistribution</code>) &#x2014;
Encoded outputs of <code>Encoder</code> represented as the mean and logvar of <code>FlaxDiagonalGaussianDistribution</code>.
<code>FlaxDiagonalGaussianDistribution</code> allows for sampling latents from the distribution.`,name:"latent_dist"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae_flax.py#L48"}}),Zo=new D({props:{name:"replace",anchor:"diffusers.models.vae_flax.FlaxAutoencoderKLOutput.replace",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/diffusers/blob/main/src/flax/struct.py#L108"}}),en=new N({}),tn=new D({props:{name:"class diffusers.FlaxAutoencoderKL",anchor:"diffusers.FlaxAutoencoderKL",parameters:[{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 3"},{name:"down_block_types",val:": typing.Tuple[str] = ('DownEncoderBlock2D',)"},{name:"up_block_types",val:": typing.Tuple[str] = ('UpDecoderBlock2D',)"},{name:"block_out_channels",val:": typing.Tuple[int] = (64,)"},{name:"layers_per_block",val:": int = 1"},{name:"act_fn",val:": str = 'silu'"},{name:"latent_channels",val:": int = 4"},{name:"norm_num_groups",val:": int = 32"},{name:"sample_size",val:": int = 32"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"parent",val:": typing.Union[typing.Type[flax.linen.module.Module], typing.Type[flax.core.scope.Scope], typing.Type[flax.linen.module._Sentinel], NoneType] = <flax.linen.module._Sentinel object at 0x7f1e7b300460>"},{name:"name",val:": str = None"}],parametersDescription:[{anchor:"diffusers.FlaxAutoencoderKL.in_channels",description:`<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Input channels`,name:"in_channels"},{anchor:"diffusers.FlaxAutoencoderKL.out_channels",description:`<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Output channels`,name:"out_channels"},{anchor:"diffusers.FlaxAutoencoderKL.down_block_types",description:`<strong>down_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <em>(DownEncoderBlock2D)</em>) &#x2014;
DownEncoder block type`,name:"down_block_types"},{anchor:"diffusers.FlaxAutoencoderKL.up_block_types",description:`<strong>up_block_types</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <em>(UpDecoderBlock2D)</em>) &#x2014;
UpDecoder block type`,name:"up_block_types"},{anchor:"diffusers.FlaxAutoencoderKL.block_out_channels",description:`<strong>block_out_channels</strong> (<code>Tuple[str]</code>, <em>optional</em>, defaults to <em>(64,)</em>) &#x2014;
Tuple containing the number of output channels for each block`,name:"block_out_channels"},{anchor:"diffusers.FlaxAutoencoderKL.layers_per_block",description:`<strong>layers_per_block</strong> (<code>int</code>, <em>optional</em>, defaults to <em>2</em>) &#x2014;
Number of Resnet layer for each block`,name:"layers_per_block"},{anchor:"diffusers.FlaxAutoencoderKL.act_fn",description:`<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <em>silu</em>) &#x2014;
Activation function`,name:"act_fn"},{anchor:"diffusers.FlaxAutoencoderKL.latent_channels",description:`<strong>latent_channels</strong> (<code>int</code>, <em>optional</em>, defaults to <em>4</em>) &#x2014;
Latent space channels`,name:"latent_channels"},{anchor:"diffusers.FlaxAutoencoderKL.norm_num_groups",description:`<strong>norm_num_groups</strong> (<code>int</code>, <em>optional</em>, defaults to <em>32</em>) &#x2014;
Norm num group`,name:"norm_num_groups"},{anchor:"diffusers.FlaxAutoencoderKL.sample_size",description:`<strong>sample_size</strong> (<code>int</code>, <em>optional</em>, defaults to <em>32</em>) &#x2014;
Sample input size`,name:"sample_size"},{anchor:"diffusers.FlaxAutoencoderKL.dtype",description:`<strong>dtype</strong> (<code>jnp.dtype</code>, <em>optional</em>, defaults to jnp.float32) &#x2014;
parameters <em>dtype</em>`,name:"dtype"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/vae_flax.py#L721"}}),{c(){x=n("meta"),M=d(),$=n("h1"),y=n("a"),k=n("span"),u(f.$$.fragment),w=d(),Y=n("span"),fe=r("Models"),B=d(),I=n("p"),pn=r(`Diffusers contains pretrained models for popular algorithms and modules for creating the next set of diffusion models.
The primary function of these models is to denoise an input sample, by modeling the distribution $p`),jn=n("em"),_i=r("\\theta(\\mathbf{x}"),vi=r(`{t-1}|\\mathbf{x}_t)$.
The models are built on the base class [\u2018ModelMixin\u2019] that is a `),qn=n("code"),bi=r("torch.nn.module"),xi=r(" with basic functionality for saving and loading models both locally and from the HuggingFace hub."),qs=d(),ue=n("h2"),Ie=n("a"),Pn=n("span"),u(Tt.$$.fragment),yi=d(),Ln=n("span"),$i=r("ModelMixin"),Ps=d(),T=n("div"),u(Et.$$.fragment),wi=d(),Bn=n("p"),Di=r("Base class for all models."),ki=d(),mn=n("p"),fn=n("a"),Mi=r("ModelMixin"),Ti=r(` takes care of storing the configuration of the models and handles methods for loading, downloading
and saving models.`),Ei=d(),In=n("ul"),ae=n("li"),Vn=n("strong"),Ni=r("config_name"),Ui=r(" ("),zn=n("code"),Ai=r("str"),Fi=r(`) \u2014 A filename under which the model should be stored when calling
`),un=n("a"),Ci=r("save_pretrained()"),Oi=r("."),ji=d(),se=n("div"),u(Nt.$$.fragment),qi=d(),Kn=n("p"),Pi=r("Deactivates gradient checkpointing for the current model."),Li=d(),Sn=n("p"),Bi=r(`Note that in other frameworks this feature can be referred to as \u201Cactivation checkpointing\u201D or \u201Ccheckpoint
activations\u201D.`),Ii=d(),re=n("div"),u(Ut.$$.fragment),Vi=d(),Xn=n("p"),zi=r("Activates gradient checkpointing for the current model."),Ki=d(),Wn=n("p"),Si=r(`Note that in other frameworks this feature can be referred to as \u201Cactivation checkpointing\u201D or \u201Ccheckpoint
activations\u201D.`),Xi=d(),U=n("div"),u(At.$$.fragment),Wi=d(),Qn=n("p"),Qi=r("Instantiate a pretrained pytorch model from a pre-trained model configuration."),Hi=d(),he=n("p"),Yi=r("The model is set in evaluation mode by default using "),Hn=n("code"),Gi=r("model.eval()"),Ji=r(` (Dropout modules are deactivated). To train
the model, you should first set it back in training mode with `),Yn=n("code"),Ri=r("model.train()"),Zi=r("."),el=d(),Ft=n("p"),tl=r("The warning "),Gn=n("em"),ol=r("Weights from XXX not initialized from pretrained model"),nl=r(` means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.`),al=d(),Ct=n("p"),sl=r("The warning "),Jn=n("em"),rl=r("Weights from XXX not used in YYY"),il=r(` means that the layer XXX is not used by YYY, therefore those
weights are discarded.`),ll=d(),u(Ve.$$.fragment),dl=d(),u(ze.$$.fragment),cl=d(),Ke=n("div"),u(Ot.$$.fragment),pl=d(),Rn=n("p"),ml=r("Get number of (optionally, trainable or non-embeddings) parameters in the module."),fl=d(),Se=n("div"),u(jt.$$.fragment),ul=d(),qt=n("p"),hl=r(`Save a model and its configuration file to a directory, so that it can be re-loaded using the
`),Zn=n("code"),gl=r("[from_pretrained()](/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin.from_pretrained)"),_l=r(" class method."),Ls=d(),ge=n("h2"),Xe=n("a"),ea=n("span"),u(Pt.$$.fragment),vl=d(),ta=n("span"),bl=r("UNet2DOutput"),Bs=d(),Lt=n("div"),u(Bt.$$.fragment),Is=d(),_e=n("h2"),We=n("a"),oa=n("span"),u(It.$$.fragment),xl=d(),na=n("span"),yl=r("UNet2DModel"),Vs=d(),V=n("div"),u(Vt.$$.fragment),$l=d(),aa=n("p"),wl=r("UNet2DModel is a 2D UNet model that takes in a noisy sample and a timestep and returns sample shaped output."),Dl=d(),zt=n("p"),kl=r("This model inherits from "),hn=n("a"),Ml=r("ModelMixin"),Tl=r(`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),El=d(),gn=n("div"),u(Kt.$$.fragment),zs=d(),ve=n("h2"),Qe=n("a"),sa=n("span"),u(St.$$.fragment),Nl=d(),ra=n("span"),Ul=r("UNet1DOutput"),Ks=d(),Xt=n("div"),u(Wt.$$.fragment),Ss=d(),be=n("h2"),He=n("a"),ia=n("span"),u(Qt.$$.fragment),Al=d(),la=n("span"),Fl=r("UNet1DModel"),Xs=d(),z=n("div"),u(Ht.$$.fragment),Cl=d(),da=n("p"),Ol=r("UNet1DModel is a 1D UNet model that takes in a noisy sample and a timestep and returns sample shaped output."),jl=d(),Yt=n("p"),ql=r("This model inherits from "),_n=n("a"),Pl=r("ModelMixin"),Ll=r(`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),Bl=d(),vn=n("div"),u(Gt.$$.fragment),Ws=d(),xe=n("h2"),Ye=n("a"),ca=n("span"),u(Jt.$$.fragment),Il=d(),pa=n("span"),Vl=r("UNet2DConditionOutput"),Qs=d(),Rt=n("div"),u(Zt.$$.fragment),Hs=d(),ye=n("h2"),Ge=n("a"),ma=n("span"),u(eo.$$.fragment),zl=d(),fa=n("span"),Kl=r("UNet2DConditionModel"),Ys=d(),K=n("div"),u(to.$$.fragment),Sl=d(),ua=n("p"),Xl=r(`UNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a timestep
and returns sample shaped output.`),Wl=d(),oo=n("p"),Ql=r("This model inherits from "),bn=n("a"),Hl=r("ModelMixin"),Yl=r(`. Check the superclass documentation for the generic methods the library
implements for all the models (such as downloading or saving, etc.)`),Gl=d(),xn=n("div"),u(no.$$.fragment),Gs=d(),$e=n("h2"),Je=n("a"),ha=n("span"),u(ao.$$.fragment),Jl=d(),ga=n("span"),Rl=r("DecoderOutput"),Js=d(),we=n("div"),u(so.$$.fragment),Zl=d(),_a=n("p"),ed=r("Output of decoding method."),Rs=d(),De=n("h2"),Re=n("a"),va=n("span"),u(ro.$$.fragment),td=d(),ba=n("span"),od=r("VQEncoderOutput"),Zs=d(),ke=n("div"),u(io.$$.fragment),nd=d(),xa=n("p"),ad=r("Output of VQModel encoding method."),er=d(),Me=n("h2"),Ze=n("a"),ya=n("span"),u(lo.$$.fragment),sd=d(),$a=n("span"),rd=r("VQModel"),tr=d(),S=n("div"),u(co.$$.fragment),id=d(),wa=n("p"),ld=r(`VQ-VAE model from the paper Neural Discrete Representation Learning by Aaron van den Oord, Oriol Vinyals and Koray
Kavukcuoglu.`),dd=d(),po=n("p"),cd=r("This model inherits from "),yn=n("a"),pd=r("ModelMixin"),md=r(`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),fd=d(),$n=n("div"),u(mo.$$.fragment),or=d(),Te=n("h2"),et=n("a"),Da=n("span"),u(fo.$$.fragment),ud=d(),ka=n("span"),hd=r("AutoencoderKLOutput"),nr=d(),Ee=n("div"),u(uo.$$.fragment),gd=d(),Ma=n("p"),_d=r("Output of AutoencoderKL encoding method."),ar=d(),Ne=n("h2"),tt=n("a"),Ta=n("span"),u(ho.$$.fragment),vd=d(),Ea=n("span"),bd=r("AutoencoderKL"),sr=d(),X=n("div"),u(go.$$.fragment),xd=d(),Na=n("p"),yd=r(`Variational Autoencoder (VAE) model with KL loss from the paper Auto-Encoding Variational Bayes by Diederik P. Kingma
and Max Welling.`),$d=d(),_o=n("p"),wd=r("This model inherits from "),wn=n("a"),Dd=r("ModelMixin"),kd=r(`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),Md=d(),Dn=n("div"),u(vo.$$.fragment),rr=d(),Ue=n("h2"),ot=n("a"),Ua=n("span"),u(bo.$$.fragment),Td=d(),Aa=n("span"),Ed=r("Transformer2DModel"),ir=d(),F=n("div"),u(xo.$$.fragment),Nd=d(),Fa=n("p"),Ud=r(`Transformer model for image-like data. Takes either discrete (classes of vector embeddings) or continuous (actual
embeddings) inputs.`),Ad=d(),Ca=n("p"),Fd=r(`When input is continuous: First, project the input (aka embedding) and reshape to b, t, d. Then apply standard
transformer action. Finally, reshape to image.`),Cd=d(),yo=n("p"),Od=r(`When input is discrete: First, input (classes of latent pixels) is converted to embeddings and has positional
embeddings applied, see `),Oa=n("code"),jd=r("ImagePositionalEmbeddings"),qd=r(`. Then apply standard transformer action. Finally, predict
classes of unnoised image.`),Pd=d(),ja=n("p"),Ld=r(`Note that it is assumed one of the input classes is the masked latent pixel. The predicted classes of the unnoised
image do not contain a prediction for the masked pixel as the unnoised image cannot be masked.`),Bd=d(),kn=n("div"),u($o.$$.fragment),lr=d(),Ae=n("h2"),nt=n("a"),qa=n("span"),u(wo.$$.fragment),Id=d(),Pa=n("span"),Vd=r("Transformer2DModelOutput"),dr=d(),Do=n("div"),u(ko.$$.fragment),cr=d(),Fe=n("h2"),at=n("a"),La=n("span"),u(Mo.$$.fragment),zd=d(),Ba=n("span"),Kd=r("FlaxModelMixin"),pr=d(),E=n("div"),u(To.$$.fragment),Sd=d(),Ia=n("p"),Xd=r("Base class for all flax models."),Wd=d(),Mn=n("p"),Tn=n("a"),Qd=r("FlaxModelMixin"),Hd=r(` takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models.`),Yd=d(),P=n("div"),u(Eo.$$.fragment),Gd=d(),Va=n("p"),Jd=r("Instantiate a pretrained flax model from a pre-trained model configuration."),Rd=d(),No=n("p"),Zd=r("The warning "),za=n("em"),ec=r("Weights from XXX not initialized from pretrained model"),tc=r(` means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.`),oc=d(),Uo=n("p"),nc=r("The warning "),Ka=n("em"),ac=r("Weights from XXX not used in YYY"),sc=r(` means that the layer XXX is not used by YYY, therefore those
weights are discarded.`),rc=d(),u(st.$$.fragment),ic=d(),rt=n("div"),u(Ao.$$.fragment),lc=d(),Fo=n("p"),dc=r(`Save a model and its configuration file to a directory, so that it can be re-loaded using the
`),Sa=n("code"),cc=r("[from_pretrained()](/docs/diffusers/main/en/using-diffusers/loading#diffusers.FlaxModelMixin.from_pretrained)"),pc=r(" class method"),mc=d(),G=n("div"),u(Co.$$.fragment),fc=d(),W=n("p"),uc=r("Cast the floating-point "),Xa=n("code"),hc=r("params"),gc=r(" to "),Wa=n("code"),_c=r("jax.numpy.bfloat16"),vc=r(". This returns a new "),Qa=n("code"),bc=r("params"),xc=r(` tree and does not cast
the `),Ha=n("code"),yc=r("params"),$c=r(" in place."),wc=d(),Ya=n("p"),Dc=r(`This method can be used on TPU to explicitly convert the model parameters to bfloat16 precision to do full
half-precision training or to save weights in bfloat16 for inference in order to save memory and improve speed.`),kc=d(),u(it.$$.fragment),Mc=d(),J=n("div"),u(Oo.$$.fragment),Tc=d(),Q=n("p"),Ec=r("Cast the floating-point "),Ga=n("code"),Nc=r("params"),Uc=r(" to "),Ja=n("code"),Ac=r("jax.numpy.float16"),Fc=r(". This returns a new "),Ra=n("code"),Cc=r("params"),Oc=r(` tree and does not cast the
`),Za=n("code"),jc=r("params"),qc=r(" in place."),Pc=d(),es=n("p"),Lc=r(`This method can be used on GPU to explicitly convert the model parameters to float16 precision to do full
half-precision training or to save weights in float16 for inference in order to save memory and improve speed.`),Bc=d(),u(lt.$$.fragment),Ic=d(),ie=n("div"),u(jo.$$.fragment),Vc=d(),H=n("p"),zc=r("Cast the floating-point "),ts=n("code"),Kc=r("params"),Sc=r(" to "),os=n("code"),Xc=r("jax.numpy.float32"),Wc=r(`. This method can be used to explicitly convert the
model parameters to fp32 precision. This returns a new `),ns=n("code"),Qc=r("params"),Hc=r(" tree and does not cast the "),as=n("code"),Yc=r("params"),Gc=r(" in place."),Jc=d(),u(dt.$$.fragment),mr=d(),Ce=n("h2"),ct=n("a"),ss=n("span"),u(qo.$$.fragment),Rc=d(),rs=n("span"),Zc=r("FlaxUNet2DConditionOutput"),fr=d(),Oe=n("div"),u(Po.$$.fragment),ep=d(),pt=n("div"),u(Lo.$$.fragment),tp=d(),is=n("p"),op=r("\u201CReturns a new object replacing the specified fields with new values."),ur=d(),je=n("h2"),mt=n("a"),ls=n("span"),u(Bo.$$.fragment),np=d(),ds=n("span"),ap=r("FlaxUNet2DConditionModel"),hr=d(),C=n("div"),u(Io.$$.fragment),sp=d(),cs=n("p"),rp=r(`FlaxUNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a
timestep and returns sample shaped output.`),ip=d(),Vo=n("p"),lp=r("This model inherits from "),En=n("a"),dp=r("FlaxModelMixin"),cp=r(`. Check the superclass documentation for the generic methods the library
implements for all the models (such as downloading or saving, etc.)`),pp=d(),zo=n("p"),mp=r("Also, this model is a Flax Linen "),Ko=n("a"),fp=r("flax.linen.Module"),up=r(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),hp=d(),ps=n("p"),gp=r("Finally, this model supports inherent JAX features such as:"),_p=d(),ee=n("ul"),ms=n("li"),So=n("a"),vp=r("Just-In-Time (JIT) compilation"),bp=d(),fs=n("li"),Xo=n("a"),xp=r("Automatic Differentiation"),yp=d(),us=n("li"),Wo=n("a"),$p=r("Vectorization"),wp=d(),hs=n("li"),Qo=n("a"),Dp=r("Parallelization"),gr=d(),qe=n("h2"),ft=n("a"),gs=n("span"),u(Ho.$$.fragment),kp=d(),_s=n("span"),Mp=r("FlaxDecoderOutput"),_r=d(),te=n("div"),u(Yo.$$.fragment),Tp=d(),vs=n("p"),Ep=r("Output of decoding method."),Np=d(),ut=n("div"),u(Go.$$.fragment),Up=d(),bs=n("p"),Ap=r("\u201CReturns a new object replacing the specified fields with new values."),vr=d(),Pe=n("h2"),ht=n("a"),xs=n("span"),u(Jo.$$.fragment),Fp=d(),ys=n("span"),Cp=r("FlaxAutoencoderKLOutput"),br=d(),oe=n("div"),u(Ro.$$.fragment),Op=d(),$s=n("p"),jp=r("Output of AutoencoderKL encoding method."),qp=d(),gt=n("div"),u(Zo.$$.fragment),Pp=d(),ws=n("p"),Lp=r("\u201CReturns a new object replacing the specified fields with new values."),xr=d(),Le=n("h2"),_t=n("a"),Ds=n("span"),u(en.$$.fragment),Bp=d(),ks=n("span"),Ip=r("FlaxAutoencoderKL"),yr=d(),j=n("div"),u(tn.$$.fragment),Vp=d(),Ms=n("p"),zp=r(`Flax Implementation of Variational Autoencoder (VAE) model with KL loss from the paper Auto-Encoding Variational
Bayes by Diederik P. Kingma and Max Welling.`),Kp=d(),on=n("p"),Sp=r("This model is a Flax Linen "),nn=n("a"),Xp=r("flax.linen.Module"),Wp=r(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Qp=d(),Ts=n("p"),Hp=r("Finally, this model supports inherent JAX features such as:"),Yp=d(),ne=n("ul"),Es=n("li"),an=n("a"),Gp=r("Just-In-Time (JIT) compilation"),Jp=d(),Ns=n("li"),sn=n("a"),Rp=r("Automatic Differentiation"),Zp=d(),Us=n("li"),rn=n("a"),em=r("Vectorization"),tm=d(),As=n("li"),ln=n("a"),om=r("Parallelization"),this.h()},l(t){const p=th('[data-svelte="svelte-1phssyn"]',document.head);x=a(p,"META",{name:!0,content:!0}),p.forEach(o),M=c(t),$=a(t,"H1",{class:!0});var dn=s($);y=a(dn,"A",{id:!0,class:!0,href:!0});var Fs=s(y);k=a(Fs,"SPAN",{});var Cs=s(k);h(f.$$.fragment,Cs),Cs.forEach(o),Fs.forEach(o),w=c(dn),Y=a(dn,"SPAN",{});var Os=s(Y);fe=i(Os,"Models"),Os.forEach(o),dn.forEach(o),B=c(t),I=a(t,"P",{});var Be=s(I);pn=i(Be,`Diffusers contains pretrained models for popular algorithms and modules for creating the next set of diffusion models.
The primary function of these models is to denoise an input sample, by modeling the distribution $p`),jn=a(Be,"EM",{});var js=s(jn);_i=i(js,"\\theta(\\mathbf{x}"),js.forEach(o),vi=i(Be,`{t-1}|\\mathbf{x}_t)$.
The models are built on the base class [\u2018ModelMixin\u2019] that is a `),qn=a(Be,"CODE",{});var sm=s(qn);bi=i(sm,"torch.nn.module"),sm.forEach(o),xi=i(Be," with basic functionality for saving and loading models both locally and from the HuggingFace hub."),Be.forEach(o),qs=c(t),ue=a(t,"H2",{class:!0});var wr=s(ue);Ie=a(wr,"A",{id:!0,class:!0,href:!0});var rm=s(Ie);Pn=a(rm,"SPAN",{});var im=s(Pn);h(Tt.$$.fragment,im),im.forEach(o),rm.forEach(o),yi=c(wr),Ln=a(wr,"SPAN",{});var lm=s(Ln);$i=i(lm,"ModelMixin"),lm.forEach(o),wr.forEach(o),Ps=c(t),T=a(t,"DIV",{class:!0});var A=s(T);h(Et.$$.fragment,A),wi=c(A),Bn=a(A,"P",{});var dm=s(Bn);Di=i(dm,"Base class for all models."),dm.forEach(o),ki=c(A),mn=a(A,"P",{});var nm=s(mn);fn=a(nm,"A",{href:!0});var cm=s(fn);Mi=i(cm,"ModelMixin"),cm.forEach(o),Ti=i(nm,` takes care of storing the configuration of the models and handles methods for loading, downloading
and saving models.`),nm.forEach(o),Ei=c(A),In=a(A,"UL",{});var pm=s(In);ae=a(pm,"LI",{});var cn=s(ae);Vn=a(cn,"STRONG",{});var mm=s(Vn);Ni=i(mm,"config_name"),mm.forEach(o),Ui=i(cn," ("),zn=a(cn,"CODE",{});var fm=s(zn);Ai=i(fm,"str"),fm.forEach(o),Fi=i(cn,`) \u2014 A filename under which the model should be stored when calling
`),un=a(cn,"A",{href:!0});var um=s(un);Ci=i(um,"save_pretrained()"),um.forEach(o),Oi=i(cn,"."),cn.forEach(o),pm.forEach(o),ji=c(A),se=a(A,"DIV",{class:!0});var Nn=s(se);h(Nt.$$.fragment,Nn),qi=c(Nn),Kn=a(Nn,"P",{});var hm=s(Kn);Pi=i(hm,"Deactivates gradient checkpointing for the current model."),hm.forEach(o),Li=c(Nn),Sn=a(Nn,"P",{});var gm=s(Sn);Bi=i(gm,`Note that in other frameworks this feature can be referred to as \u201Cactivation checkpointing\u201D or \u201Ccheckpoint
activations\u201D.`),gm.forEach(o),Nn.forEach(o),Ii=c(A),re=a(A,"DIV",{class:!0});var Un=s(re);h(Ut.$$.fragment,Un),Vi=c(Un),Xn=a(Un,"P",{});var _m=s(Xn);zi=i(_m,"Activates gradient checkpointing for the current model."),_m.forEach(o),Ki=c(Un),Wn=a(Un,"P",{});var vm=s(Wn);Si=i(vm,`Note that in other frameworks this feature can be referred to as \u201Cactivation checkpointing\u201D or \u201Ccheckpoint
activations\u201D.`),vm.forEach(o),Un.forEach(o),Xi=c(A),U=a(A,"DIV",{class:!0});var L=s(U);h(At.$$.fragment,L),Wi=c(L),Qn=a(L,"P",{});var bm=s(Qn);Qi=i(bm,"Instantiate a pretrained pytorch model from a pre-trained model configuration."),bm.forEach(o),Hi=c(L),he=a(L,"P",{});var An=s(he);Yi=i(An,"The model is set in evaluation mode by default using "),Hn=a(An,"CODE",{});var xm=s(Hn);Gi=i(xm,"model.eval()"),xm.forEach(o),Ji=i(An,` (Dropout modules are deactivated). To train
the model, you should first set it back in training mode with `),Yn=a(An,"CODE",{});var ym=s(Yn);Ri=i(ym,"model.train()"),ym.forEach(o),Zi=i(An,"."),An.forEach(o),el=c(L),Ft=a(L,"P",{});var Dr=s(Ft);tl=i(Dr,"The warning "),Gn=a(Dr,"EM",{});var $m=s(Gn);ol=i($m,"Weights from XXX not initialized from pretrained model"),$m.forEach(o),nl=i(Dr,` means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.`),Dr.forEach(o),al=c(L),Ct=a(L,"P",{});var kr=s(Ct);sl=i(kr,"The warning "),Jn=a(kr,"EM",{});var wm=s(Jn);rl=i(wm,"Weights from XXX not used in YYY"),wm.forEach(o),il=i(kr,` means that the layer XXX is not used by YYY, therefore those
weights are discarded.`),kr.forEach(o),ll=c(L),h(Ve.$$.fragment,L),dl=c(L),h(ze.$$.fragment,L),L.forEach(o),cl=c(A),Ke=a(A,"DIV",{class:!0});var Mr=s(Ke);h(Ot.$$.fragment,Mr),pl=c(Mr),Rn=a(Mr,"P",{});var Dm=s(Rn);ml=i(Dm,"Get number of (optionally, trainable or non-embeddings) parameters in the module."),Dm.forEach(o),Mr.forEach(o),fl=c(A),Se=a(A,"DIV",{class:!0});var Tr=s(Se);h(jt.$$.fragment,Tr),ul=c(Tr),qt=a(Tr,"P",{});var Er=s(qt);hl=i(Er,`Save a model and its configuration file to a directory, so that it can be re-loaded using the
`),Zn=a(Er,"CODE",{});var km=s(Zn);gl=i(km,"[from_pretrained()](/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin.from_pretrained)"),km.forEach(o),_l=i(Er," class method."),Er.forEach(o),Tr.forEach(o),A.forEach(o),Ls=c(t),ge=a(t,"H2",{class:!0});var Nr=s(ge);Xe=a(Nr,"A",{id:!0,class:!0,href:!0});var Mm=s(Xe);ea=a(Mm,"SPAN",{});var Tm=s(ea);h(Pt.$$.fragment,Tm),Tm.forEach(o),Mm.forEach(o),vl=c(Nr),ta=a(Nr,"SPAN",{});var Em=s(ta);bl=i(Em,"UNet2DOutput"),Em.forEach(o),Nr.forEach(o),Bs=c(t),Lt=a(t,"DIV",{class:!0});var Nm=s(Lt);h(Bt.$$.fragment,Nm),Nm.forEach(o),Is=c(t),_e=a(t,"H2",{class:!0});var Ur=s(_e);We=a(Ur,"A",{id:!0,class:!0,href:!0});var Um=s(We);oa=a(Um,"SPAN",{});var Am=s(oa);h(It.$$.fragment,Am),Am.forEach(o),Um.forEach(o),xl=c(Ur),na=a(Ur,"SPAN",{});var Fm=s(na);yl=i(Fm,"UNet2DModel"),Fm.forEach(o),Ur.forEach(o),Vs=c(t),V=a(t,"DIV",{class:!0});var vt=s(V);h(Vt.$$.fragment,vt),$l=c(vt),aa=a(vt,"P",{});var Cm=s(aa);wl=i(Cm,"UNet2DModel is a 2D UNet model that takes in a noisy sample and a timestep and returns sample shaped output."),Cm.forEach(o),Dl=c(vt),zt=a(vt,"P",{});var Ar=s(zt);kl=i(Ar,"This model inherits from "),hn=a(Ar,"A",{href:!0});var Om=s(hn);Ml=i(Om,"ModelMixin"),Om.forEach(o),Tl=i(Ar,`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),Ar.forEach(o),El=c(vt),gn=a(vt,"DIV",{class:!0});var jm=s(gn);h(Kt.$$.fragment,jm),jm.forEach(o),vt.forEach(o),zs=c(t),ve=a(t,"H2",{class:!0});var Fr=s(ve);Qe=a(Fr,"A",{id:!0,class:!0,href:!0});var qm=s(Qe);sa=a(qm,"SPAN",{});var Pm=s(sa);h(St.$$.fragment,Pm),Pm.forEach(o),qm.forEach(o),Nl=c(Fr),ra=a(Fr,"SPAN",{});var Lm=s(ra);Ul=i(Lm,"UNet1DOutput"),Lm.forEach(o),Fr.forEach(o),Ks=c(t),Xt=a(t,"DIV",{class:!0});var Bm=s(Xt);h(Wt.$$.fragment,Bm),Bm.forEach(o),Ss=c(t),be=a(t,"H2",{class:!0});var Cr=s(be);He=a(Cr,"A",{id:!0,class:!0,href:!0});var Im=s(He);ia=a(Im,"SPAN",{});var Vm=s(ia);h(Qt.$$.fragment,Vm),Vm.forEach(o),Im.forEach(o),Al=c(Cr),la=a(Cr,"SPAN",{});var zm=s(la);Fl=i(zm,"UNet1DModel"),zm.forEach(o),Cr.forEach(o),Xs=c(t),z=a(t,"DIV",{class:!0});var bt=s(z);h(Ht.$$.fragment,bt),Cl=c(bt),da=a(bt,"P",{});var Km=s(da);Ol=i(Km,"UNet1DModel is a 1D UNet model that takes in a noisy sample and a timestep and returns sample shaped output."),Km.forEach(o),jl=c(bt),Yt=a(bt,"P",{});var Or=s(Yt);ql=i(Or,"This model inherits from "),_n=a(Or,"A",{href:!0});var Sm=s(_n);Pl=i(Sm,"ModelMixin"),Sm.forEach(o),Ll=i(Or,`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),Or.forEach(o),Bl=c(bt),vn=a(bt,"DIV",{class:!0});var Xm=s(vn);h(Gt.$$.fragment,Xm),Xm.forEach(o),bt.forEach(o),Ws=c(t),xe=a(t,"H2",{class:!0});var jr=s(xe);Ye=a(jr,"A",{id:!0,class:!0,href:!0});var Wm=s(Ye);ca=a(Wm,"SPAN",{});var Qm=s(ca);h(Jt.$$.fragment,Qm),Qm.forEach(o),Wm.forEach(o),Il=c(jr),pa=a(jr,"SPAN",{});var Hm=s(pa);Vl=i(Hm,"UNet2DConditionOutput"),Hm.forEach(o),jr.forEach(o),Qs=c(t),Rt=a(t,"DIV",{class:!0});var Ym=s(Rt);h(Zt.$$.fragment,Ym),Ym.forEach(o),Hs=c(t),ye=a(t,"H2",{class:!0});var qr=s(ye);Ge=a(qr,"A",{id:!0,class:!0,href:!0});var Gm=s(Ge);ma=a(Gm,"SPAN",{});var Jm=s(ma);h(eo.$$.fragment,Jm),Jm.forEach(o),Gm.forEach(o),zl=c(qr),fa=a(qr,"SPAN",{});var Rm=s(fa);Kl=i(Rm,"UNet2DConditionModel"),Rm.forEach(o),qr.forEach(o),Ys=c(t),K=a(t,"DIV",{class:!0});var xt=s(K);h(to.$$.fragment,xt),Sl=c(xt),ua=a(xt,"P",{});var Zm=s(ua);Xl=i(Zm,`UNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a timestep
and returns sample shaped output.`),Zm.forEach(o),Wl=c(xt),oo=a(xt,"P",{});var Pr=s(oo);Ql=i(Pr,"This model inherits from "),bn=a(Pr,"A",{href:!0});var ef=s(bn);Hl=i(ef,"ModelMixin"),ef.forEach(o),Yl=i(Pr,`. Check the superclass documentation for the generic methods the library
implements for all the models (such as downloading or saving, etc.)`),Pr.forEach(o),Gl=c(xt),xn=a(xt,"DIV",{class:!0});var tf=s(xn);h(no.$$.fragment,tf),tf.forEach(o),xt.forEach(o),Gs=c(t),$e=a(t,"H2",{class:!0});var Lr=s($e);Je=a(Lr,"A",{id:!0,class:!0,href:!0});var of=s(Je);ha=a(of,"SPAN",{});var nf=s(ha);h(ao.$$.fragment,nf),nf.forEach(o),of.forEach(o),Jl=c(Lr),ga=a(Lr,"SPAN",{});var af=s(ga);Rl=i(af,"DecoderOutput"),af.forEach(o),Lr.forEach(o),Js=c(t),we=a(t,"DIV",{class:!0});var Br=s(we);h(so.$$.fragment,Br),Zl=c(Br),_a=a(Br,"P",{});var sf=s(_a);ed=i(sf,"Output of decoding method."),sf.forEach(o),Br.forEach(o),Rs=c(t),De=a(t,"H2",{class:!0});var Ir=s(De);Re=a(Ir,"A",{id:!0,class:!0,href:!0});var rf=s(Re);va=a(rf,"SPAN",{});var lf=s(va);h(ro.$$.fragment,lf),lf.forEach(o),rf.forEach(o),td=c(Ir),ba=a(Ir,"SPAN",{});var df=s(ba);od=i(df,"VQEncoderOutput"),df.forEach(o),Ir.forEach(o),Zs=c(t),ke=a(t,"DIV",{class:!0});var Vr=s(ke);h(io.$$.fragment,Vr),nd=c(Vr),xa=a(Vr,"P",{});var cf=s(xa);ad=i(cf,"Output of VQModel encoding method."),cf.forEach(o),Vr.forEach(o),er=c(t),Me=a(t,"H2",{class:!0});var zr=s(Me);Ze=a(zr,"A",{id:!0,class:!0,href:!0});var pf=s(Ze);ya=a(pf,"SPAN",{});var mf=s(ya);h(lo.$$.fragment,mf),mf.forEach(o),pf.forEach(o),sd=c(zr),$a=a(zr,"SPAN",{});var ff=s($a);rd=i(ff,"VQModel"),ff.forEach(o),zr.forEach(o),tr=c(t),S=a(t,"DIV",{class:!0});var yt=s(S);h(co.$$.fragment,yt),id=c(yt),wa=a(yt,"P",{});var uf=s(wa);ld=i(uf,`VQ-VAE model from the paper Neural Discrete Representation Learning by Aaron van den Oord, Oriol Vinyals and Koray
Kavukcuoglu.`),uf.forEach(o),dd=c(yt),po=a(yt,"P",{});var Kr=s(po);cd=i(Kr,"This model inherits from "),yn=a(Kr,"A",{href:!0});var hf=s(yn);pd=i(hf,"ModelMixin"),hf.forEach(o),md=i(Kr,`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),Kr.forEach(o),fd=c(yt),$n=a(yt,"DIV",{class:!0});var gf=s($n);h(mo.$$.fragment,gf),gf.forEach(o),yt.forEach(o),or=c(t),Te=a(t,"H2",{class:!0});var Sr=s(Te);et=a(Sr,"A",{id:!0,class:!0,href:!0});var _f=s(et);Da=a(_f,"SPAN",{});var vf=s(Da);h(fo.$$.fragment,vf),vf.forEach(o),_f.forEach(o),ud=c(Sr),ka=a(Sr,"SPAN",{});var bf=s(ka);hd=i(bf,"AutoencoderKLOutput"),bf.forEach(o),Sr.forEach(o),nr=c(t),Ee=a(t,"DIV",{class:!0});var Xr=s(Ee);h(uo.$$.fragment,Xr),gd=c(Xr),Ma=a(Xr,"P",{});var xf=s(Ma);_d=i(xf,"Output of AutoencoderKL encoding method."),xf.forEach(o),Xr.forEach(o),ar=c(t),Ne=a(t,"H2",{class:!0});var Wr=s(Ne);tt=a(Wr,"A",{id:!0,class:!0,href:!0});var yf=s(tt);Ta=a(yf,"SPAN",{});var $f=s(Ta);h(ho.$$.fragment,$f),$f.forEach(o),yf.forEach(o),vd=c(Wr),Ea=a(Wr,"SPAN",{});var wf=s(Ea);bd=i(wf,"AutoencoderKL"),wf.forEach(o),Wr.forEach(o),sr=c(t),X=a(t,"DIV",{class:!0});var $t=s(X);h(go.$$.fragment,$t),xd=c($t),Na=a($t,"P",{});var Df=s(Na);yd=i(Df,`Variational Autoencoder (VAE) model with KL loss from the paper Auto-Encoding Variational Bayes by Diederik P. Kingma
and Max Welling.`),Df.forEach(o),$d=c($t),_o=a($t,"P",{});var Qr=s(_o);wd=i(Qr,"This model inherits from "),wn=a(Qr,"A",{href:!0});var kf=s(wn);Dd=i(kf,"ModelMixin"),kf.forEach(o),kd=i(Qr,`. Check the superclass documentation for the generic methods the library
implements for all the model (such as downloading or saving, etc.)`),Qr.forEach(o),Md=c($t),Dn=a($t,"DIV",{class:!0});var Mf=s(Dn);h(vo.$$.fragment,Mf),Mf.forEach(o),$t.forEach(o),rr=c(t),Ue=a(t,"H2",{class:!0});var Hr=s(Ue);ot=a(Hr,"A",{id:!0,class:!0,href:!0});var Tf=s(ot);Ua=a(Tf,"SPAN",{});var Ef=s(Ua);h(bo.$$.fragment,Ef),Ef.forEach(o),Tf.forEach(o),Td=c(Hr),Aa=a(Hr,"SPAN",{});var Nf=s(Aa);Ed=i(Nf,"Transformer2DModel"),Nf.forEach(o),Hr.forEach(o),ir=c(t),F=a(t,"DIV",{class:!0});var R=s(F);h(xo.$$.fragment,R),Nd=c(R),Fa=a(R,"P",{});var Uf=s(Fa);Ud=i(Uf,`Transformer model for image-like data. Takes either discrete (classes of vector embeddings) or continuous (actual
embeddings) inputs.`),Uf.forEach(o),Ad=c(R),Ca=a(R,"P",{});var Af=s(Ca);Fd=i(Af,`When input is continuous: First, project the input (aka embedding) and reshape to b, t, d. Then apply standard
transformer action. Finally, reshape to image.`),Af.forEach(o),Cd=c(R),yo=a(R,"P",{});var Yr=s(yo);Od=i(Yr,`When input is discrete: First, input (classes of latent pixels) is converted to embeddings and has positional
embeddings applied, see `),Oa=a(Yr,"CODE",{});var Ff=s(Oa);jd=i(Ff,"ImagePositionalEmbeddings"),Ff.forEach(o),qd=i(Yr,`. Then apply standard transformer action. Finally, predict
classes of unnoised image.`),Yr.forEach(o),Pd=c(R),ja=a(R,"P",{});var Cf=s(ja);Ld=i(Cf,`Note that it is assumed one of the input classes is the masked latent pixel. The predicted classes of the unnoised
image do not contain a prediction for the masked pixel as the unnoised image cannot be masked.`),Cf.forEach(o),Bd=c(R),kn=a(R,"DIV",{class:!0});var Of=s(kn);h($o.$$.fragment,Of),Of.forEach(o),R.forEach(o),lr=c(t),Ae=a(t,"H2",{class:!0});var Gr=s(Ae);nt=a(Gr,"A",{id:!0,class:!0,href:!0});var jf=s(nt);qa=a(jf,"SPAN",{});var qf=s(qa);h(wo.$$.fragment,qf),qf.forEach(o),jf.forEach(o),Id=c(Gr),Pa=a(Gr,"SPAN",{});var Pf=s(Pa);Vd=i(Pf,"Transformer2DModelOutput"),Pf.forEach(o),Gr.forEach(o),dr=c(t),Do=a(t,"DIV",{class:!0});var Lf=s(Do);h(ko.$$.fragment,Lf),Lf.forEach(o),cr=c(t),Fe=a(t,"H2",{class:!0});var Jr=s(Fe);at=a(Jr,"A",{id:!0,class:!0,href:!0});var Bf=s(at);La=a(Bf,"SPAN",{});var If=s(La);h(Mo.$$.fragment,If),If.forEach(o),Bf.forEach(o),zd=c(Jr),Ba=a(Jr,"SPAN",{});var Vf=s(Ba);Kd=i(Vf,"FlaxModelMixin"),Vf.forEach(o),Jr.forEach(o),pr=c(t),E=a(t,"DIV",{class:!0});var O=s(E);h(To.$$.fragment,O),Sd=c(O),Ia=a(O,"P",{});var zf=s(Ia);Xd=i(zf,"Base class for all flax models."),zf.forEach(o),Wd=c(O),Mn=a(O,"P",{});var am=s(Mn);Tn=a(am,"A",{href:!0});var Kf=s(Tn);Qd=i(Kf,"FlaxModelMixin"),Kf.forEach(o),Hd=i(am,` takes care of storing the configuration of the models and handles methods for loading,
downloading and saving models.`),am.forEach(o),Yd=c(O),P=a(O,"DIV",{class:!0});var le=s(P);h(Eo.$$.fragment,le),Gd=c(le),Va=a(le,"P",{});var Sf=s(Va);Jd=i(Sf,"Instantiate a pretrained flax model from a pre-trained model configuration."),Sf.forEach(o),Rd=c(le),No=a(le,"P",{});var Rr=s(No);Zd=i(Rr,"The warning "),za=a(Rr,"EM",{});var Xf=s(za);ec=i(Xf,"Weights from XXX not initialized from pretrained model"),Xf.forEach(o),tc=i(Rr,` means that the weights of XXX do not come
pretrained with the rest of the model. It is up to you to train those weights with a downstream fine-tuning
task.`),Rr.forEach(o),oc=c(le),Uo=a(le,"P",{});var Zr=s(Uo);nc=i(Zr,"The warning "),Ka=a(Zr,"EM",{});var Wf=s(Ka);ac=i(Wf,"Weights from XXX not used in YYY"),Wf.forEach(o),sc=i(Zr,` means that the layer XXX is not used by YYY, therefore those
weights are discarded.`),Zr.forEach(o),rc=c(le),h(st.$$.fragment,le),le.forEach(o),ic=c(O),rt=a(O,"DIV",{class:!0});var ei=s(rt);h(Ao.$$.fragment,ei),lc=c(ei),Fo=a(ei,"P",{});var ti=s(Fo);dc=i(ti,`Save a model and its configuration file to a directory, so that it can be re-loaded using the
`),Sa=a(ti,"CODE",{});var Qf=s(Sa);cc=i(Qf,"[from_pretrained()](/docs/diffusers/main/en/using-diffusers/loading#diffusers.FlaxModelMixin.from_pretrained)"),Qf.forEach(o),pc=i(ti," class method"),ti.forEach(o),ei.forEach(o),mc=c(O),G=a(O,"DIV",{class:!0});var wt=s(G);h(Co.$$.fragment,wt),fc=c(wt),W=a(wt,"P",{});var de=s(W);uc=i(de,"Cast the floating-point "),Xa=a(de,"CODE",{});var Hf=s(Xa);hc=i(Hf,"params"),Hf.forEach(o),gc=i(de," to "),Wa=a(de,"CODE",{});var Yf=s(Wa);_c=i(Yf,"jax.numpy.bfloat16"),Yf.forEach(o),vc=i(de,". This returns a new "),Qa=a(de,"CODE",{});var Gf=s(Qa);bc=i(Gf,"params"),Gf.forEach(o),xc=i(de,` tree and does not cast
the `),Ha=a(de,"CODE",{});var Jf=s(Ha);yc=i(Jf,"params"),Jf.forEach(o),$c=i(de," in place."),de.forEach(o),wc=c(wt),Ya=a(wt,"P",{});var Rf=s(Ya);Dc=i(Rf,`This method can be used on TPU to explicitly convert the model parameters to bfloat16 precision to do full
half-precision training or to save weights in bfloat16 for inference in order to save memory and improve speed.`),Rf.forEach(o),kc=c(wt),h(it.$$.fragment,wt),wt.forEach(o),Mc=c(O),J=a(O,"DIV",{class:!0});var Dt=s(J);h(Oo.$$.fragment,Dt),Tc=c(Dt),Q=a(Dt,"P",{});var ce=s(Q);Ec=i(ce,"Cast the floating-point "),Ga=a(ce,"CODE",{});var Zf=s(Ga);Nc=i(Zf,"params"),Zf.forEach(o),Uc=i(ce," to "),Ja=a(ce,"CODE",{});var eu=s(Ja);Ac=i(eu,"jax.numpy.float16"),eu.forEach(o),Fc=i(ce,". This returns a new "),Ra=a(ce,"CODE",{});var tu=s(Ra);Cc=i(tu,"params"),tu.forEach(o),Oc=i(ce,` tree and does not cast the
`),Za=a(ce,"CODE",{});var ou=s(Za);jc=i(ou,"params"),ou.forEach(o),qc=i(ce," in place."),ce.forEach(o),Pc=c(Dt),es=a(Dt,"P",{});var nu=s(es);Lc=i(nu,`This method can be used on GPU to explicitly convert the model parameters to float16 precision to do full
half-precision training or to save weights in float16 for inference in order to save memory and improve speed.`),nu.forEach(o),Bc=c(Dt),h(lt.$$.fragment,Dt),Dt.forEach(o),Ic=c(O),ie=a(O,"DIV",{class:!0});var Fn=s(ie);h(jo.$$.fragment,Fn),Vc=c(Fn),H=a(Fn,"P",{});var pe=s(H);zc=i(pe,"Cast the floating-point "),ts=a(pe,"CODE",{});var au=s(ts);Kc=i(au,"params"),au.forEach(o),Sc=i(pe," to "),os=a(pe,"CODE",{});var su=s(os);Xc=i(su,"jax.numpy.float32"),su.forEach(o),Wc=i(pe,`. This method can be used to explicitly convert the
model parameters to fp32 precision. This returns a new `),ns=a(pe,"CODE",{});var ru=s(ns);Qc=i(ru,"params"),ru.forEach(o),Hc=i(pe," tree and does not cast the "),as=a(pe,"CODE",{});var iu=s(as);Yc=i(iu,"params"),iu.forEach(o),Gc=i(pe," in place."),pe.forEach(o),Jc=c(Fn),h(dt.$$.fragment,Fn),Fn.forEach(o),O.forEach(o),mr=c(t),Ce=a(t,"H2",{class:!0});var oi=s(Ce);ct=a(oi,"A",{id:!0,class:!0,href:!0});var lu=s(ct);ss=a(lu,"SPAN",{});var du=s(ss);h(qo.$$.fragment,du),du.forEach(o),lu.forEach(o),Rc=c(oi),rs=a(oi,"SPAN",{});var cu=s(rs);Zc=i(cu,"FlaxUNet2DConditionOutput"),cu.forEach(o),oi.forEach(o),fr=c(t),Oe=a(t,"DIV",{class:!0});var ni=s(Oe);h(Po.$$.fragment,ni),ep=c(ni),pt=a(ni,"DIV",{class:!0});var ai=s(pt);h(Lo.$$.fragment,ai),tp=c(ai),is=a(ai,"P",{});var pu=s(is);op=i(pu,"\u201CReturns a new object replacing the specified fields with new values."),pu.forEach(o),ai.forEach(o),ni.forEach(o),ur=c(t),je=a(t,"H2",{class:!0});var si=s(je);mt=a(si,"A",{id:!0,class:!0,href:!0});var mu=s(mt);ls=a(mu,"SPAN",{});var fu=s(ls);h(Bo.$$.fragment,fu),fu.forEach(o),mu.forEach(o),np=c(si),ds=a(si,"SPAN",{});var uu=s(ds);ap=i(uu,"FlaxUNet2DConditionModel"),uu.forEach(o),si.forEach(o),hr=c(t),C=a(t,"DIV",{class:!0});var Z=s(C);h(Io.$$.fragment,Z),sp=c(Z),cs=a(Z,"P",{});var hu=s(cs);rp=i(hu,`FlaxUNet2DConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a
timestep and returns sample shaped output.`),hu.forEach(o),ip=c(Z),Vo=a(Z,"P",{});var ri=s(Vo);lp=i(ri,"This model inherits from "),En=a(ri,"A",{href:!0});var gu=s(En);dp=i(gu,"FlaxModelMixin"),gu.forEach(o),cp=i(ri,`. Check the superclass documentation for the generic methods the library
implements for all the models (such as downloading or saving, etc.)`),ri.forEach(o),pp=c(Z),zo=a(Z,"P",{});var ii=s(zo);mp=i(ii,"Also, this model is a Flax Linen "),Ko=a(ii,"A",{href:!0,rel:!0});var _u=s(Ko);fp=i(_u,"flax.linen.Module"),_u.forEach(o),up=i(ii,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),ii.forEach(o),hp=c(Z),ps=a(Z,"P",{});var vu=s(ps);gp=i(vu,"Finally, this model supports inherent JAX features such as:"),vu.forEach(o),_p=c(Z),ee=a(Z,"UL",{});var kt=s(ee);ms=a(kt,"LI",{});var bu=s(ms);So=a(bu,"A",{href:!0,rel:!0});var xu=s(So);vp=i(xu,"Just-In-Time (JIT) compilation"),xu.forEach(o),bu.forEach(o),bp=c(kt),fs=a(kt,"LI",{});var yu=s(fs);Xo=a(yu,"A",{href:!0,rel:!0});var $u=s(Xo);xp=i($u,"Automatic Differentiation"),$u.forEach(o),yu.forEach(o),yp=c(kt),us=a(kt,"LI",{});var wu=s(us);Wo=a(wu,"A",{href:!0,rel:!0});var Du=s(Wo);$p=i(Du,"Vectorization"),Du.forEach(o),wu.forEach(o),wp=c(kt),hs=a(kt,"LI",{});var ku=s(hs);Qo=a(ku,"A",{href:!0,rel:!0});var Mu=s(Qo);Dp=i(Mu,"Parallelization"),Mu.forEach(o),ku.forEach(o),kt.forEach(o),Z.forEach(o),gr=c(t),qe=a(t,"H2",{class:!0});var li=s(qe);ft=a(li,"A",{id:!0,class:!0,href:!0});var Tu=s(ft);gs=a(Tu,"SPAN",{});var Eu=s(gs);h(Ho.$$.fragment,Eu),Eu.forEach(o),Tu.forEach(o),kp=c(li),_s=a(li,"SPAN",{});var Nu=s(_s);Mp=i(Nu,"FlaxDecoderOutput"),Nu.forEach(o),li.forEach(o),_r=c(t),te=a(t,"DIV",{class:!0});var Cn=s(te);h(Yo.$$.fragment,Cn),Tp=c(Cn),vs=a(Cn,"P",{});var Uu=s(vs);Ep=i(Uu,"Output of decoding method."),Uu.forEach(o),Np=c(Cn),ut=a(Cn,"DIV",{class:!0});var di=s(ut);h(Go.$$.fragment,di),Up=c(di),bs=a(di,"P",{});var Au=s(bs);Ap=i(Au,"\u201CReturns a new object replacing the specified fields with new values."),Au.forEach(o),di.forEach(o),Cn.forEach(o),vr=c(t),Pe=a(t,"H2",{class:!0});var ci=s(Pe);ht=a(ci,"A",{id:!0,class:!0,href:!0});var Fu=s(ht);xs=a(Fu,"SPAN",{});var Cu=s(xs);h(Jo.$$.fragment,Cu),Cu.forEach(o),Fu.forEach(o),Fp=c(ci),ys=a(ci,"SPAN",{});var Ou=s(ys);Cp=i(Ou,"FlaxAutoencoderKLOutput"),Ou.forEach(o),ci.forEach(o),br=c(t),oe=a(t,"DIV",{class:!0});var On=s(oe);h(Ro.$$.fragment,On),Op=c(On),$s=a(On,"P",{});var ju=s($s);jp=i(ju,"Output of AutoencoderKL encoding method."),ju.forEach(o),qp=c(On),gt=a(On,"DIV",{class:!0});var pi=s(gt);h(Zo.$$.fragment,pi),Pp=c(pi),ws=a(pi,"P",{});var qu=s(ws);Lp=i(qu,"\u201CReturns a new object replacing the specified fields with new values."),qu.forEach(o),pi.forEach(o),On.forEach(o),xr=c(t),Le=a(t,"H2",{class:!0});var mi=s(Le);_t=a(mi,"A",{id:!0,class:!0,href:!0});var Pu=s(_t);Ds=a(Pu,"SPAN",{});var Lu=s(Ds);h(en.$$.fragment,Lu),Lu.forEach(o),Pu.forEach(o),Bp=c(mi),ks=a(mi,"SPAN",{});var Bu=s(ks);Ip=i(Bu,"FlaxAutoencoderKL"),Bu.forEach(o),mi.forEach(o),yr=c(t),j=a(t,"DIV",{class:!0});var me=s(j);h(tn.$$.fragment,me),Vp=c(me),Ms=a(me,"P",{});var Iu=s(Ms);zp=i(Iu,`Flax Implementation of Variational Autoencoder (VAE) model with KL loss from the paper Auto-Encoding Variational
Bayes by Diederik P. Kingma and Max Welling.`),Iu.forEach(o),Kp=c(me),on=a(me,"P",{});var fi=s(on);Sp=i(fi,"This model is a Flax Linen "),nn=a(fi,"A",{href:!0,rel:!0});var Vu=s(nn);Xp=i(Vu,"flax.linen.Module"),Vu.forEach(o),Wp=i(fi,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),fi.forEach(o),Qp=c(me),Ts=a(me,"P",{});var zu=s(Ts);Hp=i(zu,"Finally, this model supports inherent JAX features such as:"),zu.forEach(o),Yp=c(me),ne=a(me,"UL",{});var Mt=s(ne);Es=a(Mt,"LI",{});var Ku=s(Es);an=a(Ku,"A",{href:!0,rel:!0});var Su=s(an);Gp=i(Su,"Just-In-Time (JIT) compilation"),Su.forEach(o),Ku.forEach(o),Jp=c(Mt),Ns=a(Mt,"LI",{});var Xu=s(Ns);sn=a(Xu,"A",{href:!0,rel:!0});var Wu=s(sn);Rp=i(Wu,"Automatic Differentiation"),Wu.forEach(o),Xu.forEach(o),Zp=c(Mt),Us=a(Mt,"LI",{});var Qu=s(Us);rn=a(Qu,"A",{href:!0,rel:!0});var Hu=s(rn);em=i(Hu,"Vectorization"),Hu.forEach(o),Qu.forEach(o),tm=c(Mt),As=a(Mt,"LI",{});var Yu=s(As);ln=a(Yu,"A",{href:!0,rel:!0});var Gu=s(ln);om=i(Gu,"Parallelization"),Gu.forEach(o),Yu.forEach(o),Mt.forEach(o),me.forEach(o),this.h()},h(){l(x,"name","hf:doc:metadata"),l(x,"content",JSON.stringify(ch)),l(y,"id","models"),l(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(y,"href","#models"),l($,"class","relative group"),l(Ie,"id","diffusers.ModelMixin"),l(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ie,"href","#diffusers.ModelMixin"),l(ue,"class","relative group"),l(fn,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin"),l(un,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin.save_pretrained"),l(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Xe,"id","diffusers.models.unet_2d.UNet2DOutput"),l(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Xe,"href","#diffusers.models.unet_2d.UNet2DOutput"),l(ge,"class","relative group"),l(Lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(We,"id","diffusers.UNet2DModel"),l(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(We,"href","#diffusers.UNet2DModel"),l(_e,"class","relative group"),l(hn,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin"),l(gn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Qe,"id","diffusers.models.unet_1d.UNet1DOutput"),l(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Qe,"href","#diffusers.models.unet_1d.UNet1DOutput"),l(ve,"class","relative group"),l(Xt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(He,"id","diffusers.UNet1DModel"),l(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(He,"href","#diffusers.UNet1DModel"),l(be,"class","relative group"),l(_n,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin"),l(vn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ye,"id","diffusers.models.unet_2d_condition.UNet2DConditionOutput"),l(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ye,"href","#diffusers.models.unet_2d_condition.UNet2DConditionOutput"),l(xe,"class","relative group"),l(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ge,"id","diffusers.UNet2DConditionModel"),l(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ge,"href","#diffusers.UNet2DConditionModel"),l(ye,"class","relative group"),l(bn,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin"),l(xn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Je,"id","diffusers.models.vae.DecoderOutput"),l(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Je,"href","#diffusers.models.vae.DecoderOutput"),l($e,"class","relative group"),l(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Re,"id","diffusers.models.vae.VQEncoderOutput"),l(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Re,"href","#diffusers.models.vae.VQEncoderOutput"),l(De,"class","relative group"),l(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ze,"id","diffusers.VQModel"),l(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ze,"href","#diffusers.VQModel"),l(Me,"class","relative group"),l(yn,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin"),l($n,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(et,"id","diffusers.models.vae.AutoencoderKLOutput"),l(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(et,"href","#diffusers.models.vae.AutoencoderKLOutput"),l(Te,"class","relative group"),l(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(tt,"id","diffusers.AutoencoderKL"),l(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(tt,"href","#diffusers.AutoencoderKL"),l(Ne,"class","relative group"),l(wn,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.ModelMixin"),l(Dn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ot,"id","diffusers.Transformer2DModel"),l(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ot,"href","#diffusers.Transformer2DModel"),l(Ue,"class","relative group"),l(kn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(nt,"id","diffusers.models.attention.Transformer2DModelOutput"),l(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(nt,"href","#diffusers.models.attention.Transformer2DModelOutput"),l(Ae,"class","relative group"),l(Do,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(at,"id","diffusers.FlaxModelMixin"),l(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(at,"href","#diffusers.FlaxModelMixin"),l(Fe,"class","relative group"),l(Tn,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.FlaxModelMixin"),l(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ct,"id","diffusers.models.unet_2d_condition_flax.FlaxUNet2DConditionOutput"),l(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ct,"href","#diffusers.models.unet_2d_condition_flax.FlaxUNet2DConditionOutput"),l(Ce,"class","relative group"),l(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(mt,"id","diffusers.FlaxUNet2DConditionModel"),l(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(mt,"href","#diffusers.FlaxUNet2DConditionModel"),l(je,"class","relative group"),l(En,"href","/docs/diffusers/main/en/using-diffusers/loading#diffusers.FlaxModelMixin"),l(Ko,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),l(Ko,"rel","nofollow"),l(So,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(So,"rel","nofollow"),l(Xo,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(Xo,"rel","nofollow"),l(Wo,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(Wo,"rel","nofollow"),l(Qo,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(Qo,"rel","nofollow"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ft,"id","diffusers.models.vae_flax.FlaxDecoderOutput"),l(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ft,"href","#diffusers.models.vae_flax.FlaxDecoderOutput"),l(qe,"class","relative group"),l(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ht,"id","diffusers.models.vae_flax.FlaxAutoencoderKLOutput"),l(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ht,"href","#diffusers.models.vae_flax.FlaxAutoencoderKLOutput"),l(Pe,"class","relative group"),l(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(_t,"id","diffusers.FlaxAutoencoderKL"),l(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_t,"href","#diffusers.FlaxAutoencoderKL"),l(Le,"class","relative group"),l(nn,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),l(nn,"rel","nofollow"),l(an,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(an,"rel","nofollow"),l(sn,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(sn,"rel","nofollow"),l(rn,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(rn,"rel","nofollow"),l(ln,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(ln,"rel","nofollow"),l(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,x),m(t,M,p),m(t,$,p),e($,y),e(y,k),g(f,k,null),e($,w),e($,Y),e(Y,fe),m(t,B,p),m(t,I,p),e(I,pn),e(I,jn),e(jn,_i),e(I,vi),e(I,qn),e(qn,bi),e(I,xi),m(t,qs,p),m(t,ue,p),e(ue,Ie),e(Ie,Pn),g(Tt,Pn,null),e(ue,yi),e(ue,Ln),e(Ln,$i),m(t,Ps,p),m(t,T,p),g(Et,T,null),e(T,wi),e(T,Bn),e(Bn,Di),e(T,ki),e(T,mn),e(mn,fn),e(fn,Mi),e(mn,Ti),e(T,Ei),e(T,In),e(In,ae),e(ae,Vn),e(Vn,Ni),e(ae,Ui),e(ae,zn),e(zn,Ai),e(ae,Fi),e(ae,un),e(un,Ci),e(ae,Oi),e(T,ji),e(T,se),g(Nt,se,null),e(se,qi),e(se,Kn),e(Kn,Pi),e(se,Li),e(se,Sn),e(Sn,Bi),e(T,Ii),e(T,re),g(Ut,re,null),e(re,Vi),e(re,Xn),e(Xn,zi),e(re,Ki),e(re,Wn),e(Wn,Si),e(T,Xi),e(T,U),g(At,U,null),e(U,Wi),e(U,Qn),e(Qn,Qi),e(U,Hi),e(U,he),e(he,Yi),e(he,Hn),e(Hn,Gi),e(he,Ji),e(he,Yn),e(Yn,Ri),e(he,Zi),e(U,el),e(U,Ft),e(Ft,tl),e(Ft,Gn),e(Gn,ol),e(Ft,nl),e(U,al),e(U,Ct),e(Ct,sl),e(Ct,Jn),e(Jn,rl),e(Ct,il),e(U,ll),g(Ve,U,null),e(U,dl),g(ze,U,null),e(T,cl),e(T,Ke),g(Ot,Ke,null),e(Ke,pl),e(Ke,Rn),e(Rn,ml),e(T,fl),e(T,Se),g(jt,Se,null),e(Se,ul),e(Se,qt),e(qt,hl),e(qt,Zn),e(Zn,gl),e(qt,_l),m(t,Ls,p),m(t,ge,p),e(ge,Xe),e(Xe,ea),g(Pt,ea,null),e(ge,vl),e(ge,ta),e(ta,bl),m(t,Bs,p),m(t,Lt,p),g(Bt,Lt,null),m(t,Is,p),m(t,_e,p),e(_e,We),e(We,oa),g(It,oa,null),e(_e,xl),e(_e,na),e(na,yl),m(t,Vs,p),m(t,V,p),g(Vt,V,null),e(V,$l),e(V,aa),e(aa,wl),e(V,Dl),e(V,zt),e(zt,kl),e(zt,hn),e(hn,Ml),e(zt,Tl),e(V,El),e(V,gn),g(Kt,gn,null),m(t,zs,p),m(t,ve,p),e(ve,Qe),e(Qe,sa),g(St,sa,null),e(ve,Nl),e(ve,ra),e(ra,Ul),m(t,Ks,p),m(t,Xt,p),g(Wt,Xt,null),m(t,Ss,p),m(t,be,p),e(be,He),e(He,ia),g(Qt,ia,null),e(be,Al),e(be,la),e(la,Fl),m(t,Xs,p),m(t,z,p),g(Ht,z,null),e(z,Cl),e(z,da),e(da,Ol),e(z,jl),e(z,Yt),e(Yt,ql),e(Yt,_n),e(_n,Pl),e(Yt,Ll),e(z,Bl),e(z,vn),g(Gt,vn,null),m(t,Ws,p),m(t,xe,p),e(xe,Ye),e(Ye,ca),g(Jt,ca,null),e(xe,Il),e(xe,pa),e(pa,Vl),m(t,Qs,p),m(t,Rt,p),g(Zt,Rt,null),m(t,Hs,p),m(t,ye,p),e(ye,Ge),e(Ge,ma),g(eo,ma,null),e(ye,zl),e(ye,fa),e(fa,Kl),m(t,Ys,p),m(t,K,p),g(to,K,null),e(K,Sl),e(K,ua),e(ua,Xl),e(K,Wl),e(K,oo),e(oo,Ql),e(oo,bn),e(bn,Hl),e(oo,Yl),e(K,Gl),e(K,xn),g(no,xn,null),m(t,Gs,p),m(t,$e,p),e($e,Je),e(Je,ha),g(ao,ha,null),e($e,Jl),e($e,ga),e(ga,Rl),m(t,Js,p),m(t,we,p),g(so,we,null),e(we,Zl),e(we,_a),e(_a,ed),m(t,Rs,p),m(t,De,p),e(De,Re),e(Re,va),g(ro,va,null),e(De,td),e(De,ba),e(ba,od),m(t,Zs,p),m(t,ke,p),g(io,ke,null),e(ke,nd),e(ke,xa),e(xa,ad),m(t,er,p),m(t,Me,p),e(Me,Ze),e(Ze,ya),g(lo,ya,null),e(Me,sd),e(Me,$a),e($a,rd),m(t,tr,p),m(t,S,p),g(co,S,null),e(S,id),e(S,wa),e(wa,ld),e(S,dd),e(S,po),e(po,cd),e(po,yn),e(yn,pd),e(po,md),e(S,fd),e(S,$n),g(mo,$n,null),m(t,or,p),m(t,Te,p),e(Te,et),e(et,Da),g(fo,Da,null),e(Te,ud),e(Te,ka),e(ka,hd),m(t,nr,p),m(t,Ee,p),g(uo,Ee,null),e(Ee,gd),e(Ee,Ma),e(Ma,_d),m(t,ar,p),m(t,Ne,p),e(Ne,tt),e(tt,Ta),g(ho,Ta,null),e(Ne,vd),e(Ne,Ea),e(Ea,bd),m(t,sr,p),m(t,X,p),g(go,X,null),e(X,xd),e(X,Na),e(Na,yd),e(X,$d),e(X,_o),e(_o,wd),e(_o,wn),e(wn,Dd),e(_o,kd),e(X,Md),e(X,Dn),g(vo,Dn,null),m(t,rr,p),m(t,Ue,p),e(Ue,ot),e(ot,Ua),g(bo,Ua,null),e(Ue,Td),e(Ue,Aa),e(Aa,Ed),m(t,ir,p),m(t,F,p),g(xo,F,null),e(F,Nd),e(F,Fa),e(Fa,Ud),e(F,Ad),e(F,Ca),e(Ca,Fd),e(F,Cd),e(F,yo),e(yo,Od),e(yo,Oa),e(Oa,jd),e(yo,qd),e(F,Pd),e(F,ja),e(ja,Ld),e(F,Bd),e(F,kn),g($o,kn,null),m(t,lr,p),m(t,Ae,p),e(Ae,nt),e(nt,qa),g(wo,qa,null),e(Ae,Id),e(Ae,Pa),e(Pa,Vd),m(t,dr,p),m(t,Do,p),g(ko,Do,null),m(t,cr,p),m(t,Fe,p),e(Fe,at),e(at,La),g(Mo,La,null),e(Fe,zd),e(Fe,Ba),e(Ba,Kd),m(t,pr,p),m(t,E,p),g(To,E,null),e(E,Sd),e(E,Ia),e(Ia,Xd),e(E,Wd),e(E,Mn),e(Mn,Tn),e(Tn,Qd),e(Mn,Hd),e(E,Yd),e(E,P),g(Eo,P,null),e(P,Gd),e(P,Va),e(Va,Jd),e(P,Rd),e(P,No),e(No,Zd),e(No,za),e(za,ec),e(No,tc),e(P,oc),e(P,Uo),e(Uo,nc),e(Uo,Ka),e(Ka,ac),e(Uo,sc),e(P,rc),g(st,P,null),e(E,ic),e(E,rt),g(Ao,rt,null),e(rt,lc),e(rt,Fo),e(Fo,dc),e(Fo,Sa),e(Sa,cc),e(Fo,pc),e(E,mc),e(E,G),g(Co,G,null),e(G,fc),e(G,W),e(W,uc),e(W,Xa),e(Xa,hc),e(W,gc),e(W,Wa),e(Wa,_c),e(W,vc),e(W,Qa),e(Qa,bc),e(W,xc),e(W,Ha),e(Ha,yc),e(W,$c),e(G,wc),e(G,Ya),e(Ya,Dc),e(G,kc),g(it,G,null),e(E,Mc),e(E,J),g(Oo,J,null),e(J,Tc),e(J,Q),e(Q,Ec),e(Q,Ga),e(Ga,Nc),e(Q,Uc),e(Q,Ja),e(Ja,Ac),e(Q,Fc),e(Q,Ra),e(Ra,Cc),e(Q,Oc),e(Q,Za),e(Za,jc),e(Q,qc),e(J,Pc),e(J,es),e(es,Lc),e(J,Bc),g(lt,J,null),e(E,Ic),e(E,ie),g(jo,ie,null),e(ie,Vc),e(ie,H),e(H,zc),e(H,ts),e(ts,Kc),e(H,Sc),e(H,os),e(os,Xc),e(H,Wc),e(H,ns),e(ns,Qc),e(H,Hc),e(H,as),e(as,Yc),e(H,Gc),e(ie,Jc),g(dt,ie,null),m(t,mr,p),m(t,Ce,p),e(Ce,ct),e(ct,ss),g(qo,ss,null),e(Ce,Rc),e(Ce,rs),e(rs,Zc),m(t,fr,p),m(t,Oe,p),g(Po,Oe,null),e(Oe,ep),e(Oe,pt),g(Lo,pt,null),e(pt,tp),e(pt,is),e(is,op),m(t,ur,p),m(t,je,p),e(je,mt),e(mt,ls),g(Bo,ls,null),e(je,np),e(je,ds),e(ds,ap),m(t,hr,p),m(t,C,p),g(Io,C,null),e(C,sp),e(C,cs),e(cs,rp),e(C,ip),e(C,Vo),e(Vo,lp),e(Vo,En),e(En,dp),e(Vo,cp),e(C,pp),e(C,zo),e(zo,mp),e(zo,Ko),e(Ko,fp),e(zo,up),e(C,hp),e(C,ps),e(ps,gp),e(C,_p),e(C,ee),e(ee,ms),e(ms,So),e(So,vp),e(ee,bp),e(ee,fs),e(fs,Xo),e(Xo,xp),e(ee,yp),e(ee,us),e(us,Wo),e(Wo,$p),e(ee,wp),e(ee,hs),e(hs,Qo),e(Qo,Dp),m(t,gr,p),m(t,qe,p),e(qe,ft),e(ft,gs),g(Ho,gs,null),e(qe,kp),e(qe,_s),e(_s,Mp),m(t,_r,p),m(t,te,p),g(Yo,te,null),e(te,Tp),e(te,vs),e(vs,Ep),e(te,Np),e(te,ut),g(Go,ut,null),e(ut,Up),e(ut,bs),e(bs,Ap),m(t,vr,p),m(t,Pe,p),e(Pe,ht),e(ht,xs),g(Jo,xs,null),e(Pe,Fp),e(Pe,ys),e(ys,Cp),m(t,br,p),m(t,oe,p),g(Ro,oe,null),e(oe,Op),e(oe,$s),e($s,jp),e(oe,qp),e(oe,gt),g(Zo,gt,null),e(gt,Pp),e(gt,ws),e(ws,Lp),m(t,xr,p),m(t,Le,p),e(Le,_t),e(_t,Ds),g(en,Ds,null),e(Le,Bp),e(Le,ks),e(ks,Ip),m(t,yr,p),m(t,j,p),g(tn,j,null),e(j,Vp),e(j,Ms),e(Ms,zp),e(j,Kp),e(j,on),e(on,Sp),e(on,nn),e(nn,Xp),e(on,Wp),e(j,Qp),e(j,Ts),e(Ts,Hp),e(j,Yp),e(j,ne),e(ne,Es),e(Es,an),e(an,Gp),e(ne,Jp),e(ne,Ns),e(Ns,sn),e(sn,Rp),e(ne,Zp),e(ne,Us),e(Us,rn),e(rn,em),e(ne,tm),e(ne,As),e(As,ln),e(ln,om),$r=!0},p(t,[p]){const dn={};p&2&&(dn.$$scope={dirty:p,ctx:t}),Ve.$set(dn);const Fs={};p&2&&(Fs.$$scope={dirty:p,ctx:t}),ze.$set(Fs);const Cs={};p&2&&(Cs.$$scope={dirty:p,ctx:t}),st.$set(Cs);const Os={};p&2&&(Os.$$scope={dirty:p,ctx:t}),it.$set(Os);const Be={};p&2&&(Be.$$scope={dirty:p,ctx:t}),lt.$set(Be);const js={};p&2&&(js.$$scope={dirty:p,ctx:t}),dt.$set(js)},i(t){$r||(_(f.$$.fragment,t),_(Tt.$$.fragment,t),_(Et.$$.fragment,t),_(Nt.$$.fragment,t),_(Ut.$$.fragment,t),_(At.$$.fragment,t),_(Ve.$$.fragment,t),_(ze.$$.fragment,t),_(Ot.$$.fragment,t),_(jt.$$.fragment,t),_(Pt.$$.fragment,t),_(Bt.$$.fragment,t),_(It.$$.fragment,t),_(Vt.$$.fragment,t),_(Kt.$$.fragment,t),_(St.$$.fragment,t),_(Wt.$$.fragment,t),_(Qt.$$.fragment,t),_(Ht.$$.fragment,t),_(Gt.$$.fragment,t),_(Jt.$$.fragment,t),_(Zt.$$.fragment,t),_(eo.$$.fragment,t),_(to.$$.fragment,t),_(no.$$.fragment,t),_(ao.$$.fragment,t),_(so.$$.fragment,t),_(ro.$$.fragment,t),_(io.$$.fragment,t),_(lo.$$.fragment,t),_(co.$$.fragment,t),_(mo.$$.fragment,t),_(fo.$$.fragment,t),_(uo.$$.fragment,t),_(ho.$$.fragment,t),_(go.$$.fragment,t),_(vo.$$.fragment,t),_(bo.$$.fragment,t),_(xo.$$.fragment,t),_($o.$$.fragment,t),_(wo.$$.fragment,t),_(ko.$$.fragment,t),_(Mo.$$.fragment,t),_(To.$$.fragment,t),_(Eo.$$.fragment,t),_(st.$$.fragment,t),_(Ao.$$.fragment,t),_(Co.$$.fragment,t),_(it.$$.fragment,t),_(Oo.$$.fragment,t),_(lt.$$.fragment,t),_(jo.$$.fragment,t),_(dt.$$.fragment,t),_(qo.$$.fragment,t),_(Po.$$.fragment,t),_(Lo.$$.fragment,t),_(Bo.$$.fragment,t),_(Io.$$.fragment,t),_(Ho.$$.fragment,t),_(Yo.$$.fragment,t),_(Go.$$.fragment,t),_(Jo.$$.fragment,t),_(Ro.$$.fragment,t),_(Zo.$$.fragment,t),_(en.$$.fragment,t),_(tn.$$.fragment,t),$r=!0)},o(t){v(f.$$.fragment,t),v(Tt.$$.fragment,t),v(Et.$$.fragment,t),v(Nt.$$.fragment,t),v(Ut.$$.fragment,t),v(At.$$.fragment,t),v(Ve.$$.fragment,t),v(ze.$$.fragment,t),v(Ot.$$.fragment,t),v(jt.$$.fragment,t),v(Pt.$$.fragment,t),v(Bt.$$.fragment,t),v(It.$$.fragment,t),v(Vt.$$.fragment,t),v(Kt.$$.fragment,t),v(St.$$.fragment,t),v(Wt.$$.fragment,t),v(Qt.$$.fragment,t),v(Ht.$$.fragment,t),v(Gt.$$.fragment,t),v(Jt.$$.fragment,t),v(Zt.$$.fragment,t),v(eo.$$.fragment,t),v(to.$$.fragment,t),v(no.$$.fragment,t),v(ao.$$.fragment,t),v(so.$$.fragment,t),v(ro.$$.fragment,t),v(io.$$.fragment,t),v(lo.$$.fragment,t),v(co.$$.fragment,t),v(mo.$$.fragment,t),v(fo.$$.fragment,t),v(uo.$$.fragment,t),v(ho.$$.fragment,t),v(go.$$.fragment,t),v(vo.$$.fragment,t),v(bo.$$.fragment,t),v(xo.$$.fragment,t),v($o.$$.fragment,t),v(wo.$$.fragment,t),v(ko.$$.fragment,t),v(Mo.$$.fragment,t),v(To.$$.fragment,t),v(Eo.$$.fragment,t),v(st.$$.fragment,t),v(Ao.$$.fragment,t),v(Co.$$.fragment,t),v(it.$$.fragment,t),v(Oo.$$.fragment,t),v(lt.$$.fragment,t),v(jo.$$.fragment,t),v(dt.$$.fragment,t),v(qo.$$.fragment,t),v(Po.$$.fragment,t),v(Lo.$$.fragment,t),v(Bo.$$.fragment,t),v(Io.$$.fragment,t),v(Ho.$$.fragment,t),v(Yo.$$.fragment,t),v(Go.$$.fragment,t),v(Jo.$$.fragment,t),v(Ro.$$.fragment,t),v(Zo.$$.fragment,t),v(en.$$.fragment,t),v(tn.$$.fragment,t),$r=!1},d(t){o(x),t&&o(M),t&&o($),b(f),t&&o(B),t&&o(I),t&&o(qs),t&&o(ue),b(Tt),t&&o(Ps),t&&o(T),b(Et),b(Nt),b(Ut),b(At),b(Ve),b(ze),b(Ot),b(jt),t&&o(Ls),t&&o(ge),b(Pt),t&&o(Bs),t&&o(Lt),b(Bt),t&&o(Is),t&&o(_e),b(It),t&&o(Vs),t&&o(V),b(Vt),b(Kt),t&&o(zs),t&&o(ve),b(St),t&&o(Ks),t&&o(Xt),b(Wt),t&&o(Ss),t&&o(be),b(Qt),t&&o(Xs),t&&o(z),b(Ht),b(Gt),t&&o(Ws),t&&o(xe),b(Jt),t&&o(Qs),t&&o(Rt),b(Zt),t&&o(Hs),t&&o(ye),b(eo),t&&o(Ys),t&&o(K),b(to),b(no),t&&o(Gs),t&&o($e),b(ao),t&&o(Js),t&&o(we),b(so),t&&o(Rs),t&&o(De),b(ro),t&&o(Zs),t&&o(ke),b(io),t&&o(er),t&&o(Me),b(lo),t&&o(tr),t&&o(S),b(co),b(mo),t&&o(or),t&&o(Te),b(fo),t&&o(nr),t&&o(Ee),b(uo),t&&o(ar),t&&o(Ne),b(ho),t&&o(sr),t&&o(X),b(go),b(vo),t&&o(rr),t&&o(Ue),b(bo),t&&o(ir),t&&o(F),b(xo),b($o),t&&o(lr),t&&o(Ae),b(wo),t&&o(dr),t&&o(Do),b(ko),t&&o(cr),t&&o(Fe),b(Mo),t&&o(pr),t&&o(E),b(To),b(Eo),b(st),b(Ao),b(Co),b(it),b(Oo),b(lt),b(jo),b(dt),t&&o(mr),t&&o(Ce),b(qo),t&&o(fr),t&&o(Oe),b(Po),b(Lo),t&&o(ur),t&&o(je),b(Bo),t&&o(hr),t&&o(C),b(Io),t&&o(gr),t&&o(qe),b(Ho),t&&o(_r),t&&o(te),b(Yo),b(Go),t&&o(vr),t&&o(Pe),b(Jo),t&&o(br),t&&o(oe),b(Ro),b(Zo),t&&o(xr),t&&o(Le),b(en),t&&o(yr),t&&o(j),b(tn)}}}const ch={local:"models",sections:[{local:"diffusers.ModelMixin",title:"ModelMixin"},{local:"diffusers.models.unet_2d.UNet2DOutput",title:"UNet2DOutput"},{local:"diffusers.UNet2DModel",title:"UNet2DModel"},{local:"diffusers.models.unet_1d.UNet1DOutput",title:"UNet1DOutput"},{local:"diffusers.UNet1DModel",title:"UNet1DModel"},{local:"diffusers.models.unet_2d_condition.UNet2DConditionOutput",title:"UNet2DConditionOutput"},{local:"diffusers.UNet2DConditionModel",title:"UNet2DConditionModel"},{local:"diffusers.models.vae.DecoderOutput",title:"DecoderOutput"},{local:"diffusers.models.vae.VQEncoderOutput",title:"VQEncoderOutput"},{local:"diffusers.VQModel",title:"VQModel"},{local:"diffusers.models.vae.AutoencoderKLOutput",title:"AutoencoderKLOutput"},{local:"diffusers.AutoencoderKL",title:"AutoencoderKL"},{local:"diffusers.Transformer2DModel",title:"Transformer2DModel"},{local:"diffusers.models.attention.Transformer2DModelOutput",title:"Transformer2DModelOutput"},{local:"diffusers.FlaxModelMixin",title:"FlaxModelMixin"},{local:"diffusers.models.unet_2d_condition_flax.FlaxUNet2DConditionOutput",title:"FlaxUNet2DConditionOutput"},{local:"diffusers.FlaxUNet2DConditionModel",title:"FlaxUNet2DConditionModel"},{local:"diffusers.models.vae_flax.FlaxDecoderOutput",title:"FlaxDecoderOutput"},{local:"diffusers.models.vae_flax.FlaxAutoencoderKLOutput",title:"FlaxAutoencoderKLOutput"},{local:"diffusers.FlaxAutoencoderKL",title:"FlaxAutoencoderKL"}],title:"Models"};function ph(q){return oh(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vh extends Ru{constructor(x){super();Zu(this,x,ph,dh,eh,{})}}export{vh as default,ch as metadata};
