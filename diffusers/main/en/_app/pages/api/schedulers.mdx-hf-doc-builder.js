import{S as R5,i as Q5,s as j5,e as s,k as l,w as p,t as o,M as W5,c as n,d as t,m as c,a as i,x as h,h as a,b as d,G as e,g as f,y as m,q as g,o as _,B as v,v as U5}from"../../chunks/vendor-hf-doc-builder.js";import{T as B5}from"../../chunks/Tip-hf-doc-builder.js";import{D as b}from"../../chunks/Docstring-hf-doc-builder.js";import{I as w}from"../../chunks/IconCopyLink-hf-doc-builder.js";function H5(sc){let Z,ut;return{c(){Z=s("p"),ut=o("Score SDE-VP is under construction.")},l(ee){Z=n(ee,"P",{});var De=i(Z);ut=a(De,"Score SDE-VP is under construction."),De.forEach(t)},m(ee,De){f(ee,Z,De),e(Z,ut)},d(ee){ee&&t(Z)}}}function G5(sc){let Z,ut,ee,De,Ri,Ir,Kf,Qi,Rf,nc,lo,Qf,oc,qe,ft,ji,Nr,jf,Wi,Wf,ic,Ce,Uf,Ui,Bf,Hf,Bi,Gf,zf,ac,pt,co,Yf,Lr,Hi,Jf,Xf,Gi,Zf,ep,Ke,tp,zi,rp,sp,Yi,np,op,dc,Re,ht,Ji,qr,ip,Xi,ap,lc,O,dp,Zi,lp,cp,uo,up,fp,fo,pp,hp,ea,mp,gp,po,_p,vp,ta,bp,Sp,cc,Qe,mt,ra,Kr,Dp,sa,xp,uc,ho,$p,fc,gt,na,Ep,yp,oa,wp,pc,je,_t,ia,Rr,Mp,aa,Pp,hc,mo,Tp,mc,ke,Qr,Cp,da,kp,Op,Ap,jr,Vp,la,Fp,Ip,Np,ca,Lp,gc,vt,qp,go,Kp,Rp,_c,We,bt,ua,Wr,Qp,fa,jp,vc,Ue,Ur,Wp,pa,Up,bc,Be,St,ha,Br,Bp,ma,Hp,Sc,He,Hr,Gp,ga,zp,Dc,Ge,Dt,_a,Gr,Yp,va,Jp,xc,ze,xt,ba,zr,Xp,Sa,Zp,$c,_o,eh,Ec,P,Yr,th,Da,rh,sh,A,vo,nh,oh,xa,ih,ah,$a,dh,lh,Ea,ch,uh,bo,fh,ph,So,hh,mh,Do,gh,_h,vh,xo,bh,Jr,Sh,Dh,$t,Xr,xh,ya,$h,Eh,Et,Zr,yh,wa,wh,Mh,yt,es,Ph,Ma,Th,yc,Ye,wt,Pa,ts,Ch,Ta,kh,wc,Mt,Oh,rs,Ah,Vh,Mc,T,ss,Fh,Ca,Ih,Nh,V,$o,Lh,qh,ka,Kh,Rh,Oa,Qh,jh,Aa,Wh,Uh,Eo,Bh,Hh,yo,Gh,zh,wo,Yh,Jh,Xh,Mo,Zh,ns,em,tm,Pt,os,rm,Va,sm,nm,Tt,is,om,Fa,im,am,Ct,as,dm,Ia,lm,Pc,Je,kt,Na,ds,cm,La,um,Tc,xe,fm,ls,pm,hm,cs,mm,gm,us,_m,vm,Cc,S,fs,bm,qa,Sm,Dm,Ot,xm,ps,$m,Em,hs,ym,wm,Xe,Mm,Ka,Pm,Tm,Ra,Cm,km,Om,Me,Am,ms,Vm,Fm,Qa,Im,Nm,ja,Lm,qm,Km,F,Po,Rm,Qm,Wa,jm,Wm,Ua,Um,Bm,Ba,Hm,Gm,To,zm,Ym,Co,Jm,Xm,ko,Zm,eg,tg,$e,gs,rg,Ha,sg,ng,Ga,og,ig,za,ag,dg,Oe,_s,lg,Ya,cg,ug,vs,fg,bs,pg,hg,mg,At,Ss,gg,Ja,_g,vg,Vt,Ds,bg,Xa,Sg,Dg,Ft,xs,xg,Za,$g,Eg,It,$s,yg,ed,wg,Mg,Nt,Es,Pg,td,Tg,kc,Ze,Lt,rd,ys,Cg,sd,kg,Oc,qt,Og,ws,Ag,Vg,Ac,x,Ms,Fg,nd,Ig,Ng,Kt,Lg,Ps,qg,Kg,Ts,Rg,Qg,I,Oo,jg,Wg,od,Ug,Bg,id,Hg,Gg,ad,zg,Yg,Ao,Jg,Xg,Vo,Zg,e_,Fo,t_,r_,s_,Cs,n_,ks,o_,i_,a_,Ae,Os,d_,dd,l_,c_,ld,u_,f_,Rt,As,p_,cd,h_,m_,Qt,Vs,g_,ud,__,v_,jt,Fs,b_,fd,S_,D_,Wt,Is,x_,pd,$_,Vc,et,Ut,hd,Ns,E_,md,y_,Fc,Bt,w_,Ls,M_,P_,Ic,C,qs,T_,Io,C_,Ks,k_,O_,N,No,A_,V_,gd,F_,I_,_d,N_,L_,vd,q_,K_,Lo,R_,Q_,qo,j_,W_,Ko,U_,B_,H_,Ht,Rs,G_,bd,z_,Y_,Gt,Qs,J_,js,X_,Sd,Z_,ev,tv,zt,Ws,rv,Dd,sv,nv,Yt,Us,ov,xd,iv,Nc,tt,Jt,$d,Bs,av,Ed,dv,Lc,Xt,lv,Hs,cv,uv,qc,$,Gs,fv,yd,pv,hv,L,Ro,mv,gv,wd,_v,vv,Md,bv,Sv,Pd,Dv,xv,Qo,$v,Ev,jo,yv,wv,Wo,Mv,Pv,Tv,Uo,Cv,zs,kv,Ov,Zt,Ys,Av,Td,Vv,Fv,er,Js,Iv,Cd,Nv,Lv,Ve,Xs,qv,kd,Kv,Rv,Pe,Qv,Od,jv,Wv,Ad,Uv,Bv,Vd,Hv,Gv,zv,tr,Zs,Yv,Fd,Jv,Xv,rr,en,Zv,Id,eb,Kc,rt,sr,Nd,tn,tb,Ld,rb,Rc,nr,sb,rn,nb,ob,Qc,E,sn,ib,qd,ab,db,Bo,lb,nn,cb,ub,q,Ho,fb,pb,Kd,hb,mb,Rd,gb,_b,Qd,vb,bb,Go,Sb,Db,zo,xb,$b,Yo,Eb,yb,wb,or,on,Mb,jd,Pb,Tb,Fe,an,Cb,Wd,kb,Ob,st,Ab,Ud,Vb,Fb,Bd,Ib,Nb,Lb,ir,dn,qb,Hd,Kb,Rb,ar,ln,Qb,Gd,jb,Wb,dr,cn,Ub,zd,Bb,jc,nt,lr,Yd,un,Hb,Jd,Gb,Wc,cr,zb,fn,Yb,Jb,Uc,k,pn,Xb,Jo,Zb,hn,e1,t1,K,Xo,r1,s1,Xd,n1,o1,Zd,i1,a1,el,d1,l1,Zo,c1,u1,ei,f1,p1,ti,h1,m1,g1,ri,_1,mn,v1,b1,ur,gn,S1,tl,D1,x1,fr,_n,$1,rl,E1,y1,pr,vn,w1,sl,M1,Bc,ot,hr,nl,bn,P1,ol,T1,Hc,mr,C1,Sn,k1,O1,Gc,gr,zc,te,Dn,A1,il,V1,F1,R,si,I1,N1,al,L1,q1,dl,K1,R1,ll,Q1,j1,ni,W1,U1,oi,B1,H1,ii,G1,z1,Y1,ai,J1,xn,X1,Z1,cl,e0,Yc,it,_r,ul,$n,t0,fl,r0,Jc,Ie,s0,En,n0,o0,yn,i0,a0,Xc,Y,wn,d0,vr,l0,Mn,c0,u0,Pn,f0,p0,Q,di,h0,m0,pl,g0,_0,hl,v0,b0,ml,S0,D0,li,x0,$0,ci,E0,y0,ui,w0,M0,P0,br,Tn,T0,Cn,C0,gl,k0,O0,A0,Sr,kn,V0,_l,F0,I0,Dr,On,N0,vl,L0,Zc,at,xr,bl,An,q0,Sl,K0,eu,fi,R0,tu,J,Vn,Q0,pi,j0,Fn,W0,U0,j,hi,B0,H0,Dl,G0,z0,xl,Y0,J0,$l,X0,Z0,mi,e2,t2,gi,r2,s2,_i,n2,o2,i2,$r,In,a2,Nn,d2,El,l2,c2,u2,Er,Ln,f2,yl,p2,h2,yr,qn,m2,wl,g2,ru,dt,wr,Ml,Kn,_2,Pl,v2,su,Rn,b2,Qn,S2,nu,y,jn,D2,Tl,x2,$2,Cl,E2,y2,W,vi,w2,M2,kl,P2,T2,Ol,C2,k2,Al,O2,A2,bi,V2,F2,Si,I2,N2,Di,L2,q2,K2,xi,R2,Wn,Q2,j2,Ne,Un,W2,Bn,U2,Vl,B2,H2,G2,Fl,z2,Y2,U,Hn,J2,Gn,X2,Il,Z2,e4,t4,Nl,r4,s4,Ll,n4,o4,ql,i4,a4,Kl,zn,d4,Rl,l4,c4,u4,lt,f4,Ql,p4,h4,jl,m4,g4,_4,Mr,Yn,v4,Wl,b4,S4,Pr,Jn,D4,Xn,x4,Ul,$4,E4,ou,ct,Tr,Bl,Zn,y4,Hl,w4,iu,Te,M4,$i,P4,T4,eo,C4,k4,to,O4,au,X,ro,A4,Gl,V4,F4,B,Ei,I4,N4,zl,L4,q4,Yl,K4,R4,Jl,Q4,j4,yi,W4,U4,wi,B4,H4,Mi,G4,z4,Y4,Pi,J4,so,X4,Z4,Cr,no,eS,Xl,tS,rS,kr,oo,sS,Zl,nS,du;return Ir=new w({}),Nr=new w({}),qr=new w({}),Kr=new w({}),Rr=new w({}),Wr=new w({}),Ur=new b({props:{name:"class diffusers.SchedulerMixin",anchor:"diffusers.SchedulerMixin",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L38"}}),Br=new w({}),Hr=new b({props:{name:"class diffusers.schedulers.scheduling_utils.SchedulerOutput",anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput",parameters:[{name:"prev_sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput.prev_sample",description:`<strong>prev_sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code> for images) &#x2014;
Computed sample (x_{t-1}) of previous timestep. <code>prev_sample</code> should be used as next model input in the
denoising loop.`,name:"prev_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L25"}}),Gr=new w({}),zr=new w({}),Yr=new b({props:{name:"class diffusers.DDIMScheduler",anchor:"diffusers.DDIMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"},{name:"set_alpha_to_one",val:": bool = True"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDIMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDIMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDIMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDIMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDIMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"},{anchor:"diffusers.DDIMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.DDIMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L78"}}),Xr=new b({props:{name:"scale_model_input",anchor:"diffusers.DDIMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDIMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L164",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Zr=new b({props:{name:"set_timesteps",anchor:"diffusers.DDIMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L188"}}),es=new b({props:{name:"step",anchor:"diffusers.DDIMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"eta",val:": float = 0.0"},{name:"use_clipped_model_output",val:": bool = False"},{name:"generator",val:" = None"},{name:"variance_noise",val:": typing.Optional[torch.FloatTensor] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDIMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDIMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DDIMScheduler.step.eta",description:"<strong>eta</strong> (<code>float</code>) &#x2014; weight of noise for added noise in diffusion step.",name:"eta"},{anchor:"diffusers.DDIMScheduler.step.use_clipped_model_output",description:`<strong>use_clipped_model_output</strong> (<code>bool</code>) &#x2014; if <code>True</code>, compute &#x201C;corrected&#x201D; <code>model_output</code> from the clipped
predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when
<code>self.config.clip_sample</code> is <code>True</code>. If no clipping has happened, &#x201C;corrected&#x201D; <code>model_output</code> would
coincide with the one provided as input and <code>use_clipped_model_output</code> will have not effect.
generator &#x2014; random number generator.`,name:"use_clipped_model_output"},{anchor:"diffusers.DDIMScheduler.step.variance_noise",description:`<strong>variance_noise</strong> (<code>torch.FloatTensor</code>) &#x2014; instead of generating noise for the variance using <code>generator</code>, we
can directly provide the noise for the variance itself. This is useful for methods such as
CycleDiffusion. (<a href="https://arxiv.org/abs/2210.05559" rel="nofollow">https://arxiv.org/abs/2210.05559</a>)`,name:"variance_noise"},{anchor:"diffusers.DDIMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDIMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L204",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> or <code>tuple</code></p>
`}}),ts=new w({}),ss=new b({props:{name:"class diffusers.DDPMScheduler",anchor:"diffusers.DDPMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"variance_type",val:": str = 'fixed_small'"},{name:"clip_sample",val:": bool = True"},{name:"predict_epsilon",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDPMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDPMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDPMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDPMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDPMScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.DDPMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"},{anchor:"diffusers.DDPMScheduler.predict_epsilon",description:`<strong>predict_epsilon</strong> (<code>bool</code>) &#x2014;
optional flag to use when the model predicts the noise (epsilon), or the samples instead of the noise.`,name:"predict_epsilon"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L76"}}),os=new b({props:{name:"scale_model_input",anchor:"diffusers.DDPMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDPMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L160",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),is=new b({props:{name:"set_timesteps",anchor:"diffusers.DDPMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L174"}}),as=new b({props:{name:"step",anchor:"diffusers.DDPMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"generator",val:" = None"},{name:"return_dict",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDPMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDPMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.DDPMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDPMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L222",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> or <code>tuple</code></p>
`}}),ds=new w({}),fs=new b({props:{name:"class diffusers.DPMSolverMultistepScheduler",anchor:"diffusers.DPMSolverMultistepScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"solver_order",val:": int = 2"},{name:"predict_epsilon",val:": bool = True"},{name:"thresholding",val:": bool = False"},{name:"dynamic_thresholding_ratio",val:": float = 0.995"},{name:"sample_max_value",val:": float = 1.0"},{name:"algorithm_type",val:": str = 'dpmsolver++'"},{name:"solver_type",val:": str = 'midpoint'"},{name:"lower_order_final",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DPMSolverMultistepScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DPMSolverMultistepScheduler.solver_order",description:`<strong>solver_order</strong> (<code>int</code>, default <code>2</code>) &#x2014;
the order of DPM-Solver; can be <code>1</code> or <code>2</code> or <code>3</code>. We recommend to use <code>solver_order=2</code> for guided
sampling, and <code>solver_order=3</code> for unconditional sampling.`,name:"solver_order"},{anchor:"diffusers.DPMSolverMultistepScheduler.predict_epsilon",description:`<strong>predict_epsilon</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
we currently support both the noise prediction model and the data prediction model. If the model predicts
the noise / epsilon, set <code>predict_epsilon</code> to <code>True</code>. If the model predicts the data / x0 directly, set
<code>predict_epsilon</code> to <code>False</code>.`,name:"predict_epsilon"},{anchor:"diffusers.DPMSolverMultistepScheduler.thresholding",description:`<strong>thresholding</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
whether to use the &#x201C;dynamic thresholding&#x201D; method (introduced by Imagen, <a href="https://arxiv.org/abs/2205.11487" rel="nofollow">https://arxiv.org/abs/2205.11487</a>).
For pixel-space diffusion models, you can set both <code>algorithm_type=dpmsolver++</code> and <code>thresholding=True</code> to
use the dynamic thresholding. Note that the thresholding method is unsuitable for latent-space diffusion
models (such as stable-diffusion).`,name:"thresholding"},{anchor:"diffusers.DPMSolverMultistepScheduler.dynamic_thresholding_ratio",description:`<strong>dynamic_thresholding_ratio</strong> (<code>float</code>, default <code>0.995</code>) &#x2014;
the ratio for the dynamic thresholding method. Default is <code>0.995</code>, the same as Imagen
(<a href="https://arxiv.org/abs/2205.11487" rel="nofollow">https://arxiv.org/abs/2205.11487</a>).`,name:"dynamic_thresholding_ratio"},{anchor:"diffusers.DPMSolverMultistepScheduler.sample_max_value",description:`<strong>sample_max_value</strong> (<code>float</code>, default <code>1.0</code>) &#x2014;
the threshold value for dynamic thresholding. Valid only when <code>thresholding=True</code> and
<code>algorithm_type=&quot;dpmsolver++</code>.`,name:"sample_max_value"},{anchor:"diffusers.DPMSolverMultistepScheduler.algorithm_type",description:`<strong>algorithm_type</strong> (<code>str</code>, default <code>dpmsolver++</code>) &#x2014;
the algorithm type for the solver. Either <code>dpmsolver</code> or <code>dpmsolver++</code>. The <code>dpmsolver</code> type implements the
algorithms in <a href="https://arxiv.org/abs/2206.00927" rel="nofollow">https://arxiv.org/abs/2206.00927</a>, and the <code>dpmsolver++</code> type implements the algorithms in
<a href="https://arxiv.org/abs/2211.01095" rel="nofollow">https://arxiv.org/abs/2211.01095</a>. We recommend to use <code>dpmsolver++</code> with <code>solver_order=2</code> for guided
sampling (e.g. stable-diffusion).`,name:"algorithm_type"},{anchor:"diffusers.DPMSolverMultistepScheduler.solver_type",description:`<strong>solver_type</strong> (<code>str</code>, default <code>midpoint</code>) &#x2014;
the solver type for the second-order solver. Either <code>midpoint</code> or <code>heun</code>. The solver type slightly affects
the sample quality, especially for small number of steps. We empirically find that <code>midpoint</code> solvers are
slightly better, so we recommend to use the <code>midpoint</code> type.`,name:"solver_type"},{anchor:"diffusers.DPMSolverMultistepScheduler.lower_order_final",description:`<strong>lower_order_final</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
whether to use lower-order solvers in the final steps. Only valid for &lt; 15 inference steps. We empirically
find this trick can stabilize the sampling of DPM-Solver for steps &lt; 15, especially for steps &lt;= 10.`,name:"lower_order_final"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L56"}}),gs=new b({props:{name:"convert_model_output",anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L206",returnDescription:`
<p>the converted model output.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),_s=new b({props:{name:"dpm_solver_first_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L255",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ss=new b({props:{name:"multistep_dpm_solver_second_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update",parameters:[{name:"model_output_list",val:": typing.List[torch.FloatTensor]"},{name:"timestep_list",val:": typing.List[int]"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.model_output_list",description:`<strong>model_output_list</strong> (<code>List[torch.FloatTensor]</code>) &#x2014;
direct outputs from learned diffusion model at current and latter timesteps.`,name:"model_output_list"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current and latter discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L287",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ds=new b({props:{name:"multistep_dpm_solver_third_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update",parameters:[{name:"model_output_list",val:": typing.List[torch.FloatTensor]"},{name:"timestep_list",val:": typing.List[int]"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.model_output_list",description:`<strong>model_output_list</strong> (<code>List[torch.FloatTensor]</code>) &#x2014;
direct outputs from learned diffusion model at current and latter timesteps.`,name:"model_output_list"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current and latter discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L346",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),xs=new b({props:{name:"scale_model_input",anchor:"diffusers.DPMSolverMultistepScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L469",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),$s=new b({props:{name:"set_timesteps",anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L183"}}),Es=new b({props:{name:"step",anchor:"diffusers.DPMSolverMultistepScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L401",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),ys=new w({}),Ms=new b({props:{name:"class diffusers.KarrasVeScheduler",anchor:"diffusers.KarrasVeScheduler",parameters:[{name:"sigma_min",val:": float = 0.02"},{name:"sigma_max",val:": float = 100"},{name:"s_noise",val:": float = 1.007"},{name:"s_churn",val:": float = 80"},{name:"s_min",val:": float = 0.05"},{name:"s_max",val:": float = 50"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.sigma_min",description:"<strong>sigma_min</strong> (<code>float</code>) &#x2014; minimum noise magnitude",name:"sigma_min"},{anchor:"diffusers.KarrasVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum noise magnitude",name:"sigma_max"},{anchor:"diffusers.KarrasVeScheduler.s_noise",description:`<strong>s_noise</strong> (<code>float</code>) &#x2014; the amount of additional noise to counteract loss of detail during sampling.
A reasonable range is [1.000, 1.011].`,name:"s_noise"},{anchor:"diffusers.KarrasVeScheduler.s_churn",description:`<strong>s_churn</strong> (<code>float</code>) &#x2014; the parameter controlling the overall amount of stochasticity.
A reasonable range is [0, 100].`,name:"s_churn"},{anchor:"diffusers.KarrasVeScheduler.s_min",description:`<strong>s_min</strong> (<code>float</code>) &#x2014; the start value of the sigma range where we add noise (enable stochasticity).
A reasonable range is [0, 10].`,name:"s_min"},{anchor:"diffusers.KarrasVeScheduler.s_max",description:`<strong>s_max</strong> (<code>float</code>) &#x2014; the end value of the sigma range where we add noise.
A reasonable range is [0.2, 80].`,name:"s_max"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L48"}}),Os=new b({props:{name:"add_noise_to_input",anchor:"diffusers.KarrasVeScheduler.add_noise_to_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"sigma",val:": float"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L133"}}),As=new b({props:{name:"scale_model_input",anchor:"diffusers.KarrasVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.KarrasVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L98",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Vs=new b({props:{name:"set_timesteps",anchor:"diffusers.KarrasVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L112"}}),Fs=new b({props:{name:"step",anchor:"diffusers.KarrasVeScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class</p>
<p>KarrasVeOutput &#x2014; updated sample in the diffusion chain and derivative (TODO double check).`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L154",returnDescription:`
<p><code>KarrasVeOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>KarrasVeOutput</code> or <code>tuple</code></p>
`}}),Is=new b({props:{name:"step_correct",anchor:"diffusers.KarrasVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"sample_prev",val:": FloatTensor"},{name:"derivative",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_prev",description:"<strong>sample_prev</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.derivative",description:"<strong>derivative</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"derivative"},{anchor:"diffusers.KarrasVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L192",returnDescription:`
<p>updated sample in the diffusion chain. derivative (TODO): TODO</p>
`,returnType:`
<p>prev_sample (TODO)</p>
`}}),Ns=new w({}),qs=new b({props:{name:"class diffusers.LMSDiscreteScheduler",anchor:"diffusers.LMSDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.LMSDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.LMSDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.LMSDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.LMSDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L47"}}),Rs=new b({props:{name:"get_lms_coefficient",anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient",parameters:[{name:"order",val:""},{name:"t",val:""},{name:"current_order",val:""}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.order",description:"<strong>order</strong> (TODO) &#x2014;",name:"order"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.t",description:"<strong>t</strong> (TODO) &#x2014;",name:"t"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.current_order",description:"<strong>current_order</strong> (TODO) &#x2014;",name:"current_order"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L138"}}),Qs=new b({props:{name:"scale_model_input",anchor:"diffusers.LMSDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L117",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ws=new b({props:{name:"set_timesteps",anchor:"diffusers.LMSDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L160"}}),Us=new b({props:{name:"step",anchor:"diffusers.LMSDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"order",val:": int = 4"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.LMSDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.LMSDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
order &#x2014; coefficient for multi-step inference.`,name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than LMSDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L186",returnDescription:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),Bs=new w({}),Gs=new b({props:{name:"class diffusers.PNDMScheduler",anchor:"diffusers.PNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"skip_prk_steps",val:": bool = False"},{name:"set_alpha_to_one",val:": bool = False"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.PNDMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.PNDMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.PNDMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.PNDMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.PNDMScheduler.skip_prk_steps",description:`<strong>skip_prk_steps</strong> (<code>bool</code>) &#x2014;
allows the scheduler to skip the Runge-Kutta steps that are defined in the original paper as being required
before plms steps; defaults to <code>False</code>.`,name:"skip_prk_steps"},{anchor:"diffusers.PNDMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.PNDMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L56"}}),Ys=new b({props:{name:"scale_model_input",anchor:"diffusers.PNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L345",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Js=new b({props:{name:"set_timesteps",anchor:"diffusers.PNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L153"}}),Xs=new b({props:{name:"step",anchor:"diffusers.PNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L192",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> or <code>tuple</code></p>
`}}),Zs=new b({props:{name:"step_plms",anchor:"diffusers.PNDMScheduler.step_plms",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_plms.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_plms.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_plms.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_plms.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L278",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),en=new b({props:{name:"step_prk",anchor:"diffusers.PNDMScheduler.step_prk",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_prk.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_prk.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_prk.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_prk.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L223",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),tn=new w({}),sn=new b({props:{name:"class diffusers.ScoreSdeVeScheduler",anchor:"diffusers.ScoreSdeVeScheduler",parameters:[{name:"num_train_timesteps",val:": int = 2000"},{name:"snr",val:": float = 0.15"},{name:"sigma_min",val:": float = 0.01"},{name:"sigma_max",val:": float = 1348.0"},{name:"sampling_eps",val:": float = 1e-05"},{name:"correct_steps",val:": int = 1"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.ScoreSdeVeScheduler.snr",description:`<strong>snr</strong> (<code>float</code>) &#x2014;
coefficient weighting the step from the model_output sample (from the network) to the random noise.`,name:"snr"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>) &#x2014;
initial noise scale for sigma sequence in sampling procedure. The minimum sigma should mirror the
distribution of the data.`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum value used for the range of continuous timesteps passed into the model.",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.sampling_eps",description:`<strong>sampling_eps</strong> (<code>float</code>) &#x2014; the end value of sampling, where timesteps decrease progressively from 1 to
epsilon. &#x2014;`,name:"sampling_eps"},{anchor:"diffusers.ScoreSdeVeScheduler.correct_steps",description:"<strong>correct_steps</strong> (<code>int</code>) &#x2014; number of correction steps performed on a produced sample.",name:"correct_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L45"}}),on=new b({props:{name:"scale_model_input",anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L87",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),an=new b({props:{name:"set_sigmas",anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas",parameters:[{name:"num_inference_steps",val:": int"},{name:"sigma_min",val:": float = None"},{name:"sigma_max",val:": float = None"},{name:"sampling_eps",val:": float = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>, optional) &#x2014;
initial noise scale value (overrides value given at Scheduler instantiation).`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>, optional) &#x2014; final noise scale value (overrides value given at Scheduler instantiation).",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L117"}}),dn=new b({props:{name:"set_timesteps",anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"sampling_eps",val:": float = None"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L101"}}),ln=new b({props:{name:"step_correct",anchor:"diffusers.ScoreSdeVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L212",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),cn=new b({props:{name:"step_pred",anchor:"diffusers.ScoreSdeVeScheduler.step_pred",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L151",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),un=new w({}),pn=new b({props:{name:"class diffusers.IPNDMScheduler",anchor:"diffusers.IPNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L24"}}),gn=new b({props:{name:"scale_model_input",anchor:"diffusers.IPNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L126",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),_n=new b({props:{name:"set_timesteps",anchor:"diffusers.IPNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L56"}}),vn=new b({props:{name:"step",anchor:"diffusers.IPNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.IPNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.IPNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.IPNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L76",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),bn=new w({}),gr=new B5({props:{warning:!0,$$slots:{default:[H5]},$$scope:{ctx:sc}}}),Dn=new b({props:{name:"class diffusers.schedulers.ScoreSdeVpScheduler",anchor:"diffusers.schedulers.ScoreSdeVpScheduler",parameters:[{name:"num_train_timesteps",val:" = 2000"},{name:"beta_min",val:" = 0.1"},{name:"beta_max",val:" = 20"},{name:"sampling_eps",val:" = 0.001"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_vp.py#L26"}}),$n=new w({}),wn=new b({props:{name:"class diffusers.EulerDiscreteScheduler",anchor:"diffusers.EulerDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L48"}}),Tn=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L117",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),kn=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L138"}}),On=new b({props:{name:"step",anchor:"diffusers.EulerDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"s_churn",val:": float = 0.0"},{name:"s_tmin",val:": float = 0.0"},{name:"s_tmax",val:": float = inf"},{name:"s_noise",val:": float = 1.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_churn",description:"<strong>s_churn</strong> (<code>float</code>) &#x2014;",name:"s_churn"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmin",description:"<strong>s_tmin</strong>  (<code>float</code>) &#x2014;",name:"s_tmin"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmax",description:"<strong>s_tmax</strong>  (<code>float</code>) &#x2014;",name:"s_tmax"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_noise",description:"<strong>s_noise</strong> (<code>float</code>) &#x2014;",name:"s_noise"},{anchor:"diffusers.EulerDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L161",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a
<code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),An=new w({}),Vn=new b({props:{name:"class diffusers.EulerAncestralDiscreteScheduler",anchor:"diffusers.EulerAncestralDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L48"}}),In=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L116",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ln=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L137"}}),qn=new b({props:{name:"step",anchor:"diffusers.EulerAncestralDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerAncestralDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L160",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise
a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),Kn=new w({}),jn=new b({props:{name:"class diffusers.VQDiffusionScheduler",anchor:"diffusers.VQDiffusionScheduler",parameters:[{name:"num_vec_classes",val:": int"},{name:"num_train_timesteps",val:": int = 100"},{name:"alpha_cum_start",val:": float = 0.99999"},{name:"alpha_cum_end",val:": float = 9e-06"},{name:"gamma_cum_start",val:": float = 9e-06"},{name:"gamma_cum_end",val:": float = 0.99999"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.num_vec_classes",description:`<strong>num_vec_classes</strong> (<code>int</code>) &#x2014;
The number of classes of the vector embeddings of the latent pixels. Includes the class for the masked
latent pixel.`,name:"num_vec_classes"},{anchor:"diffusers.VQDiffusionScheduler.num_train_timesteps",description:`<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014;
Number of diffusion steps used to train the model.`,name:"num_train_timesteps"},{anchor:"diffusers.VQDiffusionScheduler.alpha_cum_start",description:`<strong>alpha_cum_start</strong> (<code>float</code>) &#x2014;
The starting cumulative alpha value.`,name:"alpha_cum_start"},{anchor:"diffusers.VQDiffusionScheduler.alpha_cum_end",description:`<strong>alpha_cum_end</strong> (<code>float</code>) &#x2014;
The ending cumulative alpha value.`,name:"alpha_cum_end"},{anchor:"diffusers.VQDiffusionScheduler.gamma_cum_start",description:`<strong>gamma_cum_start</strong> (<code>float</code>) &#x2014;
The starting cumulative gamma value.`,name:"gamma_cum_start"},{anchor:"diffusers.VQDiffusionScheduler.gamma_cum_end",description:`<strong>gamma_cum_end</strong> (<code>float</code>) &#x2014;
The ending cumulative gamma value.`,name:"gamma_cum_end"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L106"}}),Un=new b({props:{name:"log_Q_t_transitioning_to_known_class",anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class",parameters:[{name:"t",val:": torch.int32"},{name:"x_t",val:": LongTensor"},{name:"log_onehot_x_t",val:": FloatTensor"},{name:"cumulative",val:": bool"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.t",description:`<strong>t</strong> (torch.Long) &#x2014;
The timestep that determines which transition matrix is used.`,name:"t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.x_t",description:`<strong>x_t</strong> (<code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>) &#x2014;
The classes of each latent pixel at time <code>t</code>.`,name:"x_t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.log_onehot_x_t",description:`<strong>log_onehot_x_t</strong> (<code>torch.FloatTensor</code> of shape <code>(batch size, num classes, num latent pixels)</code>) &#x2014;
The log one-hot vectors of <code>x_t</code>`,name:"log_onehot_x_t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.cumulative",description:`<strong>cumulative</strong> (<code>bool</code>) &#x2014;
If cumulative is <code>False</code>, we use the single step transition matrix <code>t-1</code>-&gt;<code>t</code>. If cumulative is <code>True</code>,
we use the cumulative transition matrix <code>0</code>-&gt;<code>t</code>.`,name:"cumulative"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L377",returnDescription:`
<p>Each <em>column</em> of the returned matrix is a <em>row</em> of log probabilities of the complete probability
transition matrix.</p>
<p>When non cumulative, returns <code>self.num_classes - 1</code> rows because the initial latent pixel cannot be
masked.</p>
<p>Where:</p>
<ul>
<li><code>q_n</code> is the probability distribution for the forward process of the <code>n</code>th latent pixel.</li>
<li>C_0 is a class of a latent pixel embedding</li>
<li>C_k is the class of the masked latent pixel</li>
</ul>
<p>non-cumulative result (omitting logarithms):</p>

	<CodeBlock 
		code={\`q_0(x_t | x_{t-1\\} = C_0) ... q_n(x_t | x_{t-1\\} = C_0)
          .      .                     .
          .               .            .
          .                      .     .
q_0(x_t | x_{t-1\\} = C_k) ... q_n(x_t | x_{t-1\\} = C_k)\`}
		highlighted={\`q<span class="hljs-constructor">_0(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_0)</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_0)</span>
          .      .                     .
          .               .            .
          .                      .     .
q<span class="hljs-constructor">_0(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_k)</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_k)</span>\`}
	/>
<p>cumulative result (omitting logarithms):</p>

	<CodeBlock 
		code={\`q_0_cumulative(x_t | x_0 = C_0)    ...  q_n_cumulative(x_t | x_0 = C_0)
          .               .                          .
          .                        .                 .
          .                               .          .
q_0_cumulative(x_t | x_0 = C_{k-1\\}) ... q_n_cumulative(x_t | x_0 = C_{k-1\\})\`}
		highlighted={\`q<span class="hljs-constructor">_0_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_0)</span><span class="hljs-operator">    ...  </span>q<span class="hljs-constructor">_n_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_0)</span>
          .               .                          .
          .                        .                 .
          .                               .          .
q<span class="hljs-constructor">_0_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_{<span class="hljs-params">k</span>-1\\})</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_{<span class="hljs-params">k</span>-1\\})</span>\`}
	/>
`,returnType:`
<p><code>torch.FloatTensor</code> of shape <code>(batch size, num classes - 1, num latent pixels)</code></p>
`}}),Hn=new b({props:{name:"q_posterior",anchor:"diffusers.VQDiffusionScheduler.q_posterior",parameters:[{name:"log_p_x_0",val:""},{name:"x_t",val:""},{name:"t",val:""}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.q_posterior.t",description:`<strong>t</strong> (torch.Long) &#x2014;
The timestep that determines which transition matrix is used.`,name:"t"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L258",returnDescription:`
<p>The log probabilities for the predicted classes of the image at timestep <code>t-1</code>. I.e. Equation (11).</p>
`,returnType:`
<p><code>torch.FloatTensor</code> of shape <code>(batch size, num classes, num latent pixels)</code></p>
`}}),Yn=new b({props:{name:"set_timesteps",anchor:"diffusers.VQDiffusionScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.VQDiffusionScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014;
device to place the timesteps and the diffusion process parameters (alpha, beta, gamma) on.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L188"}}),Jn=new b({props:{name:"step",anchor:"diffusers.VQDiffusionScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": torch.int64"},{name:"sample",val:": LongTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.step.t",description:`<strong>t</strong> (<code>torch.long</code>) &#x2014;
The timestep that determines which transition matrices are used.</p>
<p>x_t &#x2014; (<code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>):
The classes of each latent pixel at time <code>t</code></p>
<p>generator &#x2014; (<code>torch.Generator</code> or None):
RNG for the noise applied to p(x_{t-1} | x_t) before it is sampled from.`,name:"t"},{anchor:"diffusers.VQDiffusionScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014;
option for returning tuple rather than VQDiffusionSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L210",returnDescription:`
<p><code>~schedulers.scheduling_utils.VQDiffusionSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.VQDiffusionSchedulerOutput</code> or <code>tuple</code></p>
`}}),Zn=new w({}),ro=new b({props:{name:"class diffusers.RePaintScheduler",anchor:"diffusers.RePaintScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"eta",val:": float = 0.0"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.RePaintScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.RePaintScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.RePaintScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.RePaintScheduler.eta",description:`<strong>eta</strong> (<code>float</code>) &#x2014;
The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 -0.0 is DDIM and
1.0 is DDPM scheduler respectively.`,name:"eta"},{anchor:"diffusers.RePaintScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.RePaintScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.RePaintScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L74"}}),no=new b({props:{name:"scale_model_input",anchor:"diffusers.RePaintScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.RePaintScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L150",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),oo=new b({props:{name:"step",anchor:"diffusers.RePaintScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"original_image",val:": FloatTensor"},{name:"mask",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.step.model_output",description:`<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned
diffusion model.`,name:"model_output"},{anchor:"diffusers.RePaintScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.RePaintScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.RePaintScheduler.step.original_image",description:`<strong>original_image</strong> (<code>torch.FloatTensor</code>) &#x2014;
the original image to inpaint on.`,name:"original_image"},{anchor:"diffusers.RePaintScheduler.step.mask",description:`<strong>mask</strong> (<code>torch.FloatTensor</code>) &#x2014;
the mask where 0.0 values define which part of the original image to inpaint (change).`,name:"mask"},{anchor:"diffusers.RePaintScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014; random number generator.",name:"generator"},{anchor:"diffusers.RePaintScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than
DDPMSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L213",returnDescription:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> or <code>tuple</code></p>
`}}),{c(){Z=s("meta"),ut=l(),ee=s("h1"),De=s("a"),Ri=s("span"),p(Ir.$$.fragment),Kf=l(),Qi=s("span"),Rf=o("Schedulers"),nc=l(),lo=s("p"),Qf=o("Diffusers contains multiple pre-built schedule functions for the diffusion process."),oc=l(),qe=s("h2"),ft=s("a"),ji=s("span"),p(Nr.$$.fragment),jf=l(),Wi=s("span"),Wf=o("What is a scheduler?"),ic=l(),Ce=s("p"),Uf=o("The schedule functions, denoted "),Ui=s("em"),Bf=o("Schedulers"),Hf=o(" in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample. That\u2019s why schedulers may also be called "),Bi=s("em"),Gf=o("Samplers"),zf=o(" in other diffusion models implementations."),ac=l(),pt=s("ul"),co=s("li"),Yf=o("Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),Lr=s("ul"),Hi=s("li"),Jf=o("adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),Xf=l(),Gi=s("li"),Zf=o("for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),ep=l(),Ke=s("li"),tp=o("Schedulers are often defined by a "),zi=s("em"),rp=o("noise schedule"),sp=o(" and an "),Yi=s("em"),np=o("update rule"),op=o(" to solve the differential equation solution."),dc=l(),Re=s("h3"),ht=s("a"),Ji=s("span"),p(qr.$$.fragment),ip=l(),Xi=s("span"),ap=o("Discrete versus continuous schedulers"),lc=l(),O=s("p"),dp=o(`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),Zi=s("code"),lp=o("int"),cp=o(" inputs), such as the "),uo=s("a"),up=o("DDPMScheduler"),fp=o(" or "),fo=s("a"),pp=o("PNDMScheduler"),hp=o(", and continuous (accepting "),ea=s("code"),mp=o("float"),gp=o(" inputs), such as the score-based schedulers "),po=s("a"),_p=o("ScoreSdeVeScheduler"),vp=o(" or "),ta=s("code"),bp=o("ScoreSdeVpScheduler"),Sp=o("."),cc=l(),Qe=s("h2"),mt=s("a"),ra=s("span"),p(Kr.$$.fragment),Dp=l(),sa=s("span"),xp=o("Designing Re-usable schedulers"),uc=l(),ho=s("p"),$p=o(`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),fc=l(),gt=s("ul"),na=s("li"),Ep=o("Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),yp=l(),oa=s("li"),wp=o("Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),pc=l(),je=s("h2"),_t=s("a"),ia=s("span"),p(Rr.$$.fragment),Mp=l(),aa=s("span"),Pp=o("API"),hc=l(),mo=s("p"),Tp=o("The core API for any new scheduler must follow a limited structure."),mc=l(),ke=s("ul"),Qr=s("li"),Cp=o("Schedulers should provide one or more "),da=s("code"),kp=o("def step(...)"),Op=o(" functions that should be called to update the generated sample iteratively."),Ap=l(),jr=s("li"),Vp=o("Schedulers should provide a "),la=s("code"),Fp=o("set_timesteps(...)"),Ip=o(" method that configures the parameters of a schedule function for a specific inference task."),Np=l(),ca=s("li"),Lp=o("Schedulers should be framework-specific."),gc=l(),vt=s("p"),qp=o("The base class "),go=s("a"),Kp=o("SchedulerMixin"),Rp=o(" implements low level utilities used by multiple schedulers."),_c=l(),We=s("h3"),bt=s("a"),ua=s("span"),p(Wr.$$.fragment),Qp=l(),fa=s("span"),jp=o("SchedulerMixin"),vc=l(),Ue=s("div"),p(Ur.$$.fragment),Wp=l(),pa=s("p"),Up=o("Mixin containing common functions for the schedulers."),bc=l(),Be=s("h3"),St=s("a"),ha=s("span"),p(Br.$$.fragment),Bp=l(),ma=s("span"),Hp=o("SchedulerOutput"),Sc=o("\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),He=s("div"),p(Hr.$$.fragment),Gp=l(),ga=s("p"),zp=o("Base class for the scheduler\u2019s step function output."),Dc=l(),Ge=s("h3"),Dt=s("a"),_a=s("span"),p(Gr.$$.fragment),Yp=l(),va=s("span"),Jp=o("Implemented Schedulers"),xc=l(),ze=s("h4"),xt=s("a"),ba=s("span"),p(zr.$$.fragment),Xp=l(),Sa=s("span"),Zp=o("Denoising diffusion implicit models (DDIM)"),$c=l(),_o=s("p"),eh=o("Original paper can be found here."),Ec=l(),P=s("div"),p(Yr.$$.fragment),th=l(),Da=s("p"),rh=o(`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),sh=l(),A=s("p"),vo=s("a"),nh=o("~ConfigMixin"),oh=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),xa=s("code"),ih=o("__init__"),ah=o(`
function, such as `),$a=s("code"),dh=o("num_train_timesteps"),lh=o(". They can be accessed via "),Ea=s("code"),ch=o("scheduler.config.num_train_timesteps"),uh=o(`.
`),bo=s("a"),fh=o("~ConfigMixin"),ph=o(" also provides general loading and saving functionality via the "),So=s("a"),hh=o("save_config()"),mh=o(` and
`),Do=s("a"),gh=o("from_config()"),_h=o(" functions."),vh=l(),xo=s("p"),bh=o("For more details, see the original paper: "),Jr=s("a"),Sh=o("https://arxiv.org/abs/2010.02502"),Dh=l(),$t=s("div"),p(Xr.$$.fragment),xh=l(),ya=s("p"),$h=o(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Eh=l(),Et=s("div"),p(Zr.$$.fragment),yh=l(),wa=s("p"),wh=o("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Mh=l(),yt=s("div"),p(es.$$.fragment),Ph=l(),Ma=s("p"),Th=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),yc=l(),Ye=s("h4"),wt=s("a"),Pa=s("span"),p(ts.$$.fragment),Ch=l(),Ta=s("span"),kh=o("Denoising diffusion probabilistic models (DDPM)"),wc=l(),Mt=s("p"),Oh=o("Original paper can be found "),rs=s("a"),Ah=o("here"),Vh=o("."),Mc=l(),T=s("div"),p(ss.$$.fragment),Fh=l(),Ca=s("p"),Ih=o(`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),Nh=l(),V=s("p"),$o=s("a"),Lh=o("~ConfigMixin"),qh=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ka=s("code"),Kh=o("__init__"),Rh=o(`
function, such as `),Oa=s("code"),Qh=o("num_train_timesteps"),jh=o(". They can be accessed via "),Aa=s("code"),Wh=o("scheduler.config.num_train_timesteps"),Uh=o(`.
`),Eo=s("a"),Bh=o("~ConfigMixin"),Hh=o(" also provides general loading and saving functionality via the "),yo=s("a"),Gh=o("save_config()"),zh=o(` and
`),wo=s("a"),Yh=o("from_config()"),Jh=o(" functions."),Xh=l(),Mo=s("p"),Zh=o("For more details, see the original paper: "),ns=s("a"),em=o("https://arxiv.org/abs/2006.11239"),tm=l(),Pt=s("div"),p(os.$$.fragment),rm=l(),Va=s("p"),sm=o(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),nm=l(),Tt=s("div"),p(is.$$.fragment),om=l(),Fa=s("p"),im=o("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),am=l(),Ct=s("div"),p(as.$$.fragment),dm=l(),Ia=s("p"),lm=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Pc=l(),Je=s("h4"),kt=s("a"),Na=s("span"),p(ds.$$.fragment),cm=l(),La=s("span"),um=o("Multistep DPM-Solver"),Tc=l(),xe=s("p"),fm=o("Original paper can be found "),ls=s("a"),pm=o("here"),hm=o(" and the "),cs=s("a"),mm=o("improved version"),gm=o(". The original implementation can be found "),us=s("a"),_m=o("here"),vm=o("."),Cc=l(),S=s("div"),p(fs.$$.fragment),bm=l(),qa=s("p"),Sm=o(`DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with
the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality
samples, and it can generate quite good samples even in only 10 steps.`),Dm=l(),Ot=s("p"),xm=o("For more details, see the original paper: "),ps=s("a"),$m=o("https://arxiv.org/abs/2206.00927"),Em=o(" and "),hs=s("a"),ym=o("https://arxiv.org/abs/2211.01095"),wm=l(),Xe=s("p"),Mm=o(`Currently, we support the multistep DPM-Solver for both noise prediction models and data prediction models. We
recommend to use `),Ka=s("code"),Pm=o("solver_order=2"),Tm=o(" for guided sampling, and "),Ra=s("code"),Cm=o("solver_order=3"),km=o(" for unconditional sampling."),Om=l(),Me=s("p"),Am=o("We also support the \u201Cdynamic thresholding\u201D method in Imagen ("),ms=s("a"),Vm=o("https://arxiv.org/abs/2205.11487"),Fm=o(`). For pixel-space
diffusion models, you can set both `),Qa=s("code"),Im=o('algorithm_type="dpmsolver++"'),Nm=o(" and "),ja=s("code"),Lm=o("thresholding=True"),qm=o(` to use the dynamic
thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as
stable-diffusion).`),Km=l(),F=s("p"),Po=s("a"),Rm=o("~ConfigMixin"),Qm=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Wa=s("code"),jm=o("__init__"),Wm=o(`
function, such as `),Ua=s("code"),Um=o("num_train_timesteps"),Bm=o(". They can be accessed via "),Ba=s("code"),Hm=o("scheduler.config.num_train_timesteps"),Gm=o(`.
`),To=s("a"),zm=o("~ConfigMixin"),Ym=o(" also provides general loading and saving functionality via the "),Co=s("a"),Jm=o("save_config()"),Xm=o(` and
`),ko=s("a"),Zm=o("from_config()"),eg=o(" functions."),tg=l(),$e=s("div"),p(gs.$$.fragment),rg=l(),Ha=s("p"),sg=o("Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs."),ng=l(),Ga=s("p"),og=o(`DPM-Solver is designed to discretize an integral of the noise prediciton model, and DPM-Solver++ is designed to
discretize an integral of the data prediction model. So we need to first convert the model output to the
corresponding type to match the algorithm.`),ig=l(),za=s("p"),ag=o(`Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or
DPM-Solver++ for both noise prediction model and data prediction model.`),dg=l(),Oe=s("div"),p(_s.$$.fragment),lg=l(),Ya=s("p"),cg=o("One step for the first-order DPM-Solver (equivalent to DDIM)."),ug=l(),vs=s("p"),fg=o("See "),bs=s("a"),pg=o("https://arxiv.org/abs/2206.00927"),hg=o(" for the detailed derivation."),mg=l(),At=s("div"),p(Ss.$$.fragment),gg=l(),Ja=s("p"),_g=o("One step for the second-order multistep DPM-Solver."),vg=l(),Vt=s("div"),p(Ds.$$.fragment),bg=l(),Xa=s("p"),Sg=o("One step for the third-order multistep DPM-Solver."),Dg=l(),Ft=s("div"),p(xs.$$.fragment),xg=l(),Za=s("p"),$g=o(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Eg=l(),It=s("div"),p($s.$$.fragment),yg=l(),ed=s("p"),wg=o("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),Mg=l(),Nt=s("div"),p(Es.$$.fragment),Pg=l(),td=s("p"),Tg=o("Step function propagating the sample with the multistep DPM-Solver."),kc=l(),Ze=s("h4"),Lt=s("a"),rd=s("span"),p(ys.$$.fragment),Cg=l(),sd=s("span"),kg=o("Variance exploding, stochastic sampling from Karras et. al"),Oc=l(),qt=s("p"),Og=o("Original paper can be found "),ws=s("a"),Ag=o("here"),Vg=o("."),Ac=l(),x=s("div"),p(Ms.$$.fragment),Fg=l(),nd=s("p"),Ig=o(`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),Ng=l(),Kt=s("p"),Lg=o(`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Ps=s("a"),qg=o("https://arxiv.org/abs/2206.00364"),Kg=o(` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Ts=s("a"),Rg=o("https://arxiv.org/abs/2011.13456"),Qg=l(),I=s("p"),Oo=s("a"),jg=o("~ConfigMixin"),Wg=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),od=s("code"),Ug=o("__init__"),Bg=o(`
function, such as `),id=s("code"),Hg=o("num_train_timesteps"),Gg=o(". They can be accessed via "),ad=s("code"),zg=o("scheduler.config.num_train_timesteps"),Yg=o(`.
`),Ao=s("a"),Jg=o("~ConfigMixin"),Xg=o(" also provides general loading and saving functionality via the "),Vo=s("a"),Zg=o("save_config()"),e_=o(` and
`),Fo=s("a"),t_=o("from_config()"),r_=o(" functions."),s_=l(),Cs=s("p"),n_=o(`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),ks=s("a"),o_=o("https://arxiv.org/abs/2206.00364"),i_=o(`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),a_=l(),Ae=s("div"),p(Os.$$.fragment),d_=l(),dd=s("p"),l_=o(`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),c_=l(),ld=s("p"),u_=o("TODO Args:"),f_=l(),Rt=s("div"),p(As.$$.fragment),p_=l(),cd=s("p"),h_=o(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),m_=l(),Qt=s("div"),p(Vs.$$.fragment),g_=l(),ud=s("p"),__=o("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),v_=l(),jt=s("div"),p(Fs.$$.fragment),b_=l(),fd=s("p"),S_=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),D_=l(),Wt=s("div"),p(Is.$$.fragment),x_=l(),pd=s("p"),$_=o("Correct the predicted sample based on the output model_output of the network. TODO complete description"),Vc=l(),et=s("h4"),Ut=s("a"),hd=s("span"),p(Ns.$$.fragment),E_=l(),md=s("span"),y_=o("Linear multistep scheduler for discrete beta schedules"),Fc=l(),Bt=s("p"),w_=o("Original implementation can be found "),Ls=s("a"),M_=o("here"),P_=o("."),Ic=l(),C=s("div"),p(qs.$$.fragment),T_=l(),Io=s("p"),C_=o(`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),Ks=s("a"),k_=o("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),O_=l(),N=s("p"),No=s("a"),A_=o("~ConfigMixin"),V_=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),gd=s("code"),F_=o("__init__"),I_=o(`
function, such as `),_d=s("code"),N_=o("num_train_timesteps"),L_=o(". They can be accessed via "),vd=s("code"),q_=o("scheduler.config.num_train_timesteps"),K_=o(`.
`),Lo=s("a"),R_=o("~ConfigMixin"),Q_=o(" also provides general loading and saving functionality via the "),qo=s("a"),j_=o("save_config()"),W_=o(` and
`),Ko=s("a"),U_=o("from_config()"),B_=o(" functions."),H_=l(),Ht=s("div"),p(Rs.$$.fragment),G_=l(),bd=s("p"),z_=o("Compute a linear multistep coefficient."),Y_=l(),Gt=s("div"),p(Qs.$$.fragment),J_=l(),js=s("p"),X_=o("Scales the denoising model input by "),Sd=s("code"),Z_=o("(sigma**2 + 1) ** 0.5"),ev=o(" to match the K-LMS algorithm."),tv=l(),zt=s("div"),p(Ws.$$.fragment),rv=l(),Dd=s("p"),sv=o("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),nv=l(),Yt=s("div"),p(Us.$$.fragment),ov=l(),xd=s("p"),iv=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Nc=l(),tt=s("h4"),Jt=s("a"),$d=s("span"),p(Bs.$$.fragment),av=l(),Ed=s("span"),dv=o("Pseudo numerical methods for diffusion models (PNDM)"),Lc=l(),Xt=s("p"),lv=o("Original implementation can be found "),Hs=s("a"),cv=o("here"),uv=o("."),qc=l(),$=s("div"),p(Gs.$$.fragment),fv=l(),yd=s("p"),pv=o(`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),hv=l(),L=s("p"),Ro=s("a"),mv=o("~ConfigMixin"),gv=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),wd=s("code"),_v=o("__init__"),vv=o(`
function, such as `),Md=s("code"),bv=o("num_train_timesteps"),Sv=o(". They can be accessed via "),Pd=s("code"),Dv=o("scheduler.config.num_train_timesteps"),xv=o(`.
`),Qo=s("a"),$v=o("~ConfigMixin"),Ev=o(" also provides general loading and saving functionality via the "),jo=s("a"),yv=o("save_config()"),wv=o(` and
`),Wo=s("a"),Mv=o("from_config()"),Pv=o(" functions."),Tv=l(),Uo=s("p"),Cv=o("For more details, see the original paper: "),zs=s("a"),kv=o("https://arxiv.org/abs/2202.09778"),Ov=l(),Zt=s("div"),p(Ys.$$.fragment),Av=l(),Td=s("p"),Vv=o(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Fv=l(),er=s("div"),p(Js.$$.fragment),Iv=l(),Cd=s("p"),Nv=o("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Lv=l(),Ve=s("div"),p(Xs.$$.fragment),qv=l(),kd=s("p"),Kv=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Rv=l(),Pe=s("p"),Qv=o("This function calls "),Od=s("code"),jv=o("step_prk()"),Wv=o(" or "),Ad=s("code"),Uv=o("step_plms()"),Bv=o(" depending on the internal variable "),Vd=s("code"),Hv=o("counter"),Gv=o("."),zv=l(),tr=s("div"),p(Zs.$$.fragment),Yv=l(),Fd=s("p"),Jv=o(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),Xv=l(),rr=s("div"),p(en.$$.fragment),Zv=l(),Id=s("p"),eb=o(`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),Kc=l(),rt=s("h4"),sr=s("a"),Nd=s("span"),p(tn.$$.fragment),tb=l(),Ld=s("span"),rb=o("variance exploding stochastic differential equation (VE-SDE) scheduler"),Rc=l(),nr=s("p"),sb=o("Original paper can be found "),rn=s("a"),nb=o("here"),ob=o("."),Qc=l(),E=s("div"),p(sn.$$.fragment),ib=l(),qd=s("p"),ab=o("The variance exploding stochastic differential equation (SDE) scheduler."),db=l(),Bo=s("p"),lb=o("For more information, see the original paper: "),nn=s("a"),cb=o("https://arxiv.org/abs/2011.13456"),ub=l(),q=s("p"),Ho=s("a"),fb=o("~ConfigMixin"),pb=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Kd=s("code"),hb=o("__init__"),mb=o(`
function, such as `),Rd=s("code"),gb=o("num_train_timesteps"),_b=o(". They can be accessed via "),Qd=s("code"),vb=o("scheduler.config.num_train_timesteps"),bb=o(`.
`),Go=s("a"),Sb=o("~ConfigMixin"),Db=o(" also provides general loading and saving functionality via the "),zo=s("a"),xb=o("save_config()"),$b=o(` and
`),Yo=s("a"),Eb=o("from_config()"),yb=o(" functions."),wb=l(),or=s("div"),p(on.$$.fragment),Mb=l(),jd=s("p"),Pb=o(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Tb=l(),Fe=s("div"),p(an.$$.fragment),Cb=l(),Wd=s("p"),kb=o("Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),Ob=l(),st=s("p"),Ab=o("The sigmas control the weight of the "),Ud=s("code"),Vb=o("drift"),Fb=o(" and "),Bd=s("code"),Ib=o("diffusion"),Nb=o(" components of sample update."),Lb=l(),ir=s("div"),p(dn.$$.fragment),qb=l(),Hd=s("p"),Kb=o("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),Rb=l(),ar=s("div"),p(ln.$$.fragment),Qb=l(),Gd=s("p"),jb=o(`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),Wb=l(),dr=s("div"),p(cn.$$.fragment),Ub=l(),zd=s("p"),Bb=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),jc=l(),nt=s("h4"),lr=s("a"),Yd=s("span"),p(un.$$.fragment),Hb=l(),Jd=s("span"),Gb=o("improved pseudo numerical methods for diffusion models (iPNDM)"),Wc=l(),cr=s("p"),zb=o("Original implementation can be found "),fn=s("a"),Yb=o("here"),Jb=o("."),Uc=l(),k=s("div"),p(pn.$$.fragment),Xb=l(),Jo=s("p"),Zb=o(`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),hn=s("a"),e1=o("library"),t1=l(),K=s("p"),Xo=s("a"),r1=o("~ConfigMixin"),s1=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Xd=s("code"),n1=o("__init__"),o1=o(`
function, such as `),Zd=s("code"),i1=o("num_train_timesteps"),a1=o(". They can be accessed via "),el=s("code"),d1=o("scheduler.config.num_train_timesteps"),l1=o(`.
`),Zo=s("a"),c1=o("~ConfigMixin"),u1=o(" also provides general loading and saving functionality via the "),ei=s("a"),f1=o("save_config()"),p1=o(` and
`),ti=s("a"),h1=o("from_config()"),m1=o(" functions."),g1=l(),ri=s("p"),_1=o("For more details, see the original paper: "),mn=s("a"),v1=o("https://arxiv.org/abs/2202.09778"),b1=l(),ur=s("div"),p(gn.$$.fragment),S1=l(),tl=s("p"),D1=o(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),x1=l(),fr=s("div"),p(_n.$$.fragment),$1=l(),rl=s("p"),E1=o("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),y1=l(),pr=s("div"),p(vn.$$.fragment),w1=l(),sl=s("p"),M1=o(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),Bc=l(),ot=s("h4"),hr=s("a"),nl=s("span"),p(bn.$$.fragment),P1=l(),ol=s("span"),T1=o("variance preserving stochastic differential equation (VP-SDE) scheduler"),Hc=l(),mr=s("p"),C1=o("Original paper can be found "),Sn=s("a"),k1=o("here"),O1=o("."),Gc=l(),p(gr.$$.fragment),zc=l(),te=s("div"),p(Dn.$$.fragment),A1=l(),il=s("p"),V1=o("The variance preserving stochastic differential equation (SDE) scheduler."),F1=l(),R=s("p"),si=s("a"),I1=o("~ConfigMixin"),N1=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),al=s("code"),L1=o("__init__"),q1=o(`
function, such as `),dl=s("code"),K1=o("num_train_timesteps"),R1=o(". They can be accessed via "),ll=s("code"),Q1=o("scheduler.config.num_train_timesteps"),j1=o(`.
`),ni=s("a"),W1=o("~ConfigMixin"),U1=o(" also provides general loading and saving functionality via the "),oi=s("a"),B1=o("save_config()"),H1=o(` and
`),ii=s("a"),G1=o("from_config()"),z1=o(" functions."),Y1=l(),ai=s("p"),J1=o("For more information, see the original paper: "),xn=s("a"),X1=o("https://arxiv.org/abs/2011.13456"),Z1=l(),cl=s("p"),e0=o("UNDER CONSTRUCTION"),Yc=l(),it=s("h4"),_r=s("a"),ul=s("span"),p($n.$$.fragment),t0=l(),fl=s("span"),r0=o("Euler scheduler"),Jc=l(),Ie=s("p"),s0=o("Euler scheduler (Algorithm 2) from the paper "),En=s("a"),n0=o("Elucidating the Design Space of Diffusion-Based Generative Models"),o0=o(" by Karras et al. (2022). Based on the original "),yn=s("a"),i0=o("k-diffusion"),a0=o(` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Xc=l(),Y=s("div"),p(wn.$$.fragment),d0=l(),vr=s("p"),l0=o("Euler scheduler (Algorithm 2) from Karras et al. (2022) "),Mn=s("a"),c0=o("https://arxiv.org/abs/2206.00364"),u0=o(`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),Pn=s("a"),f0=o("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),p0=l(),Q=s("p"),di=s("a"),h0=o("~ConfigMixin"),m0=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),pl=s("code"),g0=o("__init__"),_0=o(`
function, such as `),hl=s("code"),v0=o("num_train_timesteps"),b0=o(". They can be accessed via "),ml=s("code"),S0=o("scheduler.config.num_train_timesteps"),D0=o(`.
`),li=s("a"),x0=o("~ConfigMixin"),$0=o(" also provides general loading and saving functionality via the "),ci=s("a"),E0=o("save_config()"),y0=o(` and
`),ui=s("a"),w0=o("from_config()"),M0=o(" functions."),P0=l(),br=s("div"),p(Tn.$$.fragment),T0=l(),Cn=s("p"),C0=o("Scales the denoising model input by "),gl=s("code"),k0=o("(sigma**2 + 1) ** 0.5"),O0=o(" to match the Euler algorithm."),A0=l(),Sr=s("div"),p(kn.$$.fragment),V0=l(),_l=s("p"),F0=o("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),I0=l(),Dr=s("div"),p(On.$$.fragment),N0=l(),vl=s("p"),L0=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Zc=l(),at=s("h4"),xr=s("a"),bl=s("span"),p(An.$$.fragment),q0=l(),Sl=s("span"),K0=o("Euler Ancestral scheduler"),eu=l(),fi=s("p"),R0=o(`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),tu=l(),J=s("div"),p(Vn.$$.fragment),Q0=l(),pi=s("p"),j0=o(`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Fn=s("a"),W0=o("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),U0=l(),j=s("p"),hi=s("a"),B0=o("~ConfigMixin"),H0=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Dl=s("code"),G0=o("__init__"),z0=o(`
function, such as `),xl=s("code"),Y0=o("num_train_timesteps"),J0=o(". They can be accessed via "),$l=s("code"),X0=o("scheduler.config.num_train_timesteps"),Z0=o(`.
`),mi=s("a"),e2=o("~ConfigMixin"),t2=o(" also provides general loading and saving functionality via the "),gi=s("a"),r2=o("save_config()"),s2=o(` and
`),_i=s("a"),n2=o("from_config()"),o2=o(" functions."),i2=l(),$r=s("div"),p(In.$$.fragment),a2=l(),Nn=s("p"),d2=o("Scales the denoising model input by "),El=s("code"),l2=o("(sigma**2 + 1) ** 0.5"),c2=o(" to match the Euler algorithm."),u2=l(),Er=s("div"),p(Ln.$$.fragment),f2=l(),yl=s("p"),p2=o("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),h2=l(),yr=s("div"),p(qn.$$.fragment),m2=l(),wl=s("p"),g2=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),ru=l(),dt=s("h4"),wr=s("a"),Ml=s("span"),p(Kn.$$.fragment),_2=l(),Pl=s("span"),v2=o("VQDiffusionScheduler"),su=l(),Rn=s("p"),b2=o("Original paper can be found "),Qn=s("a"),S2=o("here"),nu=l(),y=s("div"),p(jn.$$.fragment),D2=l(),Tl=s("p"),x2=o("The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image."),$2=l(),Cl=s("p"),E2=o(`The VQ-diffusion scheduler converts the transformer\u2019s output into a sample for the unnoised image at the previous
diffusion timestep.`),y2=l(),W=s("p"),vi=s("a"),w2=o("~ConfigMixin"),M2=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),kl=s("code"),P2=o("__init__"),T2=o(`
function, such as `),Ol=s("code"),C2=o("num_train_timesteps"),k2=o(". They can be accessed via "),Al=s("code"),O2=o("scheduler.config.num_train_timesteps"),A2=o(`.
`),bi=s("a"),V2=o("~ConfigMixin"),F2=o(" also provides general loading and saving functionality via the "),Si=s("a"),I2=o("save_config()"),N2=o(` and
`),Di=s("a"),L2=o("from_config()"),q2=o(" functions."),K2=l(),xi=s("p"),R2=o("For more details, see the original paper: "),Wn=s("a"),Q2=o("https://arxiv.org/abs/2111.14822"),j2=l(),Ne=s("div"),p(Un.$$.fragment),W2=l(),Bn=s("p"),U2=o(`Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each
latent pixel in `),Vl=s("code"),B2=o("x_t"),H2=o("."),G2=l(),Fl=s("p"),z2=o(`See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix
is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.`),Y2=l(),U=s("div"),p(Hn.$$.fragment),J2=l(),Gn=s("p"),X2=o("Calculates the log probabilities for the predicted classes of the image at timestep "),Il=s("code"),Z2=o("t-1"),e4=o(". I.e. Equation (11)."),t4=l(),Nl=s("p"),r4=o(`Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only
forward probabilities.`),s4=l(),Ll=s("p"),n4=o("Equation (11) stated in terms of forward probabilities via Equation (5):"),o4=l(),ql=s("p"),i4=o("Where:"),a4=l(),Kl=s("ul"),zn=s("li"),d4=o("the sum is over x"),Rl=s("em"),l4=o("0 = {C_0 \u2026 C"),c4=o("{k-1}} (classes for x_0)"),u4=l(),lt=s("p"),f4=o("p(x"),Ql=s("em"),p4=o("{t-1} | x_t) = sum( q(x_t | x"),h4=o("{t-1}) "),jl=s("em"),m4=o("q(x_{t-1} | x_0)"),g4=o(" p(x_0) / q(x_t | x_0) )"),_4=l(),Mr=s("div"),p(Yn.$$.fragment),v4=l(),Wl=s("p"),b4=o("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),S4=l(),Pr=s("div"),p(Jn.$$.fragment),D4=l(),Xn=s("p"),x4=o(`Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the
docstring for `),Ul=s("code"),$4=o("self.q_posterior"),E4=o(" for more in depth docs on how Equation (11) is computed."),ou=l(),ct=s("h4"),Tr=s("a"),Bl=s("span"),p(Zn.$$.fragment),y4=l(),Hl=s("span"),w4=o("RePaint scheduler"),iu=l(),Te=s("p"),M4=o(`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),$i=s("a"),P4=o("RePaintPipeline"),T4=o(`.
Based on the paper `),eo=s("a"),C4=o("RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),k4=o(`
and the original implementation by Andreas Lugmayr et al.: `),to=s("a"),O4=o("https://github.com/andreas128/RePaint"),au=l(),X=s("div"),p(ro.$$.fragment),A4=l(),Gl=s("p"),V4=o("RePaint is a schedule for DDPM inpainting inside a given mask."),F4=l(),B=s("p"),Ei=s("a"),I4=o("~ConfigMixin"),N4=o(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),zl=s("code"),L4=o("__init__"),q4=o(`
function, such as `),Yl=s("code"),K4=o("num_train_timesteps"),R4=o(". They can be accessed via "),Jl=s("code"),Q4=o("scheduler.config.num_train_timesteps"),j4=o(`.
`),yi=s("a"),W4=o("~ConfigMixin"),U4=o(" also provides general loading and saving functionality via the "),wi=s("a"),B4=o("save_config()"),H4=o(` and
`),Mi=s("a"),G4=o("from_config()"),z4=o(" functions."),Y4=l(),Pi=s("p"),J4=o("For more details, see the original paper: "),so=s("a"),X4=o("https://arxiv.org/pdf/2201.09865.pdf"),Z4=l(),Cr=s("div"),p(no.$$.fragment),eS=l(),Xl=s("p"),tS=o(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),rS=l(),kr=s("div"),p(oo.$$.fragment),sS=l(),Zl=s("p"),nS=o(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),this.h()},l(r){const u=W5('[data-svelte="svelte-1phssyn"]',document.head);Z=n(u,"META",{name:!0,content:!0}),u.forEach(t),ut=c(r),ee=n(r,"H1",{class:!0});var io=i(ee);De=n(io,"A",{id:!0,class:!0,href:!0});var vS=i(De);Ri=n(vS,"SPAN",{});var bS=i(Ri);h(Ir.$$.fragment,bS),bS.forEach(t),vS.forEach(t),Kf=c(io),Qi=n(io,"SPAN",{});var SS=i(Qi);Rf=a(SS,"Schedulers"),SS.forEach(t),io.forEach(t),nc=c(r),lo=n(r,"P",{});var DS=i(lo);Qf=a(DS,"Diffusers contains multiple pre-built schedule functions for the diffusion process."),DS.forEach(t),oc=c(r),qe=n(r,"H2",{class:!0});var lu=i(qe);ft=n(lu,"A",{id:!0,class:!0,href:!0});var xS=i(ft);ji=n(xS,"SPAN",{});var $S=i(ji);h(Nr.$$.fragment,$S),$S.forEach(t),xS.forEach(t),jf=c(lu),Wi=n(lu,"SPAN",{});var ES=i(Wi);Wf=a(ES,"What is a scheduler?"),ES.forEach(t),lu.forEach(t),ic=c(r),Ce=n(r,"P",{});var Ti=i(Ce);Uf=a(Ti,"The schedule functions, denoted "),Ui=n(Ti,"EM",{});var yS=i(Ui);Bf=a(yS,"Schedulers"),yS.forEach(t),Hf=a(Ti," in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample. That\u2019s why schedulers may also be called "),Bi=n(Ti,"EM",{});var wS=i(Bi);Gf=a(wS,"Samplers"),wS.forEach(t),zf=a(Ti," in other diffusion models implementations."),Ti.forEach(t),ac=c(r),pt=n(r,"UL",{});var cu=i(pt);co=n(cu,"LI",{});var oS=i(co);Yf=a(oS,"Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),Lr=n(oS,"UL",{});var uu=i(Lr);Hi=n(uu,"LI",{});var MS=i(Hi);Jf=a(MS,"adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),MS.forEach(t),Xf=c(uu),Gi=n(uu,"LI",{});var PS=i(Gi);Zf=a(PS,"for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),PS.forEach(t),uu.forEach(t),oS.forEach(t),ep=c(cu),Ke=n(cu,"LI",{});var Ci=i(Ke);tp=a(Ci,"Schedulers are often defined by a "),zi=n(Ci,"EM",{});var TS=i(zi);rp=a(TS,"noise schedule"),TS.forEach(t),sp=a(Ci," and an "),Yi=n(Ci,"EM",{});var CS=i(Yi);np=a(CS,"update rule"),CS.forEach(t),op=a(Ci," to solve the differential equation solution."),Ci.forEach(t),cu.forEach(t),dc=c(r),Re=n(r,"H3",{class:!0});var fu=i(Re);ht=n(fu,"A",{id:!0,class:!0,href:!0});var kS=i(ht);Ji=n(kS,"SPAN",{});var OS=i(Ji);h(qr.$$.fragment,OS),OS.forEach(t),kS.forEach(t),ip=c(fu),Xi=n(fu,"SPAN",{});var AS=i(Xi);ap=a(AS,"Discrete versus continuous schedulers"),AS.forEach(t),fu.forEach(t),lc=c(r),O=n(r,"P",{});var me=i(O);dp=a(me,`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),Zi=n(me,"CODE",{});var VS=i(Zi);lp=a(VS,"int"),VS.forEach(t),cp=a(me," inputs), such as the "),uo=n(me,"A",{href:!0});var FS=i(uo);up=a(FS,"DDPMScheduler"),FS.forEach(t),fp=a(me," or "),fo=n(me,"A",{href:!0});var IS=i(fo);pp=a(IS,"PNDMScheduler"),IS.forEach(t),hp=a(me,", and continuous (accepting "),ea=n(me,"CODE",{});var NS=i(ea);mp=a(NS,"float"),NS.forEach(t),gp=a(me," inputs), such as the score-based schedulers "),po=n(me,"A",{href:!0});var LS=i(po);_p=a(LS,"ScoreSdeVeScheduler"),LS.forEach(t),vp=a(me," or "),ta=n(me,"CODE",{});var qS=i(ta);bp=a(qS,"ScoreSdeVpScheduler"),qS.forEach(t),Sp=a(me,"."),me.forEach(t),cc=c(r),Qe=n(r,"H2",{class:!0});var pu=i(Qe);mt=n(pu,"A",{id:!0,class:!0,href:!0});var KS=i(mt);ra=n(KS,"SPAN",{});var RS=i(ra);h(Kr.$$.fragment,RS),RS.forEach(t),KS.forEach(t),Dp=c(pu),sa=n(pu,"SPAN",{});var QS=i(sa);xp=a(QS,"Designing Re-usable schedulers"),QS.forEach(t),pu.forEach(t),uc=c(r),ho=n(r,"P",{});var jS=i(ho);$p=a(jS,`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),jS.forEach(t),fc=c(r),gt=n(r,"UL",{});var hu=i(gt);na=n(hu,"LI",{});var WS=i(na);Ep=a(WS,"Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),WS.forEach(t),yp=c(hu),oa=n(hu,"LI",{});var US=i(oa);wp=a(US,"Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),US.forEach(t),hu.forEach(t),pc=c(r),je=n(r,"H2",{class:!0});var mu=i(je);_t=n(mu,"A",{id:!0,class:!0,href:!0});var BS=i(_t);ia=n(BS,"SPAN",{});var HS=i(ia);h(Rr.$$.fragment,HS),HS.forEach(t),BS.forEach(t),Mp=c(mu),aa=n(mu,"SPAN",{});var GS=i(aa);Pp=a(GS,"API"),GS.forEach(t),mu.forEach(t),hc=c(r),mo=n(r,"P",{});var zS=i(mo);Tp=a(zS,"The core API for any new scheduler must follow a limited structure."),zS.forEach(t),mc=c(r),ke=n(r,"UL",{});var ki=i(ke);Qr=n(ki,"LI",{});var gu=i(Qr);Cp=a(gu,"Schedulers should provide one or more "),da=n(gu,"CODE",{});var YS=i(da);kp=a(YS,"def step(...)"),YS.forEach(t),Op=a(gu," functions that should be called to update the generated sample iteratively."),gu.forEach(t),Ap=c(ki),jr=n(ki,"LI",{});var _u=i(jr);Vp=a(_u,"Schedulers should provide a "),la=n(_u,"CODE",{});var JS=i(la);Fp=a(JS,"set_timesteps(...)"),JS.forEach(t),Ip=a(_u," method that configures the parameters of a schedule function for a specific inference task."),_u.forEach(t),Np=c(ki),ca=n(ki,"LI",{});var XS=i(ca);Lp=a(XS,"Schedulers should be framework-specific."),XS.forEach(t),ki.forEach(t),gc=c(r),vt=n(r,"P",{});var vu=i(vt);qp=a(vu,"The base class "),go=n(vu,"A",{href:!0});var ZS=i(go);Kp=a(ZS,"SchedulerMixin"),ZS.forEach(t),Rp=a(vu," implements low level utilities used by multiple schedulers."),vu.forEach(t),_c=c(r),We=n(r,"H3",{class:!0});var bu=i(We);bt=n(bu,"A",{id:!0,class:!0,href:!0});var eD=i(bt);ua=n(eD,"SPAN",{});var tD=i(ua);h(Wr.$$.fragment,tD),tD.forEach(t),eD.forEach(t),Qp=c(bu),fa=n(bu,"SPAN",{});var rD=i(fa);jp=a(rD,"SchedulerMixin"),rD.forEach(t),bu.forEach(t),vc=c(r),Ue=n(r,"DIV",{class:!0});var Su=i(Ue);h(Ur.$$.fragment,Su),Wp=c(Su),pa=n(Su,"P",{});var sD=i(pa);Up=a(sD,"Mixin containing common functions for the schedulers."),sD.forEach(t),Su.forEach(t),bc=c(r),Be=n(r,"H3",{class:!0});var Du=i(Be);St=n(Du,"A",{id:!0,class:!0,href:!0});var nD=i(St);ha=n(nD,"SPAN",{});var oD=i(ha);h(Br.$$.fragment,oD),oD.forEach(t),nD.forEach(t),Bp=c(Du),ma=n(Du,"SPAN",{});var iD=i(ma);Hp=a(iD,"SchedulerOutput"),iD.forEach(t),Du.forEach(t),Sc=a(r,"\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),He=n(r,"DIV",{class:!0});var xu=i(He);h(Hr.$$.fragment,xu),Gp=c(xu),ga=n(xu,"P",{});var aD=i(ga);zp=a(aD,"Base class for the scheduler\u2019s step function output."),aD.forEach(t),xu.forEach(t),Dc=c(r),Ge=n(r,"H3",{class:!0});var $u=i(Ge);Dt=n($u,"A",{id:!0,class:!0,href:!0});var dD=i(Dt);_a=n(dD,"SPAN",{});var lD=i(_a);h(Gr.$$.fragment,lD),lD.forEach(t),dD.forEach(t),Yp=c($u),va=n($u,"SPAN",{});var cD=i(va);Jp=a(cD,"Implemented Schedulers"),cD.forEach(t),$u.forEach(t),xc=c(r),ze=n(r,"H4",{class:!0});var Eu=i(ze);xt=n(Eu,"A",{id:!0,class:!0,href:!0});var uD=i(xt);ba=n(uD,"SPAN",{});var fD=i(ba);h(zr.$$.fragment,fD),fD.forEach(t),uD.forEach(t),Xp=c(Eu),Sa=n(Eu,"SPAN",{});var pD=i(Sa);Zp=a(pD,"Denoising diffusion implicit models (DDIM)"),pD.forEach(t),Eu.forEach(t),$c=c(r),_o=n(r,"P",{});var hD=i(_o);eh=a(hD,"Original paper can be found here."),hD.forEach(t),Ec=c(r),P=n(r,"DIV",{class:!0});var ge=i(P);h(Yr.$$.fragment,ge),th=c(ge),Da=n(ge,"P",{});var mD=i(Da);rh=a(mD,`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),mD.forEach(t),sh=c(ge),A=n(ge,"P",{});var re=i(A);vo=n(re,"A",{href:!0});var gD=i(vo);nh=a(gD,"~ConfigMixin"),gD.forEach(t),oh=a(re," takes care of storing all config attributes that are passed in the scheduler\u2019s "),xa=n(re,"CODE",{});var _D=i(xa);ih=a(_D,"__init__"),_D.forEach(t),ah=a(re,`
function, such as `),$a=n(re,"CODE",{});var vD=i($a);dh=a(vD,"num_train_timesteps"),vD.forEach(t),lh=a(re,". They can be accessed via "),Ea=n(re,"CODE",{});var bD=i(Ea);ch=a(bD,"scheduler.config.num_train_timesteps"),bD.forEach(t),uh=a(re,`.
`),bo=n(re,"A",{href:!0});var SD=i(bo);fh=a(SD,"~ConfigMixin"),SD.forEach(t),ph=a(re," also provides general loading and saving functionality via the "),So=n(re,"A",{href:!0});var DD=i(So);hh=a(DD,"save_config()"),DD.forEach(t),mh=a(re,` and
`),Do=n(re,"A",{href:!0});var xD=i(Do);gh=a(xD,"from_config()"),xD.forEach(t),_h=a(re," functions."),re.forEach(t),vh=c(ge),xo=n(ge,"P",{});var iS=i(xo);bh=a(iS,"For more details, see the original paper: "),Jr=n(iS,"A",{href:!0,rel:!0});var $D=i(Jr);Sh=a($D,"https://arxiv.org/abs/2010.02502"),$D.forEach(t),iS.forEach(t),Dh=c(ge),$t=n(ge,"DIV",{class:!0});var yu=i($t);h(Xr.$$.fragment,yu),xh=c(yu),ya=n(yu,"P",{});var ED=i(ya);$h=a(ED,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),ED.forEach(t),yu.forEach(t),Eh=c(ge),Et=n(ge,"DIV",{class:!0});var wu=i(Et);h(Zr.$$.fragment,wu),yh=c(wu),wa=n(wu,"P",{});var yD=i(wa);wh=a(yD,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),yD.forEach(t),wu.forEach(t),Mh=c(ge),yt=n(ge,"DIV",{class:!0});var Mu=i(yt);h(es.$$.fragment,Mu),Ph=c(Mu),Ma=n(Mu,"P",{});var wD=i(Ma);Th=a(wD,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),wD.forEach(t),Mu.forEach(t),ge.forEach(t),yc=c(r),Ye=n(r,"H4",{class:!0});var Pu=i(Ye);wt=n(Pu,"A",{id:!0,class:!0,href:!0});var MD=i(wt);Pa=n(MD,"SPAN",{});var PD=i(Pa);h(ts.$$.fragment,PD),PD.forEach(t),MD.forEach(t),Ch=c(Pu),Ta=n(Pu,"SPAN",{});var TD=i(Ta);kh=a(TD,"Denoising diffusion probabilistic models (DDPM)"),TD.forEach(t),Pu.forEach(t),wc=c(r),Mt=n(r,"P",{});var Tu=i(Mt);Oh=a(Tu,"Original paper can be found "),rs=n(Tu,"A",{href:!0,rel:!0});var CD=i(rs);Ah=a(CD,"here"),CD.forEach(t),Vh=a(Tu,"."),Tu.forEach(t),Mc=c(r),T=n(r,"DIV",{class:!0});var _e=i(T);h(ss.$$.fragment,_e),Fh=c(_e),Ca=n(_e,"P",{});var kD=i(Ca);Ih=a(kD,`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),kD.forEach(t),Nh=c(_e),V=n(_e,"P",{});var se=i(V);$o=n(se,"A",{href:!0});var OD=i($o);Lh=a(OD,"~ConfigMixin"),OD.forEach(t),qh=a(se," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ka=n(se,"CODE",{});var AD=i(ka);Kh=a(AD,"__init__"),AD.forEach(t),Rh=a(se,`
function, such as `),Oa=n(se,"CODE",{});var VD=i(Oa);Qh=a(VD,"num_train_timesteps"),VD.forEach(t),jh=a(se,". They can be accessed via "),Aa=n(se,"CODE",{});var FD=i(Aa);Wh=a(FD,"scheduler.config.num_train_timesteps"),FD.forEach(t),Uh=a(se,`.
`),Eo=n(se,"A",{href:!0});var ID=i(Eo);Bh=a(ID,"~ConfigMixin"),ID.forEach(t),Hh=a(se," also provides general loading and saving functionality via the "),yo=n(se,"A",{href:!0});var ND=i(yo);Gh=a(ND,"save_config()"),ND.forEach(t),zh=a(se,` and
`),wo=n(se,"A",{href:!0});var LD=i(wo);Yh=a(LD,"from_config()"),LD.forEach(t),Jh=a(se," functions."),se.forEach(t),Xh=c(_e),Mo=n(_e,"P",{});var aS=i(Mo);Zh=a(aS,"For more details, see the original paper: "),ns=n(aS,"A",{href:!0,rel:!0});var qD=i(ns);em=a(qD,"https://arxiv.org/abs/2006.11239"),qD.forEach(t),aS.forEach(t),tm=c(_e),Pt=n(_e,"DIV",{class:!0});var Cu=i(Pt);h(os.$$.fragment,Cu),rm=c(Cu),Va=n(Cu,"P",{});var KD=i(Va);sm=a(KD,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),KD.forEach(t),Cu.forEach(t),nm=c(_e),Tt=n(_e,"DIV",{class:!0});var ku=i(Tt);h(is.$$.fragment,ku),om=c(ku),Fa=n(ku,"P",{});var RD=i(Fa);im=a(RD,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),RD.forEach(t),ku.forEach(t),am=c(_e),Ct=n(_e,"DIV",{class:!0});var Ou=i(Ct);h(as.$$.fragment,Ou),dm=c(Ou),Ia=n(Ou,"P",{});var QD=i(Ia);lm=a(QD,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),QD.forEach(t),Ou.forEach(t),_e.forEach(t),Pc=c(r),Je=n(r,"H4",{class:!0});var Au=i(Je);kt=n(Au,"A",{id:!0,class:!0,href:!0});var jD=i(kt);Na=n(jD,"SPAN",{});var WD=i(Na);h(ds.$$.fragment,WD),WD.forEach(t),jD.forEach(t),cm=c(Au),La=n(Au,"SPAN",{});var UD=i(La);um=a(UD,"Multistep DPM-Solver"),UD.forEach(t),Au.forEach(t),Tc=c(r),xe=n(r,"P",{});var Or=i(xe);fm=a(Or,"Original paper can be found "),ls=n(Or,"A",{href:!0,rel:!0});var BD=i(ls);pm=a(BD,"here"),BD.forEach(t),hm=a(Or," and the "),cs=n(Or,"A",{href:!0,rel:!0});var HD=i(cs);mm=a(HD,"improved version"),HD.forEach(t),gm=a(Or,". The original implementation can be found "),us=n(Or,"A",{href:!0,rel:!0});var GD=i(us);_m=a(GD,"here"),GD.forEach(t),vm=a(Or,"."),Or.forEach(t),Cc=c(r),S=n(r,"DIV",{class:!0});var D=i(S);h(fs.$$.fragment,D),bm=c(D),qa=n(D,"P",{});var zD=i(qa);Sm=a(zD,`DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with
the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality
samples, and it can generate quite good samples even in only 10 steps.`),zD.forEach(t),Dm=c(D),Ot=n(D,"P",{});var ec=i(Ot);xm=a(ec,"For more details, see the original paper: "),ps=n(ec,"A",{href:!0,rel:!0});var YD=i(ps);$m=a(YD,"https://arxiv.org/abs/2206.00927"),YD.forEach(t),Em=a(ec," and "),hs=n(ec,"A",{href:!0,rel:!0});var JD=i(hs);ym=a(JD,"https://arxiv.org/abs/2211.01095"),JD.forEach(t),ec.forEach(t),wm=c(D),Xe=n(D,"P",{});var Oi=i(Xe);Mm=a(Oi,`Currently, we support the multistep DPM-Solver for both noise prediction models and data prediction models. We
recommend to use `),Ka=n(Oi,"CODE",{});var XD=i(Ka);Pm=a(XD,"solver_order=2"),XD.forEach(t),Tm=a(Oi," for guided sampling, and "),Ra=n(Oi,"CODE",{});var ZD=i(Ra);Cm=a(ZD,"solver_order=3"),ZD.forEach(t),km=a(Oi," for unconditional sampling."),Oi.forEach(t),Om=c(D),Me=n(D,"P",{});var Ar=i(Me);Am=a(Ar,"We also support the \u201Cdynamic thresholding\u201D method in Imagen ("),ms=n(Ar,"A",{href:!0,rel:!0});var ex=i(ms);Vm=a(ex,"https://arxiv.org/abs/2205.11487"),ex.forEach(t),Fm=a(Ar,`). For pixel-space
diffusion models, you can set both `),Qa=n(Ar,"CODE",{});var tx=i(Qa);Im=a(tx,'algorithm_type="dpmsolver++"'),tx.forEach(t),Nm=a(Ar," and "),ja=n(Ar,"CODE",{});var rx=i(ja);Lm=a(rx,"thresholding=True"),rx.forEach(t),qm=a(Ar,` to use the dynamic
thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as
stable-diffusion).`),Ar.forEach(t),Km=c(D),F=n(D,"P",{});var ne=i(F);Po=n(ne,"A",{href:!0});var sx=i(Po);Rm=a(sx,"~ConfigMixin"),sx.forEach(t),Qm=a(ne," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Wa=n(ne,"CODE",{});var nx=i(Wa);jm=a(nx,"__init__"),nx.forEach(t),Wm=a(ne,`
function, such as `),Ua=n(ne,"CODE",{});var ox=i(Ua);Um=a(ox,"num_train_timesteps"),ox.forEach(t),Bm=a(ne,". They can be accessed via "),Ba=n(ne,"CODE",{});var ix=i(Ba);Hm=a(ix,"scheduler.config.num_train_timesteps"),ix.forEach(t),Gm=a(ne,`.
`),To=n(ne,"A",{href:!0});var ax=i(To);zm=a(ax,"~ConfigMixin"),ax.forEach(t),Ym=a(ne," also provides general loading and saving functionality via the "),Co=n(ne,"A",{href:!0});var dx=i(Co);Jm=a(dx,"save_config()"),dx.forEach(t),Xm=a(ne,` and
`),ko=n(ne,"A",{href:!0});var lx=i(ko);Zm=a(lx,"from_config()"),lx.forEach(t),eg=a(ne," functions."),ne.forEach(t),tg=c(D),$e=n(D,"DIV",{class:!0});var Vr=i($e);h(gs.$$.fragment,Vr),rg=c(Vr),Ha=n(Vr,"P",{});var cx=i(Ha);sg=a(cx,"Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs."),cx.forEach(t),ng=c(Vr),Ga=n(Vr,"P",{});var ux=i(Ga);og=a(ux,`DPM-Solver is designed to discretize an integral of the noise prediciton model, and DPM-Solver++ is designed to
discretize an integral of the data prediction model. So we need to first convert the model output to the
corresponding type to match the algorithm.`),ux.forEach(t),ig=c(Vr),za=n(Vr,"P",{});var fx=i(za);ag=a(fx,`Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or
DPM-Solver++ for both noise prediction model and data prediction model.`),fx.forEach(t),Vr.forEach(t),dg=c(D),Oe=n(D,"DIV",{class:!0});var Ai=i(Oe);h(_s.$$.fragment,Ai),lg=c(Ai),Ya=n(Ai,"P",{});var px=i(Ya);cg=a(px,"One step for the first-order DPM-Solver (equivalent to DDIM)."),px.forEach(t),ug=c(Ai),vs=n(Ai,"P",{});var Vu=i(vs);fg=a(Vu,"See "),bs=n(Vu,"A",{href:!0,rel:!0});var hx=i(bs);pg=a(hx,"https://arxiv.org/abs/2206.00927"),hx.forEach(t),hg=a(Vu," for the detailed derivation."),Vu.forEach(t),Ai.forEach(t),mg=c(D),At=n(D,"DIV",{class:!0});var Fu=i(At);h(Ss.$$.fragment,Fu),gg=c(Fu),Ja=n(Fu,"P",{});var mx=i(Ja);_g=a(mx,"One step for the second-order multistep DPM-Solver."),mx.forEach(t),Fu.forEach(t),vg=c(D),Vt=n(D,"DIV",{class:!0});var Iu=i(Vt);h(Ds.$$.fragment,Iu),bg=c(Iu),Xa=n(Iu,"P",{});var gx=i(Xa);Sg=a(gx,"One step for the third-order multistep DPM-Solver."),gx.forEach(t),Iu.forEach(t),Dg=c(D),Ft=n(D,"DIV",{class:!0});var Nu=i(Ft);h(xs.$$.fragment,Nu),xg=c(Nu),Za=n(Nu,"P",{});var _x=i(Za);$g=a(_x,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),_x.forEach(t),Nu.forEach(t),Eg=c(D),It=n(D,"DIV",{class:!0});var Lu=i(It);h($s.$$.fragment,Lu),yg=c(Lu),ed=n(Lu,"P",{});var vx=i(ed);wg=a(vx,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),vx.forEach(t),Lu.forEach(t),Mg=c(D),Nt=n(D,"DIV",{class:!0});var qu=i(Nt);h(Es.$$.fragment,qu),Pg=c(qu),td=n(qu,"P",{});var bx=i(td);Tg=a(bx,"Step function propagating the sample with the multistep DPM-Solver."),bx.forEach(t),qu.forEach(t),D.forEach(t),kc=c(r),Ze=n(r,"H4",{class:!0});var Ku=i(Ze);Lt=n(Ku,"A",{id:!0,class:!0,href:!0});var Sx=i(Lt);rd=n(Sx,"SPAN",{});var Dx=i(rd);h(ys.$$.fragment,Dx),Dx.forEach(t),Sx.forEach(t),Cg=c(Ku),sd=n(Ku,"SPAN",{});var xx=i(sd);kg=a(xx,"Variance exploding, stochastic sampling from Karras et. al"),xx.forEach(t),Ku.forEach(t),Oc=c(r),qt=n(r,"P",{});var Ru=i(qt);Og=a(Ru,"Original paper can be found "),ws=n(Ru,"A",{href:!0,rel:!0});var $x=i(ws);Ag=a($x,"here"),$x.forEach(t),Vg=a(Ru,"."),Ru.forEach(t),Ac=c(r),x=n(r,"DIV",{class:!0});var M=i(x);h(Ms.$$.fragment,M),Fg=c(M),nd=n(M,"P",{});var Ex=i(nd);Ig=a(Ex,`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),Ex.forEach(t),Ng=c(M),Kt=n(M,"P",{});var tc=i(Kt);Lg=a(tc,`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Ps=n(tc,"A",{href:!0,rel:!0});var yx=i(Ps);qg=a(yx,"https://arxiv.org/abs/2206.00364"),yx.forEach(t),Kg=a(tc,` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Ts=n(tc,"A",{href:!0,rel:!0});var wx=i(Ts);Rg=a(wx,"https://arxiv.org/abs/2011.13456"),wx.forEach(t),tc.forEach(t),Qg=c(M),I=n(M,"P",{});var oe=i(I);Oo=n(oe,"A",{href:!0});var Mx=i(Oo);jg=a(Mx,"~ConfigMixin"),Mx.forEach(t),Wg=a(oe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),od=n(oe,"CODE",{});var Px=i(od);Ug=a(Px,"__init__"),Px.forEach(t),Bg=a(oe,`
function, such as `),id=n(oe,"CODE",{});var Tx=i(id);Hg=a(Tx,"num_train_timesteps"),Tx.forEach(t),Gg=a(oe,". They can be accessed via "),ad=n(oe,"CODE",{});var Cx=i(ad);zg=a(Cx,"scheduler.config.num_train_timesteps"),Cx.forEach(t),Yg=a(oe,`.
`),Ao=n(oe,"A",{href:!0});var kx=i(Ao);Jg=a(kx,"~ConfigMixin"),kx.forEach(t),Xg=a(oe," also provides general loading and saving functionality via the "),Vo=n(oe,"A",{href:!0});var Ox=i(Vo);Zg=a(Ox,"save_config()"),Ox.forEach(t),e_=a(oe,` and
`),Fo=n(oe,"A",{href:!0});var Ax=i(Fo);t_=a(Ax,"from_config()"),Ax.forEach(t),r_=a(oe," functions."),oe.forEach(t),s_=c(M),Cs=n(M,"P",{});var Qu=i(Cs);n_=a(Qu,`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),ks=n(Qu,"A",{href:!0,rel:!0});var Vx=i(ks);o_=a(Vx,"https://arxiv.org/abs/2206.00364"),Vx.forEach(t),i_=a(Qu,`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),Qu.forEach(t),a_=c(M),Ae=n(M,"DIV",{class:!0});var Vi=i(Ae);h(Os.$$.fragment,Vi),d_=c(Vi),dd=n(Vi,"P",{});var Fx=i(dd);l_=a(Fx,`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),Fx.forEach(t),c_=c(Vi),ld=n(Vi,"P",{});var Ix=i(ld);u_=a(Ix,"TODO Args:"),Ix.forEach(t),Vi.forEach(t),f_=c(M),Rt=n(M,"DIV",{class:!0});var ju=i(Rt);h(As.$$.fragment,ju),p_=c(ju),cd=n(ju,"P",{});var Nx=i(cd);h_=a(Nx,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Nx.forEach(t),ju.forEach(t),m_=c(M),Qt=n(M,"DIV",{class:!0});var Wu=i(Qt);h(Vs.$$.fragment,Wu),g_=c(Wu),ud=n(Wu,"P",{});var Lx=i(ud);__=a(Lx,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),Lx.forEach(t),Wu.forEach(t),v_=c(M),jt=n(M,"DIV",{class:!0});var Uu=i(jt);h(Fs.$$.fragment,Uu),b_=c(Uu),fd=n(Uu,"P",{});var qx=i(fd);S_=a(qx,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),qx.forEach(t),Uu.forEach(t),D_=c(M),Wt=n(M,"DIV",{class:!0});var Bu=i(Wt);h(Is.$$.fragment,Bu),x_=c(Bu),pd=n(Bu,"P",{});var Kx=i(pd);$_=a(Kx,"Correct the predicted sample based on the output model_output of the network. TODO complete description"),Kx.forEach(t),Bu.forEach(t),M.forEach(t),Vc=c(r),et=n(r,"H4",{class:!0});var Hu=i(et);Ut=n(Hu,"A",{id:!0,class:!0,href:!0});var Rx=i(Ut);hd=n(Rx,"SPAN",{});var Qx=i(hd);h(Ns.$$.fragment,Qx),Qx.forEach(t),Rx.forEach(t),E_=c(Hu),md=n(Hu,"SPAN",{});var jx=i(md);y_=a(jx,"Linear multistep scheduler for discrete beta schedules"),jx.forEach(t),Hu.forEach(t),Fc=c(r),Bt=n(r,"P",{});var Gu=i(Bt);w_=a(Gu,"Original implementation can be found "),Ls=n(Gu,"A",{href:!0,rel:!0});var Wx=i(Ls);M_=a(Wx,"here"),Wx.forEach(t),P_=a(Gu,"."),Gu.forEach(t),Ic=c(r),C=n(r,"DIV",{class:!0});var ve=i(C);h(qs.$$.fragment,ve),T_=c(ve),Io=n(ve,"P",{});var dS=i(Io);C_=a(dS,`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),Ks=n(dS,"A",{href:!0,rel:!0});var Ux=i(Ks);k_=a(Ux,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),Ux.forEach(t),dS.forEach(t),O_=c(ve),N=n(ve,"P",{});var ie=i(N);No=n(ie,"A",{href:!0});var Bx=i(No);A_=a(Bx,"~ConfigMixin"),Bx.forEach(t),V_=a(ie," takes care of storing all config attributes that are passed in the scheduler\u2019s "),gd=n(ie,"CODE",{});var Hx=i(gd);F_=a(Hx,"__init__"),Hx.forEach(t),I_=a(ie,`
function, such as `),_d=n(ie,"CODE",{});var Gx=i(_d);N_=a(Gx,"num_train_timesteps"),Gx.forEach(t),L_=a(ie,". They can be accessed via "),vd=n(ie,"CODE",{});var zx=i(vd);q_=a(zx,"scheduler.config.num_train_timesteps"),zx.forEach(t),K_=a(ie,`.
`),Lo=n(ie,"A",{href:!0});var Yx=i(Lo);R_=a(Yx,"~ConfigMixin"),Yx.forEach(t),Q_=a(ie," also provides general loading and saving functionality via the "),qo=n(ie,"A",{href:!0});var Jx=i(qo);j_=a(Jx,"save_config()"),Jx.forEach(t),W_=a(ie,` and
`),Ko=n(ie,"A",{href:!0});var Xx=i(Ko);U_=a(Xx,"from_config()"),Xx.forEach(t),B_=a(ie," functions."),ie.forEach(t),H_=c(ve),Ht=n(ve,"DIV",{class:!0});var zu=i(Ht);h(Rs.$$.fragment,zu),G_=c(zu),bd=n(zu,"P",{});var Zx=i(bd);z_=a(Zx,"Compute a linear multistep coefficient."),Zx.forEach(t),zu.forEach(t),Y_=c(ve),Gt=n(ve,"DIV",{class:!0});var Yu=i(Gt);h(Qs.$$.fragment,Yu),J_=c(Yu),js=n(Yu,"P",{});var Ju=i(js);X_=a(Ju,"Scales the denoising model input by "),Sd=n(Ju,"CODE",{});var e$=i(Sd);Z_=a(e$,"(sigma**2 + 1) ** 0.5"),e$.forEach(t),ev=a(Ju," to match the K-LMS algorithm."),Ju.forEach(t),Yu.forEach(t),tv=c(ve),zt=n(ve,"DIV",{class:!0});var Xu=i(zt);h(Ws.$$.fragment,Xu),rv=c(Xu),Dd=n(Xu,"P",{});var t$=i(Dd);sv=a(t$,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),t$.forEach(t),Xu.forEach(t),nv=c(ve),Yt=n(ve,"DIV",{class:!0});var Zu=i(Yt);h(Us.$$.fragment,Zu),ov=c(Zu),xd=n(Zu,"P",{});var r$=i(xd);iv=a(r$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),r$.forEach(t),Zu.forEach(t),ve.forEach(t),Nc=c(r),tt=n(r,"H4",{class:!0});var ef=i(tt);Jt=n(ef,"A",{id:!0,class:!0,href:!0});var s$=i(Jt);$d=n(s$,"SPAN",{});var n$=i($d);h(Bs.$$.fragment,n$),n$.forEach(t),s$.forEach(t),av=c(ef),Ed=n(ef,"SPAN",{});var o$=i(Ed);dv=a(o$,"Pseudo numerical methods for diffusion models (PNDM)"),o$.forEach(t),ef.forEach(t),Lc=c(r),Xt=n(r,"P",{});var tf=i(Xt);lv=a(tf,"Original implementation can be found "),Hs=n(tf,"A",{href:!0,rel:!0});var i$=i(Hs);cv=a(i$,"here"),i$.forEach(t),uv=a(tf,"."),tf.forEach(t),qc=c(r),$=n(r,"DIV",{class:!0});var H=i($);h(Gs.$$.fragment,H),fv=c(H),yd=n(H,"P",{});var a$=i(yd);pv=a(a$,`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),a$.forEach(t),hv=c(H),L=n(H,"P",{});var ae=i(L);Ro=n(ae,"A",{href:!0});var d$=i(Ro);mv=a(d$,"~ConfigMixin"),d$.forEach(t),gv=a(ae," takes care of storing all config attributes that are passed in the scheduler\u2019s "),wd=n(ae,"CODE",{});var l$=i(wd);_v=a(l$,"__init__"),l$.forEach(t),vv=a(ae,`
function, such as `),Md=n(ae,"CODE",{});var c$=i(Md);bv=a(c$,"num_train_timesteps"),c$.forEach(t),Sv=a(ae,". They can be accessed via "),Pd=n(ae,"CODE",{});var u$=i(Pd);Dv=a(u$,"scheduler.config.num_train_timesteps"),u$.forEach(t),xv=a(ae,`.
`),Qo=n(ae,"A",{href:!0});var f$=i(Qo);$v=a(f$,"~ConfigMixin"),f$.forEach(t),Ev=a(ae," also provides general loading and saving functionality via the "),jo=n(ae,"A",{href:!0});var p$=i(jo);yv=a(p$,"save_config()"),p$.forEach(t),wv=a(ae,` and
`),Wo=n(ae,"A",{href:!0});var h$=i(Wo);Mv=a(h$,"from_config()"),h$.forEach(t),Pv=a(ae," functions."),ae.forEach(t),Tv=c(H),Uo=n(H,"P",{});var lS=i(Uo);Cv=a(lS,"For more details, see the original paper: "),zs=n(lS,"A",{href:!0,rel:!0});var m$=i(zs);kv=a(m$,"https://arxiv.org/abs/2202.09778"),m$.forEach(t),lS.forEach(t),Ov=c(H),Zt=n(H,"DIV",{class:!0});var rf=i(Zt);h(Ys.$$.fragment,rf),Av=c(rf),Td=n(rf,"P",{});var g$=i(Td);Vv=a(g$,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),g$.forEach(t),rf.forEach(t),Fv=c(H),er=n(H,"DIV",{class:!0});var sf=i(er);h(Js.$$.fragment,sf),Iv=c(sf),Cd=n(sf,"P",{});var _$=i(Cd);Nv=a(_$,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),_$.forEach(t),sf.forEach(t),Lv=c(H),Ve=n(H,"DIV",{class:!0});var Fi=i(Ve);h(Xs.$$.fragment,Fi),qv=c(Fi),kd=n(Fi,"P",{});var v$=i(kd);Kv=a(v$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),v$.forEach(t),Rv=c(Fi),Pe=n(Fi,"P",{});var Fr=i(Pe);Qv=a(Fr,"This function calls "),Od=n(Fr,"CODE",{});var b$=i(Od);jv=a(b$,"step_prk()"),b$.forEach(t),Wv=a(Fr," or "),Ad=n(Fr,"CODE",{});var S$=i(Ad);Uv=a(S$,"step_plms()"),S$.forEach(t),Bv=a(Fr," depending on the internal variable "),Vd=n(Fr,"CODE",{});var D$=i(Vd);Hv=a(D$,"counter"),D$.forEach(t),Gv=a(Fr,"."),Fr.forEach(t),Fi.forEach(t),zv=c(H),tr=n(H,"DIV",{class:!0});var nf=i(tr);h(Zs.$$.fragment,nf),Yv=c(nf),Fd=n(nf,"P",{});var x$=i(Fd);Jv=a(x$,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),x$.forEach(t),nf.forEach(t),Xv=c(H),rr=n(H,"DIV",{class:!0});var of=i(rr);h(en.$$.fragment,of),Zv=c(of),Id=n(of,"P",{});var $$=i(Id);eb=a($$,`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),$$.forEach(t),of.forEach(t),H.forEach(t),Kc=c(r),rt=n(r,"H4",{class:!0});var af=i(rt);sr=n(af,"A",{id:!0,class:!0,href:!0});var E$=i(sr);Nd=n(E$,"SPAN",{});var y$=i(Nd);h(tn.$$.fragment,y$),y$.forEach(t),E$.forEach(t),tb=c(af),Ld=n(af,"SPAN",{});var w$=i(Ld);rb=a(w$,"variance exploding stochastic differential equation (VE-SDE) scheduler"),w$.forEach(t),af.forEach(t),Rc=c(r),nr=n(r,"P",{});var df=i(nr);sb=a(df,"Original paper can be found "),rn=n(df,"A",{href:!0,rel:!0});var M$=i(rn);nb=a(M$,"here"),M$.forEach(t),ob=a(df,"."),df.forEach(t),Qc=c(r),E=n(r,"DIV",{class:!0});var G=i(E);h(sn.$$.fragment,G),ib=c(G),qd=n(G,"P",{});var P$=i(qd);ab=a(P$,"The variance exploding stochastic differential equation (SDE) scheduler."),P$.forEach(t),db=c(G),Bo=n(G,"P",{});var cS=i(Bo);lb=a(cS,"For more information, see the original paper: "),nn=n(cS,"A",{href:!0,rel:!0});var T$=i(nn);cb=a(T$,"https://arxiv.org/abs/2011.13456"),T$.forEach(t),cS.forEach(t),ub=c(G),q=n(G,"P",{});var de=i(q);Ho=n(de,"A",{href:!0});var C$=i(Ho);fb=a(C$,"~ConfigMixin"),C$.forEach(t),pb=a(de," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Kd=n(de,"CODE",{});var k$=i(Kd);hb=a(k$,"__init__"),k$.forEach(t),mb=a(de,`
function, such as `),Rd=n(de,"CODE",{});var O$=i(Rd);gb=a(O$,"num_train_timesteps"),O$.forEach(t),_b=a(de,". They can be accessed via "),Qd=n(de,"CODE",{});var A$=i(Qd);vb=a(A$,"scheduler.config.num_train_timesteps"),A$.forEach(t),bb=a(de,`.
`),Go=n(de,"A",{href:!0});var V$=i(Go);Sb=a(V$,"~ConfigMixin"),V$.forEach(t),Db=a(de," also provides general loading and saving functionality via the "),zo=n(de,"A",{href:!0});var F$=i(zo);xb=a(F$,"save_config()"),F$.forEach(t),$b=a(de,` and
`),Yo=n(de,"A",{href:!0});var I$=i(Yo);Eb=a(I$,"from_config()"),I$.forEach(t),yb=a(de," functions."),de.forEach(t),wb=c(G),or=n(G,"DIV",{class:!0});var lf=i(or);h(on.$$.fragment,lf),Mb=c(lf),jd=n(lf,"P",{});var N$=i(jd);Pb=a(N$,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),N$.forEach(t),lf.forEach(t),Tb=c(G),Fe=n(G,"DIV",{class:!0});var Ii=i(Fe);h(an.$$.fragment,Ii),Cb=c(Ii),Wd=n(Ii,"P",{});var L$=i(Wd);kb=a(L$,"Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),L$.forEach(t),Ob=c(Ii),st=n(Ii,"P",{});var Ni=i(st);Ab=a(Ni,"The sigmas control the weight of the "),Ud=n(Ni,"CODE",{});var q$=i(Ud);Vb=a(q$,"drift"),q$.forEach(t),Fb=a(Ni," and "),Bd=n(Ni,"CODE",{});var K$=i(Bd);Ib=a(K$,"diffusion"),K$.forEach(t),Nb=a(Ni," components of sample update."),Ni.forEach(t),Ii.forEach(t),Lb=c(G),ir=n(G,"DIV",{class:!0});var cf=i(ir);h(dn.$$.fragment,cf),qb=c(cf),Hd=n(cf,"P",{});var R$=i(Hd);Kb=a(R$,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),R$.forEach(t),cf.forEach(t),Rb=c(G),ar=n(G,"DIV",{class:!0});var uf=i(ar);h(ln.$$.fragment,uf),Qb=c(uf),Gd=n(uf,"P",{});var Q$=i(Gd);jb=a(Q$,`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),Q$.forEach(t),uf.forEach(t),Wb=c(G),dr=n(G,"DIV",{class:!0});var ff=i(dr);h(cn.$$.fragment,ff),Ub=c(ff),zd=n(ff,"P",{});var j$=i(zd);Bb=a(j$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),j$.forEach(t),ff.forEach(t),G.forEach(t),jc=c(r),nt=n(r,"H4",{class:!0});var pf=i(nt);lr=n(pf,"A",{id:!0,class:!0,href:!0});var W$=i(lr);Yd=n(W$,"SPAN",{});var U$=i(Yd);h(un.$$.fragment,U$),U$.forEach(t),W$.forEach(t),Hb=c(pf),Jd=n(pf,"SPAN",{});var B$=i(Jd);Gb=a(B$,"improved pseudo numerical methods for diffusion models (iPNDM)"),B$.forEach(t),pf.forEach(t),Wc=c(r),cr=n(r,"P",{});var hf=i(cr);zb=a(hf,"Original implementation can be found "),fn=n(hf,"A",{href:!0,rel:!0});var H$=i(fn);Yb=a(H$,"here"),H$.forEach(t),Jb=a(hf,"."),hf.forEach(t),Uc=c(r),k=n(r,"DIV",{class:!0});var be=i(k);h(pn.$$.fragment,be),Xb=c(be),Jo=n(be,"P",{});var uS=i(Jo);Zb=a(uS,`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),hn=n(uS,"A",{href:!0,rel:!0});var G$=i(hn);e1=a(G$,"library"),G$.forEach(t),uS.forEach(t),t1=c(be),K=n(be,"P",{});var le=i(K);Xo=n(le,"A",{href:!0});var z$=i(Xo);r1=a(z$,"~ConfigMixin"),z$.forEach(t),s1=a(le," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Xd=n(le,"CODE",{});var Y$=i(Xd);n1=a(Y$,"__init__"),Y$.forEach(t),o1=a(le,`
function, such as `),Zd=n(le,"CODE",{});var J$=i(Zd);i1=a(J$,"num_train_timesteps"),J$.forEach(t),a1=a(le,". They can be accessed via "),el=n(le,"CODE",{});var X$=i(el);d1=a(X$,"scheduler.config.num_train_timesteps"),X$.forEach(t),l1=a(le,`.
`),Zo=n(le,"A",{href:!0});var Z$=i(Zo);c1=a(Z$,"~ConfigMixin"),Z$.forEach(t),u1=a(le," also provides general loading and saving functionality via the "),ei=n(le,"A",{href:!0});var eE=i(ei);f1=a(eE,"save_config()"),eE.forEach(t),p1=a(le,` and
`),ti=n(le,"A",{href:!0});var tE=i(ti);h1=a(tE,"from_config()"),tE.forEach(t),m1=a(le," functions."),le.forEach(t),g1=c(be),ri=n(be,"P",{});var fS=i(ri);_1=a(fS,"For more details, see the original paper: "),mn=n(fS,"A",{href:!0,rel:!0});var rE=i(mn);v1=a(rE,"https://arxiv.org/abs/2202.09778"),rE.forEach(t),fS.forEach(t),b1=c(be),ur=n(be,"DIV",{class:!0});var mf=i(ur);h(gn.$$.fragment,mf),S1=c(mf),tl=n(mf,"P",{});var sE=i(tl);D1=a(sE,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),sE.forEach(t),mf.forEach(t),x1=c(be),fr=n(be,"DIV",{class:!0});var gf=i(fr);h(_n.$$.fragment,gf),$1=c(gf),rl=n(gf,"P",{});var nE=i(rl);E1=a(nE,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),nE.forEach(t),gf.forEach(t),y1=c(be),pr=n(be,"DIV",{class:!0});var _f=i(pr);h(vn.$$.fragment,_f),w1=c(_f),sl=n(_f,"P",{});var oE=i(sl);M1=a(oE,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),oE.forEach(t),_f.forEach(t),be.forEach(t),Bc=c(r),ot=n(r,"H4",{class:!0});var vf=i(ot);hr=n(vf,"A",{id:!0,class:!0,href:!0});var iE=i(hr);nl=n(iE,"SPAN",{});var aE=i(nl);h(bn.$$.fragment,aE),aE.forEach(t),iE.forEach(t),P1=c(vf),ol=n(vf,"SPAN",{});var dE=i(ol);T1=a(dE,"variance preserving stochastic differential equation (VP-SDE) scheduler"),dE.forEach(t),vf.forEach(t),Hc=c(r),mr=n(r,"P",{});var bf=i(mr);C1=a(bf,"Original paper can be found "),Sn=n(bf,"A",{href:!0,rel:!0});var lE=i(Sn);k1=a(lE,"here"),lE.forEach(t),O1=a(bf,"."),bf.forEach(t),Gc=c(r),h(gr.$$.fragment,r),zc=c(r),te=n(r,"DIV",{class:!0});var Le=i(te);h(Dn.$$.fragment,Le),A1=c(Le),il=n(Le,"P",{});var cE=i(il);V1=a(cE,"The variance preserving stochastic differential equation (SDE) scheduler."),cE.forEach(t),F1=c(Le),R=n(Le,"P",{});var ce=i(R);si=n(ce,"A",{href:!0});var uE=i(si);I1=a(uE,"~ConfigMixin"),uE.forEach(t),N1=a(ce," takes care of storing all config attributes that are passed in the scheduler\u2019s "),al=n(ce,"CODE",{});var fE=i(al);L1=a(fE,"__init__"),fE.forEach(t),q1=a(ce,`
function, such as `),dl=n(ce,"CODE",{});var pE=i(dl);K1=a(pE,"num_train_timesteps"),pE.forEach(t),R1=a(ce,". They can be accessed via "),ll=n(ce,"CODE",{});var hE=i(ll);Q1=a(hE,"scheduler.config.num_train_timesteps"),hE.forEach(t),j1=a(ce,`.
`),ni=n(ce,"A",{href:!0});var mE=i(ni);W1=a(mE,"~ConfigMixin"),mE.forEach(t),U1=a(ce," also provides general loading and saving functionality via the "),oi=n(ce,"A",{href:!0});var gE=i(oi);B1=a(gE,"save_config()"),gE.forEach(t),H1=a(ce,` and
`),ii=n(ce,"A",{href:!0});var _E=i(ii);G1=a(_E,"from_config()"),_E.forEach(t),z1=a(ce," functions."),ce.forEach(t),Y1=c(Le),ai=n(Le,"P",{});var pS=i(ai);J1=a(pS,"For more information, see the original paper: "),xn=n(pS,"A",{href:!0,rel:!0});var vE=i(xn);X1=a(vE,"https://arxiv.org/abs/2011.13456"),vE.forEach(t),pS.forEach(t),Z1=c(Le),cl=n(Le,"P",{});var bE=i(cl);e0=a(bE,"UNDER CONSTRUCTION"),bE.forEach(t),Le.forEach(t),Yc=c(r),it=n(r,"H4",{class:!0});var Sf=i(it);_r=n(Sf,"A",{id:!0,class:!0,href:!0});var SE=i(_r);ul=n(SE,"SPAN",{});var DE=i(ul);h($n.$$.fragment,DE),DE.forEach(t),SE.forEach(t),t0=c(Sf),fl=n(Sf,"SPAN",{});var xE=i(fl);r0=a(xE,"Euler scheduler"),xE.forEach(t),Sf.forEach(t),Jc=c(r),Ie=n(r,"P",{});var Li=i(Ie);s0=a(Li,"Euler scheduler (Algorithm 2) from the paper "),En=n(Li,"A",{href:!0,rel:!0});var $E=i(En);n0=a($E,"Elucidating the Design Space of Diffusion-Based Generative Models"),$E.forEach(t),o0=a(Li," by Karras et al. (2022). Based on the original "),yn=n(Li,"A",{href:!0,rel:!0});var EE=i(yn);i0=a(EE,"k-diffusion"),EE.forEach(t),a0=a(Li,` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Li.forEach(t),Xc=c(r),Y=n(r,"DIV",{class:!0});var Ee=i(Y);h(wn.$$.fragment,Ee),d0=c(Ee),vr=n(Ee,"P",{});var rc=i(vr);l0=a(rc,"Euler scheduler (Algorithm 2) from Karras et al. (2022) "),Mn=n(rc,"A",{href:!0,rel:!0});var yE=i(Mn);c0=a(yE,"https://arxiv.org/abs/2206.00364"),yE.forEach(t),u0=a(rc,`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),Pn=n(rc,"A",{href:!0,rel:!0});var wE=i(Pn);f0=a(wE,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),wE.forEach(t),rc.forEach(t),p0=c(Ee),Q=n(Ee,"P",{});var ue=i(Q);di=n(ue,"A",{href:!0});var ME=i(di);h0=a(ME,"~ConfigMixin"),ME.forEach(t),m0=a(ue," takes care of storing all config attributes that are passed in the scheduler\u2019s "),pl=n(ue,"CODE",{});var PE=i(pl);g0=a(PE,"__init__"),PE.forEach(t),_0=a(ue,`
function, such as `),hl=n(ue,"CODE",{});var TE=i(hl);v0=a(TE,"num_train_timesteps"),TE.forEach(t),b0=a(ue,". They can be accessed via "),ml=n(ue,"CODE",{});var CE=i(ml);S0=a(CE,"scheduler.config.num_train_timesteps"),CE.forEach(t),D0=a(ue,`.
`),li=n(ue,"A",{href:!0});var kE=i(li);x0=a(kE,"~ConfigMixin"),kE.forEach(t),$0=a(ue," also provides general loading and saving functionality via the "),ci=n(ue,"A",{href:!0});var OE=i(ci);E0=a(OE,"save_config()"),OE.forEach(t),y0=a(ue,` and
`),ui=n(ue,"A",{href:!0});var AE=i(ui);w0=a(AE,"from_config()"),AE.forEach(t),M0=a(ue," functions."),ue.forEach(t),P0=c(Ee),br=n(Ee,"DIV",{class:!0});var Df=i(br);h(Tn.$$.fragment,Df),T0=c(Df),Cn=n(Df,"P",{});var xf=i(Cn);C0=a(xf,"Scales the denoising model input by "),gl=n(xf,"CODE",{});var VE=i(gl);k0=a(VE,"(sigma**2 + 1) ** 0.5"),VE.forEach(t),O0=a(xf," to match the Euler algorithm."),xf.forEach(t),Df.forEach(t),A0=c(Ee),Sr=n(Ee,"DIV",{class:!0});var $f=i(Sr);h(kn.$$.fragment,$f),V0=c($f),_l=n($f,"P",{});var FE=i(_l);F0=a(FE,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),FE.forEach(t),$f.forEach(t),I0=c(Ee),Dr=n(Ee,"DIV",{class:!0});var Ef=i(Dr);h(On.$$.fragment,Ef),N0=c(Ef),vl=n(Ef,"P",{});var IE=i(vl);L0=a(IE,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),IE.forEach(t),Ef.forEach(t),Ee.forEach(t),Zc=c(r),at=n(r,"H4",{class:!0});var yf=i(at);xr=n(yf,"A",{id:!0,class:!0,href:!0});var NE=i(xr);bl=n(NE,"SPAN",{});var LE=i(bl);h(An.$$.fragment,LE),LE.forEach(t),NE.forEach(t),q0=c(yf),Sl=n(yf,"SPAN",{});var qE=i(Sl);K0=a(qE,"Euler Ancestral scheduler"),qE.forEach(t),yf.forEach(t),eu=c(r),fi=n(r,"P",{});var KE=i(fi);R0=a(KE,`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),KE.forEach(t),tu=c(r),J=n(r,"DIV",{class:!0});var ye=i(J);h(Vn.$$.fragment,ye),Q0=c(ye),pi=n(ye,"P",{});var hS=i(pi);j0=a(hS,`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Fn=n(hS,"A",{href:!0,rel:!0});var RE=i(Fn);W0=a(RE,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),RE.forEach(t),hS.forEach(t),U0=c(ye),j=n(ye,"P",{});var fe=i(j);hi=n(fe,"A",{href:!0});var QE=i(hi);B0=a(QE,"~ConfigMixin"),QE.forEach(t),H0=a(fe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Dl=n(fe,"CODE",{});var jE=i(Dl);G0=a(jE,"__init__"),jE.forEach(t),z0=a(fe,`
function, such as `),xl=n(fe,"CODE",{});var WE=i(xl);Y0=a(WE,"num_train_timesteps"),WE.forEach(t),J0=a(fe,". They can be accessed via "),$l=n(fe,"CODE",{});var UE=i($l);X0=a(UE,"scheduler.config.num_train_timesteps"),UE.forEach(t),Z0=a(fe,`.
`),mi=n(fe,"A",{href:!0});var BE=i(mi);e2=a(BE,"~ConfigMixin"),BE.forEach(t),t2=a(fe," also provides general loading and saving functionality via the "),gi=n(fe,"A",{href:!0});var HE=i(gi);r2=a(HE,"save_config()"),HE.forEach(t),s2=a(fe,` and
`),_i=n(fe,"A",{href:!0});var GE=i(_i);n2=a(GE,"from_config()"),GE.forEach(t),o2=a(fe," functions."),fe.forEach(t),i2=c(ye),$r=n(ye,"DIV",{class:!0});var wf=i($r);h(In.$$.fragment,wf),a2=c(wf),Nn=n(wf,"P",{});var Mf=i(Nn);d2=a(Mf,"Scales the denoising model input by "),El=n(Mf,"CODE",{});var zE=i(El);l2=a(zE,"(sigma**2 + 1) ** 0.5"),zE.forEach(t),c2=a(Mf," to match the Euler algorithm."),Mf.forEach(t),wf.forEach(t),u2=c(ye),Er=n(ye,"DIV",{class:!0});var Pf=i(Er);h(Ln.$$.fragment,Pf),f2=c(Pf),yl=n(Pf,"P",{});var YE=i(yl);p2=a(YE,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),YE.forEach(t),Pf.forEach(t),h2=c(ye),yr=n(ye,"DIV",{class:!0});var Tf=i(yr);h(qn.$$.fragment,Tf),m2=c(Tf),wl=n(Tf,"P",{});var JE=i(wl);g2=a(JE,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),JE.forEach(t),Tf.forEach(t),ye.forEach(t),ru=c(r),dt=n(r,"H4",{class:!0});var Cf=i(dt);wr=n(Cf,"A",{id:!0,class:!0,href:!0});var XE=i(wr);Ml=n(XE,"SPAN",{});var ZE=i(Ml);h(Kn.$$.fragment,ZE),ZE.forEach(t),XE.forEach(t),_2=c(Cf),Pl=n(Cf,"SPAN",{});var e5=i(Pl);v2=a(e5,"VQDiffusionScheduler"),e5.forEach(t),Cf.forEach(t),su=c(r),Rn=n(r,"P",{});var mS=i(Rn);b2=a(mS,"Original paper can be found "),Qn=n(mS,"A",{href:!0,rel:!0});var t5=i(Qn);S2=a(t5,"here"),t5.forEach(t),mS.forEach(t),nu=c(r),y=n(r,"DIV",{class:!0});var z=i(y);h(jn.$$.fragment,z),D2=c(z),Tl=n(z,"P",{});var r5=i(Tl);x2=a(r5,"The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image."),r5.forEach(t),$2=c(z),Cl=n(z,"P",{});var s5=i(Cl);E2=a(s5,`The VQ-diffusion scheduler converts the transformer\u2019s output into a sample for the unnoised image at the previous
diffusion timestep.`),s5.forEach(t),y2=c(z),W=n(z,"P",{});var pe=i(W);vi=n(pe,"A",{href:!0});var n5=i(vi);w2=a(n5,"~ConfigMixin"),n5.forEach(t),M2=a(pe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),kl=n(pe,"CODE",{});var o5=i(kl);P2=a(o5,"__init__"),o5.forEach(t),T2=a(pe,`
function, such as `),Ol=n(pe,"CODE",{});var i5=i(Ol);C2=a(i5,"num_train_timesteps"),i5.forEach(t),k2=a(pe,". They can be accessed via "),Al=n(pe,"CODE",{});var a5=i(Al);O2=a(a5,"scheduler.config.num_train_timesteps"),a5.forEach(t),A2=a(pe,`.
`),bi=n(pe,"A",{href:!0});var d5=i(bi);V2=a(d5,"~ConfigMixin"),d5.forEach(t),F2=a(pe," also provides general loading and saving functionality via the "),Si=n(pe,"A",{href:!0});var l5=i(Si);I2=a(l5,"save_config()"),l5.forEach(t),N2=a(pe,` and
`),Di=n(pe,"A",{href:!0});var c5=i(Di);L2=a(c5,"from_config()"),c5.forEach(t),q2=a(pe," functions."),pe.forEach(t),K2=c(z),xi=n(z,"P",{});var gS=i(xi);R2=a(gS,"For more details, see the original paper: "),Wn=n(gS,"A",{href:!0,rel:!0});var u5=i(Wn);Q2=a(u5,"https://arxiv.org/abs/2111.14822"),u5.forEach(t),gS.forEach(t),j2=c(z),Ne=n(z,"DIV",{class:!0});var qi=i(Ne);h(Un.$$.fragment,qi),W2=c(qi),Bn=n(qi,"P",{});var kf=i(Bn);U2=a(kf,`Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each
latent pixel in `),Vl=n(kf,"CODE",{});var f5=i(Vl);B2=a(f5,"x_t"),f5.forEach(t),H2=a(kf,"."),kf.forEach(t),G2=c(qi),Fl=n(qi,"P",{});var p5=i(Fl);z2=a(p5,`See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix
is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.`),p5.forEach(t),qi.forEach(t),Y2=c(z),U=n(z,"DIV",{class:!0});var Se=i(U);h(Hn.$$.fragment,Se),J2=c(Se),Gn=n(Se,"P",{});var Of=i(Gn);X2=a(Of,"Calculates the log probabilities for the predicted classes of the image at timestep "),Il=n(Of,"CODE",{});var h5=i(Il);Z2=a(h5,"t-1"),h5.forEach(t),e4=a(Of,". I.e. Equation (11)."),Of.forEach(t),t4=c(Se),Nl=n(Se,"P",{});var m5=i(Nl);r4=a(m5,`Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only
forward probabilities.`),m5.forEach(t),s4=c(Se),Ll=n(Se,"P",{});var g5=i(Ll);n4=a(g5,"Equation (11) stated in terms of forward probabilities via Equation (5):"),g5.forEach(t),o4=c(Se),ql=n(Se,"P",{});var _5=i(ql);i4=a(_5,"Where:"),_5.forEach(t),a4=c(Se),Kl=n(Se,"UL",{});var v5=i(Kl);zn=n(v5,"LI",{});var Af=i(zn);d4=a(Af,"the sum is over x"),Rl=n(Af,"EM",{});var b5=i(Rl);l4=a(b5,"0 = {C_0 \u2026 C"),b5.forEach(t),c4=a(Af,"{k-1}} (classes for x_0)"),Af.forEach(t),v5.forEach(t),u4=c(Se),lt=n(Se,"P",{});var Ki=i(lt);f4=a(Ki,"p(x"),Ql=n(Ki,"EM",{});var S5=i(Ql);p4=a(S5,"{t-1} | x_t) = sum( q(x_t | x"),S5.forEach(t),h4=a(Ki,"{t-1}) "),jl=n(Ki,"EM",{});var D5=i(jl);m4=a(D5,"q(x_{t-1} | x_0)"),D5.forEach(t),g4=a(Ki," p(x_0) / q(x_t | x_0) )"),Ki.forEach(t),Se.forEach(t),_4=c(z),Mr=n(z,"DIV",{class:!0});var Vf=i(Mr);h(Yn.$$.fragment,Vf),v4=c(Vf),Wl=n(Vf,"P",{});var x5=i(Wl);b4=a(x5,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),x5.forEach(t),Vf.forEach(t),S4=c(z),Pr=n(z,"DIV",{class:!0});var Ff=i(Pr);h(Jn.$$.fragment,Ff),D4=c(Ff),Xn=n(Ff,"P",{});var If=i(Xn);x4=a(If,`Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the
docstring for `),Ul=n(If,"CODE",{});var $5=i(Ul);$4=a($5,"self.q_posterior"),$5.forEach(t),E4=a(If," for more in depth docs on how Equation (11) is computed."),If.forEach(t),Ff.forEach(t),z.forEach(t),ou=c(r),ct=n(r,"H4",{class:!0});var Nf=i(ct);Tr=n(Nf,"A",{id:!0,class:!0,href:!0});var E5=i(Tr);Bl=n(E5,"SPAN",{});var y5=i(Bl);h(Zn.$$.fragment,y5),y5.forEach(t),E5.forEach(t),y4=c(Nf),Hl=n(Nf,"SPAN",{});var w5=i(Hl);w4=a(w5,"RePaint scheduler"),w5.forEach(t),Nf.forEach(t),iu=c(r),Te=n(r,"P",{});var ao=i(Te);M4=a(ao,`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),$i=n(ao,"A",{href:!0});var M5=i($i);P4=a(M5,"RePaintPipeline"),M5.forEach(t),T4=a(ao,`.
Based on the paper `),eo=n(ao,"A",{href:!0,rel:!0});var P5=i(eo);C4=a(P5,"RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),P5.forEach(t),k4=a(ao,`
and the original implementation by Andreas Lugmayr et al.: `),to=n(ao,"A",{href:!0,rel:!0});var T5=i(to);O4=a(T5,"https://github.com/andreas128/RePaint"),T5.forEach(t),ao.forEach(t),au=c(r),X=n(r,"DIV",{class:!0});var we=i(X);h(ro.$$.fragment,we),A4=c(we),Gl=n(we,"P",{});var C5=i(Gl);V4=a(C5,"RePaint is a schedule for DDPM inpainting inside a given mask."),C5.forEach(t),F4=c(we),B=n(we,"P",{});var he=i(B);Ei=n(he,"A",{href:!0});var k5=i(Ei);I4=a(k5,"~ConfigMixin"),k5.forEach(t),N4=a(he," takes care of storing all config attributes that are passed in the scheduler\u2019s "),zl=n(he,"CODE",{});var O5=i(zl);L4=a(O5,"__init__"),O5.forEach(t),q4=a(he,`
function, such as `),Yl=n(he,"CODE",{});var A5=i(Yl);K4=a(A5,"num_train_timesteps"),A5.forEach(t),R4=a(he,". They can be accessed via "),Jl=n(he,"CODE",{});var V5=i(Jl);Q4=a(V5,"scheduler.config.num_train_timesteps"),V5.forEach(t),j4=a(he,`.
`),yi=n(he,"A",{href:!0});var F5=i(yi);W4=a(F5,"~ConfigMixin"),F5.forEach(t),U4=a(he," also provides general loading and saving functionality via the "),wi=n(he,"A",{href:!0});var I5=i(wi);B4=a(I5,"save_config()"),I5.forEach(t),H4=a(he,` and
`),Mi=n(he,"A",{href:!0});var N5=i(Mi);G4=a(N5,"from_config()"),N5.forEach(t),z4=a(he," functions."),he.forEach(t),Y4=c(we),Pi=n(we,"P",{});var _S=i(Pi);J4=a(_S,"For more details, see the original paper: "),so=n(_S,"A",{href:!0,rel:!0});var L5=i(so);X4=a(L5,"https://arxiv.org/pdf/2201.09865.pdf"),L5.forEach(t),_S.forEach(t),Z4=c(we),Cr=n(we,"DIV",{class:!0});var Lf=i(Cr);h(no.$$.fragment,Lf),eS=c(Lf),Xl=n(Lf,"P",{});var q5=i(Xl);tS=a(q5,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),q5.forEach(t),Lf.forEach(t),rS=c(we),kr=n(we,"DIV",{class:!0});var qf=i(kr);h(oo.$$.fragment,qf),sS=c(qf),Zl=n(qf,"P",{});var K5=i(Zl);nS=a(K5,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),K5.forEach(t),qf.forEach(t),we.forEach(t),this.h()},h(){d(Z,"name","hf:doc:metadata"),d(Z,"content",JSON.stringify(z5)),d(De,"id","schedulers"),d(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(De,"href","#schedulers"),d(ee,"class","relative group"),d(ft,"id","what-is-a-scheduler"),d(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ft,"href","#what-is-a-scheduler"),d(qe,"class","relative group"),d(ht,"id","discrete-versus-continuous-schedulers"),d(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ht,"href","#discrete-versus-continuous-schedulers"),d(Re,"class","relative group"),d(uo,"href","/docs/diffusers/main/en/api/schedulers#diffusers.DDPMScheduler"),d(fo,"href","/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler"),d(po,"href","/docs/diffusers/main/en/api/schedulers#diffusers.ScoreSdeVeScheduler"),d(mt,"id","designing-reusable-schedulers"),d(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(mt,"href","#designing-reusable-schedulers"),d(Qe,"class","relative group"),d(_t,"id","api"),d(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_t,"href","#api"),d(je,"class","relative group"),d(go,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(bt,"id","diffusers.SchedulerMixin"),d(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(bt,"href","#diffusers.SchedulerMixin"),d(We,"class","relative group"),d(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(St,"id","diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(St,"href","#diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(Be,"class","relative group"),d(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Dt,"id","implemented-schedulers"),d(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Dt,"href","#implemented-schedulers"),d(Ge,"class","relative group"),d(xt,"id","diffusers.DDIMScheduler"),d(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xt,"href","#diffusers.DDIMScheduler"),d(ze,"class","relative group"),d(vo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(bo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(So,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Do,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Jr,"href","https://arxiv.org/abs/2010.02502"),d(Jr,"rel","nofollow"),d($t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(wt,"id","diffusers.DDPMScheduler"),d(wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wt,"href","#diffusers.DDPMScheduler"),d(Ye,"class","relative group"),d(rs,"href","https://arxiv.org/abs/2010.02502"),d(rs,"rel","nofollow"),d($o,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Eo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(yo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(wo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(ns,"href","https://arxiv.org/abs/2006.11239"),d(ns,"rel","nofollow"),d(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(kt,"id","diffusers.DPMSolverMultistepScheduler"),d(kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(kt,"href","#diffusers.DPMSolverMultistepScheduler"),d(Je,"class","relative group"),d(ls,"href","https://arxiv.org/abs/2206.00927"),d(ls,"rel","nofollow"),d(cs,"href","https://arxiv.org/abs/2211.01095"),d(cs,"rel","nofollow"),d(us,"href","https://github.com/LuChengTHU/dpm-solver"),d(us,"rel","nofollow"),d(ps,"href","https://arxiv.org/abs/2206.00927"),d(ps,"rel","nofollow"),d(hs,"href","https://arxiv.org/abs/2211.01095"),d(hs,"rel","nofollow"),d(ms,"href","https://arxiv.org/abs/2205.11487"),d(ms,"rel","nofollow"),d(Po,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(To,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Co,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ko,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(bs,"href","https://arxiv.org/abs/2206.00927"),d(bs,"rel","nofollow"),d(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(It,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Lt,"id","diffusers.KarrasVeScheduler"),d(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Lt,"href","#diffusers.KarrasVeScheduler"),d(Ze,"class","relative group"),d(ws,"href","https://arxiv.org/abs/2006.11239"),d(ws,"rel","nofollow"),d(Ps,"href","https://arxiv.org/abs/2206.00364"),d(Ps,"rel","nofollow"),d(Ts,"href","https://arxiv.org/abs/2011.13456"),d(Ts,"rel","nofollow"),d(Oo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Ao,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Vo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Fo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(ks,"href","https://arxiv.org/abs/2206.00364"),d(ks,"rel","nofollow"),d(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ut,"id","diffusers.LMSDiscreteScheduler"),d(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ut,"href","#diffusers.LMSDiscreteScheduler"),d(et,"class","relative group"),d(Ls,"href","https://arxiv.org/abs/2206.00364"),d(Ls,"rel","nofollow"),d(Ks,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(Ks,"rel","nofollow"),d(No,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Lo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(qo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Ko,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Jt,"id","diffusers.PNDMScheduler"),d(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Jt,"href","#diffusers.PNDMScheduler"),d(tt,"class","relative group"),d(Hs,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(Hs,"rel","nofollow"),d(Ro,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Qo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(jo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Wo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(zs,"href","https://arxiv.org/abs/2202.09778"),d(zs,"rel","nofollow"),d(Zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(er,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(sr,"id","diffusers.ScoreSdeVeScheduler"),d(sr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(sr,"href","#diffusers.ScoreSdeVeScheduler"),d(rt,"class","relative group"),d(rn,"href","https://arxiv.org/abs/2011.13456"),d(rn,"rel","nofollow"),d(nn,"href","https://arxiv.org/abs/2011.13456"),d(nn,"rel","nofollow"),d(Ho,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Go,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(zo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Yo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(or,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ir,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ar,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(lr,"id","diffusers.IPNDMScheduler"),d(lr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lr,"href","#diffusers.IPNDMScheduler"),d(nt,"class","relative group"),d(fn,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(fn,"rel","nofollow"),d(hn,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(hn,"rel","nofollow"),d(Xo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Zo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ei,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ti,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(mn,"href","https://arxiv.org/abs/2202.09778"),d(mn,"rel","nofollow"),d(ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(hr,"id","diffusers.schedulers.ScoreSdeVpScheduler"),d(hr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(hr,"href","#diffusers.schedulers.ScoreSdeVpScheduler"),d(ot,"class","relative group"),d(Sn,"href","https://arxiv.org/abs/2011.13456"),d(Sn,"rel","nofollow"),d(si,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ni,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(oi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ii,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(xn,"href","https://arxiv.org/abs/2011.13456"),d(xn,"rel","nofollow"),d(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_r,"id","diffusers.EulerDiscreteScheduler"),d(_r,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_r,"href","#diffusers.EulerDiscreteScheduler"),d(it,"class","relative group"),d(En,"href","https://arxiv.org/abs/2206.00364"),d(En,"rel","nofollow"),d(yn,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(yn,"rel","nofollow"),d(Mn,"href","https://arxiv.org/abs/2206.00364"),d(Mn,"rel","nofollow"),d(Pn,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(Pn,"rel","nofollow"),d(di,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(li,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ci,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ui,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(br,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(xr,"id","diffusers.EulerAncestralDiscreteScheduler"),d(xr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xr,"href","#diffusers.EulerAncestralDiscreteScheduler"),d(at,"class","relative group"),d(Fn,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),d(Fn,"rel","nofollow"),d(hi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(mi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(gi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(_i,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d($r,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Er,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(yr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(wr,"id","diffusers.VQDiffusionScheduler"),d(wr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wr,"href","#diffusers.VQDiffusionScheduler"),d(dt,"class","relative group"),d(Qn,"href","https://arxiv.org/abs/2111.14822"),d(Qn,"rel","nofollow"),d(vi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(bi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Si,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Di,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Wn,"href","https://arxiv.org/abs/2111.14822"),d(Wn,"rel","nofollow"),d(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Tr,"id","diffusers.RePaintScheduler"),d(Tr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Tr,"href","#diffusers.RePaintScheduler"),d(ct,"class","relative group"),d($i,"href","/docs/diffusers/main/en/api/pipelines/repaint#diffusers.RePaintPipeline"),d(eo,"href","https://arxiv.org/abs/2201.09865"),d(eo,"rel","nofollow"),d(to,"href","https://github.com/andreas128/RePaint"),d(to,"rel","nofollow"),d(Ei,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(yi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(wi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Mi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(so,"href","https://arxiv.org/pdf/2201.09865.pdf"),d(so,"rel","nofollow"),d(Cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(kr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,u){e(document.head,Z),f(r,ut,u),f(r,ee,u),e(ee,De),e(De,Ri),m(Ir,Ri,null),e(ee,Kf),e(ee,Qi),e(Qi,Rf),f(r,nc,u),f(r,lo,u),e(lo,Qf),f(r,oc,u),f(r,qe,u),e(qe,ft),e(ft,ji),m(Nr,ji,null),e(qe,jf),e(qe,Wi),e(Wi,Wf),f(r,ic,u),f(r,Ce,u),e(Ce,Uf),e(Ce,Ui),e(Ui,Bf),e(Ce,Hf),e(Ce,Bi),e(Bi,Gf),e(Ce,zf),f(r,ac,u),f(r,pt,u),e(pt,co),e(co,Yf),e(co,Lr),e(Lr,Hi),e(Hi,Jf),e(Lr,Xf),e(Lr,Gi),e(Gi,Zf),e(pt,ep),e(pt,Ke),e(Ke,tp),e(Ke,zi),e(zi,rp),e(Ke,sp),e(Ke,Yi),e(Yi,np),e(Ke,op),f(r,dc,u),f(r,Re,u),e(Re,ht),e(ht,Ji),m(qr,Ji,null),e(Re,ip),e(Re,Xi),e(Xi,ap),f(r,lc,u),f(r,O,u),e(O,dp),e(O,Zi),e(Zi,lp),e(O,cp),e(O,uo),e(uo,up),e(O,fp),e(O,fo),e(fo,pp),e(O,hp),e(O,ea),e(ea,mp),e(O,gp),e(O,po),e(po,_p),e(O,vp),e(O,ta),e(ta,bp),e(O,Sp),f(r,cc,u),f(r,Qe,u),e(Qe,mt),e(mt,ra),m(Kr,ra,null),e(Qe,Dp),e(Qe,sa),e(sa,xp),f(r,uc,u),f(r,ho,u),e(ho,$p),f(r,fc,u),f(r,gt,u),e(gt,na),e(na,Ep),e(gt,yp),e(gt,oa),e(oa,wp),f(r,pc,u),f(r,je,u),e(je,_t),e(_t,ia),m(Rr,ia,null),e(je,Mp),e(je,aa),e(aa,Pp),f(r,hc,u),f(r,mo,u),e(mo,Tp),f(r,mc,u),f(r,ke,u),e(ke,Qr),e(Qr,Cp),e(Qr,da),e(da,kp),e(Qr,Op),e(ke,Ap),e(ke,jr),e(jr,Vp),e(jr,la),e(la,Fp),e(jr,Ip),e(ke,Np),e(ke,ca),e(ca,Lp),f(r,gc,u),f(r,vt,u),e(vt,qp),e(vt,go),e(go,Kp),e(vt,Rp),f(r,_c,u),f(r,We,u),e(We,bt),e(bt,ua),m(Wr,ua,null),e(We,Qp),e(We,fa),e(fa,jp),f(r,vc,u),f(r,Ue,u),m(Ur,Ue,null),e(Ue,Wp),e(Ue,pa),e(pa,Up),f(r,bc,u),f(r,Be,u),e(Be,St),e(St,ha),m(Br,ha,null),e(Be,Bp),e(Be,ma),e(ma,Hp),f(r,Sc,u),f(r,He,u),m(Hr,He,null),e(He,Gp),e(He,ga),e(ga,zp),f(r,Dc,u),f(r,Ge,u),e(Ge,Dt),e(Dt,_a),m(Gr,_a,null),e(Ge,Yp),e(Ge,va),e(va,Jp),f(r,xc,u),f(r,ze,u),e(ze,xt),e(xt,ba),m(zr,ba,null),e(ze,Xp),e(ze,Sa),e(Sa,Zp),f(r,$c,u),f(r,_o,u),e(_o,eh),f(r,Ec,u),f(r,P,u),m(Yr,P,null),e(P,th),e(P,Da),e(Da,rh),e(P,sh),e(P,A),e(A,vo),e(vo,nh),e(A,oh),e(A,xa),e(xa,ih),e(A,ah),e(A,$a),e($a,dh),e(A,lh),e(A,Ea),e(Ea,ch),e(A,uh),e(A,bo),e(bo,fh),e(A,ph),e(A,So),e(So,hh),e(A,mh),e(A,Do),e(Do,gh),e(A,_h),e(P,vh),e(P,xo),e(xo,bh),e(xo,Jr),e(Jr,Sh),e(P,Dh),e(P,$t),m(Xr,$t,null),e($t,xh),e($t,ya),e(ya,$h),e(P,Eh),e(P,Et),m(Zr,Et,null),e(Et,yh),e(Et,wa),e(wa,wh),e(P,Mh),e(P,yt),m(es,yt,null),e(yt,Ph),e(yt,Ma),e(Ma,Th),f(r,yc,u),f(r,Ye,u),e(Ye,wt),e(wt,Pa),m(ts,Pa,null),e(Ye,Ch),e(Ye,Ta),e(Ta,kh),f(r,wc,u),f(r,Mt,u),e(Mt,Oh),e(Mt,rs),e(rs,Ah),e(Mt,Vh),f(r,Mc,u),f(r,T,u),m(ss,T,null),e(T,Fh),e(T,Ca),e(Ca,Ih),e(T,Nh),e(T,V),e(V,$o),e($o,Lh),e(V,qh),e(V,ka),e(ka,Kh),e(V,Rh),e(V,Oa),e(Oa,Qh),e(V,jh),e(V,Aa),e(Aa,Wh),e(V,Uh),e(V,Eo),e(Eo,Bh),e(V,Hh),e(V,yo),e(yo,Gh),e(V,zh),e(V,wo),e(wo,Yh),e(V,Jh),e(T,Xh),e(T,Mo),e(Mo,Zh),e(Mo,ns),e(ns,em),e(T,tm),e(T,Pt),m(os,Pt,null),e(Pt,rm),e(Pt,Va),e(Va,sm),e(T,nm),e(T,Tt),m(is,Tt,null),e(Tt,om),e(Tt,Fa),e(Fa,im),e(T,am),e(T,Ct),m(as,Ct,null),e(Ct,dm),e(Ct,Ia),e(Ia,lm),f(r,Pc,u),f(r,Je,u),e(Je,kt),e(kt,Na),m(ds,Na,null),e(Je,cm),e(Je,La),e(La,um),f(r,Tc,u),f(r,xe,u),e(xe,fm),e(xe,ls),e(ls,pm),e(xe,hm),e(xe,cs),e(cs,mm),e(xe,gm),e(xe,us),e(us,_m),e(xe,vm),f(r,Cc,u),f(r,S,u),m(fs,S,null),e(S,bm),e(S,qa),e(qa,Sm),e(S,Dm),e(S,Ot),e(Ot,xm),e(Ot,ps),e(ps,$m),e(Ot,Em),e(Ot,hs),e(hs,ym),e(S,wm),e(S,Xe),e(Xe,Mm),e(Xe,Ka),e(Ka,Pm),e(Xe,Tm),e(Xe,Ra),e(Ra,Cm),e(Xe,km),e(S,Om),e(S,Me),e(Me,Am),e(Me,ms),e(ms,Vm),e(Me,Fm),e(Me,Qa),e(Qa,Im),e(Me,Nm),e(Me,ja),e(ja,Lm),e(Me,qm),e(S,Km),e(S,F),e(F,Po),e(Po,Rm),e(F,Qm),e(F,Wa),e(Wa,jm),e(F,Wm),e(F,Ua),e(Ua,Um),e(F,Bm),e(F,Ba),e(Ba,Hm),e(F,Gm),e(F,To),e(To,zm),e(F,Ym),e(F,Co),e(Co,Jm),e(F,Xm),e(F,ko),e(ko,Zm),e(F,eg),e(S,tg),e(S,$e),m(gs,$e,null),e($e,rg),e($e,Ha),e(Ha,sg),e($e,ng),e($e,Ga),e(Ga,og),e($e,ig),e($e,za),e(za,ag),e(S,dg),e(S,Oe),m(_s,Oe,null),e(Oe,lg),e(Oe,Ya),e(Ya,cg),e(Oe,ug),e(Oe,vs),e(vs,fg),e(vs,bs),e(bs,pg),e(vs,hg),e(S,mg),e(S,At),m(Ss,At,null),e(At,gg),e(At,Ja),e(Ja,_g),e(S,vg),e(S,Vt),m(Ds,Vt,null),e(Vt,bg),e(Vt,Xa),e(Xa,Sg),e(S,Dg),e(S,Ft),m(xs,Ft,null),e(Ft,xg),e(Ft,Za),e(Za,$g),e(S,Eg),e(S,It),m($s,It,null),e(It,yg),e(It,ed),e(ed,wg),e(S,Mg),e(S,Nt),m(Es,Nt,null),e(Nt,Pg),e(Nt,td),e(td,Tg),f(r,kc,u),f(r,Ze,u),e(Ze,Lt),e(Lt,rd),m(ys,rd,null),e(Ze,Cg),e(Ze,sd),e(sd,kg),f(r,Oc,u),f(r,qt,u),e(qt,Og),e(qt,ws),e(ws,Ag),e(qt,Vg),f(r,Ac,u),f(r,x,u),m(Ms,x,null),e(x,Fg),e(x,nd),e(nd,Ig),e(x,Ng),e(x,Kt),e(Kt,Lg),e(Kt,Ps),e(Ps,qg),e(Kt,Kg),e(Kt,Ts),e(Ts,Rg),e(x,Qg),e(x,I),e(I,Oo),e(Oo,jg),e(I,Wg),e(I,od),e(od,Ug),e(I,Bg),e(I,id),e(id,Hg),e(I,Gg),e(I,ad),e(ad,zg),e(I,Yg),e(I,Ao),e(Ao,Jg),e(I,Xg),e(I,Vo),e(Vo,Zg),e(I,e_),e(I,Fo),e(Fo,t_),e(I,r_),e(x,s_),e(x,Cs),e(Cs,n_),e(Cs,ks),e(ks,o_),e(Cs,i_),e(x,a_),e(x,Ae),m(Os,Ae,null),e(Ae,d_),e(Ae,dd),e(dd,l_),e(Ae,c_),e(Ae,ld),e(ld,u_),e(x,f_),e(x,Rt),m(As,Rt,null),e(Rt,p_),e(Rt,cd),e(cd,h_),e(x,m_),e(x,Qt),m(Vs,Qt,null),e(Qt,g_),e(Qt,ud),e(ud,__),e(x,v_),e(x,jt),m(Fs,jt,null),e(jt,b_),e(jt,fd),e(fd,S_),e(x,D_),e(x,Wt),m(Is,Wt,null),e(Wt,x_),e(Wt,pd),e(pd,$_),f(r,Vc,u),f(r,et,u),e(et,Ut),e(Ut,hd),m(Ns,hd,null),e(et,E_),e(et,md),e(md,y_),f(r,Fc,u),f(r,Bt,u),e(Bt,w_),e(Bt,Ls),e(Ls,M_),e(Bt,P_),f(r,Ic,u),f(r,C,u),m(qs,C,null),e(C,T_),e(C,Io),e(Io,C_),e(Io,Ks),e(Ks,k_),e(C,O_),e(C,N),e(N,No),e(No,A_),e(N,V_),e(N,gd),e(gd,F_),e(N,I_),e(N,_d),e(_d,N_),e(N,L_),e(N,vd),e(vd,q_),e(N,K_),e(N,Lo),e(Lo,R_),e(N,Q_),e(N,qo),e(qo,j_),e(N,W_),e(N,Ko),e(Ko,U_),e(N,B_),e(C,H_),e(C,Ht),m(Rs,Ht,null),e(Ht,G_),e(Ht,bd),e(bd,z_),e(C,Y_),e(C,Gt),m(Qs,Gt,null),e(Gt,J_),e(Gt,js),e(js,X_),e(js,Sd),e(Sd,Z_),e(js,ev),e(C,tv),e(C,zt),m(Ws,zt,null),e(zt,rv),e(zt,Dd),e(Dd,sv),e(C,nv),e(C,Yt),m(Us,Yt,null),e(Yt,ov),e(Yt,xd),e(xd,iv),f(r,Nc,u),f(r,tt,u),e(tt,Jt),e(Jt,$d),m(Bs,$d,null),e(tt,av),e(tt,Ed),e(Ed,dv),f(r,Lc,u),f(r,Xt,u),e(Xt,lv),e(Xt,Hs),e(Hs,cv),e(Xt,uv),f(r,qc,u),f(r,$,u),m(Gs,$,null),e($,fv),e($,yd),e(yd,pv),e($,hv),e($,L),e(L,Ro),e(Ro,mv),e(L,gv),e(L,wd),e(wd,_v),e(L,vv),e(L,Md),e(Md,bv),e(L,Sv),e(L,Pd),e(Pd,Dv),e(L,xv),e(L,Qo),e(Qo,$v),e(L,Ev),e(L,jo),e(jo,yv),e(L,wv),e(L,Wo),e(Wo,Mv),e(L,Pv),e($,Tv),e($,Uo),e(Uo,Cv),e(Uo,zs),e(zs,kv),e($,Ov),e($,Zt),m(Ys,Zt,null),e(Zt,Av),e(Zt,Td),e(Td,Vv),e($,Fv),e($,er),m(Js,er,null),e(er,Iv),e(er,Cd),e(Cd,Nv),e($,Lv),e($,Ve),m(Xs,Ve,null),e(Ve,qv),e(Ve,kd),e(kd,Kv),e(Ve,Rv),e(Ve,Pe),e(Pe,Qv),e(Pe,Od),e(Od,jv),e(Pe,Wv),e(Pe,Ad),e(Ad,Uv),e(Pe,Bv),e(Pe,Vd),e(Vd,Hv),e(Pe,Gv),e($,zv),e($,tr),m(Zs,tr,null),e(tr,Yv),e(tr,Fd),e(Fd,Jv),e($,Xv),e($,rr),m(en,rr,null),e(rr,Zv),e(rr,Id),e(Id,eb),f(r,Kc,u),f(r,rt,u),e(rt,sr),e(sr,Nd),m(tn,Nd,null),e(rt,tb),e(rt,Ld),e(Ld,rb),f(r,Rc,u),f(r,nr,u),e(nr,sb),e(nr,rn),e(rn,nb),e(nr,ob),f(r,Qc,u),f(r,E,u),m(sn,E,null),e(E,ib),e(E,qd),e(qd,ab),e(E,db),e(E,Bo),e(Bo,lb),e(Bo,nn),e(nn,cb),e(E,ub),e(E,q),e(q,Ho),e(Ho,fb),e(q,pb),e(q,Kd),e(Kd,hb),e(q,mb),e(q,Rd),e(Rd,gb),e(q,_b),e(q,Qd),e(Qd,vb),e(q,bb),e(q,Go),e(Go,Sb),e(q,Db),e(q,zo),e(zo,xb),e(q,$b),e(q,Yo),e(Yo,Eb),e(q,yb),e(E,wb),e(E,or),m(on,or,null),e(or,Mb),e(or,jd),e(jd,Pb),e(E,Tb),e(E,Fe),m(an,Fe,null),e(Fe,Cb),e(Fe,Wd),e(Wd,kb),e(Fe,Ob),e(Fe,st),e(st,Ab),e(st,Ud),e(Ud,Vb),e(st,Fb),e(st,Bd),e(Bd,Ib),e(st,Nb),e(E,Lb),e(E,ir),m(dn,ir,null),e(ir,qb),e(ir,Hd),e(Hd,Kb),e(E,Rb),e(E,ar),m(ln,ar,null),e(ar,Qb),e(ar,Gd),e(Gd,jb),e(E,Wb),e(E,dr),m(cn,dr,null),e(dr,Ub),e(dr,zd),e(zd,Bb),f(r,jc,u),f(r,nt,u),e(nt,lr),e(lr,Yd),m(un,Yd,null),e(nt,Hb),e(nt,Jd),e(Jd,Gb),f(r,Wc,u),f(r,cr,u),e(cr,zb),e(cr,fn),e(fn,Yb),e(cr,Jb),f(r,Uc,u),f(r,k,u),m(pn,k,null),e(k,Xb),e(k,Jo),e(Jo,Zb),e(Jo,hn),e(hn,e1),e(k,t1),e(k,K),e(K,Xo),e(Xo,r1),e(K,s1),e(K,Xd),e(Xd,n1),e(K,o1),e(K,Zd),e(Zd,i1),e(K,a1),e(K,el),e(el,d1),e(K,l1),e(K,Zo),e(Zo,c1),e(K,u1),e(K,ei),e(ei,f1),e(K,p1),e(K,ti),e(ti,h1),e(K,m1),e(k,g1),e(k,ri),e(ri,_1),e(ri,mn),e(mn,v1),e(k,b1),e(k,ur),m(gn,ur,null),e(ur,S1),e(ur,tl),e(tl,D1),e(k,x1),e(k,fr),m(_n,fr,null),e(fr,$1),e(fr,rl),e(rl,E1),e(k,y1),e(k,pr),m(vn,pr,null),e(pr,w1),e(pr,sl),e(sl,M1),f(r,Bc,u),f(r,ot,u),e(ot,hr),e(hr,nl),m(bn,nl,null),e(ot,P1),e(ot,ol),e(ol,T1),f(r,Hc,u),f(r,mr,u),e(mr,C1),e(mr,Sn),e(Sn,k1),e(mr,O1),f(r,Gc,u),m(gr,r,u),f(r,zc,u),f(r,te,u),m(Dn,te,null),e(te,A1),e(te,il),e(il,V1),e(te,F1),e(te,R),e(R,si),e(si,I1),e(R,N1),e(R,al),e(al,L1),e(R,q1),e(R,dl),e(dl,K1),e(R,R1),e(R,ll),e(ll,Q1),e(R,j1),e(R,ni),e(ni,W1),e(R,U1),e(R,oi),e(oi,B1),e(R,H1),e(R,ii),e(ii,G1),e(R,z1),e(te,Y1),e(te,ai),e(ai,J1),e(ai,xn),e(xn,X1),e(te,Z1),e(te,cl),e(cl,e0),f(r,Yc,u),f(r,it,u),e(it,_r),e(_r,ul),m($n,ul,null),e(it,t0),e(it,fl),e(fl,r0),f(r,Jc,u),f(r,Ie,u),e(Ie,s0),e(Ie,En),e(En,n0),e(Ie,o0),e(Ie,yn),e(yn,i0),e(Ie,a0),f(r,Xc,u),f(r,Y,u),m(wn,Y,null),e(Y,d0),e(Y,vr),e(vr,l0),e(vr,Mn),e(Mn,c0),e(vr,u0),e(vr,Pn),e(Pn,f0),e(Y,p0),e(Y,Q),e(Q,di),e(di,h0),e(Q,m0),e(Q,pl),e(pl,g0),e(Q,_0),e(Q,hl),e(hl,v0),e(Q,b0),e(Q,ml),e(ml,S0),e(Q,D0),e(Q,li),e(li,x0),e(Q,$0),e(Q,ci),e(ci,E0),e(Q,y0),e(Q,ui),e(ui,w0),e(Q,M0),e(Y,P0),e(Y,br),m(Tn,br,null),e(br,T0),e(br,Cn),e(Cn,C0),e(Cn,gl),e(gl,k0),e(Cn,O0),e(Y,A0),e(Y,Sr),m(kn,Sr,null),e(Sr,V0),e(Sr,_l),e(_l,F0),e(Y,I0),e(Y,Dr),m(On,Dr,null),e(Dr,N0),e(Dr,vl),e(vl,L0),f(r,Zc,u),f(r,at,u),e(at,xr),e(xr,bl),m(An,bl,null),e(at,q0),e(at,Sl),e(Sl,K0),f(r,eu,u),f(r,fi,u),e(fi,R0),f(r,tu,u),f(r,J,u),m(Vn,J,null),e(J,Q0),e(J,pi),e(pi,j0),e(pi,Fn),e(Fn,W0),e(J,U0),e(J,j),e(j,hi),e(hi,B0),e(j,H0),e(j,Dl),e(Dl,G0),e(j,z0),e(j,xl),e(xl,Y0),e(j,J0),e(j,$l),e($l,X0),e(j,Z0),e(j,mi),e(mi,e2),e(j,t2),e(j,gi),e(gi,r2),e(j,s2),e(j,_i),e(_i,n2),e(j,o2),e(J,i2),e(J,$r),m(In,$r,null),e($r,a2),e($r,Nn),e(Nn,d2),e(Nn,El),e(El,l2),e(Nn,c2),e(J,u2),e(J,Er),m(Ln,Er,null),e(Er,f2),e(Er,yl),e(yl,p2),e(J,h2),e(J,yr),m(qn,yr,null),e(yr,m2),e(yr,wl),e(wl,g2),f(r,ru,u),f(r,dt,u),e(dt,wr),e(wr,Ml),m(Kn,Ml,null),e(dt,_2),e(dt,Pl),e(Pl,v2),f(r,su,u),f(r,Rn,u),e(Rn,b2),e(Rn,Qn),e(Qn,S2),f(r,nu,u),f(r,y,u),m(jn,y,null),e(y,D2),e(y,Tl),e(Tl,x2),e(y,$2),e(y,Cl),e(Cl,E2),e(y,y2),e(y,W),e(W,vi),e(vi,w2),e(W,M2),e(W,kl),e(kl,P2),e(W,T2),e(W,Ol),e(Ol,C2),e(W,k2),e(W,Al),e(Al,O2),e(W,A2),e(W,bi),e(bi,V2),e(W,F2),e(W,Si),e(Si,I2),e(W,N2),e(W,Di),e(Di,L2),e(W,q2),e(y,K2),e(y,xi),e(xi,R2),e(xi,Wn),e(Wn,Q2),e(y,j2),e(y,Ne),m(Un,Ne,null),e(Ne,W2),e(Ne,Bn),e(Bn,U2),e(Bn,Vl),e(Vl,B2),e(Bn,H2),e(Ne,G2),e(Ne,Fl),e(Fl,z2),e(y,Y2),e(y,U),m(Hn,U,null),e(U,J2),e(U,Gn),e(Gn,X2),e(Gn,Il),e(Il,Z2),e(Gn,e4),e(U,t4),e(U,Nl),e(Nl,r4),e(U,s4),e(U,Ll),e(Ll,n4),e(U,o4),e(U,ql),e(ql,i4),e(U,a4),e(U,Kl),e(Kl,zn),e(zn,d4),e(zn,Rl),e(Rl,l4),e(zn,c4),e(U,u4),e(U,lt),e(lt,f4),e(lt,Ql),e(Ql,p4),e(lt,h4),e(lt,jl),e(jl,m4),e(lt,g4),e(y,_4),e(y,Mr),m(Yn,Mr,null),e(Mr,v4),e(Mr,Wl),e(Wl,b4),e(y,S4),e(y,Pr),m(Jn,Pr,null),e(Pr,D4),e(Pr,Xn),e(Xn,x4),e(Xn,Ul),e(Ul,$4),e(Xn,E4),f(r,ou,u),f(r,ct,u),e(ct,Tr),e(Tr,Bl),m(Zn,Bl,null),e(ct,y4),e(ct,Hl),e(Hl,w4),f(r,iu,u),f(r,Te,u),e(Te,M4),e(Te,$i),e($i,P4),e(Te,T4),e(Te,eo),e(eo,C4),e(Te,k4),e(Te,to),e(to,O4),f(r,au,u),f(r,X,u),m(ro,X,null),e(X,A4),e(X,Gl),e(Gl,V4),e(X,F4),e(X,B),e(B,Ei),e(Ei,I4),e(B,N4),e(B,zl),e(zl,L4),e(B,q4),e(B,Yl),e(Yl,K4),e(B,R4),e(B,Jl),e(Jl,Q4),e(B,j4),e(B,yi),e(yi,W4),e(B,U4),e(B,wi),e(wi,B4),e(B,H4),e(B,Mi),e(Mi,G4),e(B,z4),e(X,Y4),e(X,Pi),e(Pi,J4),e(Pi,so),e(so,X4),e(X,Z4),e(X,Cr),m(no,Cr,null),e(Cr,eS),e(Cr,Xl),e(Xl,tS),e(X,rS),e(X,kr),m(oo,kr,null),e(kr,sS),e(kr,Zl),e(Zl,nS),du=!0},p(r,[u]){const io={};u&2&&(io.$$scope={dirty:u,ctx:r}),gr.$set(io)},i(r){du||(g(Ir.$$.fragment,r),g(Nr.$$.fragment,r),g(qr.$$.fragment,r),g(Kr.$$.fragment,r),g(Rr.$$.fragment,r),g(Wr.$$.fragment,r),g(Ur.$$.fragment,r),g(Br.$$.fragment,r),g(Hr.$$.fragment,r),g(Gr.$$.fragment,r),g(zr.$$.fragment,r),g(Yr.$$.fragment,r),g(Xr.$$.fragment,r),g(Zr.$$.fragment,r),g(es.$$.fragment,r),g(ts.$$.fragment,r),g(ss.$$.fragment,r),g(os.$$.fragment,r),g(is.$$.fragment,r),g(as.$$.fragment,r),g(ds.$$.fragment,r),g(fs.$$.fragment,r),g(gs.$$.fragment,r),g(_s.$$.fragment,r),g(Ss.$$.fragment,r),g(Ds.$$.fragment,r),g(xs.$$.fragment,r),g($s.$$.fragment,r),g(Es.$$.fragment,r),g(ys.$$.fragment,r),g(Ms.$$.fragment,r),g(Os.$$.fragment,r),g(As.$$.fragment,r),g(Vs.$$.fragment,r),g(Fs.$$.fragment,r),g(Is.$$.fragment,r),g(Ns.$$.fragment,r),g(qs.$$.fragment,r),g(Rs.$$.fragment,r),g(Qs.$$.fragment,r),g(Ws.$$.fragment,r),g(Us.$$.fragment,r),g(Bs.$$.fragment,r),g(Gs.$$.fragment,r),g(Ys.$$.fragment,r),g(Js.$$.fragment,r),g(Xs.$$.fragment,r),g(Zs.$$.fragment,r),g(en.$$.fragment,r),g(tn.$$.fragment,r),g(sn.$$.fragment,r),g(on.$$.fragment,r),g(an.$$.fragment,r),g(dn.$$.fragment,r),g(ln.$$.fragment,r),g(cn.$$.fragment,r),g(un.$$.fragment,r),g(pn.$$.fragment,r),g(gn.$$.fragment,r),g(_n.$$.fragment,r),g(vn.$$.fragment,r),g(bn.$$.fragment,r),g(gr.$$.fragment,r),g(Dn.$$.fragment,r),g($n.$$.fragment,r),g(wn.$$.fragment,r),g(Tn.$$.fragment,r),g(kn.$$.fragment,r),g(On.$$.fragment,r),g(An.$$.fragment,r),g(Vn.$$.fragment,r),g(In.$$.fragment,r),g(Ln.$$.fragment,r),g(qn.$$.fragment,r),g(Kn.$$.fragment,r),g(jn.$$.fragment,r),g(Un.$$.fragment,r),g(Hn.$$.fragment,r),g(Yn.$$.fragment,r),g(Jn.$$.fragment,r),g(Zn.$$.fragment,r),g(ro.$$.fragment,r),g(no.$$.fragment,r),g(oo.$$.fragment,r),du=!0)},o(r){_(Ir.$$.fragment,r),_(Nr.$$.fragment,r),_(qr.$$.fragment,r),_(Kr.$$.fragment,r),_(Rr.$$.fragment,r),_(Wr.$$.fragment,r),_(Ur.$$.fragment,r),_(Br.$$.fragment,r),_(Hr.$$.fragment,r),_(Gr.$$.fragment,r),_(zr.$$.fragment,r),_(Yr.$$.fragment,r),_(Xr.$$.fragment,r),_(Zr.$$.fragment,r),_(es.$$.fragment,r),_(ts.$$.fragment,r),_(ss.$$.fragment,r),_(os.$$.fragment,r),_(is.$$.fragment,r),_(as.$$.fragment,r),_(ds.$$.fragment,r),_(fs.$$.fragment,r),_(gs.$$.fragment,r),_(_s.$$.fragment,r),_(Ss.$$.fragment,r),_(Ds.$$.fragment,r),_(xs.$$.fragment,r),_($s.$$.fragment,r),_(Es.$$.fragment,r),_(ys.$$.fragment,r),_(Ms.$$.fragment,r),_(Os.$$.fragment,r),_(As.$$.fragment,r),_(Vs.$$.fragment,r),_(Fs.$$.fragment,r),_(Is.$$.fragment,r),_(Ns.$$.fragment,r),_(qs.$$.fragment,r),_(Rs.$$.fragment,r),_(Qs.$$.fragment,r),_(Ws.$$.fragment,r),_(Us.$$.fragment,r),_(Bs.$$.fragment,r),_(Gs.$$.fragment,r),_(Ys.$$.fragment,r),_(Js.$$.fragment,r),_(Xs.$$.fragment,r),_(Zs.$$.fragment,r),_(en.$$.fragment,r),_(tn.$$.fragment,r),_(sn.$$.fragment,r),_(on.$$.fragment,r),_(an.$$.fragment,r),_(dn.$$.fragment,r),_(ln.$$.fragment,r),_(cn.$$.fragment,r),_(un.$$.fragment,r),_(pn.$$.fragment,r),_(gn.$$.fragment,r),_(_n.$$.fragment,r),_(vn.$$.fragment,r),_(bn.$$.fragment,r),_(gr.$$.fragment,r),_(Dn.$$.fragment,r),_($n.$$.fragment,r),_(wn.$$.fragment,r),_(Tn.$$.fragment,r),_(kn.$$.fragment,r),_(On.$$.fragment,r),_(An.$$.fragment,r),_(Vn.$$.fragment,r),_(In.$$.fragment,r),_(Ln.$$.fragment,r),_(qn.$$.fragment,r),_(Kn.$$.fragment,r),_(jn.$$.fragment,r),_(Un.$$.fragment,r),_(Hn.$$.fragment,r),_(Yn.$$.fragment,r),_(Jn.$$.fragment,r),_(Zn.$$.fragment,r),_(ro.$$.fragment,r),_(no.$$.fragment,r),_(oo.$$.fragment,r),du=!1},d(r){t(Z),r&&t(ut),r&&t(ee),v(Ir),r&&t(nc),r&&t(lo),r&&t(oc),r&&t(qe),v(Nr),r&&t(ic),r&&t(Ce),r&&t(ac),r&&t(pt),r&&t(dc),r&&t(Re),v(qr),r&&t(lc),r&&t(O),r&&t(cc),r&&t(Qe),v(Kr),r&&t(uc),r&&t(ho),r&&t(fc),r&&t(gt),r&&t(pc),r&&t(je),v(Rr),r&&t(hc),r&&t(mo),r&&t(mc),r&&t(ke),r&&t(gc),r&&t(vt),r&&t(_c),r&&t(We),v(Wr),r&&t(vc),r&&t(Ue),v(Ur),r&&t(bc),r&&t(Be),v(Br),r&&t(Sc),r&&t(He),v(Hr),r&&t(Dc),r&&t(Ge),v(Gr),r&&t(xc),r&&t(ze),v(zr),r&&t($c),r&&t(_o),r&&t(Ec),r&&t(P),v(Yr),v(Xr),v(Zr),v(es),r&&t(yc),r&&t(Ye),v(ts),r&&t(wc),r&&t(Mt),r&&t(Mc),r&&t(T),v(ss),v(os),v(is),v(as),r&&t(Pc),r&&t(Je),v(ds),r&&t(Tc),r&&t(xe),r&&t(Cc),r&&t(S),v(fs),v(gs),v(_s),v(Ss),v(Ds),v(xs),v($s),v(Es),r&&t(kc),r&&t(Ze),v(ys),r&&t(Oc),r&&t(qt),r&&t(Ac),r&&t(x),v(Ms),v(Os),v(As),v(Vs),v(Fs),v(Is),r&&t(Vc),r&&t(et),v(Ns),r&&t(Fc),r&&t(Bt),r&&t(Ic),r&&t(C),v(qs),v(Rs),v(Qs),v(Ws),v(Us),r&&t(Nc),r&&t(tt),v(Bs),r&&t(Lc),r&&t(Xt),r&&t(qc),r&&t($),v(Gs),v(Ys),v(Js),v(Xs),v(Zs),v(en),r&&t(Kc),r&&t(rt),v(tn),r&&t(Rc),r&&t(nr),r&&t(Qc),r&&t(E),v(sn),v(on),v(an),v(dn),v(ln),v(cn),r&&t(jc),r&&t(nt),v(un),r&&t(Wc),r&&t(cr),r&&t(Uc),r&&t(k),v(pn),v(gn),v(_n),v(vn),r&&t(Bc),r&&t(ot),v(bn),r&&t(Hc),r&&t(mr),r&&t(Gc),v(gr,r),r&&t(zc),r&&t(te),v(Dn),r&&t(Yc),r&&t(it),v($n),r&&t(Jc),r&&t(Ie),r&&t(Xc),r&&t(Y),v(wn),v(Tn),v(kn),v(On),r&&t(Zc),r&&t(at),v(An),r&&t(eu),r&&t(fi),r&&t(tu),r&&t(J),v(Vn),v(In),v(Ln),v(qn),r&&t(ru),r&&t(dt),v(Kn),r&&t(su),r&&t(Rn),r&&t(nu),r&&t(y),v(jn),v(Un),v(Hn),v(Yn),v(Jn),r&&t(ou),r&&t(ct),v(Zn),r&&t(iu),r&&t(Te),r&&t(au),r&&t(X),v(ro),v(no),v(oo)}}}const z5={local:"schedulers",sections:[{local:"what-is-a-scheduler",sections:[{local:"discrete-versus-continuous-schedulers",title:"Discrete versus continuous schedulers"}],title:"What is a scheduler?"},{local:"designing-reusable-schedulers",title:"Designing Re-usable schedulers"},{local:"api",sections:[{local:"diffusers.SchedulerMixin",title:"SchedulerMixin"},{local:"diffusers.schedulers.scheduling_utils.SchedulerOutput",title:"SchedulerOutput"},{local:"implemented-schedulers",sections:[{local:"diffusers.DDIMScheduler",title:"Denoising diffusion implicit models (DDIM)"},{local:"diffusers.DDPMScheduler",title:"Denoising diffusion probabilistic models (DDPM)"},{local:"diffusers.DPMSolverMultistepScheduler",title:"Multistep DPM-Solver"},{local:"diffusers.KarrasVeScheduler",title:"Variance exploding, stochastic sampling from Karras et. al"},{local:"diffusers.LMSDiscreteScheduler",title:"Linear multistep scheduler for discrete beta schedules"},{local:"diffusers.PNDMScheduler",title:"Pseudo numerical methods for diffusion models (PNDM)"},{local:"diffusers.ScoreSdeVeScheduler",title:"variance exploding stochastic differential equation (VE-SDE) scheduler"},{local:"diffusers.IPNDMScheduler",title:"improved pseudo numerical methods for diffusion models (iPNDM)"},{local:"diffusers.schedulers.ScoreSdeVpScheduler",title:"variance preserving stochastic differential equation (VP-SDE) scheduler"},{local:"diffusers.EulerDiscreteScheduler",title:"Euler scheduler"},{local:"diffusers.EulerAncestralDiscreteScheduler",title:"Euler Ancestral scheduler"},{local:"diffusers.VQDiffusionScheduler",title:"VQDiffusionScheduler"},{local:"diffusers.RePaintScheduler",title:"RePaint scheduler"}],title:"Implemented Schedulers"}],title:"API"}],title:"Schedulers"};function Y5(sc){return U5(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ty extends R5{constructor(Z){super();Q5(this,Z,Y5,G5,j5,{})}}export{ty as default,z5 as metadata};
