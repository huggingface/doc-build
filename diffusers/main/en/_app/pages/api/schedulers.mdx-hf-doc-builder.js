import{S as A4,i as O4,s as V4,e as s,k as l,w as p,t as i,M as N4,c as n,d as r,m as c,a as o,x as h,h as a,b as d,G as e,g as f,y as m,q as g,o as _,B as v,v as F4}from"../../chunks/vendor-hf-doc-builder.js";import{T as I4}from"../../chunks/Tip-hf-doc-builder.js";import{D as b}from"../../chunks/Docstring-hf-doc-builder.js";import{I as M}from"../../chunks/IconCopyLink-hf-doc-builder.js";function L4(Ja){let G,We;return{c(){G=s("p"),We=i("Score SDE-VP is under construction.")},l(Y){G=n(Y,"P",{});var ue=o(G);We=a(ue,"Score SDE-VP is under construction."),ue.forEach(r)},m(Y,ue){f(Y,G,ue),e(G,We)},d(Y){Y&&r(G)}}}function K4(Ja){let G,We,Y,ue,bo,nr,fc,So,pc,ja,Zs,hc,Qa,Ee,Ge,$o,or,mc,Do,gc,Xa,Ye,_c,Eo,vc,bc,Za,ze,en,Sc,ir,xo,$c,Dc,yo,Ec,xc,xe,yc,wo,wc,Pc,Po,Mc,Tc,ed,ye,Je,Mo,ar,kc,To,Cc,td,T,Ac,ko,Oc,Vc,tn,Nc,Fc,rn,Ic,Lc,Co,Kc,Rc,sn,qc,Uc,Ao,Bc,Hc,rd,we,je,Oo,dr,Wc,Vo,Gc,sd,nn,Yc,nd,Qe,No,zc,Jc,Fo,jc,od,Pe,Xe,Io,lr,Qc,Lo,Xc,id,on,Zc,ad,_e,cr,eu,Ko,tu,ru,su,ur,nu,Ro,ou,iu,au,qo,du,dd,Ze,lu,an,cu,uu,ld,Me,et,Uo,fr,fu,Bo,pu,cd,Te,pr,hu,Ho,mu,ud,ke,tt,Wo,hr,gu,Go,_u,fd,Ce,mr,vu,Yo,bu,pd,Ae,rt,zo,gr,Su,Jo,$u,hd,Oe,st,jo,_r,Du,Qo,Eu,md,dn,xu,gd,x,vr,yu,Xo,wu,Pu,k,ln,Mu,Tu,Zo,ku,Cu,ei,Au,Ou,ti,Vu,Nu,cn,Fu,Iu,un,Lu,Ku,fn,Ru,qu,Uu,pn,Bu,br,Hu,Wu,nt,Sr,Gu,ri,Yu,zu,ot,$r,Ju,si,ju,Qu,it,Dr,Xu,ni,Zu,_d,Ve,at,oi,Er,ef,ii,tf,vd,dt,rf,xr,sf,nf,bd,y,yr,of,ai,af,df,C,hn,lf,cf,di,uf,ff,li,pf,hf,ci,mf,gf,mn,_f,vf,gn,bf,Sf,_n,$f,Df,Ef,vn,xf,wr,yf,wf,lt,Pr,Pf,ui,Mf,Tf,ct,Mr,kf,fi,Cf,Af,ut,Tr,Of,pi,Vf,Sd,Ne,ft,hi,kr,Nf,mi,Ff,$d,pt,If,Cr,Lf,Kf,Dd,S,Ar,Rf,gi,qf,Uf,ht,Bf,Or,Hf,Wf,Vr,Gf,Yf,A,bn,zf,Jf,_i,jf,Qf,vi,Xf,Zf,bi,ep,tp,Sn,rp,sp,$n,np,op,Dn,ip,ap,dp,Nr,lp,Fr,cp,up,fp,ve,Ir,pp,Si,hp,mp,$i,gp,_p,mt,Lr,vp,Di,bp,Sp,gt,Kr,$p,Ei,Dp,Ep,_t,Rr,xp,xi,yp,wp,vt,qr,Pp,yi,Mp,Ed,Fe,bt,wi,Ur,Tp,Pi,kp,xd,St,Cp,Br,Ap,Op,yd,w,Hr,Vp,En,Np,Wr,Fp,Ip,O,xn,Lp,Kp,Mi,Rp,qp,Ti,Up,Bp,ki,Hp,Wp,yn,Gp,Yp,wn,zp,Jp,Pn,jp,Qp,Xp,$t,Gr,Zp,Ci,eh,th,Dt,Yr,rh,zr,sh,Ai,nh,oh,ih,Et,Jr,ah,Oi,dh,lh,xt,jr,ch,Vi,uh,wd,Ie,yt,Ni,Qr,fh,Fi,ph,Pd,wt,hh,Xr,mh,gh,Md,$,Zr,_h,Ii,vh,bh,V,Mn,Sh,$h,Li,Dh,Eh,Ki,xh,yh,Ri,wh,Ph,Tn,Mh,Th,kn,kh,Ch,Cn,Ah,Oh,Vh,An,Nh,es,Fh,Ih,Pt,ts,Lh,qi,Kh,Rh,Mt,rs,qh,Ui,Uh,Bh,be,ss,Hh,Bi,Wh,Gh,me,Yh,Hi,zh,Jh,Wi,jh,Qh,Gi,Xh,Zh,em,Tt,ns,tm,Yi,rm,sm,kt,os,nm,zi,om,Td,Le,Ct,Ji,is,im,ji,am,kd,At,dm,as,lm,cm,Cd,D,ds,um,Qi,fm,pm,On,hm,ls,mm,gm,N,Vn,_m,vm,Xi,bm,Sm,Zi,$m,Dm,ea,Em,xm,Nn,ym,wm,Fn,Pm,Mm,In,Tm,km,Cm,Ot,cs,Am,ta,Om,Vm,Se,us,Nm,ra,Fm,Im,Ke,Lm,sa,Km,Rm,na,qm,Um,Bm,Vt,fs,Hm,oa,Wm,Gm,Nt,ps,Ym,ia,zm,Jm,Ft,hs,jm,aa,Qm,Ad,Re,It,da,ms,Xm,la,Zm,Od,Lt,eg,gs,tg,rg,Vd,P,_s,sg,Ln,ng,vs,og,ig,F,Kn,ag,dg,ca,lg,cg,ua,ug,fg,fa,pg,hg,Rn,mg,gg,qn,_g,vg,Un,bg,Sg,$g,Bn,Dg,bs,Eg,xg,Kt,Ss,yg,pa,wg,Pg,Rt,$s,Mg,ha,Tg,kg,qt,Ds,Cg,ma,Ag,Nd,qe,Ut,ga,Es,Og,_a,Vg,Fd,Bt,Ng,xs,Fg,Ig,Id,Ht,Ld,z,ys,Lg,va,Kg,Rg,I,Hn,qg,Ug,ba,Bg,Hg,Sa,Wg,Gg,$a,Yg,zg,Wn,Jg,jg,Gn,Qg,Xg,Yn,Zg,e_,t_,zn,r_,ws,s_,n_,Da,o_,Kd,Ue,Wt,Ea,Ps,i_,xa,a_,Rd,$e,d_,Ms,l_,c_,Ts,u_,f_,qd,B,ks,p_,Gt,h_,Cs,m_,g_,As,__,v_,L,Jn,b_,S_,ya,$_,D_,wa,E_,x_,Pa,y_,w_,jn,P_,M_,Qn,T_,k_,Xn,C_,A_,O_,Yt,Os,V_,Vs,N_,Ma,F_,I_,L_,zt,Ns,K_,Ta,R_,q_,Jt,Fs,U_,ka,B_,Ud,Be,jt,Ca,Is,H_,Aa,W_,Bd,Zn,G_,Hd,H,Ls,Y_,eo,z_,Ks,J_,j_,K,to,Q_,X_,Oa,Z_,ev,Va,tv,rv,Na,sv,nv,ro,ov,iv,so,av,dv,no,lv,cv,uv,Qt,Rs,fv,qs,pv,Fa,hv,mv,gv,Xt,Us,_v,Ia,vv,bv,Zt,Bs,Sv,La,$v,Wd,He,er,Ka,Hs,Dv,Ra,Ev,Gd,ge,xv,oo,yv,wv,Ws,Pv,Mv,Gs,Tv,Yd,W,Ys,kv,qa,Cv,Av,R,io,Ov,Vv,Ua,Nv,Fv,Ba,Iv,Lv,Ha,Kv,Rv,ao,qv,Uv,lo,Bv,Hv,co,Wv,Gv,Yv,uo,zv,zs,Jv,jv,tr,Js,Qv,Wa,Xv,Zv,rr,js,eb,Ga,tb,zd;return nr=new M({}),or=new M({}),ar=new M({}),dr=new M({}),lr=new M({}),fr=new M({}),pr=new b({props:{name:"class diffusers.SchedulerMixin",anchor:"diffusers.SchedulerMixin",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L38"}}),hr=new M({}),mr=new b({props:{name:"class diffusers.schedulers.scheduling_utils.SchedulerOutput",anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput",parameters:[{name:"prev_sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput.prev_sample",description:`<strong>prev_sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code> for images) &#x2014;
Computed sample (x_{t-1}) of previous timestep. <code>prev_sample</code> should be used as next model input in the
denoising loop.`,name:"prev_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L25"}}),gr=new M({}),_r=new M({}),vr=new b({props:{name:"class diffusers.DDIMScheduler",anchor:"diffusers.DDIMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"},{name:"set_alpha_to_one",val:": bool = True"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDIMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDIMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDIMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDIMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDIMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"},{anchor:"diffusers.DDIMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.DDIMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L78"}}),Sr=new b({props:{name:"scale_model_input",anchor:"diffusers.DDIMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDIMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L163",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),$r=new b({props:{name:"set_timesteps",anchor:"diffusers.DDIMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L187"}}),Dr=new b({props:{name:"step",anchor:"diffusers.DDIMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"eta",val:": float = 0.0"},{name:"use_clipped_model_output",val:": bool = False"},{name:"generator",val:" = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDIMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDIMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DDIMScheduler.step.eta",description:"<strong>eta</strong> (<code>float</code>) &#x2014; weight of noise for added noise in diffusion step.",name:"eta"},{anchor:"diffusers.DDIMScheduler.step.use_clipped_model_output",description:`<strong>use_clipped_model_output</strong> (<code>bool</code>) &#x2014; if <code>True</code>, compute &#x201C;corrected&#x201D; <code>model_output</code> from the clipped
predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when
<code>self.config.clip_sample</code> is <code>True</code>. If no clipping has happened, &#x201C;corrected&#x201D; <code>model_output</code> would
coincide with the one provided as input and <code>use_clipped_model_output</code> will have not effect.
generator &#x2014; random number generator.`,name:"use_clipped_model_output"},{anchor:"diffusers.DDIMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDIMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L203",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> or <code>tuple</code></p>
`}}),Er=new M({}),yr=new b({props:{name:"class diffusers.DDPMScheduler",anchor:"diffusers.DDPMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"variance_type",val:": str = 'fixed_small'"},{name:"clip_sample",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDPMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDPMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDPMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDPMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDPMScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.DDPMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L76"}}),Pr=new b({props:{name:"scale_model_input",anchor:"diffusers.DDPMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDPMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L156",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Mr=new b({props:{name:"set_timesteps",anchor:"diffusers.DDPMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L170"}}),Tr=new b({props:{name:"step",anchor:"diffusers.DDPMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"predict_epsilon",val:" = True"},{name:"generator",val:" = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDPMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDPMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DDPMScheduler.step.predict_epsilon",description:`<strong>predict_epsilon</strong> (<code>bool</code>) &#x2014;
optional flag to use when model predicts the samples directly instead of the noise, epsilon.
generator &#x2014; random number generator.`,name:"predict_epsilon"},{anchor:"diffusers.DDPMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDPMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L218",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> or <code>tuple</code></p>
`}}),kr=new M({}),Ar=new b({props:{name:"class diffusers.KarrasVeScheduler",anchor:"diffusers.KarrasVeScheduler",parameters:[{name:"sigma_min",val:": float = 0.02"},{name:"sigma_max",val:": float = 100"},{name:"s_noise",val:": float = 1.007"},{name:"s_churn",val:": float = 80"},{name:"s_min",val:": float = 0.05"},{name:"s_max",val:": float = 50"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.sigma_min",description:"<strong>sigma_min</strong> (<code>float</code>) &#x2014; minimum noise magnitude",name:"sigma_min"},{anchor:"diffusers.KarrasVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum noise magnitude",name:"sigma_max"},{anchor:"diffusers.KarrasVeScheduler.s_noise",description:`<strong>s_noise</strong> (<code>float</code>) &#x2014; the amount of additional noise to counteract loss of detail during sampling.
A reasonable range is [1.000, 1.011].`,name:"s_noise"},{anchor:"diffusers.KarrasVeScheduler.s_churn",description:`<strong>s_churn</strong> (<code>float</code>) &#x2014; the parameter controlling the overall amount of stochasticity.
A reasonable range is [0, 100].`,name:"s_churn"},{anchor:"diffusers.KarrasVeScheduler.s_min",description:`<strong>s_min</strong> (<code>float</code>) &#x2014; the start value of the sigma range where we add noise (enable stochasticity).
A reasonable range is [0, 10].`,name:"s_min"},{anchor:"diffusers.KarrasVeScheduler.s_max",description:`<strong>s_max</strong> (<code>float</code>) &#x2014; the end value of the sigma range where we add noise.
A reasonable range is [0.2, 80].`,name:"s_max"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L48"}}),Ir=new b({props:{name:"add_noise_to_input",anchor:"diffusers.KarrasVeScheduler.add_noise_to_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"sigma",val:": float"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L133"}}),Lr=new b({props:{name:"scale_model_input",anchor:"diffusers.KarrasVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.KarrasVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L98",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Kr=new b({props:{name:"set_timesteps",anchor:"diffusers.KarrasVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L112"}}),Rr=new b({props:{name:"step",anchor:"diffusers.KarrasVeScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class</p>
<p>KarrasVeOutput &#x2014; updated sample in the diffusion chain and derivative (TODO double check).`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L154",returnDescription:`
<p><code>KarrasVeOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>KarrasVeOutput</code> or <code>tuple</code></p>
`}}),qr=new b({props:{name:"step_correct",anchor:"diffusers.KarrasVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"sample_prev",val:": FloatTensor"},{name:"derivative",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_prev",description:"<strong>sample_prev</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.derivative",description:"<strong>derivative</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"derivative"},{anchor:"diffusers.KarrasVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L192",returnDescription:`
<p>updated sample in the diffusion chain. derivative (TODO): TODO</p>
`,returnType:`
<p>prev_sample (TODO)</p>
`}}),Ur=new M({}),Hr=new b({props:{name:"class diffusers.LMSDiscreteScheduler",anchor:"diffusers.LMSDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.LMSDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.LMSDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.LMSDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.LMSDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L47"}}),Gr=new b({props:{name:"get_lms_coefficient",anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient",parameters:[{name:"order",val:""},{name:"t",val:""},{name:"current_order",val:""}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.order",description:"<strong>order</strong> (TODO) &#x2014;",name:"order"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.t",description:"<strong>t</strong> (TODO) &#x2014;",name:"t"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.current_order",description:"<strong>current_order</strong> (TODO) &#x2014;",name:"current_order"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L137"}}),Yr=new b({props:{name:"scale_model_input",anchor:"diffusers.LMSDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L116",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Jr=new b({props:{name:"set_timesteps",anchor:"diffusers.LMSDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L159"}}),jr=new b({props:{name:"step",anchor:"diffusers.LMSDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"order",val:": int = 4"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.LMSDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.LMSDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
order &#x2014; coefficient for multi-step inference.`,name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than LMSDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L180",returnDescription:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),Qr=new M({}),Zr=new b({props:{name:"class diffusers.PNDMScheduler",anchor:"diffusers.PNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"skip_prk_steps",val:": bool = False"},{name:"set_alpha_to_one",val:": bool = False"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.PNDMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.PNDMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.PNDMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.PNDMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.PNDMScheduler.skip_prk_steps",description:`<strong>skip_prk_steps</strong> (<code>bool</code>) &#x2014;
allows the scheduler to skip the Runge-Kutta steps that are defined in the original paper as being required
before plms steps; defaults to <code>False</code>.`,name:"skip_prk_steps"},{anchor:"diffusers.PNDMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.PNDMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L56"}}),ts=new b({props:{name:"scale_model_input",anchor:"diffusers.PNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L344",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),rs=new b({props:{name:"set_timesteps",anchor:"diffusers.PNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L152"}}),ss=new b({props:{name:"step",anchor:"diffusers.PNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L191",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> or <code>tuple</code></p>
`}}),ns=new b({props:{name:"step_plms",anchor:"diffusers.PNDMScheduler.step_plms",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_plms.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_plms.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_plms.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_plms.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L277",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),os=new b({props:{name:"step_prk",anchor:"diffusers.PNDMScheduler.step_prk",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_prk.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_prk.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_prk.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_prk.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L222",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),is=new M({}),ds=new b({props:{name:"class diffusers.ScoreSdeVeScheduler",anchor:"diffusers.ScoreSdeVeScheduler",parameters:[{name:"num_train_timesteps",val:": int = 2000"},{name:"snr",val:": float = 0.15"},{name:"sigma_min",val:": float = 0.01"},{name:"sigma_max",val:": float = 1348.0"},{name:"sampling_eps",val:": float = 1e-05"},{name:"correct_steps",val:": int = 1"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.ScoreSdeVeScheduler.snr",description:`<strong>snr</strong> (<code>float</code>) &#x2014;
coefficient weighting the step from the model_output sample (from the network) to the random noise.`,name:"snr"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>) &#x2014;
initial noise scale for sigma sequence in sampling procedure. The minimum sigma should mirror the
distribution of the data.`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum value used for the range of continuous timesteps passed into the model.",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.sampling_eps",description:`<strong>sampling_eps</strong> (<code>float</code>) &#x2014; the end value of sampling, where timesteps decrease progressively from 1 to
epsilon. &#x2014;`,name:"sampling_eps"},{anchor:"diffusers.ScoreSdeVeScheduler.correct_steps",description:"<strong>correct_steps</strong> (<code>int</code>) &#x2014; number of correction steps performed on a produced sample.",name:"correct_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L45"}}),cs=new b({props:{name:"scale_model_input",anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L87",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),us=new b({props:{name:"set_sigmas",anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas",parameters:[{name:"num_inference_steps",val:": int"},{name:"sigma_min",val:": float = None"},{name:"sigma_max",val:": float = None"},{name:"sampling_eps",val:": float = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>, optional) &#x2014;
initial noise scale value (overrides value given at Scheduler instantiation).`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>, optional) &#x2014; final noise scale value (overrides value given at Scheduler instantiation).",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L117"}}),fs=new b({props:{name:"set_timesteps",anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"sampling_eps",val:": float = None"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L101"}}),ps=new b({props:{name:"step_correct",anchor:"diffusers.ScoreSdeVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L212",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),hs=new b({props:{name:"step_pred",anchor:"diffusers.ScoreSdeVeScheduler.step_pred",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L151",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),ms=new M({}),_s=new b({props:{name:"class diffusers.IPNDMScheduler",anchor:"diffusers.IPNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L24"}}),Ss=new b({props:{name:"scale_model_input",anchor:"diffusers.IPNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L126",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),$s=new b({props:{name:"set_timesteps",anchor:"diffusers.IPNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L56"}}),Ds=new b({props:{name:"step",anchor:"diffusers.IPNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.IPNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.IPNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.IPNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L76",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),Es=new M({}),Ht=new I4({props:{warning:!0,$$slots:{default:[L4]},$$scope:{ctx:Ja}}}),ys=new b({props:{name:"class diffusers.schedulers.ScoreSdeVpScheduler",anchor:"diffusers.schedulers.ScoreSdeVpScheduler",parameters:[{name:"num_train_timesteps",val:" = 2000"},{name:"beta_min",val:" = 0.1"},{name:"beta_max",val:" = 20"},{name:"sampling_eps",val:" = 0.001"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_vp.py#L26"}}),Ps=new M({}),ks=new b({props:{name:"class diffusers.EulerDiscreteScheduler",anchor:"diffusers.EulerDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L48"}}),Os=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L116",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ns=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L137"}}),Fs=new b({props:{name:"step",anchor:"diffusers.EulerDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"s_churn",val:": float = 0.0"},{name:"s_tmin",val:": float = 0.0"},{name:"s_tmax",val:": float = inf"},{name:"s_noise",val:": float = 1.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_churn",description:"<strong>s_churn</strong> (<code>float</code>) &#x2014;",name:"s_churn"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmin",description:"<strong>s_tmin</strong>  (<code>float</code>) &#x2014;",name:"s_tmin"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmax",description:"<strong>s_tmax</strong>  (<code>float</code>) &#x2014;",name:"s_tmax"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_noise",description:"<strong>s_noise</strong> (<code>float</code>) &#x2014;",name:"s_noise"},{anchor:"diffusers.EulerDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L156",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a
<code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),Is=new M({}),Ls=new b({props:{name:"class diffusers.EulerAncestralDiscreteScheduler",anchor:"diffusers.EulerAncestralDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L48"}}),Rs=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L115",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Us=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L136"}}),Bs=new b({props:{name:"step",anchor:"diffusers.EulerAncestralDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerAncestralDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L155",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise
a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),Hs=new M({}),Ys=new b({props:{name:"class diffusers.RePaintScheduler",anchor:"diffusers.RePaintScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"eta",val:": float = 0.0"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.RePaintScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.RePaintScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.RePaintScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.RePaintScheduler.eta",description:`<strong>eta</strong> (<code>float</code>) &#x2014;
The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 -0.0 is DDIM and
1.0 is DDPM scheduler respectively.`,name:"eta"},{anchor:"diffusers.RePaintScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.RePaintScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.RePaintScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L74"}}),Js=new b({props:{name:"scale_model_input",anchor:"diffusers.RePaintScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.RePaintScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L150",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),js=new b({props:{name:"step",anchor:"diffusers.RePaintScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"original_image",val:": FloatTensor"},{name:"mask",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.step.model_output",description:`<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned
diffusion model.`,name:"model_output"},{anchor:"diffusers.RePaintScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.RePaintScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.RePaintScheduler.step.original_image",description:`<strong>original_image</strong> (<code>torch.FloatTensor</code>) &#x2014;
the original image to inpaint on.`,name:"original_image"},{anchor:"diffusers.RePaintScheduler.step.mask",description:`<strong>mask</strong> (<code>torch.FloatTensor</code>) &#x2014;
the mask where 0.0 values define which part of the original image to inpaint (change).`,name:"mask"},{anchor:"diffusers.RePaintScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014; random number generator.",name:"generator"},{anchor:"diffusers.RePaintScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than
DDPMSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L213",returnDescription:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> or <code>tuple</code></p>
`}}),{c(){G=s("meta"),We=l(),Y=s("h1"),ue=s("a"),bo=s("span"),p(nr.$$.fragment),fc=l(),So=s("span"),pc=i("Schedulers"),ja=l(),Zs=s("p"),hc=i("Diffusers contains multiple pre-built schedule functions for the diffusion process."),Qa=l(),Ee=s("h2"),Ge=s("a"),$o=s("span"),p(or.$$.fragment),mc=l(),Do=s("span"),gc=i("What is a scheduler?"),Xa=l(),Ye=s("p"),_c=i("The schedule functions, denoted "),Eo=s("em"),vc=i("Schedulers"),bc=i(" in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample."),Za=l(),ze=s("ul"),en=s("li"),Sc=i("Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),ir=s("ul"),xo=s("li"),$c=i("adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),Dc=l(),yo=s("li"),Ec=i("for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),xc=l(),xe=s("li"),yc=i("Schedulers are often defined by a "),wo=s("em"),wc=i("noise schedule"),Pc=i(" and an "),Po=s("em"),Mc=i("update rule"),Tc=i(" to solve the differential equation solution."),ed=l(),ye=s("h3"),Je=s("a"),Mo=s("span"),p(ar.$$.fragment),kc=l(),To=s("span"),Cc=i("Discrete versus continuous schedulers"),td=l(),T=s("p"),Ac=i(`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),ko=s("code"),Oc=i("int"),Vc=i(" inputs), such as the "),tn=s("a"),Nc=i("DDPMScheduler"),Fc=i(" or "),rn=s("a"),Ic=i("PNDMScheduler"),Lc=i(", and continuous (accepting "),Co=s("code"),Kc=i("float"),Rc=i(" inputs), such as the score-based schedulers "),sn=s("a"),qc=i("ScoreSdeVeScheduler"),Uc=i(" or "),Ao=s("code"),Bc=i("ScoreSdeVpScheduler"),Hc=i("."),rd=l(),we=s("h2"),je=s("a"),Oo=s("span"),p(dr.$$.fragment),Wc=l(),Vo=s("span"),Gc=i("Designing Re-usable schedulers"),sd=l(),nn=s("p"),Yc=i(`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),nd=l(),Qe=s("ul"),No=s("li"),zc=i("Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),Jc=l(),Fo=s("li"),jc=i("Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),od=l(),Pe=s("h2"),Xe=s("a"),Io=s("span"),p(lr.$$.fragment),Qc=l(),Lo=s("span"),Xc=i("API"),id=l(),on=s("p"),Zc=i("The core API for any new scheduler must follow a limited structure."),ad=l(),_e=s("ul"),cr=s("li"),eu=i("Schedulers should provide one or more "),Ko=s("code"),tu=i("def step(...)"),ru=i(" functions that should be called to update the generated sample iteratively."),su=l(),ur=s("li"),nu=i("Schedulers should provide a "),Ro=s("code"),ou=i("set_timesteps(...)"),iu=i(" method that configures the parameters of a schedule function for a specific inference task."),au=l(),qo=s("li"),du=i("Schedulers should be framework-specific."),dd=l(),Ze=s("p"),lu=i("The base class "),an=s("a"),cu=i("SchedulerMixin"),uu=i(" implements low level utilities used by multiple schedulers."),ld=l(),Me=s("h3"),et=s("a"),Uo=s("span"),p(fr.$$.fragment),fu=l(),Bo=s("span"),pu=i("SchedulerMixin"),cd=l(),Te=s("div"),p(pr.$$.fragment),hu=l(),Ho=s("p"),mu=i("Mixin containing common functions for the schedulers."),ud=l(),ke=s("h3"),tt=s("a"),Wo=s("span"),p(hr.$$.fragment),gu=l(),Go=s("span"),_u=i("SchedulerOutput"),fd=i("\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),Ce=s("div"),p(mr.$$.fragment),vu=l(),Yo=s("p"),bu=i("Base class for the scheduler\u2019s step function output."),pd=l(),Ae=s("h3"),rt=s("a"),zo=s("span"),p(gr.$$.fragment),Su=l(),Jo=s("span"),$u=i("Implemented Schedulers"),hd=l(),Oe=s("h4"),st=s("a"),jo=s("span"),p(_r.$$.fragment),Du=l(),Qo=s("span"),Eu=i("Denoising diffusion implicit models (DDIM)"),md=l(),dn=s("p"),xu=i("Original paper can be found here."),gd=l(),x=s("div"),p(vr.$$.fragment),yu=l(),Xo=s("p"),wu=i(`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),Pu=l(),k=s("p"),ln=s("a"),Mu=i("~ConfigMixin"),Tu=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Zo=s("code"),ku=i("__init__"),Cu=i(`
function, such as `),ei=s("code"),Au=i("num_train_timesteps"),Ou=i(". They can be accessed via "),ti=s("code"),Vu=i("scheduler.config.num_train_timesteps"),Nu=i(`.
`),cn=s("a"),Fu=i("~ConfigMixin"),Iu=i(" also provides general loading and saving functionality via the "),un=s("a"),Lu=i("save_config()"),Ku=i(` and
`),fn=s("a"),Ru=i("from_config()"),qu=i(" functions."),Uu=l(),pn=s("p"),Bu=i("For more details, see the original paper: "),br=s("a"),Hu=i("https://arxiv.org/abs/2010.02502"),Wu=l(),nt=s("div"),p(Sr.$$.fragment),Gu=l(),ri=s("p"),Yu=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),zu=l(),ot=s("div"),p($r.$$.fragment),Ju=l(),si=s("p"),ju=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Qu=l(),it=s("div"),p(Dr.$$.fragment),Xu=l(),ni=s("p"),Zu=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),_d=l(),Ve=s("h4"),at=s("a"),oi=s("span"),p(Er.$$.fragment),ef=l(),ii=s("span"),tf=i("Denoising diffusion probabilistic models (DDPM)"),vd=l(),dt=s("p"),rf=i("Original paper can be found "),xr=s("a"),sf=i("here"),nf=i("."),bd=l(),y=s("div"),p(yr.$$.fragment),of=l(),ai=s("p"),af=i(`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),df=l(),C=s("p"),hn=s("a"),lf=i("~ConfigMixin"),cf=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),di=s("code"),uf=i("__init__"),ff=i(`
function, such as `),li=s("code"),pf=i("num_train_timesteps"),hf=i(". They can be accessed via "),ci=s("code"),mf=i("scheduler.config.num_train_timesteps"),gf=i(`.
`),mn=s("a"),_f=i("~ConfigMixin"),vf=i(" also provides general loading and saving functionality via the "),gn=s("a"),bf=i("save_config()"),Sf=i(` and
`),_n=s("a"),$f=i("from_config()"),Df=i(" functions."),Ef=l(),vn=s("p"),xf=i("For more details, see the original paper: "),wr=s("a"),yf=i("https://arxiv.org/abs/2006.11239"),wf=l(),lt=s("div"),p(Pr.$$.fragment),Pf=l(),ui=s("p"),Mf=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Tf=l(),ct=s("div"),p(Mr.$$.fragment),kf=l(),fi=s("p"),Cf=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Af=l(),ut=s("div"),p(Tr.$$.fragment),Of=l(),pi=s("p"),Vf=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Sd=l(),Ne=s("h4"),ft=s("a"),hi=s("span"),p(kr.$$.fragment),Nf=l(),mi=s("span"),Ff=i("Variance exploding, stochastic sampling from Karras et. al"),$d=l(),pt=s("p"),If=i("Original paper can be found "),Cr=s("a"),Lf=i("here"),Kf=i("."),Dd=l(),S=s("div"),p(Ar.$$.fragment),Rf=l(),gi=s("p"),qf=i(`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),Uf=l(),ht=s("p"),Bf=i(`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Or=s("a"),Hf=i("https://arxiv.org/abs/2206.00364"),Wf=i(` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Vr=s("a"),Gf=i("https://arxiv.org/abs/2011.13456"),Yf=l(),A=s("p"),bn=s("a"),zf=i("~ConfigMixin"),Jf=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),_i=s("code"),jf=i("__init__"),Qf=i(`
function, such as `),vi=s("code"),Xf=i("num_train_timesteps"),Zf=i(". They can be accessed via "),bi=s("code"),ep=i("scheduler.config.num_train_timesteps"),tp=i(`.
`),Sn=s("a"),rp=i("~ConfigMixin"),sp=i(" also provides general loading and saving functionality via the "),$n=s("a"),np=i("save_config()"),op=i(` and
`),Dn=s("a"),ip=i("from_config()"),ap=i(" functions."),dp=l(),Nr=s("p"),lp=i(`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),Fr=s("a"),cp=i("https://arxiv.org/abs/2206.00364"),up=i(`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),fp=l(),ve=s("div"),p(Ir.$$.fragment),pp=l(),Si=s("p"),hp=i(`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),mp=l(),$i=s("p"),gp=i("TODO Args:"),_p=l(),mt=s("div"),p(Lr.$$.fragment),vp=l(),Di=s("p"),bp=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Sp=l(),gt=s("div"),p(Kr.$$.fragment),$p=l(),Ei=s("p"),Dp=i("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),Ep=l(),_t=s("div"),p(Rr.$$.fragment),xp=l(),xi=s("p"),yp=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),wp=l(),vt=s("div"),p(qr.$$.fragment),Pp=l(),yi=s("p"),Mp=i("Correct the predicted sample based on the output model_output of the network. TODO complete description"),Ed=l(),Fe=s("h4"),bt=s("a"),wi=s("span"),p(Ur.$$.fragment),Tp=l(),Pi=s("span"),kp=i("Linear multistep scheduler for discrete beta schedules"),xd=l(),St=s("p"),Cp=i("Original implementation can be found "),Br=s("a"),Ap=i("here"),Op=i("."),yd=l(),w=s("div"),p(Hr.$$.fragment),Vp=l(),En=s("p"),Np=i(`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),Wr=s("a"),Fp=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),Ip=l(),O=s("p"),xn=s("a"),Lp=i("~ConfigMixin"),Kp=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Mi=s("code"),Rp=i("__init__"),qp=i(`
function, such as `),Ti=s("code"),Up=i("num_train_timesteps"),Bp=i(". They can be accessed via "),ki=s("code"),Hp=i("scheduler.config.num_train_timesteps"),Wp=i(`.
`),yn=s("a"),Gp=i("~ConfigMixin"),Yp=i(" also provides general loading and saving functionality via the "),wn=s("a"),zp=i("save_config()"),Jp=i(` and
`),Pn=s("a"),jp=i("from_config()"),Qp=i(" functions."),Xp=l(),$t=s("div"),p(Gr.$$.fragment),Zp=l(),Ci=s("p"),eh=i("Compute a linear multistep coefficient."),th=l(),Dt=s("div"),p(Yr.$$.fragment),rh=l(),zr=s("p"),sh=i("Scales the denoising model input by "),Ai=s("code"),nh=i("(sigma**2 + 1) ** 0.5"),oh=i(" to match the K-LMS algorithm."),ih=l(),Et=s("div"),p(Jr.$$.fragment),ah=l(),Oi=s("p"),dh=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),lh=l(),xt=s("div"),p(jr.$$.fragment),ch=l(),Vi=s("p"),uh=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),wd=l(),Ie=s("h4"),yt=s("a"),Ni=s("span"),p(Qr.$$.fragment),fh=l(),Fi=s("span"),ph=i("Pseudo numerical methods for diffusion models (PNDM)"),Pd=l(),wt=s("p"),hh=i("Original implementation can be found "),Xr=s("a"),mh=i("here"),gh=i("."),Md=l(),$=s("div"),p(Zr.$$.fragment),_h=l(),Ii=s("p"),vh=i(`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),bh=l(),V=s("p"),Mn=s("a"),Sh=i("~ConfigMixin"),$h=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Li=s("code"),Dh=i("__init__"),Eh=i(`
function, such as `),Ki=s("code"),xh=i("num_train_timesteps"),yh=i(". They can be accessed via "),Ri=s("code"),wh=i("scheduler.config.num_train_timesteps"),Ph=i(`.
`),Tn=s("a"),Mh=i("~ConfigMixin"),Th=i(" also provides general loading and saving functionality via the "),kn=s("a"),kh=i("save_config()"),Ch=i(` and
`),Cn=s("a"),Ah=i("from_config()"),Oh=i(" functions."),Vh=l(),An=s("p"),Nh=i("For more details, see the original paper: "),es=s("a"),Fh=i("https://arxiv.org/abs/2202.09778"),Ih=l(),Pt=s("div"),p(ts.$$.fragment),Lh=l(),qi=s("p"),Kh=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Rh=l(),Mt=s("div"),p(rs.$$.fragment),qh=l(),Ui=s("p"),Uh=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Bh=l(),be=s("div"),p(ss.$$.fragment),Hh=l(),Bi=s("p"),Wh=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Gh=l(),me=s("p"),Yh=i("This function calls "),Hi=s("code"),zh=i("step_prk()"),Jh=i(" or "),Wi=s("code"),jh=i("step_plms()"),Qh=i(" depending on the internal variable "),Gi=s("code"),Xh=i("counter"),Zh=i("."),em=l(),Tt=s("div"),p(ns.$$.fragment),tm=l(),Yi=s("p"),rm=i(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),sm=l(),kt=s("div"),p(os.$$.fragment),nm=l(),zi=s("p"),om=i(`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),Td=l(),Le=s("h4"),Ct=s("a"),Ji=s("span"),p(is.$$.fragment),im=l(),ji=s("span"),am=i("variance exploding stochastic differential equation (VE-SDE) scheduler"),kd=l(),At=s("p"),dm=i("Original paper can be found "),as=s("a"),lm=i("here"),cm=i("."),Cd=l(),D=s("div"),p(ds.$$.fragment),um=l(),Qi=s("p"),fm=i("The variance exploding stochastic differential equation (SDE) scheduler."),pm=l(),On=s("p"),hm=i("For more information, see the original paper: "),ls=s("a"),mm=i("https://arxiv.org/abs/2011.13456"),gm=l(),N=s("p"),Vn=s("a"),_m=i("~ConfigMixin"),vm=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Xi=s("code"),bm=i("__init__"),Sm=i(`
function, such as `),Zi=s("code"),$m=i("num_train_timesteps"),Dm=i(". They can be accessed via "),ea=s("code"),Em=i("scheduler.config.num_train_timesteps"),xm=i(`.
`),Nn=s("a"),ym=i("~ConfigMixin"),wm=i(" also provides general loading and saving functionality via the "),Fn=s("a"),Pm=i("save_config()"),Mm=i(` and
`),In=s("a"),Tm=i("from_config()"),km=i(" functions."),Cm=l(),Ot=s("div"),p(cs.$$.fragment),Am=l(),ta=s("p"),Om=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Vm=l(),Se=s("div"),p(us.$$.fragment),Nm=l(),ra=s("p"),Fm=i("Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),Im=l(),Ke=s("p"),Lm=i("The sigmas control the weight of the "),sa=s("code"),Km=i("drift"),Rm=i(" and "),na=s("code"),qm=i("diffusion"),Um=i(" components of sample update."),Bm=l(),Vt=s("div"),p(fs.$$.fragment),Hm=l(),oa=s("p"),Wm=i("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),Gm=l(),Nt=s("div"),p(ps.$$.fragment),Ym=l(),ia=s("p"),zm=i(`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),Jm=l(),Ft=s("div"),p(hs.$$.fragment),jm=l(),aa=s("p"),Qm=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Ad=l(),Re=s("h4"),It=s("a"),da=s("span"),p(ms.$$.fragment),Xm=l(),la=s("span"),Zm=i("improved pseudo numerical methods for diffusion models (iPNDM)"),Od=l(),Lt=s("p"),eg=i("Original implementation can be found "),gs=s("a"),tg=i("here"),rg=i("."),Vd=l(),P=s("div"),p(_s.$$.fragment),sg=l(),Ln=s("p"),ng=i(`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),vs=s("a"),og=i("library"),ig=l(),F=s("p"),Kn=s("a"),ag=i("~ConfigMixin"),dg=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ca=s("code"),lg=i("__init__"),cg=i(`
function, such as `),ua=s("code"),ug=i("num_train_timesteps"),fg=i(". They can be accessed via "),fa=s("code"),pg=i("scheduler.config.num_train_timesteps"),hg=i(`.
`),Rn=s("a"),mg=i("~ConfigMixin"),gg=i(" also provides general loading and saving functionality via the "),qn=s("a"),_g=i("save_config()"),vg=i(` and
`),Un=s("a"),bg=i("from_config()"),Sg=i(" functions."),$g=l(),Bn=s("p"),Dg=i("For more details, see the original paper: "),bs=s("a"),Eg=i("https://arxiv.org/abs/2202.09778"),xg=l(),Kt=s("div"),p(Ss.$$.fragment),yg=l(),pa=s("p"),wg=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Pg=l(),Rt=s("div"),p($s.$$.fragment),Mg=l(),ha=s("p"),Tg=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),kg=l(),qt=s("div"),p(Ds.$$.fragment),Cg=l(),ma=s("p"),Ag=i(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),Nd=l(),qe=s("h4"),Ut=s("a"),ga=s("span"),p(Es.$$.fragment),Og=l(),_a=s("span"),Vg=i("variance preserving stochastic differential equation (VP-SDE) scheduler"),Fd=l(),Bt=s("p"),Ng=i("Original paper can be found "),xs=s("a"),Fg=i("here"),Ig=i("."),Id=l(),p(Ht.$$.fragment),Ld=l(),z=s("div"),p(ys.$$.fragment),Lg=l(),va=s("p"),Kg=i("The variance preserving stochastic differential equation (SDE) scheduler."),Rg=l(),I=s("p"),Hn=s("a"),qg=i("~ConfigMixin"),Ug=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ba=s("code"),Bg=i("__init__"),Hg=i(`
function, such as `),Sa=s("code"),Wg=i("num_train_timesteps"),Gg=i(". They can be accessed via "),$a=s("code"),Yg=i("scheduler.config.num_train_timesteps"),zg=i(`.
`),Wn=s("a"),Jg=i("~ConfigMixin"),jg=i(" also provides general loading and saving functionality via the "),Gn=s("a"),Qg=i("save_config()"),Xg=i(` and
`),Yn=s("a"),Zg=i("from_config()"),e_=i(" functions."),t_=l(),zn=s("p"),r_=i("For more information, see the original paper: "),ws=s("a"),s_=i("https://arxiv.org/abs/2011.13456"),n_=l(),Da=s("p"),o_=i("UNDER CONSTRUCTION"),Kd=l(),Ue=s("h4"),Wt=s("a"),Ea=s("span"),p(Ps.$$.fragment),i_=l(),xa=s("span"),a_=i("Euler scheduler"),Rd=l(),$e=s("p"),d_=i("Euler scheduler (Algorithm 2) from the paper "),Ms=s("a"),l_=i("Elucidating the Design Space of Diffusion-Based Generative Models"),c_=i(" by Karras et al. (2022). Based on the original "),Ts=s("a"),u_=i("k-diffusion"),f_=i(` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),qd=l(),B=s("div"),p(ks.$$.fragment),p_=l(),Gt=s("p"),h_=i("Euler scheduler (Algorithm 2) from Karras et al. (2022) "),Cs=s("a"),m_=i("https://arxiv.org/abs/2206.00364"),g_=i(`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),As=s("a"),__=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),v_=l(),L=s("p"),Jn=s("a"),b_=i("~ConfigMixin"),S_=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ya=s("code"),$_=i("__init__"),D_=i(`
function, such as `),wa=s("code"),E_=i("num_train_timesteps"),x_=i(". They can be accessed via "),Pa=s("code"),y_=i("scheduler.config.num_train_timesteps"),w_=i(`.
`),jn=s("a"),P_=i("~ConfigMixin"),M_=i(" also provides general loading and saving functionality via the "),Qn=s("a"),T_=i("save_config()"),k_=i(` and
`),Xn=s("a"),C_=i("from_config()"),A_=i(" functions."),O_=l(),Yt=s("div"),p(Os.$$.fragment),V_=l(),Vs=s("p"),N_=i("Scales the denoising model input by "),Ma=s("code"),F_=i("(sigma**2 + 1) ** 0.5"),I_=i(" to match the Euler algorithm."),L_=l(),zt=s("div"),p(Ns.$$.fragment),K_=l(),Ta=s("p"),R_=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),q_=l(),Jt=s("div"),p(Fs.$$.fragment),U_=l(),ka=s("p"),B_=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Ud=l(),Be=s("h4"),jt=s("a"),Ca=s("span"),p(Is.$$.fragment),H_=l(),Aa=s("span"),W_=i("Euler Ancestral scheduler"),Bd=l(),Zn=s("p"),G_=i(`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Hd=l(),H=s("div"),p(Ls.$$.fragment),Y_=l(),eo=s("p"),z_=i(`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Ks=s("a"),J_=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),j_=l(),K=s("p"),to=s("a"),Q_=i("~ConfigMixin"),X_=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Oa=s("code"),Z_=i("__init__"),ev=i(`
function, such as `),Va=s("code"),tv=i("num_train_timesteps"),rv=i(". They can be accessed via "),Na=s("code"),sv=i("scheduler.config.num_train_timesteps"),nv=i(`.
`),ro=s("a"),ov=i("~ConfigMixin"),iv=i(" also provides general loading and saving functionality via the "),so=s("a"),av=i("save_config()"),dv=i(` and
`),no=s("a"),lv=i("from_config()"),cv=i(" functions."),uv=l(),Qt=s("div"),p(Rs.$$.fragment),fv=l(),qs=s("p"),pv=i("Scales the denoising model input by "),Fa=s("code"),hv=i("(sigma**2 + 1) ** 0.5"),mv=i(" to match the Euler algorithm."),gv=l(),Xt=s("div"),p(Us.$$.fragment),_v=l(),Ia=s("p"),vv=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),bv=l(),Zt=s("div"),p(Bs.$$.fragment),Sv=l(),La=s("p"),$v=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Wd=l(),He=s("h4"),er=s("a"),Ka=s("span"),p(Hs.$$.fragment),Dv=l(),Ra=s("span"),Ev=i("RePaint scheduler"),Gd=l(),ge=s("p"),xv=i(`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),oo=s("a"),yv=i("RePaintPipeline"),wv=i(`.
Based on the paper `),Ws=s("a"),Pv=i("RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),Mv=i(`
and the original implementation by Andreas Lugmayr et al.: `),Gs=s("a"),Tv=i("https://github.com/andreas128/RePaint"),Yd=l(),W=s("div"),p(Ys.$$.fragment),kv=l(),qa=s("p"),Cv=i("RePaint is a schedule for DDPM inpainting inside a given mask."),Av=l(),R=s("p"),io=s("a"),Ov=i("~ConfigMixin"),Vv=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ua=s("code"),Nv=i("__init__"),Fv=i(`
function, such as `),Ba=s("code"),Iv=i("num_train_timesteps"),Lv=i(". They can be accessed via "),Ha=s("code"),Kv=i("scheduler.config.num_train_timesteps"),Rv=i(`.
`),ao=s("a"),qv=i("~ConfigMixin"),Uv=i(" also provides general loading and saving functionality via the "),lo=s("a"),Bv=i("save_config()"),Hv=i(` and
`),co=s("a"),Wv=i("from_config()"),Gv=i(" functions."),Yv=l(),uo=s("p"),zv=i("For more details, see the original paper: "),zs=s("a"),Jv=i("https://arxiv.org/pdf/2201.09865.pdf"),jv=l(),tr=s("div"),p(Js.$$.fragment),Qv=l(),Wa=s("p"),Xv=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Zv=l(),rr=s("div"),p(js.$$.fragment),eb=l(),Ga=s("p"),tb=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),this.h()},l(t){const u=N4('[data-svelte="svelte-1phssyn"]',document.head);G=n(u,"META",{name:!0,content:!0}),u.forEach(r),We=c(t),Y=n(t,"H1",{class:!0});var Qs=o(Y);ue=n(Qs,"A",{id:!0,class:!0,href:!0});var pb=o(ue);bo=n(pb,"SPAN",{});var hb=o(bo);h(nr.$$.fragment,hb),hb.forEach(r),pb.forEach(r),fc=c(Qs),So=n(Qs,"SPAN",{});var mb=o(So);pc=a(mb,"Schedulers"),mb.forEach(r),Qs.forEach(r),ja=c(t),Zs=n(t,"P",{});var gb=o(Zs);hc=a(gb,"Diffusers contains multiple pre-built schedule functions for the diffusion process."),gb.forEach(r),Qa=c(t),Ee=n(t,"H2",{class:!0});var Jd=o(Ee);Ge=n(Jd,"A",{id:!0,class:!0,href:!0});var _b=o(Ge);$o=n(_b,"SPAN",{});var vb=o($o);h(or.$$.fragment,vb),vb.forEach(r),_b.forEach(r),mc=c(Jd),Do=n(Jd,"SPAN",{});var bb=o(Do);gc=a(bb,"What is a scheduler?"),bb.forEach(r),Jd.forEach(r),Xa=c(t),Ye=n(t,"P",{});var jd=o(Ye);_c=a(jd,"The schedule functions, denoted "),Eo=n(jd,"EM",{});var Sb=o(Eo);vc=a(Sb,"Schedulers"),Sb.forEach(r),bc=a(jd," in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample."),jd.forEach(r),Za=c(t),ze=n(t,"UL",{});var Qd=o(ze);en=n(Qd,"LI",{});var rb=o(en);Sc=a(rb,"Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),ir=n(rb,"UL",{});var Xd=o(ir);xo=n(Xd,"LI",{});var $b=o(xo);$c=a($b,"adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),$b.forEach(r),Dc=c(Xd),yo=n(Xd,"LI",{});var Db=o(yo);Ec=a(Db,"for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),Db.forEach(r),Xd.forEach(r),rb.forEach(r),xc=c(Qd),xe=n(Qd,"LI",{});var fo=o(xe);yc=a(fo,"Schedulers are often defined by a "),wo=n(fo,"EM",{});var Eb=o(wo);wc=a(Eb,"noise schedule"),Eb.forEach(r),Pc=a(fo," and an "),Po=n(fo,"EM",{});var xb=o(Po);Mc=a(xb,"update rule"),xb.forEach(r),Tc=a(fo," to solve the differential equation solution."),fo.forEach(r),Qd.forEach(r),ed=c(t),ye=n(t,"H3",{class:!0});var Zd=o(ye);Je=n(Zd,"A",{id:!0,class:!0,href:!0});var yb=o(Je);Mo=n(yb,"SPAN",{});var wb=o(Mo);h(ar.$$.fragment,wb),wb.forEach(r),yb.forEach(r),kc=c(Zd),To=n(Zd,"SPAN",{});var Pb=o(To);Cc=a(Pb,"Discrete versus continuous schedulers"),Pb.forEach(r),Zd.forEach(r),td=c(t),T=n(t,"P",{});var ie=o(T);Ac=a(ie,`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),ko=n(ie,"CODE",{});var Mb=o(ko);Oc=a(Mb,"int"),Mb.forEach(r),Vc=a(ie," inputs), such as the "),tn=n(ie,"A",{href:!0});var Tb=o(tn);Nc=a(Tb,"DDPMScheduler"),Tb.forEach(r),Fc=a(ie," or "),rn=n(ie,"A",{href:!0});var kb=o(rn);Ic=a(kb,"PNDMScheduler"),kb.forEach(r),Lc=a(ie,", and continuous (accepting "),Co=n(ie,"CODE",{});var Cb=o(Co);Kc=a(Cb,"float"),Cb.forEach(r),Rc=a(ie," inputs), such as the score-based schedulers "),sn=n(ie,"A",{href:!0});var Ab=o(sn);qc=a(Ab,"ScoreSdeVeScheduler"),Ab.forEach(r),Uc=a(ie," or "),Ao=n(ie,"CODE",{});var Ob=o(Ao);Bc=a(Ob,"ScoreSdeVpScheduler"),Ob.forEach(r),Hc=a(ie,"."),ie.forEach(r),rd=c(t),we=n(t,"H2",{class:!0});var el=o(we);je=n(el,"A",{id:!0,class:!0,href:!0});var Vb=o(je);Oo=n(Vb,"SPAN",{});var Nb=o(Oo);h(dr.$$.fragment,Nb),Nb.forEach(r),Vb.forEach(r),Wc=c(el),Vo=n(el,"SPAN",{});var Fb=o(Vo);Gc=a(Fb,"Designing Re-usable schedulers"),Fb.forEach(r),el.forEach(r),sd=c(t),nn=n(t,"P",{});var Ib=o(nn);Yc=a(Ib,`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),Ib.forEach(r),nd=c(t),Qe=n(t,"UL",{});var tl=o(Qe);No=n(tl,"LI",{});var Lb=o(No);zc=a(Lb,"Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),Lb.forEach(r),Jc=c(tl),Fo=n(tl,"LI",{});var Kb=o(Fo);jc=a(Kb,"Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),Kb.forEach(r),tl.forEach(r),od=c(t),Pe=n(t,"H2",{class:!0});var rl=o(Pe);Xe=n(rl,"A",{id:!0,class:!0,href:!0});var Rb=o(Xe);Io=n(Rb,"SPAN",{});var qb=o(Io);h(lr.$$.fragment,qb),qb.forEach(r),Rb.forEach(r),Qc=c(rl),Lo=n(rl,"SPAN",{});var Ub=o(Lo);Xc=a(Ub,"API"),Ub.forEach(r),rl.forEach(r),id=c(t),on=n(t,"P",{});var Bb=o(on);Zc=a(Bb,"The core API for any new scheduler must follow a limited structure."),Bb.forEach(r),ad=c(t),_e=n(t,"UL",{});var po=o(_e);cr=n(po,"LI",{});var sl=o(cr);eu=a(sl,"Schedulers should provide one or more "),Ko=n(sl,"CODE",{});var Hb=o(Ko);tu=a(Hb,"def step(...)"),Hb.forEach(r),ru=a(sl," functions that should be called to update the generated sample iteratively."),sl.forEach(r),su=c(po),ur=n(po,"LI",{});var nl=o(ur);nu=a(nl,"Schedulers should provide a "),Ro=n(nl,"CODE",{});var Wb=o(Ro);ou=a(Wb,"set_timesteps(...)"),Wb.forEach(r),iu=a(nl," method that configures the parameters of a schedule function for a specific inference task."),nl.forEach(r),au=c(po),qo=n(po,"LI",{});var Gb=o(qo);du=a(Gb,"Schedulers should be framework-specific."),Gb.forEach(r),po.forEach(r),dd=c(t),Ze=n(t,"P",{});var ol=o(Ze);lu=a(ol,"The base class "),an=n(ol,"A",{href:!0});var Yb=o(an);cu=a(Yb,"SchedulerMixin"),Yb.forEach(r),uu=a(ol," implements low level utilities used by multiple schedulers."),ol.forEach(r),ld=c(t),Me=n(t,"H3",{class:!0});var il=o(Me);et=n(il,"A",{id:!0,class:!0,href:!0});var zb=o(et);Uo=n(zb,"SPAN",{});var Jb=o(Uo);h(fr.$$.fragment,Jb),Jb.forEach(r),zb.forEach(r),fu=c(il),Bo=n(il,"SPAN",{});var jb=o(Bo);pu=a(jb,"SchedulerMixin"),jb.forEach(r),il.forEach(r),cd=c(t),Te=n(t,"DIV",{class:!0});var al=o(Te);h(pr.$$.fragment,al),hu=c(al),Ho=n(al,"P",{});var Qb=o(Ho);mu=a(Qb,"Mixin containing common functions for the schedulers."),Qb.forEach(r),al.forEach(r),ud=c(t),ke=n(t,"H3",{class:!0});var dl=o(ke);tt=n(dl,"A",{id:!0,class:!0,href:!0});var Xb=o(tt);Wo=n(Xb,"SPAN",{});var Zb=o(Wo);h(hr.$$.fragment,Zb),Zb.forEach(r),Xb.forEach(r),gu=c(dl),Go=n(dl,"SPAN",{});var e1=o(Go);_u=a(e1,"SchedulerOutput"),e1.forEach(r),dl.forEach(r),fd=a(t,"\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),Ce=n(t,"DIV",{class:!0});var ll=o(Ce);h(mr.$$.fragment,ll),vu=c(ll),Yo=n(ll,"P",{});var t1=o(Yo);bu=a(t1,"Base class for the scheduler\u2019s step function output."),t1.forEach(r),ll.forEach(r),pd=c(t),Ae=n(t,"H3",{class:!0});var cl=o(Ae);rt=n(cl,"A",{id:!0,class:!0,href:!0});var r1=o(rt);zo=n(r1,"SPAN",{});var s1=o(zo);h(gr.$$.fragment,s1),s1.forEach(r),r1.forEach(r),Su=c(cl),Jo=n(cl,"SPAN",{});var n1=o(Jo);$u=a(n1,"Implemented Schedulers"),n1.forEach(r),cl.forEach(r),hd=c(t),Oe=n(t,"H4",{class:!0});var ul=o(Oe);st=n(ul,"A",{id:!0,class:!0,href:!0});var o1=o(st);jo=n(o1,"SPAN",{});var i1=o(jo);h(_r.$$.fragment,i1),i1.forEach(r),o1.forEach(r),Du=c(ul),Qo=n(ul,"SPAN",{});var a1=o(Qo);Eu=a(a1,"Denoising diffusion implicit models (DDIM)"),a1.forEach(r),ul.forEach(r),md=c(t),dn=n(t,"P",{});var d1=o(dn);xu=a(d1,"Original paper can be found here."),d1.forEach(r),gd=c(t),x=n(t,"DIV",{class:!0});var ae=o(x);h(vr.$$.fragment,ae),yu=c(ae),Xo=n(ae,"P",{});var l1=o(Xo);wu=a(l1,`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),l1.forEach(r),Pu=c(ae),k=n(ae,"P",{});var J=o(k);ln=n(J,"A",{href:!0});var c1=o(ln);Mu=a(c1,"~ConfigMixin"),c1.forEach(r),Tu=a(J," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Zo=n(J,"CODE",{});var u1=o(Zo);ku=a(u1,"__init__"),u1.forEach(r),Cu=a(J,`
function, such as `),ei=n(J,"CODE",{});var f1=o(ei);Au=a(f1,"num_train_timesteps"),f1.forEach(r),Ou=a(J,". They can be accessed via "),ti=n(J,"CODE",{});var p1=o(ti);Vu=a(p1,"scheduler.config.num_train_timesteps"),p1.forEach(r),Nu=a(J,`.
`),cn=n(J,"A",{href:!0});var h1=o(cn);Fu=a(h1,"~ConfigMixin"),h1.forEach(r),Iu=a(J," also provides general loading and saving functionality via the "),un=n(J,"A",{href:!0});var m1=o(un);Lu=a(m1,"save_config()"),m1.forEach(r),Ku=a(J,` and
`),fn=n(J,"A",{href:!0});var g1=o(fn);Ru=a(g1,"from_config()"),g1.forEach(r),qu=a(J," functions."),J.forEach(r),Uu=c(ae),pn=n(ae,"P",{});var sb=o(pn);Bu=a(sb,"For more details, see the original paper: "),br=n(sb,"A",{href:!0,rel:!0});var _1=o(br);Hu=a(_1,"https://arxiv.org/abs/2010.02502"),_1.forEach(r),sb.forEach(r),Wu=c(ae),nt=n(ae,"DIV",{class:!0});var fl=o(nt);h(Sr.$$.fragment,fl),Gu=c(fl),ri=n(fl,"P",{});var v1=o(ri);Yu=a(v1,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),v1.forEach(r),fl.forEach(r),zu=c(ae),ot=n(ae,"DIV",{class:!0});var pl=o(ot);h($r.$$.fragment,pl),Ju=c(pl),si=n(pl,"P",{});var b1=o(si);ju=a(b1,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),b1.forEach(r),pl.forEach(r),Qu=c(ae),it=n(ae,"DIV",{class:!0});var hl=o(it);h(Dr.$$.fragment,hl),Xu=c(hl),ni=n(hl,"P",{});var S1=o(ni);Zu=a(S1,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),S1.forEach(r),hl.forEach(r),ae.forEach(r),_d=c(t),Ve=n(t,"H4",{class:!0});var ml=o(Ve);at=n(ml,"A",{id:!0,class:!0,href:!0});var $1=o(at);oi=n($1,"SPAN",{});var D1=o(oi);h(Er.$$.fragment,D1),D1.forEach(r),$1.forEach(r),ef=c(ml),ii=n(ml,"SPAN",{});var E1=o(ii);tf=a(E1,"Denoising diffusion probabilistic models (DDPM)"),E1.forEach(r),ml.forEach(r),vd=c(t),dt=n(t,"P",{});var gl=o(dt);rf=a(gl,"Original paper can be found "),xr=n(gl,"A",{href:!0,rel:!0});var x1=o(xr);sf=a(x1,"here"),x1.forEach(r),nf=a(gl,"."),gl.forEach(r),bd=c(t),y=n(t,"DIV",{class:!0});var de=o(y);h(yr.$$.fragment,de),of=c(de),ai=n(de,"P",{});var y1=o(ai);af=a(y1,`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),y1.forEach(r),df=c(de),C=n(de,"P",{});var j=o(C);hn=n(j,"A",{href:!0});var w1=o(hn);lf=a(w1,"~ConfigMixin"),w1.forEach(r),cf=a(j," takes care of storing all config attributes that are passed in the scheduler\u2019s "),di=n(j,"CODE",{});var P1=o(di);uf=a(P1,"__init__"),P1.forEach(r),ff=a(j,`
function, such as `),li=n(j,"CODE",{});var M1=o(li);pf=a(M1,"num_train_timesteps"),M1.forEach(r),hf=a(j,". They can be accessed via "),ci=n(j,"CODE",{});var T1=o(ci);mf=a(T1,"scheduler.config.num_train_timesteps"),T1.forEach(r),gf=a(j,`.
`),mn=n(j,"A",{href:!0});var k1=o(mn);_f=a(k1,"~ConfigMixin"),k1.forEach(r),vf=a(j," also provides general loading and saving functionality via the "),gn=n(j,"A",{href:!0});var C1=o(gn);bf=a(C1,"save_config()"),C1.forEach(r),Sf=a(j,` and
`),_n=n(j,"A",{href:!0});var A1=o(_n);$f=a(A1,"from_config()"),A1.forEach(r),Df=a(j," functions."),j.forEach(r),Ef=c(de),vn=n(de,"P",{});var nb=o(vn);xf=a(nb,"For more details, see the original paper: "),wr=n(nb,"A",{href:!0,rel:!0});var O1=o(wr);yf=a(O1,"https://arxiv.org/abs/2006.11239"),O1.forEach(r),nb.forEach(r),wf=c(de),lt=n(de,"DIV",{class:!0});var _l=o(lt);h(Pr.$$.fragment,_l),Pf=c(_l),ui=n(_l,"P",{});var V1=o(ui);Mf=a(V1,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),V1.forEach(r),_l.forEach(r),Tf=c(de),ct=n(de,"DIV",{class:!0});var vl=o(ct);h(Mr.$$.fragment,vl),kf=c(vl),fi=n(vl,"P",{});var N1=o(fi);Cf=a(N1,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),N1.forEach(r),vl.forEach(r),Af=c(de),ut=n(de,"DIV",{class:!0});var bl=o(ut);h(Tr.$$.fragment,bl),Of=c(bl),pi=n(bl,"P",{});var F1=o(pi);Vf=a(F1,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),F1.forEach(r),bl.forEach(r),de.forEach(r),Sd=c(t),Ne=n(t,"H4",{class:!0});var Sl=o(Ne);ft=n(Sl,"A",{id:!0,class:!0,href:!0});var I1=o(ft);hi=n(I1,"SPAN",{});var L1=o(hi);h(kr.$$.fragment,L1),L1.forEach(r),I1.forEach(r),Nf=c(Sl),mi=n(Sl,"SPAN",{});var K1=o(mi);Ff=a(K1,"Variance exploding, stochastic sampling from Karras et. al"),K1.forEach(r),Sl.forEach(r),$d=c(t),pt=n(t,"P",{});var $l=o(pt);If=a($l,"Original paper can be found "),Cr=n($l,"A",{href:!0,rel:!0});var R1=o(Cr);Lf=a(R1,"here"),R1.forEach(r),Kf=a($l,"."),$l.forEach(r),Dd=c(t),S=n(t,"DIV",{class:!0});var E=o(S);h(Ar.$$.fragment,E),Rf=c(E),gi=n(E,"P",{});var q1=o(gi);qf=a(q1,`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),q1.forEach(r),Uf=c(E),ht=n(E,"P",{});var Ya=o(ht);Bf=a(Ya,`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Or=n(Ya,"A",{href:!0,rel:!0});var U1=o(Or);Hf=a(U1,"https://arxiv.org/abs/2206.00364"),U1.forEach(r),Wf=a(Ya,` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Vr=n(Ya,"A",{href:!0,rel:!0});var B1=o(Vr);Gf=a(B1,"https://arxiv.org/abs/2011.13456"),B1.forEach(r),Ya.forEach(r),Yf=c(E),A=n(E,"P",{});var Q=o(A);bn=n(Q,"A",{href:!0});var H1=o(bn);zf=a(H1,"~ConfigMixin"),H1.forEach(r),Jf=a(Q," takes care of storing all config attributes that are passed in the scheduler\u2019s "),_i=n(Q,"CODE",{});var W1=o(_i);jf=a(W1,"__init__"),W1.forEach(r),Qf=a(Q,`
function, such as `),vi=n(Q,"CODE",{});var G1=o(vi);Xf=a(G1,"num_train_timesteps"),G1.forEach(r),Zf=a(Q,". They can be accessed via "),bi=n(Q,"CODE",{});var Y1=o(bi);ep=a(Y1,"scheduler.config.num_train_timesteps"),Y1.forEach(r),tp=a(Q,`.
`),Sn=n(Q,"A",{href:!0});var z1=o(Sn);rp=a(z1,"~ConfigMixin"),z1.forEach(r),sp=a(Q," also provides general loading and saving functionality via the "),$n=n(Q,"A",{href:!0});var J1=o($n);np=a(J1,"save_config()"),J1.forEach(r),op=a(Q,` and
`),Dn=n(Q,"A",{href:!0});var j1=o(Dn);ip=a(j1,"from_config()"),j1.forEach(r),ap=a(Q," functions."),Q.forEach(r),dp=c(E),Nr=n(E,"P",{});var Dl=o(Nr);lp=a(Dl,`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),Fr=n(Dl,"A",{href:!0,rel:!0});var Q1=o(Fr);cp=a(Q1,"https://arxiv.org/abs/2206.00364"),Q1.forEach(r),up=a(Dl,`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),Dl.forEach(r),fp=c(E),ve=n(E,"DIV",{class:!0});var ho=o(ve);h(Ir.$$.fragment,ho),pp=c(ho),Si=n(ho,"P",{});var X1=o(Si);hp=a(X1,`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),X1.forEach(r),mp=c(ho),$i=n(ho,"P",{});var Z1=o($i);gp=a(Z1,"TODO Args:"),Z1.forEach(r),ho.forEach(r),_p=c(E),mt=n(E,"DIV",{class:!0});var El=o(mt);h(Lr.$$.fragment,El),vp=c(El),Di=n(El,"P",{});var e2=o(Di);bp=a(e2,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),e2.forEach(r),El.forEach(r),Sp=c(E),gt=n(E,"DIV",{class:!0});var xl=o(gt);h(Kr.$$.fragment,xl),$p=c(xl),Ei=n(xl,"P",{});var t2=o(Ei);Dp=a(t2,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),t2.forEach(r),xl.forEach(r),Ep=c(E),_t=n(E,"DIV",{class:!0});var yl=o(_t);h(Rr.$$.fragment,yl),xp=c(yl),xi=n(yl,"P",{});var r2=o(xi);yp=a(r2,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),r2.forEach(r),yl.forEach(r),wp=c(E),vt=n(E,"DIV",{class:!0});var wl=o(vt);h(qr.$$.fragment,wl),Pp=c(wl),yi=n(wl,"P",{});var s2=o(yi);Mp=a(s2,"Correct the predicted sample based on the output model_output of the network. TODO complete description"),s2.forEach(r),wl.forEach(r),E.forEach(r),Ed=c(t),Fe=n(t,"H4",{class:!0});var Pl=o(Fe);bt=n(Pl,"A",{id:!0,class:!0,href:!0});var n2=o(bt);wi=n(n2,"SPAN",{});var o2=o(wi);h(Ur.$$.fragment,o2),o2.forEach(r),n2.forEach(r),Tp=c(Pl),Pi=n(Pl,"SPAN",{});var i2=o(Pi);kp=a(i2,"Linear multistep scheduler for discrete beta schedules"),i2.forEach(r),Pl.forEach(r),xd=c(t),St=n(t,"P",{});var Ml=o(St);Cp=a(Ml,"Original implementation can be found "),Br=n(Ml,"A",{href:!0,rel:!0});var a2=o(Br);Ap=a(a2,"here"),a2.forEach(r),Op=a(Ml,"."),Ml.forEach(r),yd=c(t),w=n(t,"DIV",{class:!0});var le=o(w);h(Hr.$$.fragment,le),Vp=c(le),En=n(le,"P",{});var ob=o(En);Np=a(ob,`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),Wr=n(ob,"A",{href:!0,rel:!0});var d2=o(Wr);Fp=a(d2,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d2.forEach(r),ob.forEach(r),Ip=c(le),O=n(le,"P",{});var X=o(O);xn=n(X,"A",{href:!0});var l2=o(xn);Lp=a(l2,"~ConfigMixin"),l2.forEach(r),Kp=a(X," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Mi=n(X,"CODE",{});var c2=o(Mi);Rp=a(c2,"__init__"),c2.forEach(r),qp=a(X,`
function, such as `),Ti=n(X,"CODE",{});var u2=o(Ti);Up=a(u2,"num_train_timesteps"),u2.forEach(r),Bp=a(X,". They can be accessed via "),ki=n(X,"CODE",{});var f2=o(ki);Hp=a(f2,"scheduler.config.num_train_timesteps"),f2.forEach(r),Wp=a(X,`.
`),yn=n(X,"A",{href:!0});var p2=o(yn);Gp=a(p2,"~ConfigMixin"),p2.forEach(r),Yp=a(X," also provides general loading and saving functionality via the "),wn=n(X,"A",{href:!0});var h2=o(wn);zp=a(h2,"save_config()"),h2.forEach(r),Jp=a(X,` and
`),Pn=n(X,"A",{href:!0});var m2=o(Pn);jp=a(m2,"from_config()"),m2.forEach(r),Qp=a(X," functions."),X.forEach(r),Xp=c(le),$t=n(le,"DIV",{class:!0});var Tl=o($t);h(Gr.$$.fragment,Tl),Zp=c(Tl),Ci=n(Tl,"P",{});var g2=o(Ci);eh=a(g2,"Compute a linear multistep coefficient."),g2.forEach(r),Tl.forEach(r),th=c(le),Dt=n(le,"DIV",{class:!0});var kl=o(Dt);h(Yr.$$.fragment,kl),rh=c(kl),zr=n(kl,"P",{});var Cl=o(zr);sh=a(Cl,"Scales the denoising model input by "),Ai=n(Cl,"CODE",{});var _2=o(Ai);nh=a(_2,"(sigma**2 + 1) ** 0.5"),_2.forEach(r),oh=a(Cl," to match the K-LMS algorithm."),Cl.forEach(r),kl.forEach(r),ih=c(le),Et=n(le,"DIV",{class:!0});var Al=o(Et);h(Jr.$$.fragment,Al),ah=c(Al),Oi=n(Al,"P",{});var v2=o(Oi);dh=a(v2,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),v2.forEach(r),Al.forEach(r),lh=c(le),xt=n(le,"DIV",{class:!0});var Ol=o(xt);h(jr.$$.fragment,Ol),ch=c(Ol),Vi=n(Ol,"P",{});var b2=o(Vi);uh=a(b2,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),b2.forEach(r),Ol.forEach(r),le.forEach(r),wd=c(t),Ie=n(t,"H4",{class:!0});var Vl=o(Ie);yt=n(Vl,"A",{id:!0,class:!0,href:!0});var S2=o(yt);Ni=n(S2,"SPAN",{});var $2=o(Ni);h(Qr.$$.fragment,$2),$2.forEach(r),S2.forEach(r),fh=c(Vl),Fi=n(Vl,"SPAN",{});var D2=o(Fi);ph=a(D2,"Pseudo numerical methods for diffusion models (PNDM)"),D2.forEach(r),Vl.forEach(r),Pd=c(t),wt=n(t,"P",{});var Nl=o(wt);hh=a(Nl,"Original implementation can be found "),Xr=n(Nl,"A",{href:!0,rel:!0});var E2=o(Xr);mh=a(E2,"here"),E2.forEach(r),gh=a(Nl,"."),Nl.forEach(r),Md=c(t),$=n(t,"DIV",{class:!0});var q=o($);h(Zr.$$.fragment,q),_h=c(q),Ii=n(q,"P",{});var x2=o(Ii);vh=a(x2,`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),x2.forEach(r),bh=c(q),V=n(q,"P",{});var Z=o(V);Mn=n(Z,"A",{href:!0});var y2=o(Mn);Sh=a(y2,"~ConfigMixin"),y2.forEach(r),$h=a(Z," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Li=n(Z,"CODE",{});var w2=o(Li);Dh=a(w2,"__init__"),w2.forEach(r),Eh=a(Z,`
function, such as `),Ki=n(Z,"CODE",{});var P2=o(Ki);xh=a(P2,"num_train_timesteps"),P2.forEach(r),yh=a(Z,". They can be accessed via "),Ri=n(Z,"CODE",{});var M2=o(Ri);wh=a(M2,"scheduler.config.num_train_timesteps"),M2.forEach(r),Ph=a(Z,`.
`),Tn=n(Z,"A",{href:!0});var T2=o(Tn);Mh=a(T2,"~ConfigMixin"),T2.forEach(r),Th=a(Z," also provides general loading and saving functionality via the "),kn=n(Z,"A",{href:!0});var k2=o(kn);kh=a(k2,"save_config()"),k2.forEach(r),Ch=a(Z,` and
`),Cn=n(Z,"A",{href:!0});var C2=o(Cn);Ah=a(C2,"from_config()"),C2.forEach(r),Oh=a(Z," functions."),Z.forEach(r),Vh=c(q),An=n(q,"P",{});var ib=o(An);Nh=a(ib,"For more details, see the original paper: "),es=n(ib,"A",{href:!0,rel:!0});var A2=o(es);Fh=a(A2,"https://arxiv.org/abs/2202.09778"),A2.forEach(r),ib.forEach(r),Ih=c(q),Pt=n(q,"DIV",{class:!0});var Fl=o(Pt);h(ts.$$.fragment,Fl),Lh=c(Fl),qi=n(Fl,"P",{});var O2=o(qi);Kh=a(O2,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),O2.forEach(r),Fl.forEach(r),Rh=c(q),Mt=n(q,"DIV",{class:!0});var Il=o(Mt);h(rs.$$.fragment,Il),qh=c(Il),Ui=n(Il,"P",{});var V2=o(Ui);Uh=a(V2,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),V2.forEach(r),Il.forEach(r),Bh=c(q),be=n(q,"DIV",{class:!0});var mo=o(be);h(ss.$$.fragment,mo),Hh=c(mo),Bi=n(mo,"P",{});var N2=o(Bi);Wh=a(N2,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),N2.forEach(r),Gh=c(mo),me=n(mo,"P",{});var sr=o(me);Yh=a(sr,"This function calls "),Hi=n(sr,"CODE",{});var F2=o(Hi);zh=a(F2,"step_prk()"),F2.forEach(r),Jh=a(sr," or "),Wi=n(sr,"CODE",{});var I2=o(Wi);jh=a(I2,"step_plms()"),I2.forEach(r),Qh=a(sr," depending on the internal variable "),Gi=n(sr,"CODE",{});var L2=o(Gi);Xh=a(L2,"counter"),L2.forEach(r),Zh=a(sr,"."),sr.forEach(r),mo.forEach(r),em=c(q),Tt=n(q,"DIV",{class:!0});var Ll=o(Tt);h(ns.$$.fragment,Ll),tm=c(Ll),Yi=n(Ll,"P",{});var K2=o(Yi);rm=a(K2,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),K2.forEach(r),Ll.forEach(r),sm=c(q),kt=n(q,"DIV",{class:!0});var Kl=o(kt);h(os.$$.fragment,Kl),nm=c(Kl),zi=n(Kl,"P",{});var R2=o(zi);om=a(R2,`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),R2.forEach(r),Kl.forEach(r),q.forEach(r),Td=c(t),Le=n(t,"H4",{class:!0});var Rl=o(Le);Ct=n(Rl,"A",{id:!0,class:!0,href:!0});var q2=o(Ct);Ji=n(q2,"SPAN",{});var U2=o(Ji);h(is.$$.fragment,U2),U2.forEach(r),q2.forEach(r),im=c(Rl),ji=n(Rl,"SPAN",{});var B2=o(ji);am=a(B2,"variance exploding stochastic differential equation (VE-SDE) scheduler"),B2.forEach(r),Rl.forEach(r),kd=c(t),At=n(t,"P",{});var ql=o(At);dm=a(ql,"Original paper can be found "),as=n(ql,"A",{href:!0,rel:!0});var H2=o(as);lm=a(H2,"here"),H2.forEach(r),cm=a(ql,"."),ql.forEach(r),Cd=c(t),D=n(t,"DIV",{class:!0});var U=o(D);h(ds.$$.fragment,U),um=c(U),Qi=n(U,"P",{});var W2=o(Qi);fm=a(W2,"The variance exploding stochastic differential equation (SDE) scheduler."),W2.forEach(r),pm=c(U),On=n(U,"P",{});var ab=o(On);hm=a(ab,"For more information, see the original paper: "),ls=n(ab,"A",{href:!0,rel:!0});var G2=o(ls);mm=a(G2,"https://arxiv.org/abs/2011.13456"),G2.forEach(r),ab.forEach(r),gm=c(U),N=n(U,"P",{});var ee=o(N);Vn=n(ee,"A",{href:!0});var Y2=o(Vn);_m=a(Y2,"~ConfigMixin"),Y2.forEach(r),vm=a(ee," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Xi=n(ee,"CODE",{});var z2=o(Xi);bm=a(z2,"__init__"),z2.forEach(r),Sm=a(ee,`
function, such as `),Zi=n(ee,"CODE",{});var J2=o(Zi);$m=a(J2,"num_train_timesteps"),J2.forEach(r),Dm=a(ee,". They can be accessed via "),ea=n(ee,"CODE",{});var j2=o(ea);Em=a(j2,"scheduler.config.num_train_timesteps"),j2.forEach(r),xm=a(ee,`.
`),Nn=n(ee,"A",{href:!0});var Q2=o(Nn);ym=a(Q2,"~ConfigMixin"),Q2.forEach(r),wm=a(ee," also provides general loading and saving functionality via the "),Fn=n(ee,"A",{href:!0});var X2=o(Fn);Pm=a(X2,"save_config()"),X2.forEach(r),Mm=a(ee,` and
`),In=n(ee,"A",{href:!0});var Z2=o(In);Tm=a(Z2,"from_config()"),Z2.forEach(r),km=a(ee," functions."),ee.forEach(r),Cm=c(U),Ot=n(U,"DIV",{class:!0});var Ul=o(Ot);h(cs.$$.fragment,Ul),Am=c(Ul),ta=n(Ul,"P",{});var e0=o(ta);Om=a(e0,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),e0.forEach(r),Ul.forEach(r),Vm=c(U),Se=n(U,"DIV",{class:!0});var go=o(Se);h(us.$$.fragment,go),Nm=c(go),ra=n(go,"P",{});var t0=o(ra);Fm=a(t0,"Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),t0.forEach(r),Im=c(go),Ke=n(go,"P",{});var _o=o(Ke);Lm=a(_o,"The sigmas control the weight of the "),sa=n(_o,"CODE",{});var r0=o(sa);Km=a(r0,"drift"),r0.forEach(r),Rm=a(_o," and "),na=n(_o,"CODE",{});var s0=o(na);qm=a(s0,"diffusion"),s0.forEach(r),Um=a(_o," components of sample update."),_o.forEach(r),go.forEach(r),Bm=c(U),Vt=n(U,"DIV",{class:!0});var Bl=o(Vt);h(fs.$$.fragment,Bl),Hm=c(Bl),oa=n(Bl,"P",{});var n0=o(oa);Wm=a(n0,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),n0.forEach(r),Bl.forEach(r),Gm=c(U),Nt=n(U,"DIV",{class:!0});var Hl=o(Nt);h(ps.$$.fragment,Hl),Ym=c(Hl),ia=n(Hl,"P",{});var o0=o(ia);zm=a(o0,`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),o0.forEach(r),Hl.forEach(r),Jm=c(U),Ft=n(U,"DIV",{class:!0});var Wl=o(Ft);h(hs.$$.fragment,Wl),jm=c(Wl),aa=n(Wl,"P",{});var i0=o(aa);Qm=a(i0,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),i0.forEach(r),Wl.forEach(r),U.forEach(r),Ad=c(t),Re=n(t,"H4",{class:!0});var Gl=o(Re);It=n(Gl,"A",{id:!0,class:!0,href:!0});var a0=o(It);da=n(a0,"SPAN",{});var d0=o(da);h(ms.$$.fragment,d0),d0.forEach(r),a0.forEach(r),Xm=c(Gl),la=n(Gl,"SPAN",{});var l0=o(la);Zm=a(l0,"improved pseudo numerical methods for diffusion models (iPNDM)"),l0.forEach(r),Gl.forEach(r),Od=c(t),Lt=n(t,"P",{});var Yl=o(Lt);eg=a(Yl,"Original implementation can be found "),gs=n(Yl,"A",{href:!0,rel:!0});var c0=o(gs);tg=a(c0,"here"),c0.forEach(r),rg=a(Yl,"."),Yl.forEach(r),Vd=c(t),P=n(t,"DIV",{class:!0});var ce=o(P);h(_s.$$.fragment,ce),sg=c(ce),Ln=n(ce,"P",{});var db=o(Ln);ng=a(db,`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),vs=n(db,"A",{href:!0,rel:!0});var u0=o(vs);og=a(u0,"library"),u0.forEach(r),db.forEach(r),ig=c(ce),F=n(ce,"P",{});var te=o(F);Kn=n(te,"A",{href:!0});var f0=o(Kn);ag=a(f0,"~ConfigMixin"),f0.forEach(r),dg=a(te," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ca=n(te,"CODE",{});var p0=o(ca);lg=a(p0,"__init__"),p0.forEach(r),cg=a(te,`
function, such as `),ua=n(te,"CODE",{});var h0=o(ua);ug=a(h0,"num_train_timesteps"),h0.forEach(r),fg=a(te,". They can be accessed via "),fa=n(te,"CODE",{});var m0=o(fa);pg=a(m0,"scheduler.config.num_train_timesteps"),m0.forEach(r),hg=a(te,`.
`),Rn=n(te,"A",{href:!0});var g0=o(Rn);mg=a(g0,"~ConfigMixin"),g0.forEach(r),gg=a(te," also provides general loading and saving functionality via the "),qn=n(te,"A",{href:!0});var _0=o(qn);_g=a(_0,"save_config()"),_0.forEach(r),vg=a(te,` and
`),Un=n(te,"A",{href:!0});var v0=o(Un);bg=a(v0,"from_config()"),v0.forEach(r),Sg=a(te," functions."),te.forEach(r),$g=c(ce),Bn=n(ce,"P",{});var lb=o(Bn);Dg=a(lb,"For more details, see the original paper: "),bs=n(lb,"A",{href:!0,rel:!0});var b0=o(bs);Eg=a(b0,"https://arxiv.org/abs/2202.09778"),b0.forEach(r),lb.forEach(r),xg=c(ce),Kt=n(ce,"DIV",{class:!0});var zl=o(Kt);h(Ss.$$.fragment,zl),yg=c(zl),pa=n(zl,"P",{});var S0=o(pa);wg=a(S0,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),S0.forEach(r),zl.forEach(r),Pg=c(ce),Rt=n(ce,"DIV",{class:!0});var Jl=o(Rt);h($s.$$.fragment,Jl),Mg=c(Jl),ha=n(Jl,"P",{});var $0=o(ha);Tg=a($0,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),$0.forEach(r),Jl.forEach(r),kg=c(ce),qt=n(ce,"DIV",{class:!0});var jl=o(qt);h(Ds.$$.fragment,jl),Cg=c(jl),ma=n(jl,"P",{});var D0=o(ma);Ag=a(D0,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),D0.forEach(r),jl.forEach(r),ce.forEach(r),Nd=c(t),qe=n(t,"H4",{class:!0});var Ql=o(qe);Ut=n(Ql,"A",{id:!0,class:!0,href:!0});var E0=o(Ut);ga=n(E0,"SPAN",{});var x0=o(ga);h(Es.$$.fragment,x0),x0.forEach(r),E0.forEach(r),Og=c(Ql),_a=n(Ql,"SPAN",{});var y0=o(_a);Vg=a(y0,"variance preserving stochastic differential equation (VP-SDE) scheduler"),y0.forEach(r),Ql.forEach(r),Fd=c(t),Bt=n(t,"P",{});var Xl=o(Bt);Ng=a(Xl,"Original paper can be found "),xs=n(Xl,"A",{href:!0,rel:!0});var w0=o(xs);Fg=a(w0,"here"),w0.forEach(r),Ig=a(Xl,"."),Xl.forEach(r),Id=c(t),h(Ht.$$.fragment,t),Ld=c(t),z=n(t,"DIV",{class:!0});var De=o(z);h(ys.$$.fragment,De),Lg=c(De),va=n(De,"P",{});var P0=o(va);Kg=a(P0,"The variance preserving stochastic differential equation (SDE) scheduler."),P0.forEach(r),Rg=c(De),I=n(De,"P",{});var re=o(I);Hn=n(re,"A",{href:!0});var M0=o(Hn);qg=a(M0,"~ConfigMixin"),M0.forEach(r),Ug=a(re," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ba=n(re,"CODE",{});var T0=o(ba);Bg=a(T0,"__init__"),T0.forEach(r),Hg=a(re,`
function, such as `),Sa=n(re,"CODE",{});var k0=o(Sa);Wg=a(k0,"num_train_timesteps"),k0.forEach(r),Gg=a(re,". They can be accessed via "),$a=n(re,"CODE",{});var C0=o($a);Yg=a(C0,"scheduler.config.num_train_timesteps"),C0.forEach(r),zg=a(re,`.
`),Wn=n(re,"A",{href:!0});var A0=o(Wn);Jg=a(A0,"~ConfigMixin"),A0.forEach(r),jg=a(re," also provides general loading and saving functionality via the "),Gn=n(re,"A",{href:!0});var O0=o(Gn);Qg=a(O0,"save_config()"),O0.forEach(r),Xg=a(re,` and
`),Yn=n(re,"A",{href:!0});var V0=o(Yn);Zg=a(V0,"from_config()"),V0.forEach(r),e_=a(re," functions."),re.forEach(r),t_=c(De),zn=n(De,"P",{});var cb=o(zn);r_=a(cb,"For more information, see the original paper: "),ws=n(cb,"A",{href:!0,rel:!0});var N0=o(ws);s_=a(N0,"https://arxiv.org/abs/2011.13456"),N0.forEach(r),cb.forEach(r),n_=c(De),Da=n(De,"P",{});var F0=o(Da);o_=a(F0,"UNDER CONSTRUCTION"),F0.forEach(r),De.forEach(r),Kd=c(t),Ue=n(t,"H4",{class:!0});var Zl=o(Ue);Wt=n(Zl,"A",{id:!0,class:!0,href:!0});var I0=o(Wt);Ea=n(I0,"SPAN",{});var L0=o(Ea);h(Ps.$$.fragment,L0),L0.forEach(r),I0.forEach(r),i_=c(Zl),xa=n(Zl,"SPAN",{});var K0=o(xa);a_=a(K0,"Euler scheduler"),K0.forEach(r),Zl.forEach(r),Rd=c(t),$e=n(t,"P",{});var vo=o($e);d_=a(vo,"Euler scheduler (Algorithm 2) from the paper "),Ms=n(vo,"A",{href:!0,rel:!0});var R0=o(Ms);l_=a(R0,"Elucidating the Design Space of Diffusion-Based Generative Models"),R0.forEach(r),c_=a(vo," by Karras et al. (2022). Based on the original "),Ts=n(vo,"A",{href:!0,rel:!0});var q0=o(Ts);u_=a(q0,"k-diffusion"),q0.forEach(r),f_=a(vo,` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),vo.forEach(r),qd=c(t),B=n(t,"DIV",{class:!0});var fe=o(B);h(ks.$$.fragment,fe),p_=c(fe),Gt=n(fe,"P",{});var za=o(Gt);h_=a(za,"Euler scheduler (Algorithm 2) from Karras et al. (2022) "),Cs=n(za,"A",{href:!0,rel:!0});var U0=o(Cs);m_=a(U0,"https://arxiv.org/abs/2206.00364"),U0.forEach(r),g_=a(za,`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),As=n(za,"A",{href:!0,rel:!0});var B0=o(As);__=a(B0,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),B0.forEach(r),za.forEach(r),v_=c(fe),L=n(fe,"P",{});var se=o(L);Jn=n(se,"A",{href:!0});var H0=o(Jn);b_=a(H0,"~ConfigMixin"),H0.forEach(r),S_=a(se," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ya=n(se,"CODE",{});var W0=o(ya);$_=a(W0,"__init__"),W0.forEach(r),D_=a(se,`
function, such as `),wa=n(se,"CODE",{});var G0=o(wa);E_=a(G0,"num_train_timesteps"),G0.forEach(r),x_=a(se,". They can be accessed via "),Pa=n(se,"CODE",{});var Y0=o(Pa);y_=a(Y0,"scheduler.config.num_train_timesteps"),Y0.forEach(r),w_=a(se,`.
`),jn=n(se,"A",{href:!0});var z0=o(jn);P_=a(z0,"~ConfigMixin"),z0.forEach(r),M_=a(se," also provides general loading and saving functionality via the "),Qn=n(se,"A",{href:!0});var J0=o(Qn);T_=a(J0,"save_config()"),J0.forEach(r),k_=a(se,` and
`),Xn=n(se,"A",{href:!0});var j0=o(Xn);C_=a(j0,"from_config()"),j0.forEach(r),A_=a(se," functions."),se.forEach(r),O_=c(fe),Yt=n(fe,"DIV",{class:!0});var ec=o(Yt);h(Os.$$.fragment,ec),V_=c(ec),Vs=n(ec,"P",{});var tc=o(Vs);N_=a(tc,"Scales the denoising model input by "),Ma=n(tc,"CODE",{});var Q0=o(Ma);F_=a(Q0,"(sigma**2 + 1) ** 0.5"),Q0.forEach(r),I_=a(tc," to match the Euler algorithm."),tc.forEach(r),ec.forEach(r),L_=c(fe),zt=n(fe,"DIV",{class:!0});var rc=o(zt);h(Ns.$$.fragment,rc),K_=c(rc),Ta=n(rc,"P",{});var X0=o(Ta);R_=a(X0,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),X0.forEach(r),rc.forEach(r),q_=c(fe),Jt=n(fe,"DIV",{class:!0});var sc=o(Jt);h(Fs.$$.fragment,sc),U_=c(sc),ka=n(sc,"P",{});var Z0=o(ka);B_=a(Z0,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Z0.forEach(r),sc.forEach(r),fe.forEach(r),Ud=c(t),Be=n(t,"H4",{class:!0});var nc=o(Be);jt=n(nc,"A",{id:!0,class:!0,href:!0});var e4=o(jt);Ca=n(e4,"SPAN",{});var t4=o(Ca);h(Is.$$.fragment,t4),t4.forEach(r),e4.forEach(r),H_=c(nc),Aa=n(nc,"SPAN",{});var r4=o(Aa);W_=a(r4,"Euler Ancestral scheduler"),r4.forEach(r),nc.forEach(r),Bd=c(t),Zn=n(t,"P",{});var s4=o(Zn);G_=a(s4,`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),s4.forEach(r),Hd=c(t),H=n(t,"DIV",{class:!0});var pe=o(H);h(Ls.$$.fragment,pe),Y_=c(pe),eo=n(pe,"P",{});var ub=o(eo);z_=a(ub,`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Ks=n(ub,"A",{href:!0,rel:!0});var n4=o(Ks);J_=a(n4,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),n4.forEach(r),ub.forEach(r),j_=c(pe),K=n(pe,"P",{});var ne=o(K);to=n(ne,"A",{href:!0});var o4=o(to);Q_=a(o4,"~ConfigMixin"),o4.forEach(r),X_=a(ne," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Oa=n(ne,"CODE",{});var i4=o(Oa);Z_=a(i4,"__init__"),i4.forEach(r),ev=a(ne,`
function, such as `),Va=n(ne,"CODE",{});var a4=o(Va);tv=a(a4,"num_train_timesteps"),a4.forEach(r),rv=a(ne,". They can be accessed via "),Na=n(ne,"CODE",{});var d4=o(Na);sv=a(d4,"scheduler.config.num_train_timesteps"),d4.forEach(r),nv=a(ne,`.
`),ro=n(ne,"A",{href:!0});var l4=o(ro);ov=a(l4,"~ConfigMixin"),l4.forEach(r),iv=a(ne," also provides general loading and saving functionality via the "),so=n(ne,"A",{href:!0});var c4=o(so);av=a(c4,"save_config()"),c4.forEach(r),dv=a(ne,` and
`),no=n(ne,"A",{href:!0});var u4=o(no);lv=a(u4,"from_config()"),u4.forEach(r),cv=a(ne," functions."),ne.forEach(r),uv=c(pe),Qt=n(pe,"DIV",{class:!0});var oc=o(Qt);h(Rs.$$.fragment,oc),fv=c(oc),qs=n(oc,"P",{});var ic=o(qs);pv=a(ic,"Scales the denoising model input by "),Fa=n(ic,"CODE",{});var f4=o(Fa);hv=a(f4,"(sigma**2 + 1) ** 0.5"),f4.forEach(r),mv=a(ic," to match the Euler algorithm."),ic.forEach(r),oc.forEach(r),gv=c(pe),Xt=n(pe,"DIV",{class:!0});var ac=o(Xt);h(Us.$$.fragment,ac),_v=c(ac),Ia=n(ac,"P",{});var p4=o(Ia);vv=a(p4,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),p4.forEach(r),ac.forEach(r),bv=c(pe),Zt=n(pe,"DIV",{class:!0});var dc=o(Zt);h(Bs.$$.fragment,dc),Sv=c(dc),La=n(dc,"P",{});var h4=o(La);$v=a(h4,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),h4.forEach(r),dc.forEach(r),pe.forEach(r),Wd=c(t),He=n(t,"H4",{class:!0});var lc=o(He);er=n(lc,"A",{id:!0,class:!0,href:!0});var m4=o(er);Ka=n(m4,"SPAN",{});var g4=o(Ka);h(Hs.$$.fragment,g4),g4.forEach(r),m4.forEach(r),Dv=c(lc),Ra=n(lc,"SPAN",{});var _4=o(Ra);Ev=a(_4,"RePaint scheduler"),_4.forEach(r),lc.forEach(r),Gd=c(t),ge=n(t,"P",{});var Xs=o(ge);xv=a(Xs,`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),oo=n(Xs,"A",{href:!0});var v4=o(oo);yv=a(v4,"RePaintPipeline"),v4.forEach(r),wv=a(Xs,`.
Based on the paper `),Ws=n(Xs,"A",{href:!0,rel:!0});var b4=o(Ws);Pv=a(b4,"RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),b4.forEach(r),Mv=a(Xs,`
and the original implementation by Andreas Lugmayr et al.: `),Gs=n(Xs,"A",{href:!0,rel:!0});var S4=o(Gs);Tv=a(S4,"https://github.com/andreas128/RePaint"),S4.forEach(r),Xs.forEach(r),Yd=c(t),W=n(t,"DIV",{class:!0});var he=o(W);h(Ys.$$.fragment,he),kv=c(he),qa=n(he,"P",{});var $4=o(qa);Cv=a($4,"RePaint is a schedule for DDPM inpainting inside a given mask."),$4.forEach(r),Av=c(he),R=n(he,"P",{});var oe=o(R);io=n(oe,"A",{href:!0});var D4=o(io);Ov=a(D4,"~ConfigMixin"),D4.forEach(r),Vv=a(oe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ua=n(oe,"CODE",{});var E4=o(Ua);Nv=a(E4,"__init__"),E4.forEach(r),Fv=a(oe,`
function, such as `),Ba=n(oe,"CODE",{});var x4=o(Ba);Iv=a(x4,"num_train_timesteps"),x4.forEach(r),Lv=a(oe,". They can be accessed via "),Ha=n(oe,"CODE",{});var y4=o(Ha);Kv=a(y4,"scheduler.config.num_train_timesteps"),y4.forEach(r),Rv=a(oe,`.
`),ao=n(oe,"A",{href:!0});var w4=o(ao);qv=a(w4,"~ConfigMixin"),w4.forEach(r),Uv=a(oe," also provides general loading and saving functionality via the "),lo=n(oe,"A",{href:!0});var P4=o(lo);Bv=a(P4,"save_config()"),P4.forEach(r),Hv=a(oe,` and
`),co=n(oe,"A",{href:!0});var M4=o(co);Wv=a(M4,"from_config()"),M4.forEach(r),Gv=a(oe," functions."),oe.forEach(r),Yv=c(he),uo=n(he,"P",{});var fb=o(uo);zv=a(fb,"For more details, see the original paper: "),zs=n(fb,"A",{href:!0,rel:!0});var T4=o(zs);Jv=a(T4,"https://arxiv.org/pdf/2201.09865.pdf"),T4.forEach(r),fb.forEach(r),jv=c(he),tr=n(he,"DIV",{class:!0});var cc=o(tr);h(Js.$$.fragment,cc),Qv=c(cc),Wa=n(cc,"P",{});var k4=o(Wa);Xv=a(k4,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),k4.forEach(r),cc.forEach(r),Zv=c(he),rr=n(he,"DIV",{class:!0});var uc=o(rr);h(js.$$.fragment,uc),eb=c(uc),Ga=n(uc,"P",{});var C4=o(Ga);tb=a(C4,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),C4.forEach(r),uc.forEach(r),he.forEach(r),this.h()},h(){d(G,"name","hf:doc:metadata"),d(G,"content",JSON.stringify(R4)),d(ue,"id","schedulers"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#schedulers"),d(Y,"class","relative group"),d(Ge,"id","what-is-a-scheduler"),d(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ge,"href","#what-is-a-scheduler"),d(Ee,"class","relative group"),d(Je,"id","discrete-versus-continuous-schedulers"),d(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Je,"href","#discrete-versus-continuous-schedulers"),d(ye,"class","relative group"),d(tn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.DDPMScheduler"),d(rn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler"),d(sn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.ScoreSdeVeScheduler"),d(je,"id","designing-reusable-schedulers"),d(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(je,"href","#designing-reusable-schedulers"),d(we,"class","relative group"),d(Xe,"id","api"),d(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xe,"href","#api"),d(Pe,"class","relative group"),d(an,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(et,"id","diffusers.SchedulerMixin"),d(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(et,"href","#diffusers.SchedulerMixin"),d(Me,"class","relative group"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(tt,"id","diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(tt,"href","#diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(ke,"class","relative group"),d(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(rt,"id","implemented-schedulers"),d(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(rt,"href","#implemented-schedulers"),d(Ae,"class","relative group"),d(st,"id","diffusers.DDIMScheduler"),d(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(st,"href","#diffusers.DDIMScheduler"),d(Oe,"class","relative group"),d(ln,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(cn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(un,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(fn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(br,"href","https://arxiv.org/abs/2010.02502"),d(br,"rel","nofollow"),d(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(at,"id","diffusers.DDPMScheduler"),d(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(at,"href","#diffusers.DDPMScheduler"),d(Ve,"class","relative group"),d(xr,"href","https://arxiv.org/abs/2010.02502"),d(xr,"rel","nofollow"),d(hn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(mn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(gn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(_n,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(wr,"href","https://arxiv.org/abs/2006.11239"),d(wr,"rel","nofollow"),d(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ft,"id","diffusers.KarrasVeScheduler"),d(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ft,"href","#diffusers.KarrasVeScheduler"),d(Ne,"class","relative group"),d(Cr,"href","https://arxiv.org/abs/2006.11239"),d(Cr,"rel","nofollow"),d(Or,"href","https://arxiv.org/abs/2206.00364"),d(Or,"rel","nofollow"),d(Vr,"href","https://arxiv.org/abs/2011.13456"),d(Vr,"rel","nofollow"),d(bn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Sn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d($n,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Dn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Fr,"href","https://arxiv.org/abs/2206.00364"),d(Fr,"rel","nofollow"),d(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(bt,"id","diffusers.LMSDiscreteScheduler"),d(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(bt,"href","#diffusers.LMSDiscreteScheduler"),d(Fe,"class","relative group"),d(Br,"href","https://arxiv.org/abs/2206.00364"),d(Br,"rel","nofollow"),d(Wr,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(Wr,"rel","nofollow"),d(xn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(yn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(wn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Pn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d($t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(xt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(yt,"id","diffusers.PNDMScheduler"),d(yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(yt,"href","#diffusers.PNDMScheduler"),d(Ie,"class","relative group"),d(Xr,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(Xr,"rel","nofollow"),d(Mn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Tn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(kn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Cn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(es,"href","https://arxiv.org/abs/2202.09778"),d(es,"rel","nofollow"),d(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ct,"id","diffusers.ScoreSdeVeScheduler"),d(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ct,"href","#diffusers.ScoreSdeVeScheduler"),d(Le,"class","relative group"),d(as,"href","https://arxiv.org/abs/2011.13456"),d(as,"rel","nofollow"),d(ls,"href","https://arxiv.org/abs/2011.13456"),d(ls,"rel","nofollow"),d(Vn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Nn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Fn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(In,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(It,"id","diffusers.IPNDMScheduler"),d(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(It,"href","#diffusers.IPNDMScheduler"),d(Re,"class","relative group"),d(gs,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(gs,"rel","nofollow"),d(vs,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(vs,"rel","nofollow"),d(Kn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Rn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(qn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Un,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(bs,"href","https://arxiv.org/abs/2202.09778"),d(bs,"rel","nofollow"),d(Kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ut,"id","diffusers.schedulers.ScoreSdeVpScheduler"),d(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ut,"href","#diffusers.schedulers.ScoreSdeVpScheduler"),d(qe,"class","relative group"),d(xs,"href","https://arxiv.org/abs/2011.13456"),d(xs,"rel","nofollow"),d(Hn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Wn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Gn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Yn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(ws,"href","https://arxiv.org/abs/2011.13456"),d(ws,"rel","nofollow"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Wt,"id","diffusers.EulerDiscreteScheduler"),d(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Wt,"href","#diffusers.EulerDiscreteScheduler"),d(Ue,"class","relative group"),d(Ms,"href","https://arxiv.org/abs/2206.00364"),d(Ms,"rel","nofollow"),d(Ts,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(Ts,"rel","nofollow"),d(Cs,"href","https://arxiv.org/abs/2206.00364"),d(Cs,"rel","nofollow"),d(As,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(As,"rel","nofollow"),d(Jn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(jn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Qn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Xn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(jt,"id","diffusers.EulerAncestralDiscreteScheduler"),d(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(jt,"href","#diffusers.EulerAncestralDiscreteScheduler"),d(Be,"class","relative group"),d(Ks,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),d(Ks,"rel","nofollow"),d(to,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ro,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(so,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(no,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(er,"id","diffusers.RePaintScheduler"),d(er,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(er,"href","#diffusers.RePaintScheduler"),d(He,"class","relative group"),d(oo,"href","/docs/diffusers/main/en/api/pipelines/repaint#diffusers.RePaintPipeline"),d(Ws,"href","https://arxiv.org/abs/2201.09865"),d(Ws,"rel","nofollow"),d(Gs,"href","https://github.com/andreas128/RePaint"),d(Gs,"rel","nofollow"),d(io,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ao,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(lo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(co,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(zs,"href","https://arxiv.org/pdf/2201.09865.pdf"),d(zs,"rel","nofollow"),d(tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,G),f(t,We,u),f(t,Y,u),e(Y,ue),e(ue,bo),m(nr,bo,null),e(Y,fc),e(Y,So),e(So,pc),f(t,ja,u),f(t,Zs,u),e(Zs,hc),f(t,Qa,u),f(t,Ee,u),e(Ee,Ge),e(Ge,$o),m(or,$o,null),e(Ee,mc),e(Ee,Do),e(Do,gc),f(t,Xa,u),f(t,Ye,u),e(Ye,_c),e(Ye,Eo),e(Eo,vc),e(Ye,bc),f(t,Za,u),f(t,ze,u),e(ze,en),e(en,Sc),e(en,ir),e(ir,xo),e(xo,$c),e(ir,Dc),e(ir,yo),e(yo,Ec),e(ze,xc),e(ze,xe),e(xe,yc),e(xe,wo),e(wo,wc),e(xe,Pc),e(xe,Po),e(Po,Mc),e(xe,Tc),f(t,ed,u),f(t,ye,u),e(ye,Je),e(Je,Mo),m(ar,Mo,null),e(ye,kc),e(ye,To),e(To,Cc),f(t,td,u),f(t,T,u),e(T,Ac),e(T,ko),e(ko,Oc),e(T,Vc),e(T,tn),e(tn,Nc),e(T,Fc),e(T,rn),e(rn,Ic),e(T,Lc),e(T,Co),e(Co,Kc),e(T,Rc),e(T,sn),e(sn,qc),e(T,Uc),e(T,Ao),e(Ao,Bc),e(T,Hc),f(t,rd,u),f(t,we,u),e(we,je),e(je,Oo),m(dr,Oo,null),e(we,Wc),e(we,Vo),e(Vo,Gc),f(t,sd,u),f(t,nn,u),e(nn,Yc),f(t,nd,u),f(t,Qe,u),e(Qe,No),e(No,zc),e(Qe,Jc),e(Qe,Fo),e(Fo,jc),f(t,od,u),f(t,Pe,u),e(Pe,Xe),e(Xe,Io),m(lr,Io,null),e(Pe,Qc),e(Pe,Lo),e(Lo,Xc),f(t,id,u),f(t,on,u),e(on,Zc),f(t,ad,u),f(t,_e,u),e(_e,cr),e(cr,eu),e(cr,Ko),e(Ko,tu),e(cr,ru),e(_e,su),e(_e,ur),e(ur,nu),e(ur,Ro),e(Ro,ou),e(ur,iu),e(_e,au),e(_e,qo),e(qo,du),f(t,dd,u),f(t,Ze,u),e(Ze,lu),e(Ze,an),e(an,cu),e(Ze,uu),f(t,ld,u),f(t,Me,u),e(Me,et),e(et,Uo),m(fr,Uo,null),e(Me,fu),e(Me,Bo),e(Bo,pu),f(t,cd,u),f(t,Te,u),m(pr,Te,null),e(Te,hu),e(Te,Ho),e(Ho,mu),f(t,ud,u),f(t,ke,u),e(ke,tt),e(tt,Wo),m(hr,Wo,null),e(ke,gu),e(ke,Go),e(Go,_u),f(t,fd,u),f(t,Ce,u),m(mr,Ce,null),e(Ce,vu),e(Ce,Yo),e(Yo,bu),f(t,pd,u),f(t,Ae,u),e(Ae,rt),e(rt,zo),m(gr,zo,null),e(Ae,Su),e(Ae,Jo),e(Jo,$u),f(t,hd,u),f(t,Oe,u),e(Oe,st),e(st,jo),m(_r,jo,null),e(Oe,Du),e(Oe,Qo),e(Qo,Eu),f(t,md,u),f(t,dn,u),e(dn,xu),f(t,gd,u),f(t,x,u),m(vr,x,null),e(x,yu),e(x,Xo),e(Xo,wu),e(x,Pu),e(x,k),e(k,ln),e(ln,Mu),e(k,Tu),e(k,Zo),e(Zo,ku),e(k,Cu),e(k,ei),e(ei,Au),e(k,Ou),e(k,ti),e(ti,Vu),e(k,Nu),e(k,cn),e(cn,Fu),e(k,Iu),e(k,un),e(un,Lu),e(k,Ku),e(k,fn),e(fn,Ru),e(k,qu),e(x,Uu),e(x,pn),e(pn,Bu),e(pn,br),e(br,Hu),e(x,Wu),e(x,nt),m(Sr,nt,null),e(nt,Gu),e(nt,ri),e(ri,Yu),e(x,zu),e(x,ot),m($r,ot,null),e(ot,Ju),e(ot,si),e(si,ju),e(x,Qu),e(x,it),m(Dr,it,null),e(it,Xu),e(it,ni),e(ni,Zu),f(t,_d,u),f(t,Ve,u),e(Ve,at),e(at,oi),m(Er,oi,null),e(Ve,ef),e(Ve,ii),e(ii,tf),f(t,vd,u),f(t,dt,u),e(dt,rf),e(dt,xr),e(xr,sf),e(dt,nf),f(t,bd,u),f(t,y,u),m(yr,y,null),e(y,of),e(y,ai),e(ai,af),e(y,df),e(y,C),e(C,hn),e(hn,lf),e(C,cf),e(C,di),e(di,uf),e(C,ff),e(C,li),e(li,pf),e(C,hf),e(C,ci),e(ci,mf),e(C,gf),e(C,mn),e(mn,_f),e(C,vf),e(C,gn),e(gn,bf),e(C,Sf),e(C,_n),e(_n,$f),e(C,Df),e(y,Ef),e(y,vn),e(vn,xf),e(vn,wr),e(wr,yf),e(y,wf),e(y,lt),m(Pr,lt,null),e(lt,Pf),e(lt,ui),e(ui,Mf),e(y,Tf),e(y,ct),m(Mr,ct,null),e(ct,kf),e(ct,fi),e(fi,Cf),e(y,Af),e(y,ut),m(Tr,ut,null),e(ut,Of),e(ut,pi),e(pi,Vf),f(t,Sd,u),f(t,Ne,u),e(Ne,ft),e(ft,hi),m(kr,hi,null),e(Ne,Nf),e(Ne,mi),e(mi,Ff),f(t,$d,u),f(t,pt,u),e(pt,If),e(pt,Cr),e(Cr,Lf),e(pt,Kf),f(t,Dd,u),f(t,S,u),m(Ar,S,null),e(S,Rf),e(S,gi),e(gi,qf),e(S,Uf),e(S,ht),e(ht,Bf),e(ht,Or),e(Or,Hf),e(ht,Wf),e(ht,Vr),e(Vr,Gf),e(S,Yf),e(S,A),e(A,bn),e(bn,zf),e(A,Jf),e(A,_i),e(_i,jf),e(A,Qf),e(A,vi),e(vi,Xf),e(A,Zf),e(A,bi),e(bi,ep),e(A,tp),e(A,Sn),e(Sn,rp),e(A,sp),e(A,$n),e($n,np),e(A,op),e(A,Dn),e(Dn,ip),e(A,ap),e(S,dp),e(S,Nr),e(Nr,lp),e(Nr,Fr),e(Fr,cp),e(Nr,up),e(S,fp),e(S,ve),m(Ir,ve,null),e(ve,pp),e(ve,Si),e(Si,hp),e(ve,mp),e(ve,$i),e($i,gp),e(S,_p),e(S,mt),m(Lr,mt,null),e(mt,vp),e(mt,Di),e(Di,bp),e(S,Sp),e(S,gt),m(Kr,gt,null),e(gt,$p),e(gt,Ei),e(Ei,Dp),e(S,Ep),e(S,_t),m(Rr,_t,null),e(_t,xp),e(_t,xi),e(xi,yp),e(S,wp),e(S,vt),m(qr,vt,null),e(vt,Pp),e(vt,yi),e(yi,Mp),f(t,Ed,u),f(t,Fe,u),e(Fe,bt),e(bt,wi),m(Ur,wi,null),e(Fe,Tp),e(Fe,Pi),e(Pi,kp),f(t,xd,u),f(t,St,u),e(St,Cp),e(St,Br),e(Br,Ap),e(St,Op),f(t,yd,u),f(t,w,u),m(Hr,w,null),e(w,Vp),e(w,En),e(En,Np),e(En,Wr),e(Wr,Fp),e(w,Ip),e(w,O),e(O,xn),e(xn,Lp),e(O,Kp),e(O,Mi),e(Mi,Rp),e(O,qp),e(O,Ti),e(Ti,Up),e(O,Bp),e(O,ki),e(ki,Hp),e(O,Wp),e(O,yn),e(yn,Gp),e(O,Yp),e(O,wn),e(wn,zp),e(O,Jp),e(O,Pn),e(Pn,jp),e(O,Qp),e(w,Xp),e(w,$t),m(Gr,$t,null),e($t,Zp),e($t,Ci),e(Ci,eh),e(w,th),e(w,Dt),m(Yr,Dt,null),e(Dt,rh),e(Dt,zr),e(zr,sh),e(zr,Ai),e(Ai,nh),e(zr,oh),e(w,ih),e(w,Et),m(Jr,Et,null),e(Et,ah),e(Et,Oi),e(Oi,dh),e(w,lh),e(w,xt),m(jr,xt,null),e(xt,ch),e(xt,Vi),e(Vi,uh),f(t,wd,u),f(t,Ie,u),e(Ie,yt),e(yt,Ni),m(Qr,Ni,null),e(Ie,fh),e(Ie,Fi),e(Fi,ph),f(t,Pd,u),f(t,wt,u),e(wt,hh),e(wt,Xr),e(Xr,mh),e(wt,gh),f(t,Md,u),f(t,$,u),m(Zr,$,null),e($,_h),e($,Ii),e(Ii,vh),e($,bh),e($,V),e(V,Mn),e(Mn,Sh),e(V,$h),e(V,Li),e(Li,Dh),e(V,Eh),e(V,Ki),e(Ki,xh),e(V,yh),e(V,Ri),e(Ri,wh),e(V,Ph),e(V,Tn),e(Tn,Mh),e(V,Th),e(V,kn),e(kn,kh),e(V,Ch),e(V,Cn),e(Cn,Ah),e(V,Oh),e($,Vh),e($,An),e(An,Nh),e(An,es),e(es,Fh),e($,Ih),e($,Pt),m(ts,Pt,null),e(Pt,Lh),e(Pt,qi),e(qi,Kh),e($,Rh),e($,Mt),m(rs,Mt,null),e(Mt,qh),e(Mt,Ui),e(Ui,Uh),e($,Bh),e($,be),m(ss,be,null),e(be,Hh),e(be,Bi),e(Bi,Wh),e(be,Gh),e(be,me),e(me,Yh),e(me,Hi),e(Hi,zh),e(me,Jh),e(me,Wi),e(Wi,jh),e(me,Qh),e(me,Gi),e(Gi,Xh),e(me,Zh),e($,em),e($,Tt),m(ns,Tt,null),e(Tt,tm),e(Tt,Yi),e(Yi,rm),e($,sm),e($,kt),m(os,kt,null),e(kt,nm),e(kt,zi),e(zi,om),f(t,Td,u),f(t,Le,u),e(Le,Ct),e(Ct,Ji),m(is,Ji,null),e(Le,im),e(Le,ji),e(ji,am),f(t,kd,u),f(t,At,u),e(At,dm),e(At,as),e(as,lm),e(At,cm),f(t,Cd,u),f(t,D,u),m(ds,D,null),e(D,um),e(D,Qi),e(Qi,fm),e(D,pm),e(D,On),e(On,hm),e(On,ls),e(ls,mm),e(D,gm),e(D,N),e(N,Vn),e(Vn,_m),e(N,vm),e(N,Xi),e(Xi,bm),e(N,Sm),e(N,Zi),e(Zi,$m),e(N,Dm),e(N,ea),e(ea,Em),e(N,xm),e(N,Nn),e(Nn,ym),e(N,wm),e(N,Fn),e(Fn,Pm),e(N,Mm),e(N,In),e(In,Tm),e(N,km),e(D,Cm),e(D,Ot),m(cs,Ot,null),e(Ot,Am),e(Ot,ta),e(ta,Om),e(D,Vm),e(D,Se),m(us,Se,null),e(Se,Nm),e(Se,ra),e(ra,Fm),e(Se,Im),e(Se,Ke),e(Ke,Lm),e(Ke,sa),e(sa,Km),e(Ke,Rm),e(Ke,na),e(na,qm),e(Ke,Um),e(D,Bm),e(D,Vt),m(fs,Vt,null),e(Vt,Hm),e(Vt,oa),e(oa,Wm),e(D,Gm),e(D,Nt),m(ps,Nt,null),e(Nt,Ym),e(Nt,ia),e(ia,zm),e(D,Jm),e(D,Ft),m(hs,Ft,null),e(Ft,jm),e(Ft,aa),e(aa,Qm),f(t,Ad,u),f(t,Re,u),e(Re,It),e(It,da),m(ms,da,null),e(Re,Xm),e(Re,la),e(la,Zm),f(t,Od,u),f(t,Lt,u),e(Lt,eg),e(Lt,gs),e(gs,tg),e(Lt,rg),f(t,Vd,u),f(t,P,u),m(_s,P,null),e(P,sg),e(P,Ln),e(Ln,ng),e(Ln,vs),e(vs,og),e(P,ig),e(P,F),e(F,Kn),e(Kn,ag),e(F,dg),e(F,ca),e(ca,lg),e(F,cg),e(F,ua),e(ua,ug),e(F,fg),e(F,fa),e(fa,pg),e(F,hg),e(F,Rn),e(Rn,mg),e(F,gg),e(F,qn),e(qn,_g),e(F,vg),e(F,Un),e(Un,bg),e(F,Sg),e(P,$g),e(P,Bn),e(Bn,Dg),e(Bn,bs),e(bs,Eg),e(P,xg),e(P,Kt),m(Ss,Kt,null),e(Kt,yg),e(Kt,pa),e(pa,wg),e(P,Pg),e(P,Rt),m($s,Rt,null),e(Rt,Mg),e(Rt,ha),e(ha,Tg),e(P,kg),e(P,qt),m(Ds,qt,null),e(qt,Cg),e(qt,ma),e(ma,Ag),f(t,Nd,u),f(t,qe,u),e(qe,Ut),e(Ut,ga),m(Es,ga,null),e(qe,Og),e(qe,_a),e(_a,Vg),f(t,Fd,u),f(t,Bt,u),e(Bt,Ng),e(Bt,xs),e(xs,Fg),e(Bt,Ig),f(t,Id,u),m(Ht,t,u),f(t,Ld,u),f(t,z,u),m(ys,z,null),e(z,Lg),e(z,va),e(va,Kg),e(z,Rg),e(z,I),e(I,Hn),e(Hn,qg),e(I,Ug),e(I,ba),e(ba,Bg),e(I,Hg),e(I,Sa),e(Sa,Wg),e(I,Gg),e(I,$a),e($a,Yg),e(I,zg),e(I,Wn),e(Wn,Jg),e(I,jg),e(I,Gn),e(Gn,Qg),e(I,Xg),e(I,Yn),e(Yn,Zg),e(I,e_),e(z,t_),e(z,zn),e(zn,r_),e(zn,ws),e(ws,s_),e(z,n_),e(z,Da),e(Da,o_),f(t,Kd,u),f(t,Ue,u),e(Ue,Wt),e(Wt,Ea),m(Ps,Ea,null),e(Ue,i_),e(Ue,xa),e(xa,a_),f(t,Rd,u),f(t,$e,u),e($e,d_),e($e,Ms),e(Ms,l_),e($e,c_),e($e,Ts),e(Ts,u_),e($e,f_),f(t,qd,u),f(t,B,u),m(ks,B,null),e(B,p_),e(B,Gt),e(Gt,h_),e(Gt,Cs),e(Cs,m_),e(Gt,g_),e(Gt,As),e(As,__),e(B,v_),e(B,L),e(L,Jn),e(Jn,b_),e(L,S_),e(L,ya),e(ya,$_),e(L,D_),e(L,wa),e(wa,E_),e(L,x_),e(L,Pa),e(Pa,y_),e(L,w_),e(L,jn),e(jn,P_),e(L,M_),e(L,Qn),e(Qn,T_),e(L,k_),e(L,Xn),e(Xn,C_),e(L,A_),e(B,O_),e(B,Yt),m(Os,Yt,null),e(Yt,V_),e(Yt,Vs),e(Vs,N_),e(Vs,Ma),e(Ma,F_),e(Vs,I_),e(B,L_),e(B,zt),m(Ns,zt,null),e(zt,K_),e(zt,Ta),e(Ta,R_),e(B,q_),e(B,Jt),m(Fs,Jt,null),e(Jt,U_),e(Jt,ka),e(ka,B_),f(t,Ud,u),f(t,Be,u),e(Be,jt),e(jt,Ca),m(Is,Ca,null),e(Be,H_),e(Be,Aa),e(Aa,W_),f(t,Bd,u),f(t,Zn,u),e(Zn,G_),f(t,Hd,u),f(t,H,u),m(Ls,H,null),e(H,Y_),e(H,eo),e(eo,z_),e(eo,Ks),e(Ks,J_),e(H,j_),e(H,K),e(K,to),e(to,Q_),e(K,X_),e(K,Oa),e(Oa,Z_),e(K,ev),e(K,Va),e(Va,tv),e(K,rv),e(K,Na),e(Na,sv),e(K,nv),e(K,ro),e(ro,ov),e(K,iv),e(K,so),e(so,av),e(K,dv),e(K,no),e(no,lv),e(K,cv),e(H,uv),e(H,Qt),m(Rs,Qt,null),e(Qt,fv),e(Qt,qs),e(qs,pv),e(qs,Fa),e(Fa,hv),e(qs,mv),e(H,gv),e(H,Xt),m(Us,Xt,null),e(Xt,_v),e(Xt,Ia),e(Ia,vv),e(H,bv),e(H,Zt),m(Bs,Zt,null),e(Zt,Sv),e(Zt,La),e(La,$v),f(t,Wd,u),f(t,He,u),e(He,er),e(er,Ka),m(Hs,Ka,null),e(He,Dv),e(He,Ra),e(Ra,Ev),f(t,Gd,u),f(t,ge,u),e(ge,xv),e(ge,oo),e(oo,yv),e(ge,wv),e(ge,Ws),e(Ws,Pv),e(ge,Mv),e(ge,Gs),e(Gs,Tv),f(t,Yd,u),f(t,W,u),m(Ys,W,null),e(W,kv),e(W,qa),e(qa,Cv),e(W,Av),e(W,R),e(R,io),e(io,Ov),e(R,Vv),e(R,Ua),e(Ua,Nv),e(R,Fv),e(R,Ba),e(Ba,Iv),e(R,Lv),e(R,Ha),e(Ha,Kv),e(R,Rv),e(R,ao),e(ao,qv),e(R,Uv),e(R,lo),e(lo,Bv),e(R,Hv),e(R,co),e(co,Wv),e(R,Gv),e(W,Yv),e(W,uo),e(uo,zv),e(uo,zs),e(zs,Jv),e(W,jv),e(W,tr),m(Js,tr,null),e(tr,Qv),e(tr,Wa),e(Wa,Xv),e(W,Zv),e(W,rr),m(js,rr,null),e(rr,eb),e(rr,Ga),e(Ga,tb),zd=!0},p(t,[u]){const Qs={};u&2&&(Qs.$$scope={dirty:u,ctx:t}),Ht.$set(Qs)},i(t){zd||(g(nr.$$.fragment,t),g(or.$$.fragment,t),g(ar.$$.fragment,t),g(dr.$$.fragment,t),g(lr.$$.fragment,t),g(fr.$$.fragment,t),g(pr.$$.fragment,t),g(hr.$$.fragment,t),g(mr.$$.fragment,t),g(gr.$$.fragment,t),g(_r.$$.fragment,t),g(vr.$$.fragment,t),g(Sr.$$.fragment,t),g($r.$$.fragment,t),g(Dr.$$.fragment,t),g(Er.$$.fragment,t),g(yr.$$.fragment,t),g(Pr.$$.fragment,t),g(Mr.$$.fragment,t),g(Tr.$$.fragment,t),g(kr.$$.fragment,t),g(Ar.$$.fragment,t),g(Ir.$$.fragment,t),g(Lr.$$.fragment,t),g(Kr.$$.fragment,t),g(Rr.$$.fragment,t),g(qr.$$.fragment,t),g(Ur.$$.fragment,t),g(Hr.$$.fragment,t),g(Gr.$$.fragment,t),g(Yr.$$.fragment,t),g(Jr.$$.fragment,t),g(jr.$$.fragment,t),g(Qr.$$.fragment,t),g(Zr.$$.fragment,t),g(ts.$$.fragment,t),g(rs.$$.fragment,t),g(ss.$$.fragment,t),g(ns.$$.fragment,t),g(os.$$.fragment,t),g(is.$$.fragment,t),g(ds.$$.fragment,t),g(cs.$$.fragment,t),g(us.$$.fragment,t),g(fs.$$.fragment,t),g(ps.$$.fragment,t),g(hs.$$.fragment,t),g(ms.$$.fragment,t),g(_s.$$.fragment,t),g(Ss.$$.fragment,t),g($s.$$.fragment,t),g(Ds.$$.fragment,t),g(Es.$$.fragment,t),g(Ht.$$.fragment,t),g(ys.$$.fragment,t),g(Ps.$$.fragment,t),g(ks.$$.fragment,t),g(Os.$$.fragment,t),g(Ns.$$.fragment,t),g(Fs.$$.fragment,t),g(Is.$$.fragment,t),g(Ls.$$.fragment,t),g(Rs.$$.fragment,t),g(Us.$$.fragment,t),g(Bs.$$.fragment,t),g(Hs.$$.fragment,t),g(Ys.$$.fragment,t),g(Js.$$.fragment,t),g(js.$$.fragment,t),zd=!0)},o(t){_(nr.$$.fragment,t),_(or.$$.fragment,t),_(ar.$$.fragment,t),_(dr.$$.fragment,t),_(lr.$$.fragment,t),_(fr.$$.fragment,t),_(pr.$$.fragment,t),_(hr.$$.fragment,t),_(mr.$$.fragment,t),_(gr.$$.fragment,t),_(_r.$$.fragment,t),_(vr.$$.fragment,t),_(Sr.$$.fragment,t),_($r.$$.fragment,t),_(Dr.$$.fragment,t),_(Er.$$.fragment,t),_(yr.$$.fragment,t),_(Pr.$$.fragment,t),_(Mr.$$.fragment,t),_(Tr.$$.fragment,t),_(kr.$$.fragment,t),_(Ar.$$.fragment,t),_(Ir.$$.fragment,t),_(Lr.$$.fragment,t),_(Kr.$$.fragment,t),_(Rr.$$.fragment,t),_(qr.$$.fragment,t),_(Ur.$$.fragment,t),_(Hr.$$.fragment,t),_(Gr.$$.fragment,t),_(Yr.$$.fragment,t),_(Jr.$$.fragment,t),_(jr.$$.fragment,t),_(Qr.$$.fragment,t),_(Zr.$$.fragment,t),_(ts.$$.fragment,t),_(rs.$$.fragment,t),_(ss.$$.fragment,t),_(ns.$$.fragment,t),_(os.$$.fragment,t),_(is.$$.fragment,t),_(ds.$$.fragment,t),_(cs.$$.fragment,t),_(us.$$.fragment,t),_(fs.$$.fragment,t),_(ps.$$.fragment,t),_(hs.$$.fragment,t),_(ms.$$.fragment,t),_(_s.$$.fragment,t),_(Ss.$$.fragment,t),_($s.$$.fragment,t),_(Ds.$$.fragment,t),_(Es.$$.fragment,t),_(Ht.$$.fragment,t),_(ys.$$.fragment,t),_(Ps.$$.fragment,t),_(ks.$$.fragment,t),_(Os.$$.fragment,t),_(Ns.$$.fragment,t),_(Fs.$$.fragment,t),_(Is.$$.fragment,t),_(Ls.$$.fragment,t),_(Rs.$$.fragment,t),_(Us.$$.fragment,t),_(Bs.$$.fragment,t),_(Hs.$$.fragment,t),_(Ys.$$.fragment,t),_(Js.$$.fragment,t),_(js.$$.fragment,t),zd=!1},d(t){r(G),t&&r(We),t&&r(Y),v(nr),t&&r(ja),t&&r(Zs),t&&r(Qa),t&&r(Ee),v(or),t&&r(Xa),t&&r(Ye),t&&r(Za),t&&r(ze),t&&r(ed),t&&r(ye),v(ar),t&&r(td),t&&r(T),t&&r(rd),t&&r(we),v(dr),t&&r(sd),t&&r(nn),t&&r(nd),t&&r(Qe),t&&r(od),t&&r(Pe),v(lr),t&&r(id),t&&r(on),t&&r(ad),t&&r(_e),t&&r(dd),t&&r(Ze),t&&r(ld),t&&r(Me),v(fr),t&&r(cd),t&&r(Te),v(pr),t&&r(ud),t&&r(ke),v(hr),t&&r(fd),t&&r(Ce),v(mr),t&&r(pd),t&&r(Ae),v(gr),t&&r(hd),t&&r(Oe),v(_r),t&&r(md),t&&r(dn),t&&r(gd),t&&r(x),v(vr),v(Sr),v($r),v(Dr),t&&r(_d),t&&r(Ve),v(Er),t&&r(vd),t&&r(dt),t&&r(bd),t&&r(y),v(yr),v(Pr),v(Mr),v(Tr),t&&r(Sd),t&&r(Ne),v(kr),t&&r($d),t&&r(pt),t&&r(Dd),t&&r(S),v(Ar),v(Ir),v(Lr),v(Kr),v(Rr),v(qr),t&&r(Ed),t&&r(Fe),v(Ur),t&&r(xd),t&&r(St),t&&r(yd),t&&r(w),v(Hr),v(Gr),v(Yr),v(Jr),v(jr),t&&r(wd),t&&r(Ie),v(Qr),t&&r(Pd),t&&r(wt),t&&r(Md),t&&r($),v(Zr),v(ts),v(rs),v(ss),v(ns),v(os),t&&r(Td),t&&r(Le),v(is),t&&r(kd),t&&r(At),t&&r(Cd),t&&r(D),v(ds),v(cs),v(us),v(fs),v(ps),v(hs),t&&r(Ad),t&&r(Re),v(ms),t&&r(Od),t&&r(Lt),t&&r(Vd),t&&r(P),v(_s),v(Ss),v($s),v(Ds),t&&r(Nd),t&&r(qe),v(Es),t&&r(Fd),t&&r(Bt),t&&r(Id),v(Ht,t),t&&r(Ld),t&&r(z),v(ys),t&&r(Kd),t&&r(Ue),v(Ps),t&&r(Rd),t&&r($e),t&&r(qd),t&&r(B),v(ks),v(Os),v(Ns),v(Fs),t&&r(Ud),t&&r(Be),v(Is),t&&r(Bd),t&&r(Zn),t&&r(Hd),t&&r(H),v(Ls),v(Rs),v(Us),v(Bs),t&&r(Wd),t&&r(He),v(Hs),t&&r(Gd),t&&r(ge),t&&r(Yd),t&&r(W),v(Ys),v(Js),v(js)}}}const R4={local:"schedulers",sections:[{local:"what-is-a-scheduler",sections:[{local:"discrete-versus-continuous-schedulers",title:"Discrete versus continuous schedulers"}],title:"What is a scheduler?"},{local:"designing-reusable-schedulers",title:"Designing Re-usable schedulers"},{local:"api",sections:[{local:"diffusers.SchedulerMixin",title:"SchedulerMixin"},{local:"diffusers.schedulers.scheduling_utils.SchedulerOutput",title:"SchedulerOutput"},{local:"implemented-schedulers",sections:[{local:"diffusers.DDIMScheduler",title:"Denoising diffusion implicit models (DDIM)"},{local:"diffusers.DDPMScheduler",title:"Denoising diffusion probabilistic models (DDPM)"},{local:"diffusers.KarrasVeScheduler",title:"Variance exploding, stochastic sampling from Karras et. al"},{local:"diffusers.LMSDiscreteScheduler",title:"Linear multistep scheduler for discrete beta schedules"},{local:"diffusers.PNDMScheduler",title:"Pseudo numerical methods for diffusion models (PNDM)"},{local:"diffusers.ScoreSdeVeScheduler",title:"variance exploding stochastic differential equation (VE-SDE) scheduler"},{local:"diffusers.IPNDMScheduler",title:"improved pseudo numerical methods for diffusion models (iPNDM)"},{local:"diffusers.schedulers.ScoreSdeVpScheduler",title:"variance preserving stochastic differential equation (VP-SDE) scheduler"},{local:"diffusers.EulerDiscreteScheduler",title:"Euler scheduler"},{local:"diffusers.EulerAncestralDiscreteScheduler",title:"Euler Ancestral scheduler"},{local:"diffusers.RePaintScheduler",title:"RePaint scheduler"}],title:"Implemented Schedulers"}],title:"API"}],title:"Schedulers"};function q4(Ja){return F4(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class G4 extends A4{constructor(G){super();O4(this,G,q4,K4,V4,{})}}export{G4 as default,R4 as metadata};
