import{S as N5,i as L5,s as q5,e as s,k as l,w as p,t as i,M as K5,c as n,d as t,m as c,a as o,x as h,h as a,b as d,G as e,g as f,y as m,q as g,o as _,B as v,v as R5}from"../../chunks/vendor-hf-doc-builder.js";import{T as Q5}from"../../chunks/Tip-hf-doc-builder.js";import{D as b}from"../../chunks/Docstring-hf-doc-builder.js";import{I as w}from"../../chunks/IconCopyLink-hf-doc-builder.js";function j5(tc){let Z,ct;return{c(){Z=s("p"),ct=i("Score SDE-VP is under construction.")},l(ee){Z=n(ee,"P",{});var De=o(Z);ct=a(De,"Score SDE-VP is under construction."),De.forEach(t)},m(ee,De){f(ee,Z,De),e(Z,ct)},d(ee){ee&&t(Z)}}}function W5(tc){let Z,ct,ee,De,Ki,Ir,qf,Ri,Kf,rc,lo,Rf,sc,Le,ut,Qi,Nr,Qf,ji,jf,nc,ft,Wf,Wi,Uf,Bf,oc,pt,co,Hf,Lr,Ui,Gf,zf,Bi,Yf,Jf,qe,Xf,Hi,Zf,ep,Gi,tp,rp,ic,Ke,ht,zi,qr,sp,Yi,np,ac,O,op,Ji,ip,ap,uo,dp,lp,fo,cp,up,Xi,fp,pp,po,hp,mp,Zi,gp,_p,dc,Re,mt,ea,Kr,vp,ta,bp,lc,ho,Sp,cc,gt,ra,Dp,xp,sa,$p,uc,Qe,_t,na,Rr,Ep,oa,yp,fc,mo,wp,pc,Ce,Qr,Mp,ia,Pp,Tp,Cp,jr,kp,aa,Op,Ap,Vp,da,Fp,hc,vt,Ip,go,Np,Lp,mc,je,bt,la,Wr,qp,ca,Kp,gc,We,Ur,Rp,ua,Qp,_c,Ue,St,fa,Br,jp,pa,Wp,vc,Be,Hr,Up,ha,Bp,bc,He,Dt,ma,Gr,Hp,ga,Gp,Sc,Ge,xt,_a,zr,zp,va,Yp,Dc,_o,Jp,xc,P,Yr,Xp,ba,Zp,eh,A,vo,th,rh,Sa,sh,nh,Da,oh,ih,xa,ah,dh,bo,lh,ch,So,uh,fh,Do,ph,hh,mh,xo,gh,Jr,_h,vh,$t,Xr,bh,$a,Sh,Dh,Et,Zr,xh,Ea,$h,Eh,yt,es,yh,ya,wh,$c,ze,wt,wa,ts,Mh,Ma,Ph,Ec,Mt,Th,rs,Ch,kh,yc,T,ss,Oh,Pa,Ah,Vh,V,$o,Fh,Ih,Ta,Nh,Lh,Ca,qh,Kh,ka,Rh,Qh,Eo,jh,Wh,yo,Uh,Bh,wo,Hh,Gh,zh,Mo,Yh,ns,Jh,Xh,Pt,os,Zh,Oa,em,tm,Tt,is,rm,Aa,sm,nm,Ct,as,om,Va,im,wc,Ye,kt,Fa,ds,am,Ia,dm,Mc,xe,lm,ls,cm,um,cs,fm,pm,us,hm,mm,Pc,S,fs,gm,Na,_m,vm,Ot,bm,ps,Sm,Dm,hs,xm,$m,Je,Em,La,ym,wm,qa,Mm,Pm,Tm,Me,Cm,ms,km,Om,Ka,Am,Vm,Ra,Fm,Im,Nm,F,Po,Lm,qm,Qa,Km,Rm,ja,Qm,jm,Wa,Wm,Um,To,Bm,Hm,Co,Gm,zm,ko,Ym,Jm,Xm,$e,gs,Zm,Ua,eg,tg,Ba,rg,sg,Ha,ng,og,ke,_s,ig,Ga,ag,dg,vs,lg,bs,cg,ug,fg,At,Ss,pg,za,hg,mg,Vt,Ds,gg,Ya,_g,vg,Ft,xs,bg,Ja,Sg,Dg,It,$s,xg,Xa,$g,Eg,Nt,Es,yg,Za,wg,Tc,Xe,Lt,ed,ys,Mg,td,Pg,Cc,qt,Tg,ws,Cg,kg,kc,x,Ms,Og,rd,Ag,Vg,Kt,Fg,Ps,Ig,Ng,Ts,Lg,qg,I,Oo,Kg,Rg,sd,Qg,jg,nd,Wg,Ug,od,Bg,Hg,Ao,Gg,zg,Vo,Yg,Jg,Fo,Xg,Zg,e_,Cs,t_,ks,r_,s_,n_,Oe,Os,o_,id,i_,a_,ad,d_,l_,Rt,As,c_,dd,u_,f_,Qt,Vs,p_,ld,h_,m_,jt,Fs,g_,cd,__,v_,Wt,Is,b_,ud,S_,Oc,Ze,Ut,fd,Ns,D_,pd,x_,Ac,Bt,$_,Ls,E_,y_,Vc,C,qs,w_,Io,M_,Ks,P_,T_,N,No,C_,k_,hd,O_,A_,md,V_,F_,gd,I_,N_,Lo,L_,q_,qo,K_,R_,Ko,Q_,j_,W_,Ht,Rs,U_,_d,B_,H_,Gt,Qs,G_,js,z_,vd,Y_,J_,X_,zt,Ws,Z_,bd,ev,tv,Yt,Us,rv,Sd,sv,Fc,et,Jt,Dd,Bs,nv,xd,ov,Ic,Xt,iv,Hs,av,dv,Nc,$,Gs,lv,$d,cv,uv,L,Ro,fv,pv,Ed,hv,mv,yd,gv,_v,wd,vv,bv,Qo,Sv,Dv,jo,xv,$v,Wo,Ev,yv,wv,Uo,Mv,zs,Pv,Tv,Zt,Ys,Cv,Md,kv,Ov,er,Js,Av,Pd,Vv,Fv,Ae,Xs,Iv,Td,Nv,Lv,Pe,qv,Cd,Kv,Rv,kd,Qv,jv,Od,Wv,Uv,Bv,tr,Zs,Hv,Ad,Gv,zv,rr,en,Yv,Vd,Jv,Lc,tt,sr,Fd,tn,Xv,Id,Zv,qc,nr,eb,rn,tb,rb,Kc,E,sn,sb,Nd,nb,ob,Bo,ib,nn,ab,db,q,Ho,lb,cb,Ld,ub,fb,qd,pb,hb,Kd,mb,gb,Go,_b,vb,zo,bb,Sb,Yo,Db,xb,$b,or,on,Eb,Rd,yb,wb,Ve,an,Mb,Qd,Pb,Tb,rt,Cb,jd,kb,Ob,Wd,Ab,Vb,Fb,ir,dn,Ib,Ud,Nb,Lb,ar,ln,qb,Bd,Kb,Rb,dr,cn,Qb,Hd,jb,Rc,st,lr,Gd,un,Wb,zd,Ub,Qc,cr,Bb,fn,Hb,Gb,jc,k,pn,zb,Jo,Yb,hn,Jb,Xb,K,Xo,Zb,e1,Yd,t1,r1,Jd,s1,n1,Xd,o1,i1,Zo,a1,d1,ei,l1,c1,ti,u1,f1,p1,ri,h1,mn,m1,g1,ur,gn,_1,Zd,v1,b1,fr,_n,S1,el,D1,x1,pr,vn,$1,tl,E1,Wc,nt,hr,rl,bn,y1,sl,w1,Uc,mr,M1,Sn,P1,T1,Bc,gr,Hc,te,Dn,C1,nl,k1,O1,R,si,A1,V1,ol,F1,I1,il,N1,L1,al,q1,K1,ni,R1,Q1,oi,j1,W1,ii,U1,B1,H1,ai,G1,xn,z1,Y1,dl,J1,Gc,ot,_r,ll,$n,X1,cl,Z1,zc,Fe,e0,En,t0,r0,yn,s0,n0,Yc,Y,wn,o0,vr,i0,Mn,a0,d0,Pn,l0,c0,Q,di,u0,f0,ul,p0,h0,fl,m0,g0,pl,_0,v0,li,b0,S0,ci,D0,x0,ui,$0,E0,y0,br,Tn,w0,Cn,M0,hl,P0,T0,C0,Sr,kn,k0,ml,O0,A0,Dr,On,V0,gl,F0,Jc,it,xr,_l,An,I0,vl,N0,Xc,fi,L0,Zc,J,Vn,q0,pi,K0,Fn,R0,Q0,j,hi,j0,W0,bl,U0,B0,Sl,H0,G0,Dl,z0,Y0,mi,J0,X0,gi,Z0,e2,_i,t2,r2,s2,$r,In,n2,Nn,o2,xl,i2,a2,d2,Er,Ln,l2,$l,c2,u2,yr,qn,f2,El,p2,eu,at,wr,yl,Kn,h2,wl,m2,tu,Rn,g2,Qn,_2,ru,y,jn,v2,Ml,b2,S2,Pl,D2,x2,W,vi,$2,E2,Tl,y2,w2,Cl,M2,P2,kl,T2,C2,bi,k2,O2,Si,A2,V2,Di,F2,I2,N2,xi,L2,Wn,q2,K2,Ie,Un,R2,Bn,Q2,Ol,j2,W2,U2,Al,B2,H2,U,Hn,G2,Gn,z2,Vl,Y2,J2,X2,Fl,Z2,e4,Il,t4,r4,Nl,s4,n4,Ll,zn,o4,ql,i4,a4,d4,dt,l4,Kl,c4,u4,Rl,f4,p4,h4,Mr,Yn,m4,Ql,g4,_4,Pr,Jn,v4,Xn,b4,jl,S4,D4,su,lt,Tr,Wl,Zn,x4,Ul,$4,nu,Te,E4,$i,y4,w4,eo,M4,P4,to,T4,ou,X,ro,C4,Bl,k4,O4,B,Ei,A4,V4,Hl,F4,I4,Gl,N4,L4,zl,q4,K4,yi,R4,Q4,wi,j4,W4,Mi,U4,B4,H4,Pi,G4,so,z4,Y4,Cr,no,J4,Yl,X4,Z4,kr,oo,eS,Jl,tS,iu;return Ir=new w({}),Nr=new w({}),qr=new w({}),Kr=new w({}),Rr=new w({}),Wr=new w({}),Ur=new b({props:{name:"class diffusers.SchedulerMixin",anchor:"diffusers.SchedulerMixin",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L38"}}),Br=new w({}),Hr=new b({props:{name:"class diffusers.schedulers.scheduling_utils.SchedulerOutput",anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput",parameters:[{name:"prev_sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput.prev_sample",description:`<strong>prev_sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code> for images) &#x2014;
Computed sample (x_{t-1}) of previous timestep. <code>prev_sample</code> should be used as next model input in the
denoising loop.`,name:"prev_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L25"}}),Gr=new w({}),zr=new w({}),Yr=new b({props:{name:"class diffusers.DDIMScheduler",anchor:"diffusers.DDIMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"},{name:"set_alpha_to_one",val:": bool = True"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDIMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDIMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDIMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDIMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDIMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"},{anchor:"diffusers.DDIMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.DDIMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L78"}}),Xr=new b({props:{name:"scale_model_input",anchor:"diffusers.DDIMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDIMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L164",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Zr=new b({props:{name:"set_timesteps",anchor:"diffusers.DDIMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L188"}}),es=new b({props:{name:"step",anchor:"diffusers.DDIMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"eta",val:": float = 0.0"},{name:"use_clipped_model_output",val:": bool = False"},{name:"generator",val:" = None"},{name:"variance_noise",val:": typing.Optional[torch.FloatTensor] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDIMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDIMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DDIMScheduler.step.eta",description:"<strong>eta</strong> (<code>float</code>) &#x2014; weight of noise for added noise in diffusion step.",name:"eta"},{anchor:"diffusers.DDIMScheduler.step.use_clipped_model_output",description:`<strong>use_clipped_model_output</strong> (<code>bool</code>) &#x2014; if <code>True</code>, compute &#x201C;corrected&#x201D; <code>model_output</code> from the clipped
predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when
<code>self.config.clip_sample</code> is <code>True</code>. If no clipping has happened, &#x201C;corrected&#x201D; <code>model_output</code> would
coincide with the one provided as input and <code>use_clipped_model_output</code> will have not effect.
generator &#x2014; random number generator.`,name:"use_clipped_model_output"},{anchor:"diffusers.DDIMScheduler.step.variance_noise",description:`<strong>variance_noise</strong> (<code>torch.FloatTensor</code>) &#x2014; instead of generating noise for the variance using <code>generator</code>, we
can directly provide the noise for the variance itself. This is useful for methods such as
CycleDiffusion. (<a href="https://arxiv.org/abs/2210.05559" rel="nofollow">https://arxiv.org/abs/2210.05559</a>)`,name:"variance_noise"},{anchor:"diffusers.DDIMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDIMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L204",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> or <code>tuple</code></p>
`}}),ts=new w({}),ss=new b({props:{name:"class diffusers.DDPMScheduler",anchor:"diffusers.DDPMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"variance_type",val:": str = 'fixed_small'"},{name:"clip_sample",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDPMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDPMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDPMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDPMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDPMScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.DDPMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L76"}}),os=new b({props:{name:"scale_model_input",anchor:"diffusers.DDPMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDPMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L157",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),is=new b({props:{name:"set_timesteps",anchor:"diffusers.DDPMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L171"}}),as=new b({props:{name:"step",anchor:"diffusers.DDPMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"predict_epsilon",val:" = True"},{name:"generator",val:" = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDPMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDPMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DDPMScheduler.step.predict_epsilon",description:`<strong>predict_epsilon</strong> (<code>bool</code>) &#x2014;
optional flag to use when model predicts the samples directly instead of the noise, epsilon.
generator &#x2014; random number generator.`,name:"predict_epsilon"},{anchor:"diffusers.DDPMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDPMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L219",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> or <code>tuple</code></p>
`}}),ds=new w({}),fs=new b({props:{name:"class diffusers.DPMSolverMultistepScheduler",anchor:"diffusers.DPMSolverMultistepScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"solver_order",val:": int = 2"},{name:"predict_epsilon",val:": bool = True"},{name:"thresholding",val:": bool = False"},{name:"dynamic_thresholding_ratio",val:": float = 0.995"},{name:"sample_max_value",val:": float = 1.0"},{name:"algorithm_type",val:": str = 'dpmsolver++'"},{name:"solver_type",val:": str = 'midpoint'"},{name:"lower_order_final",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DPMSolverMultistepScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DPMSolverMultistepScheduler.solver_order",description:`<strong>solver_order</strong> (<code>int</code>, default <code>2</code>) &#x2014;
the order of DPM-Solver; can be <code>1</code> or <code>2</code> or <code>3</code>. We recommend to use <code>solver_order=2</code> for guided
sampling, and <code>solver_order=3</code> for unconditional sampling.`,name:"solver_order"},{anchor:"diffusers.DPMSolverMultistepScheduler.predict_epsilon",description:`<strong>predict_epsilon</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
we currently support both the noise prediction model and the data prediction model. If the model predicts
the noise / epsilon, set <code>predict_epsilon</code> to <code>True</code>. If the model predicts the data / x0 directly, set
<code>predict_epsilon</code> to <code>False</code>.`,name:"predict_epsilon"},{anchor:"diffusers.DPMSolverMultistepScheduler.thresholding",description:`<strong>thresholding</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
whether to use the &#x201C;dynamic thresholding&#x201D; method (introduced by Imagen, <a href="https://arxiv.org/abs/2205.11487" rel="nofollow">https://arxiv.org/abs/2205.11487</a>).
For pixel-space diffusion models, you can set both <code>algorithm_type=dpmsolver++</code> and <code>thresholding=True</code> to
use the dynamic thresholding. Note that the thresholding method is unsuitable for latent-space diffusion
models (such as stable-diffusion).`,name:"thresholding"},{anchor:"diffusers.DPMSolverMultistepScheduler.dynamic_thresholding_ratio",description:`<strong>dynamic_thresholding_ratio</strong> (<code>float</code>, default <code>0.995</code>) &#x2014;
the ratio for the dynamic thresholding method. Default is <code>0.995</code>, the same as Imagen
(<a href="https://arxiv.org/abs/2205.11487" rel="nofollow">https://arxiv.org/abs/2205.11487</a>).`,name:"dynamic_thresholding_ratio"},{anchor:"diffusers.DPMSolverMultistepScheduler.sample_max_value",description:`<strong>sample_max_value</strong> (<code>float</code>, default <code>1.0</code>) &#x2014;
the threshold value for dynamic thresholding. Valid only when <code>thresholding=True</code> and
<code>algorithm_type=&quot;dpmsolver++</code>.`,name:"sample_max_value"},{anchor:"diffusers.DPMSolverMultistepScheduler.algorithm_type",description:`<strong>algorithm_type</strong> (<code>str</code>, default <code>dpmsolver++</code>) &#x2014;
the algorithm type for the solver. Either <code>dpmsolver</code> or <code>dpmsolver++</code>. The <code>dpmsolver</code> type implements the
algorithms in <a href="https://arxiv.org/abs/2206.00927" rel="nofollow">https://arxiv.org/abs/2206.00927</a>, and the <code>dpmsolver++</code> type implements the algorithms in
<a href="https://arxiv.org/abs/2211.01095" rel="nofollow">https://arxiv.org/abs/2211.01095</a>. We recommend to use <code>dpmsolver++</code> with <code>solver_order=2</code> for guided
sampling (e.g. stable-diffusion).`,name:"algorithm_type"},{anchor:"diffusers.DPMSolverMultistepScheduler.solver_type",description:`<strong>solver_type</strong> (<code>str</code>, default <code>midpoint</code>) &#x2014;
the solver type for the second-order solver. Either <code>midpoint</code> or <code>heun</code>. The solver type slightly affects
the sample quality, especially for small number of steps. We empirically find that <code>midpoint</code> solvers are
slightly better, so we recommend to use the <code>midpoint</code> type.`,name:"solver_type"},{anchor:"diffusers.DPMSolverMultistepScheduler.lower_order_final",description:`<strong>lower_order_final</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
whether to use lower-order solvers in the final steps. Only valid for &lt; 15 inference steps. We empirically
find this trick can stabilize the sampling of DPM-Solver for steps &lt; 15, especially for steps &lt;= 10.`,name:"lower_order_final"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L56"}}),gs=new b({props:{name:"convert_model_output",anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L206",returnDescription:`
<p>the converted model output.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),_s=new b({props:{name:"dpm_solver_first_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L255",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ss=new b({props:{name:"multistep_dpm_solver_second_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update",parameters:[{name:"model_output_list",val:": typing.List[torch.FloatTensor]"},{name:"timestep_list",val:": typing.List[int]"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.model_output_list",description:`<strong>model_output_list</strong> (<code>List[torch.FloatTensor]</code>) &#x2014;
direct outputs from learned diffusion model at current and latter timesteps.`,name:"model_output_list"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current and latter discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L287",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ds=new b({props:{name:"multistep_dpm_solver_third_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update",parameters:[{name:"model_output_list",val:": typing.List[torch.FloatTensor]"},{name:"timestep_list",val:": typing.List[int]"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.model_output_list",description:`<strong>model_output_list</strong> (<code>List[torch.FloatTensor]</code>) &#x2014;
direct outputs from learned diffusion model at current and latter timesteps.`,name:"model_output_list"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current and latter discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L346",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),xs=new b({props:{name:"scale_model_input",anchor:"diffusers.DPMSolverMultistepScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L469",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),$s=new b({props:{name:"set_timesteps",anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L183"}}),Es=new b({props:{name:"step",anchor:"diffusers.DPMSolverMultistepScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L401",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),ys=new w({}),Ms=new b({props:{name:"class diffusers.KarrasVeScheduler",anchor:"diffusers.KarrasVeScheduler",parameters:[{name:"sigma_min",val:": float = 0.02"},{name:"sigma_max",val:": float = 100"},{name:"s_noise",val:": float = 1.007"},{name:"s_churn",val:": float = 80"},{name:"s_min",val:": float = 0.05"},{name:"s_max",val:": float = 50"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.sigma_min",description:"<strong>sigma_min</strong> (<code>float</code>) &#x2014; minimum noise magnitude",name:"sigma_min"},{anchor:"diffusers.KarrasVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum noise magnitude",name:"sigma_max"},{anchor:"diffusers.KarrasVeScheduler.s_noise",description:`<strong>s_noise</strong> (<code>float</code>) &#x2014; the amount of additional noise to counteract loss of detail during sampling.
A reasonable range is [1.000, 1.011].`,name:"s_noise"},{anchor:"diffusers.KarrasVeScheduler.s_churn",description:`<strong>s_churn</strong> (<code>float</code>) &#x2014; the parameter controlling the overall amount of stochasticity.
A reasonable range is [0, 100].`,name:"s_churn"},{anchor:"diffusers.KarrasVeScheduler.s_min",description:`<strong>s_min</strong> (<code>float</code>) &#x2014; the start value of the sigma range where we add noise (enable stochasticity).
A reasonable range is [0, 10].`,name:"s_min"},{anchor:"diffusers.KarrasVeScheduler.s_max",description:`<strong>s_max</strong> (<code>float</code>) &#x2014; the end value of the sigma range where we add noise.
A reasonable range is [0.2, 80].`,name:"s_max"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L48"}}),Os=new b({props:{name:"add_noise_to_input",anchor:"diffusers.KarrasVeScheduler.add_noise_to_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"sigma",val:": float"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L133"}}),As=new b({props:{name:"scale_model_input",anchor:"diffusers.KarrasVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.KarrasVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L98",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Vs=new b({props:{name:"set_timesteps",anchor:"diffusers.KarrasVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L112"}}),Fs=new b({props:{name:"step",anchor:"diffusers.KarrasVeScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class</p>
<p>KarrasVeOutput &#x2014; updated sample in the diffusion chain and derivative (TODO double check).`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L154",returnDescription:`
<p><code>KarrasVeOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>KarrasVeOutput</code> or <code>tuple</code></p>
`}}),Is=new b({props:{name:"step_correct",anchor:"diffusers.KarrasVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"sample_prev",val:": FloatTensor"},{name:"derivative",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_prev",description:"<strong>sample_prev</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.derivative",description:"<strong>derivative</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"derivative"},{anchor:"diffusers.KarrasVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L192",returnDescription:`
<p>updated sample in the diffusion chain. derivative (TODO): TODO</p>
`,returnType:`
<p>prev_sample (TODO)</p>
`}}),Ns=new w({}),qs=new b({props:{name:"class diffusers.LMSDiscreteScheduler",anchor:"diffusers.LMSDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.LMSDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.LMSDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.LMSDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.LMSDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L47"}}),Rs=new b({props:{name:"get_lms_coefficient",anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient",parameters:[{name:"order",val:""},{name:"t",val:""},{name:"current_order",val:""}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.order",description:"<strong>order</strong> (TODO) &#x2014;",name:"order"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.t",description:"<strong>t</strong> (TODO) &#x2014;",name:"t"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.current_order",description:"<strong>current_order</strong> (TODO) &#x2014;",name:"current_order"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L138"}}),Qs=new b({props:{name:"scale_model_input",anchor:"diffusers.LMSDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L117",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ws=new b({props:{name:"set_timesteps",anchor:"diffusers.LMSDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L160"}}),Us=new b({props:{name:"step",anchor:"diffusers.LMSDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"order",val:": int = 4"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.LMSDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.LMSDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
order &#x2014; coefficient for multi-step inference.`,name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than LMSDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L181",returnDescription:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),Bs=new w({}),Gs=new b({props:{name:"class diffusers.PNDMScheduler",anchor:"diffusers.PNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"skip_prk_steps",val:": bool = False"},{name:"set_alpha_to_one",val:": bool = False"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.PNDMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.PNDMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.PNDMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.PNDMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.PNDMScheduler.skip_prk_steps",description:`<strong>skip_prk_steps</strong> (<code>bool</code>) &#x2014;
allows the scheduler to skip the Runge-Kutta steps that are defined in the original paper as being required
before plms steps; defaults to <code>False</code>.`,name:"skip_prk_steps"},{anchor:"diffusers.PNDMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.PNDMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L56"}}),Ys=new b({props:{name:"scale_model_input",anchor:"diffusers.PNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L345",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Js=new b({props:{name:"set_timesteps",anchor:"diffusers.PNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L153"}}),Xs=new b({props:{name:"step",anchor:"diffusers.PNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L192",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> or <code>tuple</code></p>
`}}),Zs=new b({props:{name:"step_plms",anchor:"diffusers.PNDMScheduler.step_plms",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_plms.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_plms.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_plms.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_plms.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L278",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),en=new b({props:{name:"step_prk",anchor:"diffusers.PNDMScheduler.step_prk",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_prk.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_prk.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_prk.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_prk.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L223",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),tn=new w({}),sn=new b({props:{name:"class diffusers.ScoreSdeVeScheduler",anchor:"diffusers.ScoreSdeVeScheduler",parameters:[{name:"num_train_timesteps",val:": int = 2000"},{name:"snr",val:": float = 0.15"},{name:"sigma_min",val:": float = 0.01"},{name:"sigma_max",val:": float = 1348.0"},{name:"sampling_eps",val:": float = 1e-05"},{name:"correct_steps",val:": int = 1"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.ScoreSdeVeScheduler.snr",description:`<strong>snr</strong> (<code>float</code>) &#x2014;
coefficient weighting the step from the model_output sample (from the network) to the random noise.`,name:"snr"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>) &#x2014;
initial noise scale for sigma sequence in sampling procedure. The minimum sigma should mirror the
distribution of the data.`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum value used for the range of continuous timesteps passed into the model.",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.sampling_eps",description:`<strong>sampling_eps</strong> (<code>float</code>) &#x2014; the end value of sampling, where timesteps decrease progressively from 1 to
epsilon. &#x2014;`,name:"sampling_eps"},{anchor:"diffusers.ScoreSdeVeScheduler.correct_steps",description:"<strong>correct_steps</strong> (<code>int</code>) &#x2014; number of correction steps performed on a produced sample.",name:"correct_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L45"}}),on=new b({props:{name:"scale_model_input",anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L87",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),an=new b({props:{name:"set_sigmas",anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas",parameters:[{name:"num_inference_steps",val:": int"},{name:"sigma_min",val:": float = None"},{name:"sigma_max",val:": float = None"},{name:"sampling_eps",val:": float = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>, optional) &#x2014;
initial noise scale value (overrides value given at Scheduler instantiation).`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>, optional) &#x2014; final noise scale value (overrides value given at Scheduler instantiation).",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L117"}}),dn=new b({props:{name:"set_timesteps",anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"sampling_eps",val:": float = None"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L101"}}),ln=new b({props:{name:"step_correct",anchor:"diffusers.ScoreSdeVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L212",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),cn=new b({props:{name:"step_pred",anchor:"diffusers.ScoreSdeVeScheduler.step_pred",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L151",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),un=new w({}),pn=new b({props:{name:"class diffusers.IPNDMScheduler",anchor:"diffusers.IPNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L24"}}),gn=new b({props:{name:"scale_model_input",anchor:"diffusers.IPNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L126",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),_n=new b({props:{name:"set_timesteps",anchor:"diffusers.IPNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L56"}}),vn=new b({props:{name:"step",anchor:"diffusers.IPNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.IPNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.IPNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.IPNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L76",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),bn=new w({}),gr=new Q5({props:{warning:!0,$$slots:{default:[j5]},$$scope:{ctx:tc}}}),Dn=new b({props:{name:"class diffusers.schedulers.ScoreSdeVpScheduler",anchor:"diffusers.schedulers.ScoreSdeVpScheduler",parameters:[{name:"num_train_timesteps",val:" = 2000"},{name:"beta_min",val:" = 0.1"},{name:"beta_max",val:" = 20"},{name:"sampling_eps",val:" = 0.001"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_vp.py#L26"}}),$n=new w({}),wn=new b({props:{name:"class diffusers.EulerDiscreteScheduler",anchor:"diffusers.EulerDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L48"}}),Tn=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L117",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),kn=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L138"}}),On=new b({props:{name:"step",anchor:"diffusers.EulerDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"s_churn",val:": float = 0.0"},{name:"s_tmin",val:": float = 0.0"},{name:"s_tmax",val:": float = inf"},{name:"s_noise",val:": float = 1.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_churn",description:"<strong>s_churn</strong> (<code>float</code>) &#x2014;",name:"s_churn"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmin",description:"<strong>s_tmin</strong>  (<code>float</code>) &#x2014;",name:"s_tmin"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmax",description:"<strong>s_tmax</strong>  (<code>float</code>) &#x2014;",name:"s_tmax"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_noise",description:"<strong>s_noise</strong> (<code>float</code>) &#x2014;",name:"s_noise"},{anchor:"diffusers.EulerDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L157",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a
<code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),An=new w({}),Vn=new b({props:{name:"class diffusers.EulerAncestralDiscreteScheduler",anchor:"diffusers.EulerAncestralDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L48"}}),In=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L116",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ln=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L137"}}),qn=new b({props:{name:"step",anchor:"diffusers.EulerAncestralDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerAncestralDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L156",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise
a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),Kn=new w({}),jn=new b({props:{name:"class diffusers.VQDiffusionScheduler",anchor:"diffusers.VQDiffusionScheduler",parameters:[{name:"num_vec_classes",val:": int"},{name:"num_train_timesteps",val:": int = 100"},{name:"alpha_cum_start",val:": float = 0.99999"},{name:"alpha_cum_end",val:": float = 9e-06"},{name:"gamma_cum_start",val:": float = 9e-06"},{name:"gamma_cum_end",val:": float = 0.99999"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.num_vec_classes",description:`<strong>num_vec_classes</strong> (<code>int</code>) &#x2014;
The number of classes of the vector embeddings of the latent pixels. Includes the class for the masked
latent pixel.`,name:"num_vec_classes"},{anchor:"diffusers.VQDiffusionScheduler.num_train_timesteps",description:`<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014;
Number of diffusion steps used to train the model.`,name:"num_train_timesteps"},{anchor:"diffusers.VQDiffusionScheduler.alpha_cum_start",description:`<strong>alpha_cum_start</strong> (<code>float</code>) &#x2014;
The starting cumulative alpha value.`,name:"alpha_cum_start"},{anchor:"diffusers.VQDiffusionScheduler.alpha_cum_end",description:`<strong>alpha_cum_end</strong> (<code>float</code>) &#x2014;
The ending cumulative alpha value.`,name:"alpha_cum_end"},{anchor:"diffusers.VQDiffusionScheduler.gamma_cum_start",description:`<strong>gamma_cum_start</strong> (<code>float</code>) &#x2014;
The starting cumulative gamma value.`,name:"gamma_cum_start"},{anchor:"diffusers.VQDiffusionScheduler.gamma_cum_end",description:`<strong>gamma_cum_end</strong> (<code>float</code>) &#x2014;
The ending cumulative gamma value.`,name:"gamma_cum_end"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L106"}}),Un=new b({props:{name:"log_Q_t_transitioning_to_known_class",anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class",parameters:[{name:"t",val:": torch.int32"},{name:"x_t",val:": LongTensor"},{name:"log_onehot_x_t",val:": FloatTensor"},{name:"cumulative",val:": bool"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.t",description:`<strong>t</strong> (torch.Long) &#x2014;
The timestep that determines which transition matrix is used.`,name:"t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.x_t",description:`<strong>x_t</strong> (<code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>) &#x2014;
The classes of each latent pixel at time <code>t</code>.`,name:"x_t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.log_onehot_x_t",description:`<strong>log_onehot_x_t</strong> (<code>torch.FloatTensor</code> of shape <code>(batch size, num classes, num latent pixels)</code>) &#x2014;
The log one-hot vectors of <code>x_t</code>`,name:"log_onehot_x_t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.cumulative",description:`<strong>cumulative</strong> (<code>bool</code>) &#x2014;
If cumulative is <code>False</code>, we use the single step transition matrix <code>t-1</code>-&gt;<code>t</code>. If cumulative is <code>True</code>,
we use the cumulative transition matrix <code>0</code>-&gt;<code>t</code>.`,name:"cumulative"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L377",returnDescription:`
<p>Each <em>column</em> of the returned matrix is a <em>row</em> of log probabilities of the complete probability
transition matrix.</p>
<p>When non cumulative, returns <code>self.num_classes - 1</code> rows because the initial latent pixel cannot be
masked.</p>
<p>Where:</p>
<ul>
<li><code>q_n</code> is the probability distribution for the forward process of the <code>n</code>th latent pixel.</li>
<li>C_0 is a class of a latent pixel embedding</li>
<li>C_k is the class of the masked latent pixel</li>
</ul>
<p>non-cumulative result (omitting logarithms):</p>

	<CodeBlock 
		code={\`q_0(x_t | x_{t-1\\} = C_0) ... q_n(x_t | x_{t-1\\} = C_0)
          .      .                     .
          .               .            .
          .                      .     .
q_0(x_t | x_{t-1\\} = C_k) ... q_n(x_t | x_{t-1\\} = C_k)\`}
		highlighted={\`q<span class="hljs-constructor">_0(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_0)</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_0)</span>
          .      .                     .
          .               .            .
          .                      .     .
q<span class="hljs-constructor">_0(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_k)</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_k)</span>\`}
	/>
<p>cumulative result (omitting logarithms):</p>

	<CodeBlock 
		code={\`q_0_cumulative(x_t | x_0 = C_0)    ...  q_n_cumulative(x_t | x_0 = C_0)
          .               .                          .
          .                        .                 .
          .                               .          .
q_0_cumulative(x_t | x_0 = C_{k-1\\}) ... q_n_cumulative(x_t | x_0 = C_{k-1\\})\`}
		highlighted={\`q<span class="hljs-constructor">_0_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_0)</span><span class="hljs-operator">    ...  </span>q<span class="hljs-constructor">_n_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_0)</span>
          .               .                          .
          .                        .                 .
          .                               .          .
q<span class="hljs-constructor">_0_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_{<span class="hljs-params">k</span>-1\\})</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_{<span class="hljs-params">k</span>-1\\})</span>\`}
	/>
`,returnType:`
<p><code>torch.FloatTensor</code> of shape <code>(batch size, num classes - 1, num latent pixels)</code></p>
`}}),Hn=new b({props:{name:"q_posterior",anchor:"diffusers.VQDiffusionScheduler.q_posterior",parameters:[{name:"log_p_x_0",val:""},{name:"x_t",val:""},{name:"t",val:""}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.q_posterior.t",description:`<strong>t</strong> (torch.Long) &#x2014;
The timestep that determines which transition matrix is used.`,name:"t"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L258",returnDescription:`
<p>The log probabilities for the predicted classes of the image at timestep <code>t-1</code>. I.e. Equation (11).</p>
`,returnType:`
<p><code>torch.FloatTensor</code> of shape <code>(batch size, num classes, num latent pixels)</code></p>
`}}),Yn=new b({props:{name:"set_timesteps",anchor:"diffusers.VQDiffusionScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.VQDiffusionScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014;
device to place the timesteps and the diffusion process parameters (alpha, beta, gamma) on.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L188"}}),Jn=new b({props:{name:"step",anchor:"diffusers.VQDiffusionScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": torch.int64"},{name:"sample",val:": LongTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.step.t",description:`<strong>t</strong> (<code>torch.long</code>) &#x2014;
The timestep that determines which transition matrices are used.</p>
<p>x_t &#x2014; (<code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>):
The classes of each latent pixel at time <code>t</code></p>
<p>generator &#x2014; (<code>torch.Generator</code> or None):
RNG for the noise applied to p(x_{t-1} | x_t) before it is sampled from.`,name:"t"},{anchor:"diffusers.VQDiffusionScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014;
option for returning tuple rather than VQDiffusionSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L210",returnDescription:`
<p><code>~schedulers.scheduling_utils.VQDiffusionSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.VQDiffusionSchedulerOutput</code> or <code>tuple</code></p>
`}}),Zn=new w({}),ro=new b({props:{name:"class diffusers.RePaintScheduler",anchor:"diffusers.RePaintScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"eta",val:": float = 0.0"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.RePaintScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.RePaintScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.RePaintScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.RePaintScheduler.eta",description:`<strong>eta</strong> (<code>float</code>) &#x2014;
The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 -0.0 is DDIM and
1.0 is DDPM scheduler respectively.`,name:"eta"},{anchor:"diffusers.RePaintScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.RePaintScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.RePaintScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L74"}}),no=new b({props:{name:"scale_model_input",anchor:"diffusers.RePaintScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.RePaintScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L150",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),oo=new b({props:{name:"step",anchor:"diffusers.RePaintScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"original_image",val:": FloatTensor"},{name:"mask",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.step.model_output",description:`<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned
diffusion model.`,name:"model_output"},{anchor:"diffusers.RePaintScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.RePaintScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.RePaintScheduler.step.original_image",description:`<strong>original_image</strong> (<code>torch.FloatTensor</code>) &#x2014;
the original image to inpaint on.`,name:"original_image"},{anchor:"diffusers.RePaintScheduler.step.mask",description:`<strong>mask</strong> (<code>torch.FloatTensor</code>) &#x2014;
the mask where 0.0 values define which part of the original image to inpaint (change).`,name:"mask"},{anchor:"diffusers.RePaintScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014; random number generator.",name:"generator"},{anchor:"diffusers.RePaintScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than
DDPMSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L213",returnDescription:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> or <code>tuple</code></p>
`}}),{c(){Z=s("meta"),ct=l(),ee=s("h1"),De=s("a"),Ki=s("span"),p(Ir.$$.fragment),qf=l(),Ri=s("span"),Kf=i("Schedulers"),rc=l(),lo=s("p"),Rf=i("Diffusers contains multiple pre-built schedule functions for the diffusion process."),sc=l(),Le=s("h2"),ut=s("a"),Qi=s("span"),p(Nr.$$.fragment),Qf=l(),ji=s("span"),jf=i("What is a scheduler?"),nc=l(),ft=s("p"),Wf=i("The schedule functions, denoted "),Wi=s("em"),Uf=i("Schedulers"),Bf=i(" in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample."),oc=l(),pt=s("ul"),co=s("li"),Hf=i("Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),Lr=s("ul"),Ui=s("li"),Gf=i("adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),zf=l(),Bi=s("li"),Yf=i("for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),Jf=l(),qe=s("li"),Xf=i("Schedulers are often defined by a "),Hi=s("em"),Zf=i("noise schedule"),ep=i(" and an "),Gi=s("em"),tp=i("update rule"),rp=i(" to solve the differential equation solution."),ic=l(),Ke=s("h3"),ht=s("a"),zi=s("span"),p(qr.$$.fragment),sp=l(),Yi=s("span"),np=i("Discrete versus continuous schedulers"),ac=l(),O=s("p"),op=i(`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),Ji=s("code"),ip=i("int"),ap=i(" inputs), such as the "),uo=s("a"),dp=i("DDPMScheduler"),lp=i(" or "),fo=s("a"),cp=i("PNDMScheduler"),up=i(", and continuous (accepting "),Xi=s("code"),fp=i("float"),pp=i(" inputs), such as the score-based schedulers "),po=s("a"),hp=i("ScoreSdeVeScheduler"),mp=i(" or "),Zi=s("code"),gp=i("ScoreSdeVpScheduler"),_p=i("."),dc=l(),Re=s("h2"),mt=s("a"),ea=s("span"),p(Kr.$$.fragment),vp=l(),ta=s("span"),bp=i("Designing Re-usable schedulers"),lc=l(),ho=s("p"),Sp=i(`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),cc=l(),gt=s("ul"),ra=s("li"),Dp=i("Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),xp=l(),sa=s("li"),$p=i("Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),uc=l(),Qe=s("h2"),_t=s("a"),na=s("span"),p(Rr.$$.fragment),Ep=l(),oa=s("span"),yp=i("API"),fc=l(),mo=s("p"),wp=i("The core API for any new scheduler must follow a limited structure."),pc=l(),Ce=s("ul"),Qr=s("li"),Mp=i("Schedulers should provide one or more "),ia=s("code"),Pp=i("def step(...)"),Tp=i(" functions that should be called to update the generated sample iteratively."),Cp=l(),jr=s("li"),kp=i("Schedulers should provide a "),aa=s("code"),Op=i("set_timesteps(...)"),Ap=i(" method that configures the parameters of a schedule function for a specific inference task."),Vp=l(),da=s("li"),Fp=i("Schedulers should be framework-specific."),hc=l(),vt=s("p"),Ip=i("The base class "),go=s("a"),Np=i("SchedulerMixin"),Lp=i(" implements low level utilities used by multiple schedulers."),mc=l(),je=s("h3"),bt=s("a"),la=s("span"),p(Wr.$$.fragment),qp=l(),ca=s("span"),Kp=i("SchedulerMixin"),gc=l(),We=s("div"),p(Ur.$$.fragment),Rp=l(),ua=s("p"),Qp=i("Mixin containing common functions for the schedulers."),_c=l(),Ue=s("h3"),St=s("a"),fa=s("span"),p(Br.$$.fragment),jp=l(),pa=s("span"),Wp=i("SchedulerOutput"),vc=i("\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),Be=s("div"),p(Hr.$$.fragment),Up=l(),ha=s("p"),Bp=i("Base class for the scheduler\u2019s step function output."),bc=l(),He=s("h3"),Dt=s("a"),ma=s("span"),p(Gr.$$.fragment),Hp=l(),ga=s("span"),Gp=i("Implemented Schedulers"),Sc=l(),Ge=s("h4"),xt=s("a"),_a=s("span"),p(zr.$$.fragment),zp=l(),va=s("span"),Yp=i("Denoising diffusion implicit models (DDIM)"),Dc=l(),_o=s("p"),Jp=i("Original paper can be found here."),xc=l(),P=s("div"),p(Yr.$$.fragment),Xp=l(),ba=s("p"),Zp=i(`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),eh=l(),A=s("p"),vo=s("a"),th=i("~ConfigMixin"),rh=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Sa=s("code"),sh=i("__init__"),nh=i(`
function, such as `),Da=s("code"),oh=i("num_train_timesteps"),ih=i(". They can be accessed via "),xa=s("code"),ah=i("scheduler.config.num_train_timesteps"),dh=i(`.
`),bo=s("a"),lh=i("~ConfigMixin"),ch=i(" also provides general loading and saving functionality via the "),So=s("a"),uh=i("save_config()"),fh=i(` and
`),Do=s("a"),ph=i("from_config()"),hh=i(" functions."),mh=l(),xo=s("p"),gh=i("For more details, see the original paper: "),Jr=s("a"),_h=i("https://arxiv.org/abs/2010.02502"),vh=l(),$t=s("div"),p(Xr.$$.fragment),bh=l(),$a=s("p"),Sh=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Dh=l(),Et=s("div"),p(Zr.$$.fragment),xh=l(),Ea=s("p"),$h=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Eh=l(),yt=s("div"),p(es.$$.fragment),yh=l(),ya=s("p"),wh=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),$c=l(),ze=s("h4"),wt=s("a"),wa=s("span"),p(ts.$$.fragment),Mh=l(),Ma=s("span"),Ph=i("Denoising diffusion probabilistic models (DDPM)"),Ec=l(),Mt=s("p"),Th=i("Original paper can be found "),rs=s("a"),Ch=i("here"),kh=i("."),yc=l(),T=s("div"),p(ss.$$.fragment),Oh=l(),Pa=s("p"),Ah=i(`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),Vh=l(),V=s("p"),$o=s("a"),Fh=i("~ConfigMixin"),Ih=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ta=s("code"),Nh=i("__init__"),Lh=i(`
function, such as `),Ca=s("code"),qh=i("num_train_timesteps"),Kh=i(". They can be accessed via "),ka=s("code"),Rh=i("scheduler.config.num_train_timesteps"),Qh=i(`.
`),Eo=s("a"),jh=i("~ConfigMixin"),Wh=i(" also provides general loading and saving functionality via the "),yo=s("a"),Uh=i("save_config()"),Bh=i(` and
`),wo=s("a"),Hh=i("from_config()"),Gh=i(" functions."),zh=l(),Mo=s("p"),Yh=i("For more details, see the original paper: "),ns=s("a"),Jh=i("https://arxiv.org/abs/2006.11239"),Xh=l(),Pt=s("div"),p(os.$$.fragment),Zh=l(),Oa=s("p"),em=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),tm=l(),Tt=s("div"),p(is.$$.fragment),rm=l(),Aa=s("p"),sm=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),nm=l(),Ct=s("div"),p(as.$$.fragment),om=l(),Va=s("p"),im=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),wc=l(),Ye=s("h4"),kt=s("a"),Fa=s("span"),p(ds.$$.fragment),am=l(),Ia=s("span"),dm=i("Multistep DPM-Solver"),Mc=l(),xe=s("p"),lm=i("Original paper can be found "),ls=s("a"),cm=i("here"),um=i(" and the "),cs=s("a"),fm=i("improved version"),pm=i(". The original implementation can be found "),us=s("a"),hm=i("here"),mm=i("."),Pc=l(),S=s("div"),p(fs.$$.fragment),gm=l(),Na=s("p"),_m=i(`DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with
the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality
samples, and it can generate quite good samples even in only 10 steps.`),vm=l(),Ot=s("p"),bm=i("For more details, see the original paper: "),ps=s("a"),Sm=i("https://arxiv.org/abs/2206.00927"),Dm=i(" and "),hs=s("a"),xm=i("https://arxiv.org/abs/2211.01095"),$m=l(),Je=s("p"),Em=i(`Currently, we support the multistep DPM-Solver for both noise prediction models and data prediction models. We
recommend to use `),La=s("code"),ym=i("solver_order=2"),wm=i(" for guided sampling, and "),qa=s("code"),Mm=i("solver_order=3"),Pm=i(" for unconditional sampling."),Tm=l(),Me=s("p"),Cm=i("We also support the \u201Cdynamic thresholding\u201D method in Imagen ("),ms=s("a"),km=i("https://arxiv.org/abs/2205.11487"),Om=i(`). For pixel-space
diffusion models, you can set both `),Ka=s("code"),Am=i('algorithm_type="dpmsolver++"'),Vm=i(" and "),Ra=s("code"),Fm=i("thresholding=True"),Im=i(` to use the dynamic
thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as
stable-diffusion).`),Nm=l(),F=s("p"),Po=s("a"),Lm=i("~ConfigMixin"),qm=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Qa=s("code"),Km=i("__init__"),Rm=i(`
function, such as `),ja=s("code"),Qm=i("num_train_timesteps"),jm=i(". They can be accessed via "),Wa=s("code"),Wm=i("scheduler.config.num_train_timesteps"),Um=i(`.
`),To=s("a"),Bm=i("~ConfigMixin"),Hm=i(" also provides general loading and saving functionality via the "),Co=s("a"),Gm=i("save_config()"),zm=i(` and
`),ko=s("a"),Ym=i("from_config()"),Jm=i(" functions."),Xm=l(),$e=s("div"),p(gs.$$.fragment),Zm=l(),Ua=s("p"),eg=i("Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs."),tg=l(),Ba=s("p"),rg=i(`DPM-Solver is designed to discretize an integral of the noise prediciton model, and DPM-Solver++ is designed to
discretize an integral of the data prediction model. So we need to first convert the model output to the
corresponding type to match the algorithm.`),sg=l(),Ha=s("p"),ng=i(`Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or
DPM-Solver++ for both noise prediction model and data prediction model.`),og=l(),ke=s("div"),p(_s.$$.fragment),ig=l(),Ga=s("p"),ag=i("One step for the first-order DPM-Solver (equivalent to DDIM)."),dg=l(),vs=s("p"),lg=i("See "),bs=s("a"),cg=i("https://arxiv.org/abs/2206.00927"),ug=i(" for the detailed derivation."),fg=l(),At=s("div"),p(Ss.$$.fragment),pg=l(),za=s("p"),hg=i("One step for the second-order multistep DPM-Solver."),mg=l(),Vt=s("div"),p(Ds.$$.fragment),gg=l(),Ya=s("p"),_g=i("One step for the third-order multistep DPM-Solver."),vg=l(),Ft=s("div"),p(xs.$$.fragment),bg=l(),Ja=s("p"),Sg=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Dg=l(),It=s("div"),p($s.$$.fragment),xg=l(),Xa=s("p"),$g=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),Eg=l(),Nt=s("div"),p(Es.$$.fragment),yg=l(),Za=s("p"),wg=i("Step function propagating the sample with the multistep DPM-Solver."),Tc=l(),Xe=s("h4"),Lt=s("a"),ed=s("span"),p(ys.$$.fragment),Mg=l(),td=s("span"),Pg=i("Variance exploding, stochastic sampling from Karras et. al"),Cc=l(),qt=s("p"),Tg=i("Original paper can be found "),ws=s("a"),Cg=i("here"),kg=i("."),kc=l(),x=s("div"),p(Ms.$$.fragment),Og=l(),rd=s("p"),Ag=i(`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),Vg=l(),Kt=s("p"),Fg=i(`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Ps=s("a"),Ig=i("https://arxiv.org/abs/2206.00364"),Ng=i(` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Ts=s("a"),Lg=i("https://arxiv.org/abs/2011.13456"),qg=l(),I=s("p"),Oo=s("a"),Kg=i("~ConfigMixin"),Rg=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),sd=s("code"),Qg=i("__init__"),jg=i(`
function, such as `),nd=s("code"),Wg=i("num_train_timesteps"),Ug=i(". They can be accessed via "),od=s("code"),Bg=i("scheduler.config.num_train_timesteps"),Hg=i(`.
`),Ao=s("a"),Gg=i("~ConfigMixin"),zg=i(" also provides general loading and saving functionality via the "),Vo=s("a"),Yg=i("save_config()"),Jg=i(` and
`),Fo=s("a"),Xg=i("from_config()"),Zg=i(" functions."),e_=l(),Cs=s("p"),t_=i(`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),ks=s("a"),r_=i("https://arxiv.org/abs/2206.00364"),s_=i(`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),n_=l(),Oe=s("div"),p(Os.$$.fragment),o_=l(),id=s("p"),i_=i(`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),a_=l(),ad=s("p"),d_=i("TODO Args:"),l_=l(),Rt=s("div"),p(As.$$.fragment),c_=l(),dd=s("p"),u_=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),f_=l(),Qt=s("div"),p(Vs.$$.fragment),p_=l(),ld=s("p"),h_=i("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),m_=l(),jt=s("div"),p(Fs.$$.fragment),g_=l(),cd=s("p"),__=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),v_=l(),Wt=s("div"),p(Is.$$.fragment),b_=l(),ud=s("p"),S_=i("Correct the predicted sample based on the output model_output of the network. TODO complete description"),Oc=l(),Ze=s("h4"),Ut=s("a"),fd=s("span"),p(Ns.$$.fragment),D_=l(),pd=s("span"),x_=i("Linear multistep scheduler for discrete beta schedules"),Ac=l(),Bt=s("p"),$_=i("Original implementation can be found "),Ls=s("a"),E_=i("here"),y_=i("."),Vc=l(),C=s("div"),p(qs.$$.fragment),w_=l(),Io=s("p"),M_=i(`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),Ks=s("a"),P_=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),T_=l(),N=s("p"),No=s("a"),C_=i("~ConfigMixin"),k_=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),hd=s("code"),O_=i("__init__"),A_=i(`
function, such as `),md=s("code"),V_=i("num_train_timesteps"),F_=i(". They can be accessed via "),gd=s("code"),I_=i("scheduler.config.num_train_timesteps"),N_=i(`.
`),Lo=s("a"),L_=i("~ConfigMixin"),q_=i(" also provides general loading and saving functionality via the "),qo=s("a"),K_=i("save_config()"),R_=i(` and
`),Ko=s("a"),Q_=i("from_config()"),j_=i(" functions."),W_=l(),Ht=s("div"),p(Rs.$$.fragment),U_=l(),_d=s("p"),B_=i("Compute a linear multistep coefficient."),H_=l(),Gt=s("div"),p(Qs.$$.fragment),G_=l(),js=s("p"),z_=i("Scales the denoising model input by "),vd=s("code"),Y_=i("(sigma**2 + 1) ** 0.5"),J_=i(" to match the K-LMS algorithm."),X_=l(),zt=s("div"),p(Ws.$$.fragment),Z_=l(),bd=s("p"),ev=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),tv=l(),Yt=s("div"),p(Us.$$.fragment),rv=l(),Sd=s("p"),sv=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Fc=l(),et=s("h4"),Jt=s("a"),Dd=s("span"),p(Bs.$$.fragment),nv=l(),xd=s("span"),ov=i("Pseudo numerical methods for diffusion models (PNDM)"),Ic=l(),Xt=s("p"),iv=i("Original implementation can be found "),Hs=s("a"),av=i("here"),dv=i("."),Nc=l(),$=s("div"),p(Gs.$$.fragment),lv=l(),$d=s("p"),cv=i(`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),uv=l(),L=s("p"),Ro=s("a"),fv=i("~ConfigMixin"),pv=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ed=s("code"),hv=i("__init__"),mv=i(`
function, such as `),yd=s("code"),gv=i("num_train_timesteps"),_v=i(". They can be accessed via "),wd=s("code"),vv=i("scheduler.config.num_train_timesteps"),bv=i(`.
`),Qo=s("a"),Sv=i("~ConfigMixin"),Dv=i(" also provides general loading and saving functionality via the "),jo=s("a"),xv=i("save_config()"),$v=i(` and
`),Wo=s("a"),Ev=i("from_config()"),yv=i(" functions."),wv=l(),Uo=s("p"),Mv=i("For more details, see the original paper: "),zs=s("a"),Pv=i("https://arxiv.org/abs/2202.09778"),Tv=l(),Zt=s("div"),p(Ys.$$.fragment),Cv=l(),Md=s("p"),kv=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Ov=l(),er=s("div"),p(Js.$$.fragment),Av=l(),Pd=s("p"),Vv=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Fv=l(),Ae=s("div"),p(Xs.$$.fragment),Iv=l(),Td=s("p"),Nv=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Lv=l(),Pe=s("p"),qv=i("This function calls "),Cd=s("code"),Kv=i("step_prk()"),Rv=i(" or "),kd=s("code"),Qv=i("step_plms()"),jv=i(" depending on the internal variable "),Od=s("code"),Wv=i("counter"),Uv=i("."),Bv=l(),tr=s("div"),p(Zs.$$.fragment),Hv=l(),Ad=s("p"),Gv=i(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),zv=l(),rr=s("div"),p(en.$$.fragment),Yv=l(),Vd=s("p"),Jv=i(`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),Lc=l(),tt=s("h4"),sr=s("a"),Fd=s("span"),p(tn.$$.fragment),Xv=l(),Id=s("span"),Zv=i("variance exploding stochastic differential equation (VE-SDE) scheduler"),qc=l(),nr=s("p"),eb=i("Original paper can be found "),rn=s("a"),tb=i("here"),rb=i("."),Kc=l(),E=s("div"),p(sn.$$.fragment),sb=l(),Nd=s("p"),nb=i("The variance exploding stochastic differential equation (SDE) scheduler."),ob=l(),Bo=s("p"),ib=i("For more information, see the original paper: "),nn=s("a"),ab=i("https://arxiv.org/abs/2011.13456"),db=l(),q=s("p"),Ho=s("a"),lb=i("~ConfigMixin"),cb=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ld=s("code"),ub=i("__init__"),fb=i(`
function, such as `),qd=s("code"),pb=i("num_train_timesteps"),hb=i(". They can be accessed via "),Kd=s("code"),mb=i("scheduler.config.num_train_timesteps"),gb=i(`.
`),Go=s("a"),_b=i("~ConfigMixin"),vb=i(" also provides general loading and saving functionality via the "),zo=s("a"),bb=i("save_config()"),Sb=i(` and
`),Yo=s("a"),Db=i("from_config()"),xb=i(" functions."),$b=l(),or=s("div"),p(on.$$.fragment),Eb=l(),Rd=s("p"),yb=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),wb=l(),Ve=s("div"),p(an.$$.fragment),Mb=l(),Qd=s("p"),Pb=i("Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),Tb=l(),rt=s("p"),Cb=i("The sigmas control the weight of the "),jd=s("code"),kb=i("drift"),Ob=i(" and "),Wd=s("code"),Ab=i("diffusion"),Vb=i(" components of sample update."),Fb=l(),ir=s("div"),p(dn.$$.fragment),Ib=l(),Ud=s("p"),Nb=i("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),Lb=l(),ar=s("div"),p(ln.$$.fragment),qb=l(),Bd=s("p"),Kb=i(`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),Rb=l(),dr=s("div"),p(cn.$$.fragment),Qb=l(),Hd=s("p"),jb=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Rc=l(),st=s("h4"),lr=s("a"),Gd=s("span"),p(un.$$.fragment),Wb=l(),zd=s("span"),Ub=i("improved pseudo numerical methods for diffusion models (iPNDM)"),Qc=l(),cr=s("p"),Bb=i("Original implementation can be found "),fn=s("a"),Hb=i("here"),Gb=i("."),jc=l(),k=s("div"),p(pn.$$.fragment),zb=l(),Jo=s("p"),Yb=i(`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),hn=s("a"),Jb=i("library"),Xb=l(),K=s("p"),Xo=s("a"),Zb=i("~ConfigMixin"),e1=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Yd=s("code"),t1=i("__init__"),r1=i(`
function, such as `),Jd=s("code"),s1=i("num_train_timesteps"),n1=i(". They can be accessed via "),Xd=s("code"),o1=i("scheduler.config.num_train_timesteps"),i1=i(`.
`),Zo=s("a"),a1=i("~ConfigMixin"),d1=i(" also provides general loading and saving functionality via the "),ei=s("a"),l1=i("save_config()"),c1=i(` and
`),ti=s("a"),u1=i("from_config()"),f1=i(" functions."),p1=l(),ri=s("p"),h1=i("For more details, see the original paper: "),mn=s("a"),m1=i("https://arxiv.org/abs/2202.09778"),g1=l(),ur=s("div"),p(gn.$$.fragment),_1=l(),Zd=s("p"),v1=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),b1=l(),fr=s("div"),p(_n.$$.fragment),S1=l(),el=s("p"),D1=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),x1=l(),pr=s("div"),p(vn.$$.fragment),$1=l(),tl=s("p"),E1=i(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),Wc=l(),nt=s("h4"),hr=s("a"),rl=s("span"),p(bn.$$.fragment),y1=l(),sl=s("span"),w1=i("variance preserving stochastic differential equation (VP-SDE) scheduler"),Uc=l(),mr=s("p"),M1=i("Original paper can be found "),Sn=s("a"),P1=i("here"),T1=i("."),Bc=l(),p(gr.$$.fragment),Hc=l(),te=s("div"),p(Dn.$$.fragment),C1=l(),nl=s("p"),k1=i("The variance preserving stochastic differential equation (SDE) scheduler."),O1=l(),R=s("p"),si=s("a"),A1=i("~ConfigMixin"),V1=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ol=s("code"),F1=i("__init__"),I1=i(`
function, such as `),il=s("code"),N1=i("num_train_timesteps"),L1=i(". They can be accessed via "),al=s("code"),q1=i("scheduler.config.num_train_timesteps"),K1=i(`.
`),ni=s("a"),R1=i("~ConfigMixin"),Q1=i(" also provides general loading and saving functionality via the "),oi=s("a"),j1=i("save_config()"),W1=i(` and
`),ii=s("a"),U1=i("from_config()"),B1=i(" functions."),H1=l(),ai=s("p"),G1=i("For more information, see the original paper: "),xn=s("a"),z1=i("https://arxiv.org/abs/2011.13456"),Y1=l(),dl=s("p"),J1=i("UNDER CONSTRUCTION"),Gc=l(),ot=s("h4"),_r=s("a"),ll=s("span"),p($n.$$.fragment),X1=l(),cl=s("span"),Z1=i("Euler scheduler"),zc=l(),Fe=s("p"),e0=i("Euler scheduler (Algorithm 2) from the paper "),En=s("a"),t0=i("Elucidating the Design Space of Diffusion-Based Generative Models"),r0=i(" by Karras et al. (2022). Based on the original "),yn=s("a"),s0=i("k-diffusion"),n0=i(` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Yc=l(),Y=s("div"),p(wn.$$.fragment),o0=l(),vr=s("p"),i0=i("Euler scheduler (Algorithm 2) from Karras et al. (2022) "),Mn=s("a"),a0=i("https://arxiv.org/abs/2206.00364"),d0=i(`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),Pn=s("a"),l0=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),c0=l(),Q=s("p"),di=s("a"),u0=i("~ConfigMixin"),f0=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ul=s("code"),p0=i("__init__"),h0=i(`
function, such as `),fl=s("code"),m0=i("num_train_timesteps"),g0=i(". They can be accessed via "),pl=s("code"),_0=i("scheduler.config.num_train_timesteps"),v0=i(`.
`),li=s("a"),b0=i("~ConfigMixin"),S0=i(" also provides general loading and saving functionality via the "),ci=s("a"),D0=i("save_config()"),x0=i(` and
`),ui=s("a"),$0=i("from_config()"),E0=i(" functions."),y0=l(),br=s("div"),p(Tn.$$.fragment),w0=l(),Cn=s("p"),M0=i("Scales the denoising model input by "),hl=s("code"),P0=i("(sigma**2 + 1) ** 0.5"),T0=i(" to match the Euler algorithm."),C0=l(),Sr=s("div"),p(kn.$$.fragment),k0=l(),ml=s("p"),O0=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),A0=l(),Dr=s("div"),p(On.$$.fragment),V0=l(),gl=s("p"),F0=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Jc=l(),it=s("h4"),xr=s("a"),_l=s("span"),p(An.$$.fragment),I0=l(),vl=s("span"),N0=i("Euler Ancestral scheduler"),Xc=l(),fi=s("p"),L0=i(`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Zc=l(),J=s("div"),p(Vn.$$.fragment),q0=l(),pi=s("p"),K0=i(`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Fn=s("a"),R0=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),Q0=l(),j=s("p"),hi=s("a"),j0=i("~ConfigMixin"),W0=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),bl=s("code"),U0=i("__init__"),B0=i(`
function, such as `),Sl=s("code"),H0=i("num_train_timesteps"),G0=i(". They can be accessed via "),Dl=s("code"),z0=i("scheduler.config.num_train_timesteps"),Y0=i(`.
`),mi=s("a"),J0=i("~ConfigMixin"),X0=i(" also provides general loading and saving functionality via the "),gi=s("a"),Z0=i("save_config()"),e2=i(` and
`),_i=s("a"),t2=i("from_config()"),r2=i(" functions."),s2=l(),$r=s("div"),p(In.$$.fragment),n2=l(),Nn=s("p"),o2=i("Scales the denoising model input by "),xl=s("code"),i2=i("(sigma**2 + 1) ** 0.5"),a2=i(" to match the Euler algorithm."),d2=l(),Er=s("div"),p(Ln.$$.fragment),l2=l(),$l=s("p"),c2=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),u2=l(),yr=s("div"),p(qn.$$.fragment),f2=l(),El=s("p"),p2=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),eu=l(),at=s("h4"),wr=s("a"),yl=s("span"),p(Kn.$$.fragment),h2=l(),wl=s("span"),m2=i("VQDiffusionScheduler"),tu=l(),Rn=s("p"),g2=i("Original paper can be found "),Qn=s("a"),_2=i("here"),ru=l(),y=s("div"),p(jn.$$.fragment),v2=l(),Ml=s("p"),b2=i("The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image."),S2=l(),Pl=s("p"),D2=i(`The VQ-diffusion scheduler converts the transformer\u2019s output into a sample for the unnoised image at the previous
diffusion timestep.`),x2=l(),W=s("p"),vi=s("a"),$2=i("~ConfigMixin"),E2=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Tl=s("code"),y2=i("__init__"),w2=i(`
function, such as `),Cl=s("code"),M2=i("num_train_timesteps"),P2=i(". They can be accessed via "),kl=s("code"),T2=i("scheduler.config.num_train_timesteps"),C2=i(`.
`),bi=s("a"),k2=i("~ConfigMixin"),O2=i(" also provides general loading and saving functionality via the "),Si=s("a"),A2=i("save_config()"),V2=i(` and
`),Di=s("a"),F2=i("from_config()"),I2=i(" functions."),N2=l(),xi=s("p"),L2=i("For more details, see the original paper: "),Wn=s("a"),q2=i("https://arxiv.org/abs/2111.14822"),K2=l(),Ie=s("div"),p(Un.$$.fragment),R2=l(),Bn=s("p"),Q2=i(`Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each
latent pixel in `),Ol=s("code"),j2=i("x_t"),W2=i("."),U2=l(),Al=s("p"),B2=i(`See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix
is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.`),H2=l(),U=s("div"),p(Hn.$$.fragment),G2=l(),Gn=s("p"),z2=i("Calculates the log probabilities for the predicted classes of the image at timestep "),Vl=s("code"),Y2=i("t-1"),J2=i(". I.e. Equation (11)."),X2=l(),Fl=s("p"),Z2=i(`Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only
forward probabilities.`),e4=l(),Il=s("p"),t4=i("Equation (11) stated in terms of forward probabilities via Equation (5):"),r4=l(),Nl=s("p"),s4=i("Where:"),n4=l(),Ll=s("ul"),zn=s("li"),o4=i("the sum is over x"),ql=s("em"),i4=i("0 = {C_0 \u2026 C"),a4=i("{k-1}} (classes for x_0)"),d4=l(),dt=s("p"),l4=i("p(x"),Kl=s("em"),c4=i("{t-1} | x_t) = sum( q(x_t | x"),u4=i("{t-1}) "),Rl=s("em"),f4=i("q(x_{t-1} | x_0)"),p4=i(" p(x_0) / q(x_t | x_0) )"),h4=l(),Mr=s("div"),p(Yn.$$.fragment),m4=l(),Ql=s("p"),g4=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),_4=l(),Pr=s("div"),p(Jn.$$.fragment),v4=l(),Xn=s("p"),b4=i(`Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the
docstring for `),jl=s("code"),S4=i("self.q_posterior"),D4=i(" for more in depth docs on how Equation (11) is computed."),su=l(),lt=s("h4"),Tr=s("a"),Wl=s("span"),p(Zn.$$.fragment),x4=l(),Ul=s("span"),$4=i("RePaint scheduler"),nu=l(),Te=s("p"),E4=i(`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),$i=s("a"),y4=i("RePaintPipeline"),w4=i(`.
Based on the paper `),eo=s("a"),M4=i("RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),P4=i(`
and the original implementation by Andreas Lugmayr et al.: `),to=s("a"),T4=i("https://github.com/andreas128/RePaint"),ou=l(),X=s("div"),p(ro.$$.fragment),C4=l(),Bl=s("p"),k4=i("RePaint is a schedule for DDPM inpainting inside a given mask."),O4=l(),B=s("p"),Ei=s("a"),A4=i("~ConfigMixin"),V4=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Hl=s("code"),F4=i("__init__"),I4=i(`
function, such as `),Gl=s("code"),N4=i("num_train_timesteps"),L4=i(". They can be accessed via "),zl=s("code"),q4=i("scheduler.config.num_train_timesteps"),K4=i(`.
`),yi=s("a"),R4=i("~ConfigMixin"),Q4=i(" also provides general loading and saving functionality via the "),wi=s("a"),j4=i("save_config()"),W4=i(` and
`),Mi=s("a"),U4=i("from_config()"),B4=i(" functions."),H4=l(),Pi=s("p"),G4=i("For more details, see the original paper: "),so=s("a"),z4=i("https://arxiv.org/pdf/2201.09865.pdf"),Y4=l(),Cr=s("div"),p(no.$$.fragment),J4=l(),Yl=s("p"),X4=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Z4=l(),kr=s("div"),p(oo.$$.fragment),eS=l(),Jl=s("p"),tS=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),this.h()},l(r){const u=K5('[data-svelte="svelte-1phssyn"]',document.head);Z=n(u,"META",{name:!0,content:!0}),u.forEach(t),ct=c(r),ee=n(r,"H1",{class:!0});var io=o(ee);De=n(io,"A",{id:!0,class:!0,href:!0});var mS=o(De);Ki=n(mS,"SPAN",{});var gS=o(Ki);h(Ir.$$.fragment,gS),gS.forEach(t),mS.forEach(t),qf=c(io),Ri=n(io,"SPAN",{});var _S=o(Ri);Kf=a(_S,"Schedulers"),_S.forEach(t),io.forEach(t),rc=c(r),lo=n(r,"P",{});var vS=o(lo);Rf=a(vS,"Diffusers contains multiple pre-built schedule functions for the diffusion process."),vS.forEach(t),sc=c(r),Le=n(r,"H2",{class:!0});var au=o(Le);ut=n(au,"A",{id:!0,class:!0,href:!0});var bS=o(ut);Qi=n(bS,"SPAN",{});var SS=o(Qi);h(Nr.$$.fragment,SS),SS.forEach(t),bS.forEach(t),Qf=c(au),ji=n(au,"SPAN",{});var DS=o(ji);jf=a(DS,"What is a scheduler?"),DS.forEach(t),au.forEach(t),nc=c(r),ft=n(r,"P",{});var du=o(ft);Wf=a(du,"The schedule functions, denoted "),Wi=n(du,"EM",{});var xS=o(Wi);Uf=a(xS,"Schedulers"),xS.forEach(t),Bf=a(du," in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample."),du.forEach(t),oc=c(r),pt=n(r,"UL",{});var lu=o(pt);co=n(lu,"LI",{});var rS=o(co);Hf=a(rS,"Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),Lr=n(rS,"UL",{});var cu=o(Lr);Ui=n(cu,"LI",{});var $S=o(Ui);Gf=a($S,"adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),$S.forEach(t),zf=c(cu),Bi=n(cu,"LI",{});var ES=o(Bi);Yf=a(ES,"for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),ES.forEach(t),cu.forEach(t),rS.forEach(t),Jf=c(lu),qe=n(lu,"LI",{});var Ti=o(qe);Xf=a(Ti,"Schedulers are often defined by a "),Hi=n(Ti,"EM",{});var yS=o(Hi);Zf=a(yS,"noise schedule"),yS.forEach(t),ep=a(Ti," and an "),Gi=n(Ti,"EM",{});var wS=o(Gi);tp=a(wS,"update rule"),wS.forEach(t),rp=a(Ti," to solve the differential equation solution."),Ti.forEach(t),lu.forEach(t),ic=c(r),Ke=n(r,"H3",{class:!0});var uu=o(Ke);ht=n(uu,"A",{id:!0,class:!0,href:!0});var MS=o(ht);zi=n(MS,"SPAN",{});var PS=o(zi);h(qr.$$.fragment,PS),PS.forEach(t),MS.forEach(t),sp=c(uu),Yi=n(uu,"SPAN",{});var TS=o(Yi);np=a(TS,"Discrete versus continuous schedulers"),TS.forEach(t),uu.forEach(t),ac=c(r),O=n(r,"P",{});var me=o(O);op=a(me,`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),Ji=n(me,"CODE",{});var CS=o(Ji);ip=a(CS,"int"),CS.forEach(t),ap=a(me," inputs), such as the "),uo=n(me,"A",{href:!0});var kS=o(uo);dp=a(kS,"DDPMScheduler"),kS.forEach(t),lp=a(me," or "),fo=n(me,"A",{href:!0});var OS=o(fo);cp=a(OS,"PNDMScheduler"),OS.forEach(t),up=a(me,", and continuous (accepting "),Xi=n(me,"CODE",{});var AS=o(Xi);fp=a(AS,"float"),AS.forEach(t),pp=a(me," inputs), such as the score-based schedulers "),po=n(me,"A",{href:!0});var VS=o(po);hp=a(VS,"ScoreSdeVeScheduler"),VS.forEach(t),mp=a(me," or "),Zi=n(me,"CODE",{});var FS=o(Zi);gp=a(FS,"ScoreSdeVpScheduler"),FS.forEach(t),_p=a(me,"."),me.forEach(t),dc=c(r),Re=n(r,"H2",{class:!0});var fu=o(Re);mt=n(fu,"A",{id:!0,class:!0,href:!0});var IS=o(mt);ea=n(IS,"SPAN",{});var NS=o(ea);h(Kr.$$.fragment,NS),NS.forEach(t),IS.forEach(t),vp=c(fu),ta=n(fu,"SPAN",{});var LS=o(ta);bp=a(LS,"Designing Re-usable schedulers"),LS.forEach(t),fu.forEach(t),lc=c(r),ho=n(r,"P",{});var qS=o(ho);Sp=a(qS,`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),qS.forEach(t),cc=c(r),gt=n(r,"UL",{});var pu=o(gt);ra=n(pu,"LI",{});var KS=o(ra);Dp=a(KS,"Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),KS.forEach(t),xp=c(pu),sa=n(pu,"LI",{});var RS=o(sa);$p=a(RS,"Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),RS.forEach(t),pu.forEach(t),uc=c(r),Qe=n(r,"H2",{class:!0});var hu=o(Qe);_t=n(hu,"A",{id:!0,class:!0,href:!0});var QS=o(_t);na=n(QS,"SPAN",{});var jS=o(na);h(Rr.$$.fragment,jS),jS.forEach(t),QS.forEach(t),Ep=c(hu),oa=n(hu,"SPAN",{});var WS=o(oa);yp=a(WS,"API"),WS.forEach(t),hu.forEach(t),fc=c(r),mo=n(r,"P",{});var US=o(mo);wp=a(US,"The core API for any new scheduler must follow a limited structure."),US.forEach(t),pc=c(r),Ce=n(r,"UL",{});var Ci=o(Ce);Qr=n(Ci,"LI",{});var mu=o(Qr);Mp=a(mu,"Schedulers should provide one or more "),ia=n(mu,"CODE",{});var BS=o(ia);Pp=a(BS,"def step(...)"),BS.forEach(t),Tp=a(mu," functions that should be called to update the generated sample iteratively."),mu.forEach(t),Cp=c(Ci),jr=n(Ci,"LI",{});var gu=o(jr);kp=a(gu,"Schedulers should provide a "),aa=n(gu,"CODE",{});var HS=o(aa);Op=a(HS,"set_timesteps(...)"),HS.forEach(t),Ap=a(gu," method that configures the parameters of a schedule function for a specific inference task."),gu.forEach(t),Vp=c(Ci),da=n(Ci,"LI",{});var GS=o(da);Fp=a(GS,"Schedulers should be framework-specific."),GS.forEach(t),Ci.forEach(t),hc=c(r),vt=n(r,"P",{});var _u=o(vt);Ip=a(_u,"The base class "),go=n(_u,"A",{href:!0});var zS=o(go);Np=a(zS,"SchedulerMixin"),zS.forEach(t),Lp=a(_u," implements low level utilities used by multiple schedulers."),_u.forEach(t),mc=c(r),je=n(r,"H3",{class:!0});var vu=o(je);bt=n(vu,"A",{id:!0,class:!0,href:!0});var YS=o(bt);la=n(YS,"SPAN",{});var JS=o(la);h(Wr.$$.fragment,JS),JS.forEach(t),YS.forEach(t),qp=c(vu),ca=n(vu,"SPAN",{});var XS=o(ca);Kp=a(XS,"SchedulerMixin"),XS.forEach(t),vu.forEach(t),gc=c(r),We=n(r,"DIV",{class:!0});var bu=o(We);h(Ur.$$.fragment,bu),Rp=c(bu),ua=n(bu,"P",{});var ZS=o(ua);Qp=a(ZS,"Mixin containing common functions for the schedulers."),ZS.forEach(t),bu.forEach(t),_c=c(r),Ue=n(r,"H3",{class:!0});var Su=o(Ue);St=n(Su,"A",{id:!0,class:!0,href:!0});var eD=o(St);fa=n(eD,"SPAN",{});var tD=o(fa);h(Br.$$.fragment,tD),tD.forEach(t),eD.forEach(t),jp=c(Su),pa=n(Su,"SPAN",{});var rD=o(pa);Wp=a(rD,"SchedulerOutput"),rD.forEach(t),Su.forEach(t),vc=a(r,"\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),Be=n(r,"DIV",{class:!0});var Du=o(Be);h(Hr.$$.fragment,Du),Up=c(Du),ha=n(Du,"P",{});var sD=o(ha);Bp=a(sD,"Base class for the scheduler\u2019s step function output."),sD.forEach(t),Du.forEach(t),bc=c(r),He=n(r,"H3",{class:!0});var xu=o(He);Dt=n(xu,"A",{id:!0,class:!0,href:!0});var nD=o(Dt);ma=n(nD,"SPAN",{});var oD=o(ma);h(Gr.$$.fragment,oD),oD.forEach(t),nD.forEach(t),Hp=c(xu),ga=n(xu,"SPAN",{});var iD=o(ga);Gp=a(iD,"Implemented Schedulers"),iD.forEach(t),xu.forEach(t),Sc=c(r),Ge=n(r,"H4",{class:!0});var $u=o(Ge);xt=n($u,"A",{id:!0,class:!0,href:!0});var aD=o(xt);_a=n(aD,"SPAN",{});var dD=o(_a);h(zr.$$.fragment,dD),dD.forEach(t),aD.forEach(t),zp=c($u),va=n($u,"SPAN",{});var lD=o(va);Yp=a(lD,"Denoising diffusion implicit models (DDIM)"),lD.forEach(t),$u.forEach(t),Dc=c(r),_o=n(r,"P",{});var cD=o(_o);Jp=a(cD,"Original paper can be found here."),cD.forEach(t),xc=c(r),P=n(r,"DIV",{class:!0});var ge=o(P);h(Yr.$$.fragment,ge),Xp=c(ge),ba=n(ge,"P",{});var uD=o(ba);Zp=a(uD,`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),uD.forEach(t),eh=c(ge),A=n(ge,"P",{});var re=o(A);vo=n(re,"A",{href:!0});var fD=o(vo);th=a(fD,"~ConfigMixin"),fD.forEach(t),rh=a(re," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Sa=n(re,"CODE",{});var pD=o(Sa);sh=a(pD,"__init__"),pD.forEach(t),nh=a(re,`
function, such as `),Da=n(re,"CODE",{});var hD=o(Da);oh=a(hD,"num_train_timesteps"),hD.forEach(t),ih=a(re,". They can be accessed via "),xa=n(re,"CODE",{});var mD=o(xa);ah=a(mD,"scheduler.config.num_train_timesteps"),mD.forEach(t),dh=a(re,`.
`),bo=n(re,"A",{href:!0});var gD=o(bo);lh=a(gD,"~ConfigMixin"),gD.forEach(t),ch=a(re," also provides general loading and saving functionality via the "),So=n(re,"A",{href:!0});var _D=o(So);uh=a(_D,"save_config()"),_D.forEach(t),fh=a(re,` and
`),Do=n(re,"A",{href:!0});var vD=o(Do);ph=a(vD,"from_config()"),vD.forEach(t),hh=a(re," functions."),re.forEach(t),mh=c(ge),xo=n(ge,"P",{});var sS=o(xo);gh=a(sS,"For more details, see the original paper: "),Jr=n(sS,"A",{href:!0,rel:!0});var bD=o(Jr);_h=a(bD,"https://arxiv.org/abs/2010.02502"),bD.forEach(t),sS.forEach(t),vh=c(ge),$t=n(ge,"DIV",{class:!0});var Eu=o($t);h(Xr.$$.fragment,Eu),bh=c(Eu),$a=n(Eu,"P",{});var SD=o($a);Sh=a(SD,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),SD.forEach(t),Eu.forEach(t),Dh=c(ge),Et=n(ge,"DIV",{class:!0});var yu=o(Et);h(Zr.$$.fragment,yu),xh=c(yu),Ea=n(yu,"P",{});var DD=o(Ea);$h=a(DD,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),DD.forEach(t),yu.forEach(t),Eh=c(ge),yt=n(ge,"DIV",{class:!0});var wu=o(yt);h(es.$$.fragment,wu),yh=c(wu),ya=n(wu,"P",{});var xD=o(ya);wh=a(xD,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),xD.forEach(t),wu.forEach(t),ge.forEach(t),$c=c(r),ze=n(r,"H4",{class:!0});var Mu=o(ze);wt=n(Mu,"A",{id:!0,class:!0,href:!0});var $D=o(wt);wa=n($D,"SPAN",{});var ED=o(wa);h(ts.$$.fragment,ED),ED.forEach(t),$D.forEach(t),Mh=c(Mu),Ma=n(Mu,"SPAN",{});var yD=o(Ma);Ph=a(yD,"Denoising diffusion probabilistic models (DDPM)"),yD.forEach(t),Mu.forEach(t),Ec=c(r),Mt=n(r,"P",{});var Pu=o(Mt);Th=a(Pu,"Original paper can be found "),rs=n(Pu,"A",{href:!0,rel:!0});var wD=o(rs);Ch=a(wD,"here"),wD.forEach(t),kh=a(Pu,"."),Pu.forEach(t),yc=c(r),T=n(r,"DIV",{class:!0});var _e=o(T);h(ss.$$.fragment,_e),Oh=c(_e),Pa=n(_e,"P",{});var MD=o(Pa);Ah=a(MD,`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),MD.forEach(t),Vh=c(_e),V=n(_e,"P",{});var se=o(V);$o=n(se,"A",{href:!0});var PD=o($o);Fh=a(PD,"~ConfigMixin"),PD.forEach(t),Ih=a(se," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ta=n(se,"CODE",{});var TD=o(Ta);Nh=a(TD,"__init__"),TD.forEach(t),Lh=a(se,`
function, such as `),Ca=n(se,"CODE",{});var CD=o(Ca);qh=a(CD,"num_train_timesteps"),CD.forEach(t),Kh=a(se,". They can be accessed via "),ka=n(se,"CODE",{});var kD=o(ka);Rh=a(kD,"scheduler.config.num_train_timesteps"),kD.forEach(t),Qh=a(se,`.
`),Eo=n(se,"A",{href:!0});var OD=o(Eo);jh=a(OD,"~ConfigMixin"),OD.forEach(t),Wh=a(se," also provides general loading and saving functionality via the "),yo=n(se,"A",{href:!0});var AD=o(yo);Uh=a(AD,"save_config()"),AD.forEach(t),Bh=a(se,` and
`),wo=n(se,"A",{href:!0});var VD=o(wo);Hh=a(VD,"from_config()"),VD.forEach(t),Gh=a(se," functions."),se.forEach(t),zh=c(_e),Mo=n(_e,"P",{});var nS=o(Mo);Yh=a(nS,"For more details, see the original paper: "),ns=n(nS,"A",{href:!0,rel:!0});var FD=o(ns);Jh=a(FD,"https://arxiv.org/abs/2006.11239"),FD.forEach(t),nS.forEach(t),Xh=c(_e),Pt=n(_e,"DIV",{class:!0});var Tu=o(Pt);h(os.$$.fragment,Tu),Zh=c(Tu),Oa=n(Tu,"P",{});var ID=o(Oa);em=a(ID,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),ID.forEach(t),Tu.forEach(t),tm=c(_e),Tt=n(_e,"DIV",{class:!0});var Cu=o(Tt);h(is.$$.fragment,Cu),rm=c(Cu),Aa=n(Cu,"P",{});var ND=o(Aa);sm=a(ND,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),ND.forEach(t),Cu.forEach(t),nm=c(_e),Ct=n(_e,"DIV",{class:!0});var ku=o(Ct);h(as.$$.fragment,ku),om=c(ku),Va=n(ku,"P",{});var LD=o(Va);im=a(LD,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),LD.forEach(t),ku.forEach(t),_e.forEach(t),wc=c(r),Ye=n(r,"H4",{class:!0});var Ou=o(Ye);kt=n(Ou,"A",{id:!0,class:!0,href:!0});var qD=o(kt);Fa=n(qD,"SPAN",{});var KD=o(Fa);h(ds.$$.fragment,KD),KD.forEach(t),qD.forEach(t),am=c(Ou),Ia=n(Ou,"SPAN",{});var RD=o(Ia);dm=a(RD,"Multistep DPM-Solver"),RD.forEach(t),Ou.forEach(t),Mc=c(r),xe=n(r,"P",{});var Or=o(xe);lm=a(Or,"Original paper can be found "),ls=n(Or,"A",{href:!0,rel:!0});var QD=o(ls);cm=a(QD,"here"),QD.forEach(t),um=a(Or," and the "),cs=n(Or,"A",{href:!0,rel:!0});var jD=o(cs);fm=a(jD,"improved version"),jD.forEach(t),pm=a(Or,". The original implementation can be found "),us=n(Or,"A",{href:!0,rel:!0});var WD=o(us);hm=a(WD,"here"),WD.forEach(t),mm=a(Or,"."),Or.forEach(t),Pc=c(r),S=n(r,"DIV",{class:!0});var D=o(S);h(fs.$$.fragment,D),gm=c(D),Na=n(D,"P",{});var UD=o(Na);_m=a(UD,`DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with
the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality
samples, and it can generate quite good samples even in only 10 steps.`),UD.forEach(t),vm=c(D),Ot=n(D,"P",{});var Xl=o(Ot);bm=a(Xl,"For more details, see the original paper: "),ps=n(Xl,"A",{href:!0,rel:!0});var BD=o(ps);Sm=a(BD,"https://arxiv.org/abs/2206.00927"),BD.forEach(t),Dm=a(Xl," and "),hs=n(Xl,"A",{href:!0,rel:!0});var HD=o(hs);xm=a(HD,"https://arxiv.org/abs/2211.01095"),HD.forEach(t),Xl.forEach(t),$m=c(D),Je=n(D,"P",{});var ki=o(Je);Em=a(ki,`Currently, we support the multistep DPM-Solver for both noise prediction models and data prediction models. We
recommend to use `),La=n(ki,"CODE",{});var GD=o(La);ym=a(GD,"solver_order=2"),GD.forEach(t),wm=a(ki," for guided sampling, and "),qa=n(ki,"CODE",{});var zD=o(qa);Mm=a(zD,"solver_order=3"),zD.forEach(t),Pm=a(ki," for unconditional sampling."),ki.forEach(t),Tm=c(D),Me=n(D,"P",{});var Ar=o(Me);Cm=a(Ar,"We also support the \u201Cdynamic thresholding\u201D method in Imagen ("),ms=n(Ar,"A",{href:!0,rel:!0});var YD=o(ms);km=a(YD,"https://arxiv.org/abs/2205.11487"),YD.forEach(t),Om=a(Ar,`). For pixel-space
diffusion models, you can set both `),Ka=n(Ar,"CODE",{});var JD=o(Ka);Am=a(JD,'algorithm_type="dpmsolver++"'),JD.forEach(t),Vm=a(Ar," and "),Ra=n(Ar,"CODE",{});var XD=o(Ra);Fm=a(XD,"thresholding=True"),XD.forEach(t),Im=a(Ar,` to use the dynamic
thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as
stable-diffusion).`),Ar.forEach(t),Nm=c(D),F=n(D,"P",{});var ne=o(F);Po=n(ne,"A",{href:!0});var ZD=o(Po);Lm=a(ZD,"~ConfigMixin"),ZD.forEach(t),qm=a(ne," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Qa=n(ne,"CODE",{});var ex=o(Qa);Km=a(ex,"__init__"),ex.forEach(t),Rm=a(ne,`
function, such as `),ja=n(ne,"CODE",{});var tx=o(ja);Qm=a(tx,"num_train_timesteps"),tx.forEach(t),jm=a(ne,". They can be accessed via "),Wa=n(ne,"CODE",{});var rx=o(Wa);Wm=a(rx,"scheduler.config.num_train_timesteps"),rx.forEach(t),Um=a(ne,`.
`),To=n(ne,"A",{href:!0});var sx=o(To);Bm=a(sx,"~ConfigMixin"),sx.forEach(t),Hm=a(ne," also provides general loading and saving functionality via the "),Co=n(ne,"A",{href:!0});var nx=o(Co);Gm=a(nx,"save_config()"),nx.forEach(t),zm=a(ne,` and
`),ko=n(ne,"A",{href:!0});var ox=o(ko);Ym=a(ox,"from_config()"),ox.forEach(t),Jm=a(ne," functions."),ne.forEach(t),Xm=c(D),$e=n(D,"DIV",{class:!0});var Vr=o($e);h(gs.$$.fragment,Vr),Zm=c(Vr),Ua=n(Vr,"P",{});var ix=o(Ua);eg=a(ix,"Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs."),ix.forEach(t),tg=c(Vr),Ba=n(Vr,"P",{});var ax=o(Ba);rg=a(ax,`DPM-Solver is designed to discretize an integral of the noise prediciton model, and DPM-Solver++ is designed to
discretize an integral of the data prediction model. So we need to first convert the model output to the
corresponding type to match the algorithm.`),ax.forEach(t),sg=c(Vr),Ha=n(Vr,"P",{});var dx=o(Ha);ng=a(dx,`Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or
DPM-Solver++ for both noise prediction model and data prediction model.`),dx.forEach(t),Vr.forEach(t),og=c(D),ke=n(D,"DIV",{class:!0});var Oi=o(ke);h(_s.$$.fragment,Oi),ig=c(Oi),Ga=n(Oi,"P",{});var lx=o(Ga);ag=a(lx,"One step for the first-order DPM-Solver (equivalent to DDIM)."),lx.forEach(t),dg=c(Oi),vs=n(Oi,"P",{});var Au=o(vs);lg=a(Au,"See "),bs=n(Au,"A",{href:!0,rel:!0});var cx=o(bs);cg=a(cx,"https://arxiv.org/abs/2206.00927"),cx.forEach(t),ug=a(Au," for the detailed derivation."),Au.forEach(t),Oi.forEach(t),fg=c(D),At=n(D,"DIV",{class:!0});var Vu=o(At);h(Ss.$$.fragment,Vu),pg=c(Vu),za=n(Vu,"P",{});var ux=o(za);hg=a(ux,"One step for the second-order multistep DPM-Solver."),ux.forEach(t),Vu.forEach(t),mg=c(D),Vt=n(D,"DIV",{class:!0});var Fu=o(Vt);h(Ds.$$.fragment,Fu),gg=c(Fu),Ya=n(Fu,"P",{});var fx=o(Ya);_g=a(fx,"One step for the third-order multistep DPM-Solver."),fx.forEach(t),Fu.forEach(t),vg=c(D),Ft=n(D,"DIV",{class:!0});var Iu=o(Ft);h(xs.$$.fragment,Iu),bg=c(Iu),Ja=n(Iu,"P",{});var px=o(Ja);Sg=a(px,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),px.forEach(t),Iu.forEach(t),Dg=c(D),It=n(D,"DIV",{class:!0});var Nu=o(It);h($s.$$.fragment,Nu),xg=c(Nu),Xa=n(Nu,"P",{});var hx=o(Xa);$g=a(hx,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),hx.forEach(t),Nu.forEach(t),Eg=c(D),Nt=n(D,"DIV",{class:!0});var Lu=o(Nt);h(Es.$$.fragment,Lu),yg=c(Lu),Za=n(Lu,"P",{});var mx=o(Za);wg=a(mx,"Step function propagating the sample with the multistep DPM-Solver."),mx.forEach(t),Lu.forEach(t),D.forEach(t),Tc=c(r),Xe=n(r,"H4",{class:!0});var qu=o(Xe);Lt=n(qu,"A",{id:!0,class:!0,href:!0});var gx=o(Lt);ed=n(gx,"SPAN",{});var _x=o(ed);h(ys.$$.fragment,_x),_x.forEach(t),gx.forEach(t),Mg=c(qu),td=n(qu,"SPAN",{});var vx=o(td);Pg=a(vx,"Variance exploding, stochastic sampling from Karras et. al"),vx.forEach(t),qu.forEach(t),Cc=c(r),qt=n(r,"P",{});var Ku=o(qt);Tg=a(Ku,"Original paper can be found "),ws=n(Ku,"A",{href:!0,rel:!0});var bx=o(ws);Cg=a(bx,"here"),bx.forEach(t),kg=a(Ku,"."),Ku.forEach(t),kc=c(r),x=n(r,"DIV",{class:!0});var M=o(x);h(Ms.$$.fragment,M),Og=c(M),rd=n(M,"P",{});var Sx=o(rd);Ag=a(Sx,`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),Sx.forEach(t),Vg=c(M),Kt=n(M,"P",{});var Zl=o(Kt);Fg=a(Zl,`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Ps=n(Zl,"A",{href:!0,rel:!0});var Dx=o(Ps);Ig=a(Dx,"https://arxiv.org/abs/2206.00364"),Dx.forEach(t),Ng=a(Zl,` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Ts=n(Zl,"A",{href:!0,rel:!0});var xx=o(Ts);Lg=a(xx,"https://arxiv.org/abs/2011.13456"),xx.forEach(t),Zl.forEach(t),qg=c(M),I=n(M,"P",{});var oe=o(I);Oo=n(oe,"A",{href:!0});var $x=o(Oo);Kg=a($x,"~ConfigMixin"),$x.forEach(t),Rg=a(oe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),sd=n(oe,"CODE",{});var Ex=o(sd);Qg=a(Ex,"__init__"),Ex.forEach(t),jg=a(oe,`
function, such as `),nd=n(oe,"CODE",{});var yx=o(nd);Wg=a(yx,"num_train_timesteps"),yx.forEach(t),Ug=a(oe,". They can be accessed via "),od=n(oe,"CODE",{});var wx=o(od);Bg=a(wx,"scheduler.config.num_train_timesteps"),wx.forEach(t),Hg=a(oe,`.
`),Ao=n(oe,"A",{href:!0});var Mx=o(Ao);Gg=a(Mx,"~ConfigMixin"),Mx.forEach(t),zg=a(oe," also provides general loading and saving functionality via the "),Vo=n(oe,"A",{href:!0});var Px=o(Vo);Yg=a(Px,"save_config()"),Px.forEach(t),Jg=a(oe,` and
`),Fo=n(oe,"A",{href:!0});var Tx=o(Fo);Xg=a(Tx,"from_config()"),Tx.forEach(t),Zg=a(oe," functions."),oe.forEach(t),e_=c(M),Cs=n(M,"P",{});var Ru=o(Cs);t_=a(Ru,`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),ks=n(Ru,"A",{href:!0,rel:!0});var Cx=o(ks);r_=a(Cx,"https://arxiv.org/abs/2206.00364"),Cx.forEach(t),s_=a(Ru,`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),Ru.forEach(t),n_=c(M),Oe=n(M,"DIV",{class:!0});var Ai=o(Oe);h(Os.$$.fragment,Ai),o_=c(Ai),id=n(Ai,"P",{});var kx=o(id);i_=a(kx,`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),kx.forEach(t),a_=c(Ai),ad=n(Ai,"P",{});var Ox=o(ad);d_=a(Ox,"TODO Args:"),Ox.forEach(t),Ai.forEach(t),l_=c(M),Rt=n(M,"DIV",{class:!0});var Qu=o(Rt);h(As.$$.fragment,Qu),c_=c(Qu),dd=n(Qu,"P",{});var Ax=o(dd);u_=a(Ax,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Ax.forEach(t),Qu.forEach(t),f_=c(M),Qt=n(M,"DIV",{class:!0});var ju=o(Qt);h(Vs.$$.fragment,ju),p_=c(ju),ld=n(ju,"P",{});var Vx=o(ld);h_=a(Vx,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),Vx.forEach(t),ju.forEach(t),m_=c(M),jt=n(M,"DIV",{class:!0});var Wu=o(jt);h(Fs.$$.fragment,Wu),g_=c(Wu),cd=n(Wu,"P",{});var Fx=o(cd);__=a(Fx,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Fx.forEach(t),Wu.forEach(t),v_=c(M),Wt=n(M,"DIV",{class:!0});var Uu=o(Wt);h(Is.$$.fragment,Uu),b_=c(Uu),ud=n(Uu,"P",{});var Ix=o(ud);S_=a(Ix,"Correct the predicted sample based on the output model_output of the network. TODO complete description"),Ix.forEach(t),Uu.forEach(t),M.forEach(t),Oc=c(r),Ze=n(r,"H4",{class:!0});var Bu=o(Ze);Ut=n(Bu,"A",{id:!0,class:!0,href:!0});var Nx=o(Ut);fd=n(Nx,"SPAN",{});var Lx=o(fd);h(Ns.$$.fragment,Lx),Lx.forEach(t),Nx.forEach(t),D_=c(Bu),pd=n(Bu,"SPAN",{});var qx=o(pd);x_=a(qx,"Linear multistep scheduler for discrete beta schedules"),qx.forEach(t),Bu.forEach(t),Ac=c(r),Bt=n(r,"P",{});var Hu=o(Bt);$_=a(Hu,"Original implementation can be found "),Ls=n(Hu,"A",{href:!0,rel:!0});var Kx=o(Ls);E_=a(Kx,"here"),Kx.forEach(t),y_=a(Hu,"."),Hu.forEach(t),Vc=c(r),C=n(r,"DIV",{class:!0});var ve=o(C);h(qs.$$.fragment,ve),w_=c(ve),Io=n(ve,"P",{});var oS=o(Io);M_=a(oS,`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),Ks=n(oS,"A",{href:!0,rel:!0});var Rx=o(Ks);P_=a(Rx,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),Rx.forEach(t),oS.forEach(t),T_=c(ve),N=n(ve,"P",{});var ie=o(N);No=n(ie,"A",{href:!0});var Qx=o(No);C_=a(Qx,"~ConfigMixin"),Qx.forEach(t),k_=a(ie," takes care of storing all config attributes that are passed in the scheduler\u2019s "),hd=n(ie,"CODE",{});var jx=o(hd);O_=a(jx,"__init__"),jx.forEach(t),A_=a(ie,`
function, such as `),md=n(ie,"CODE",{});var Wx=o(md);V_=a(Wx,"num_train_timesteps"),Wx.forEach(t),F_=a(ie,". They can be accessed via "),gd=n(ie,"CODE",{});var Ux=o(gd);I_=a(Ux,"scheduler.config.num_train_timesteps"),Ux.forEach(t),N_=a(ie,`.
`),Lo=n(ie,"A",{href:!0});var Bx=o(Lo);L_=a(Bx,"~ConfigMixin"),Bx.forEach(t),q_=a(ie," also provides general loading and saving functionality via the "),qo=n(ie,"A",{href:!0});var Hx=o(qo);K_=a(Hx,"save_config()"),Hx.forEach(t),R_=a(ie,` and
`),Ko=n(ie,"A",{href:!0});var Gx=o(Ko);Q_=a(Gx,"from_config()"),Gx.forEach(t),j_=a(ie," functions."),ie.forEach(t),W_=c(ve),Ht=n(ve,"DIV",{class:!0});var Gu=o(Ht);h(Rs.$$.fragment,Gu),U_=c(Gu),_d=n(Gu,"P",{});var zx=o(_d);B_=a(zx,"Compute a linear multistep coefficient."),zx.forEach(t),Gu.forEach(t),H_=c(ve),Gt=n(ve,"DIV",{class:!0});var zu=o(Gt);h(Qs.$$.fragment,zu),G_=c(zu),js=n(zu,"P",{});var Yu=o(js);z_=a(Yu,"Scales the denoising model input by "),vd=n(Yu,"CODE",{});var Yx=o(vd);Y_=a(Yx,"(sigma**2 + 1) ** 0.5"),Yx.forEach(t),J_=a(Yu," to match the K-LMS algorithm."),Yu.forEach(t),zu.forEach(t),X_=c(ve),zt=n(ve,"DIV",{class:!0});var Ju=o(zt);h(Ws.$$.fragment,Ju),Z_=c(Ju),bd=n(Ju,"P",{});var Jx=o(bd);ev=a(Jx,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),Jx.forEach(t),Ju.forEach(t),tv=c(ve),Yt=n(ve,"DIV",{class:!0});var Xu=o(Yt);h(Us.$$.fragment,Xu),rv=c(Xu),Sd=n(Xu,"P",{});var Xx=o(Sd);sv=a(Xx,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Xx.forEach(t),Xu.forEach(t),ve.forEach(t),Fc=c(r),et=n(r,"H4",{class:!0});var Zu=o(et);Jt=n(Zu,"A",{id:!0,class:!0,href:!0});var Zx=o(Jt);Dd=n(Zx,"SPAN",{});var e$=o(Dd);h(Bs.$$.fragment,e$),e$.forEach(t),Zx.forEach(t),nv=c(Zu),xd=n(Zu,"SPAN",{});var t$=o(xd);ov=a(t$,"Pseudo numerical methods for diffusion models (PNDM)"),t$.forEach(t),Zu.forEach(t),Ic=c(r),Xt=n(r,"P",{});var ef=o(Xt);iv=a(ef,"Original implementation can be found "),Hs=n(ef,"A",{href:!0,rel:!0});var r$=o(Hs);av=a(r$,"here"),r$.forEach(t),dv=a(ef,"."),ef.forEach(t),Nc=c(r),$=n(r,"DIV",{class:!0});var H=o($);h(Gs.$$.fragment,H),lv=c(H),$d=n(H,"P",{});var s$=o($d);cv=a(s$,`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),s$.forEach(t),uv=c(H),L=n(H,"P",{});var ae=o(L);Ro=n(ae,"A",{href:!0});var n$=o(Ro);fv=a(n$,"~ConfigMixin"),n$.forEach(t),pv=a(ae," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ed=n(ae,"CODE",{});var o$=o(Ed);hv=a(o$,"__init__"),o$.forEach(t),mv=a(ae,`
function, such as `),yd=n(ae,"CODE",{});var i$=o(yd);gv=a(i$,"num_train_timesteps"),i$.forEach(t),_v=a(ae,". They can be accessed via "),wd=n(ae,"CODE",{});var a$=o(wd);vv=a(a$,"scheduler.config.num_train_timesteps"),a$.forEach(t),bv=a(ae,`.
`),Qo=n(ae,"A",{href:!0});var d$=o(Qo);Sv=a(d$,"~ConfigMixin"),d$.forEach(t),Dv=a(ae," also provides general loading and saving functionality via the "),jo=n(ae,"A",{href:!0});var l$=o(jo);xv=a(l$,"save_config()"),l$.forEach(t),$v=a(ae,` and
`),Wo=n(ae,"A",{href:!0});var c$=o(Wo);Ev=a(c$,"from_config()"),c$.forEach(t),yv=a(ae," functions."),ae.forEach(t),wv=c(H),Uo=n(H,"P",{});var iS=o(Uo);Mv=a(iS,"For more details, see the original paper: "),zs=n(iS,"A",{href:!0,rel:!0});var u$=o(zs);Pv=a(u$,"https://arxiv.org/abs/2202.09778"),u$.forEach(t),iS.forEach(t),Tv=c(H),Zt=n(H,"DIV",{class:!0});var tf=o(Zt);h(Ys.$$.fragment,tf),Cv=c(tf),Md=n(tf,"P",{});var f$=o(Md);kv=a(f$,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),f$.forEach(t),tf.forEach(t),Ov=c(H),er=n(H,"DIV",{class:!0});var rf=o(er);h(Js.$$.fragment,rf),Av=c(rf),Pd=n(rf,"P",{});var p$=o(Pd);Vv=a(p$,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),p$.forEach(t),rf.forEach(t),Fv=c(H),Ae=n(H,"DIV",{class:!0});var Vi=o(Ae);h(Xs.$$.fragment,Vi),Iv=c(Vi),Td=n(Vi,"P",{});var h$=o(Td);Nv=a(h$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),h$.forEach(t),Lv=c(Vi),Pe=n(Vi,"P",{});var Fr=o(Pe);qv=a(Fr,"This function calls "),Cd=n(Fr,"CODE",{});var m$=o(Cd);Kv=a(m$,"step_prk()"),m$.forEach(t),Rv=a(Fr," or "),kd=n(Fr,"CODE",{});var g$=o(kd);Qv=a(g$,"step_plms()"),g$.forEach(t),jv=a(Fr," depending on the internal variable "),Od=n(Fr,"CODE",{});var _$=o(Od);Wv=a(_$,"counter"),_$.forEach(t),Uv=a(Fr,"."),Fr.forEach(t),Vi.forEach(t),Bv=c(H),tr=n(H,"DIV",{class:!0});var sf=o(tr);h(Zs.$$.fragment,sf),Hv=c(sf),Ad=n(sf,"P",{});var v$=o(Ad);Gv=a(v$,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),v$.forEach(t),sf.forEach(t),zv=c(H),rr=n(H,"DIV",{class:!0});var nf=o(rr);h(en.$$.fragment,nf),Yv=c(nf),Vd=n(nf,"P",{});var b$=o(Vd);Jv=a(b$,`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),b$.forEach(t),nf.forEach(t),H.forEach(t),Lc=c(r),tt=n(r,"H4",{class:!0});var of=o(tt);sr=n(of,"A",{id:!0,class:!0,href:!0});var S$=o(sr);Fd=n(S$,"SPAN",{});var D$=o(Fd);h(tn.$$.fragment,D$),D$.forEach(t),S$.forEach(t),Xv=c(of),Id=n(of,"SPAN",{});var x$=o(Id);Zv=a(x$,"variance exploding stochastic differential equation (VE-SDE) scheduler"),x$.forEach(t),of.forEach(t),qc=c(r),nr=n(r,"P",{});var af=o(nr);eb=a(af,"Original paper can be found "),rn=n(af,"A",{href:!0,rel:!0});var $$=o(rn);tb=a($$,"here"),$$.forEach(t),rb=a(af,"."),af.forEach(t),Kc=c(r),E=n(r,"DIV",{class:!0});var G=o(E);h(sn.$$.fragment,G),sb=c(G),Nd=n(G,"P",{});var E$=o(Nd);nb=a(E$,"The variance exploding stochastic differential equation (SDE) scheduler."),E$.forEach(t),ob=c(G),Bo=n(G,"P",{});var aS=o(Bo);ib=a(aS,"For more information, see the original paper: "),nn=n(aS,"A",{href:!0,rel:!0});var y$=o(nn);ab=a(y$,"https://arxiv.org/abs/2011.13456"),y$.forEach(t),aS.forEach(t),db=c(G),q=n(G,"P",{});var de=o(q);Ho=n(de,"A",{href:!0});var w$=o(Ho);lb=a(w$,"~ConfigMixin"),w$.forEach(t),cb=a(de," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ld=n(de,"CODE",{});var M$=o(Ld);ub=a(M$,"__init__"),M$.forEach(t),fb=a(de,`
function, such as `),qd=n(de,"CODE",{});var P$=o(qd);pb=a(P$,"num_train_timesteps"),P$.forEach(t),hb=a(de,". They can be accessed via "),Kd=n(de,"CODE",{});var T$=o(Kd);mb=a(T$,"scheduler.config.num_train_timesteps"),T$.forEach(t),gb=a(de,`.
`),Go=n(de,"A",{href:!0});var C$=o(Go);_b=a(C$,"~ConfigMixin"),C$.forEach(t),vb=a(de," also provides general loading and saving functionality via the "),zo=n(de,"A",{href:!0});var k$=o(zo);bb=a(k$,"save_config()"),k$.forEach(t),Sb=a(de,` and
`),Yo=n(de,"A",{href:!0});var O$=o(Yo);Db=a(O$,"from_config()"),O$.forEach(t),xb=a(de," functions."),de.forEach(t),$b=c(G),or=n(G,"DIV",{class:!0});var df=o(or);h(on.$$.fragment,df),Eb=c(df),Rd=n(df,"P",{});var A$=o(Rd);yb=a(A$,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),A$.forEach(t),df.forEach(t),wb=c(G),Ve=n(G,"DIV",{class:!0});var Fi=o(Ve);h(an.$$.fragment,Fi),Mb=c(Fi),Qd=n(Fi,"P",{});var V$=o(Qd);Pb=a(V$,"Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),V$.forEach(t),Tb=c(Fi),rt=n(Fi,"P",{});var Ii=o(rt);Cb=a(Ii,"The sigmas control the weight of the "),jd=n(Ii,"CODE",{});var F$=o(jd);kb=a(F$,"drift"),F$.forEach(t),Ob=a(Ii," and "),Wd=n(Ii,"CODE",{});var I$=o(Wd);Ab=a(I$,"diffusion"),I$.forEach(t),Vb=a(Ii," components of sample update."),Ii.forEach(t),Fi.forEach(t),Fb=c(G),ir=n(G,"DIV",{class:!0});var lf=o(ir);h(dn.$$.fragment,lf),Ib=c(lf),Ud=n(lf,"P",{});var N$=o(Ud);Nb=a(N$,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),N$.forEach(t),lf.forEach(t),Lb=c(G),ar=n(G,"DIV",{class:!0});var cf=o(ar);h(ln.$$.fragment,cf),qb=c(cf),Bd=n(cf,"P",{});var L$=o(Bd);Kb=a(L$,`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),L$.forEach(t),cf.forEach(t),Rb=c(G),dr=n(G,"DIV",{class:!0});var uf=o(dr);h(cn.$$.fragment,uf),Qb=c(uf),Hd=n(uf,"P",{});var q$=o(Hd);jb=a(q$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),q$.forEach(t),uf.forEach(t),G.forEach(t),Rc=c(r),st=n(r,"H4",{class:!0});var ff=o(st);lr=n(ff,"A",{id:!0,class:!0,href:!0});var K$=o(lr);Gd=n(K$,"SPAN",{});var R$=o(Gd);h(un.$$.fragment,R$),R$.forEach(t),K$.forEach(t),Wb=c(ff),zd=n(ff,"SPAN",{});var Q$=o(zd);Ub=a(Q$,"improved pseudo numerical methods for diffusion models (iPNDM)"),Q$.forEach(t),ff.forEach(t),Qc=c(r),cr=n(r,"P",{});var pf=o(cr);Bb=a(pf,"Original implementation can be found "),fn=n(pf,"A",{href:!0,rel:!0});var j$=o(fn);Hb=a(j$,"here"),j$.forEach(t),Gb=a(pf,"."),pf.forEach(t),jc=c(r),k=n(r,"DIV",{class:!0});var be=o(k);h(pn.$$.fragment,be),zb=c(be),Jo=n(be,"P",{});var dS=o(Jo);Yb=a(dS,`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),hn=n(dS,"A",{href:!0,rel:!0});var W$=o(hn);Jb=a(W$,"library"),W$.forEach(t),dS.forEach(t),Xb=c(be),K=n(be,"P",{});var le=o(K);Xo=n(le,"A",{href:!0});var U$=o(Xo);Zb=a(U$,"~ConfigMixin"),U$.forEach(t),e1=a(le," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Yd=n(le,"CODE",{});var B$=o(Yd);t1=a(B$,"__init__"),B$.forEach(t),r1=a(le,`
function, such as `),Jd=n(le,"CODE",{});var H$=o(Jd);s1=a(H$,"num_train_timesteps"),H$.forEach(t),n1=a(le,". They can be accessed via "),Xd=n(le,"CODE",{});var G$=o(Xd);o1=a(G$,"scheduler.config.num_train_timesteps"),G$.forEach(t),i1=a(le,`.
`),Zo=n(le,"A",{href:!0});var z$=o(Zo);a1=a(z$,"~ConfigMixin"),z$.forEach(t),d1=a(le," also provides general loading and saving functionality via the "),ei=n(le,"A",{href:!0});var Y$=o(ei);l1=a(Y$,"save_config()"),Y$.forEach(t),c1=a(le,` and
`),ti=n(le,"A",{href:!0});var J$=o(ti);u1=a(J$,"from_config()"),J$.forEach(t),f1=a(le," functions."),le.forEach(t),p1=c(be),ri=n(be,"P",{});var lS=o(ri);h1=a(lS,"For more details, see the original paper: "),mn=n(lS,"A",{href:!0,rel:!0});var X$=o(mn);m1=a(X$,"https://arxiv.org/abs/2202.09778"),X$.forEach(t),lS.forEach(t),g1=c(be),ur=n(be,"DIV",{class:!0});var hf=o(ur);h(gn.$$.fragment,hf),_1=c(hf),Zd=n(hf,"P",{});var Z$=o(Zd);v1=a(Z$,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Z$.forEach(t),hf.forEach(t),b1=c(be),fr=n(be,"DIV",{class:!0});var mf=o(fr);h(_n.$$.fragment,mf),S1=c(mf),el=n(mf,"P",{});var eE=o(el);D1=a(eE,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),eE.forEach(t),mf.forEach(t),x1=c(be),pr=n(be,"DIV",{class:!0});var gf=o(pr);h(vn.$$.fragment,gf),$1=c(gf),tl=n(gf,"P",{});var tE=o(tl);E1=a(tE,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),tE.forEach(t),gf.forEach(t),be.forEach(t),Wc=c(r),nt=n(r,"H4",{class:!0});var _f=o(nt);hr=n(_f,"A",{id:!0,class:!0,href:!0});var rE=o(hr);rl=n(rE,"SPAN",{});var sE=o(rl);h(bn.$$.fragment,sE),sE.forEach(t),rE.forEach(t),y1=c(_f),sl=n(_f,"SPAN",{});var nE=o(sl);w1=a(nE,"variance preserving stochastic differential equation (VP-SDE) scheduler"),nE.forEach(t),_f.forEach(t),Uc=c(r),mr=n(r,"P",{});var vf=o(mr);M1=a(vf,"Original paper can be found "),Sn=n(vf,"A",{href:!0,rel:!0});var oE=o(Sn);P1=a(oE,"here"),oE.forEach(t),T1=a(vf,"."),vf.forEach(t),Bc=c(r),h(gr.$$.fragment,r),Hc=c(r),te=n(r,"DIV",{class:!0});var Ne=o(te);h(Dn.$$.fragment,Ne),C1=c(Ne),nl=n(Ne,"P",{});var iE=o(nl);k1=a(iE,"The variance preserving stochastic differential equation (SDE) scheduler."),iE.forEach(t),O1=c(Ne),R=n(Ne,"P",{});var ce=o(R);si=n(ce,"A",{href:!0});var aE=o(si);A1=a(aE,"~ConfigMixin"),aE.forEach(t),V1=a(ce," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ol=n(ce,"CODE",{});var dE=o(ol);F1=a(dE,"__init__"),dE.forEach(t),I1=a(ce,`
function, such as `),il=n(ce,"CODE",{});var lE=o(il);N1=a(lE,"num_train_timesteps"),lE.forEach(t),L1=a(ce,". They can be accessed via "),al=n(ce,"CODE",{});var cE=o(al);q1=a(cE,"scheduler.config.num_train_timesteps"),cE.forEach(t),K1=a(ce,`.
`),ni=n(ce,"A",{href:!0});var uE=o(ni);R1=a(uE,"~ConfigMixin"),uE.forEach(t),Q1=a(ce," also provides general loading and saving functionality via the "),oi=n(ce,"A",{href:!0});var fE=o(oi);j1=a(fE,"save_config()"),fE.forEach(t),W1=a(ce,` and
`),ii=n(ce,"A",{href:!0});var pE=o(ii);U1=a(pE,"from_config()"),pE.forEach(t),B1=a(ce," functions."),ce.forEach(t),H1=c(Ne),ai=n(Ne,"P",{});var cS=o(ai);G1=a(cS,"For more information, see the original paper: "),xn=n(cS,"A",{href:!0,rel:!0});var hE=o(xn);z1=a(hE,"https://arxiv.org/abs/2011.13456"),hE.forEach(t),cS.forEach(t),Y1=c(Ne),dl=n(Ne,"P",{});var mE=o(dl);J1=a(mE,"UNDER CONSTRUCTION"),mE.forEach(t),Ne.forEach(t),Gc=c(r),ot=n(r,"H4",{class:!0});var bf=o(ot);_r=n(bf,"A",{id:!0,class:!0,href:!0});var gE=o(_r);ll=n(gE,"SPAN",{});var _E=o(ll);h($n.$$.fragment,_E),_E.forEach(t),gE.forEach(t),X1=c(bf),cl=n(bf,"SPAN",{});var vE=o(cl);Z1=a(vE,"Euler scheduler"),vE.forEach(t),bf.forEach(t),zc=c(r),Fe=n(r,"P",{});var Ni=o(Fe);e0=a(Ni,"Euler scheduler (Algorithm 2) from the paper "),En=n(Ni,"A",{href:!0,rel:!0});var bE=o(En);t0=a(bE,"Elucidating the Design Space of Diffusion-Based Generative Models"),bE.forEach(t),r0=a(Ni," by Karras et al. (2022). Based on the original "),yn=n(Ni,"A",{href:!0,rel:!0});var SE=o(yn);s0=a(SE,"k-diffusion"),SE.forEach(t),n0=a(Ni,` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Ni.forEach(t),Yc=c(r),Y=n(r,"DIV",{class:!0});var Ee=o(Y);h(wn.$$.fragment,Ee),o0=c(Ee),vr=n(Ee,"P",{});var ec=o(vr);i0=a(ec,"Euler scheduler (Algorithm 2) from Karras et al. (2022) "),Mn=n(ec,"A",{href:!0,rel:!0});var DE=o(Mn);a0=a(DE,"https://arxiv.org/abs/2206.00364"),DE.forEach(t),d0=a(ec,`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),Pn=n(ec,"A",{href:!0,rel:!0});var xE=o(Pn);l0=a(xE,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),xE.forEach(t),ec.forEach(t),c0=c(Ee),Q=n(Ee,"P",{});var ue=o(Q);di=n(ue,"A",{href:!0});var $E=o(di);u0=a($E,"~ConfigMixin"),$E.forEach(t),f0=a(ue," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ul=n(ue,"CODE",{});var EE=o(ul);p0=a(EE,"__init__"),EE.forEach(t),h0=a(ue,`
function, such as `),fl=n(ue,"CODE",{});var yE=o(fl);m0=a(yE,"num_train_timesteps"),yE.forEach(t),g0=a(ue,". They can be accessed via "),pl=n(ue,"CODE",{});var wE=o(pl);_0=a(wE,"scheduler.config.num_train_timesteps"),wE.forEach(t),v0=a(ue,`.
`),li=n(ue,"A",{href:!0});var ME=o(li);b0=a(ME,"~ConfigMixin"),ME.forEach(t),S0=a(ue," also provides general loading and saving functionality via the "),ci=n(ue,"A",{href:!0});var PE=o(ci);D0=a(PE,"save_config()"),PE.forEach(t),x0=a(ue,` and
`),ui=n(ue,"A",{href:!0});var TE=o(ui);$0=a(TE,"from_config()"),TE.forEach(t),E0=a(ue," functions."),ue.forEach(t),y0=c(Ee),br=n(Ee,"DIV",{class:!0});var Sf=o(br);h(Tn.$$.fragment,Sf),w0=c(Sf),Cn=n(Sf,"P",{});var Df=o(Cn);M0=a(Df,"Scales the denoising model input by "),hl=n(Df,"CODE",{});var CE=o(hl);P0=a(CE,"(sigma**2 + 1) ** 0.5"),CE.forEach(t),T0=a(Df," to match the Euler algorithm."),Df.forEach(t),Sf.forEach(t),C0=c(Ee),Sr=n(Ee,"DIV",{class:!0});var xf=o(Sr);h(kn.$$.fragment,xf),k0=c(xf),ml=n(xf,"P",{});var kE=o(ml);O0=a(kE,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),kE.forEach(t),xf.forEach(t),A0=c(Ee),Dr=n(Ee,"DIV",{class:!0});var $f=o(Dr);h(On.$$.fragment,$f),V0=c($f),gl=n($f,"P",{});var OE=o(gl);F0=a(OE,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),OE.forEach(t),$f.forEach(t),Ee.forEach(t),Jc=c(r),it=n(r,"H4",{class:!0});var Ef=o(it);xr=n(Ef,"A",{id:!0,class:!0,href:!0});var AE=o(xr);_l=n(AE,"SPAN",{});var VE=o(_l);h(An.$$.fragment,VE),VE.forEach(t),AE.forEach(t),I0=c(Ef),vl=n(Ef,"SPAN",{});var FE=o(vl);N0=a(FE,"Euler Ancestral scheduler"),FE.forEach(t),Ef.forEach(t),Xc=c(r),fi=n(r,"P",{});var IE=o(fi);L0=a(IE,`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),IE.forEach(t),Zc=c(r),J=n(r,"DIV",{class:!0});var ye=o(J);h(Vn.$$.fragment,ye),q0=c(ye),pi=n(ye,"P",{});var uS=o(pi);K0=a(uS,`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Fn=n(uS,"A",{href:!0,rel:!0});var NE=o(Fn);R0=a(NE,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),NE.forEach(t),uS.forEach(t),Q0=c(ye),j=n(ye,"P",{});var fe=o(j);hi=n(fe,"A",{href:!0});var LE=o(hi);j0=a(LE,"~ConfigMixin"),LE.forEach(t),W0=a(fe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),bl=n(fe,"CODE",{});var qE=o(bl);U0=a(qE,"__init__"),qE.forEach(t),B0=a(fe,`
function, such as `),Sl=n(fe,"CODE",{});var KE=o(Sl);H0=a(KE,"num_train_timesteps"),KE.forEach(t),G0=a(fe,". They can be accessed via "),Dl=n(fe,"CODE",{});var RE=o(Dl);z0=a(RE,"scheduler.config.num_train_timesteps"),RE.forEach(t),Y0=a(fe,`.
`),mi=n(fe,"A",{href:!0});var QE=o(mi);J0=a(QE,"~ConfigMixin"),QE.forEach(t),X0=a(fe," also provides general loading and saving functionality via the "),gi=n(fe,"A",{href:!0});var jE=o(gi);Z0=a(jE,"save_config()"),jE.forEach(t),e2=a(fe,` and
`),_i=n(fe,"A",{href:!0});var WE=o(_i);t2=a(WE,"from_config()"),WE.forEach(t),r2=a(fe," functions."),fe.forEach(t),s2=c(ye),$r=n(ye,"DIV",{class:!0});var yf=o($r);h(In.$$.fragment,yf),n2=c(yf),Nn=n(yf,"P",{});var wf=o(Nn);o2=a(wf,"Scales the denoising model input by "),xl=n(wf,"CODE",{});var UE=o(xl);i2=a(UE,"(sigma**2 + 1) ** 0.5"),UE.forEach(t),a2=a(wf," to match the Euler algorithm."),wf.forEach(t),yf.forEach(t),d2=c(ye),Er=n(ye,"DIV",{class:!0});var Mf=o(Er);h(Ln.$$.fragment,Mf),l2=c(Mf),$l=n(Mf,"P",{});var BE=o($l);c2=a(BE,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),BE.forEach(t),Mf.forEach(t),u2=c(ye),yr=n(ye,"DIV",{class:!0});var Pf=o(yr);h(qn.$$.fragment,Pf),f2=c(Pf),El=n(Pf,"P",{});var HE=o(El);p2=a(HE,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),HE.forEach(t),Pf.forEach(t),ye.forEach(t),eu=c(r),at=n(r,"H4",{class:!0});var Tf=o(at);wr=n(Tf,"A",{id:!0,class:!0,href:!0});var GE=o(wr);yl=n(GE,"SPAN",{});var zE=o(yl);h(Kn.$$.fragment,zE),zE.forEach(t),GE.forEach(t),h2=c(Tf),wl=n(Tf,"SPAN",{});var YE=o(wl);m2=a(YE,"VQDiffusionScheduler"),YE.forEach(t),Tf.forEach(t),tu=c(r),Rn=n(r,"P",{});var fS=o(Rn);g2=a(fS,"Original paper can be found "),Qn=n(fS,"A",{href:!0,rel:!0});var JE=o(Qn);_2=a(JE,"here"),JE.forEach(t),fS.forEach(t),ru=c(r),y=n(r,"DIV",{class:!0});var z=o(y);h(jn.$$.fragment,z),v2=c(z),Ml=n(z,"P",{});var XE=o(Ml);b2=a(XE,"The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image."),XE.forEach(t),S2=c(z),Pl=n(z,"P",{});var ZE=o(Pl);D2=a(ZE,`The VQ-diffusion scheduler converts the transformer\u2019s output into a sample for the unnoised image at the previous
diffusion timestep.`),ZE.forEach(t),x2=c(z),W=n(z,"P",{});var pe=o(W);vi=n(pe,"A",{href:!0});var e5=o(vi);$2=a(e5,"~ConfigMixin"),e5.forEach(t),E2=a(pe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Tl=n(pe,"CODE",{});var t5=o(Tl);y2=a(t5,"__init__"),t5.forEach(t),w2=a(pe,`
function, such as `),Cl=n(pe,"CODE",{});var r5=o(Cl);M2=a(r5,"num_train_timesteps"),r5.forEach(t),P2=a(pe,". They can be accessed via "),kl=n(pe,"CODE",{});var s5=o(kl);T2=a(s5,"scheduler.config.num_train_timesteps"),s5.forEach(t),C2=a(pe,`.
`),bi=n(pe,"A",{href:!0});var n5=o(bi);k2=a(n5,"~ConfigMixin"),n5.forEach(t),O2=a(pe," also provides general loading and saving functionality via the "),Si=n(pe,"A",{href:!0});var o5=o(Si);A2=a(o5,"save_config()"),o5.forEach(t),V2=a(pe,` and
`),Di=n(pe,"A",{href:!0});var i5=o(Di);F2=a(i5,"from_config()"),i5.forEach(t),I2=a(pe," functions."),pe.forEach(t),N2=c(z),xi=n(z,"P",{});var pS=o(xi);L2=a(pS,"For more details, see the original paper: "),Wn=n(pS,"A",{href:!0,rel:!0});var a5=o(Wn);q2=a(a5,"https://arxiv.org/abs/2111.14822"),a5.forEach(t),pS.forEach(t),K2=c(z),Ie=n(z,"DIV",{class:!0});var Li=o(Ie);h(Un.$$.fragment,Li),R2=c(Li),Bn=n(Li,"P",{});var Cf=o(Bn);Q2=a(Cf,`Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each
latent pixel in `),Ol=n(Cf,"CODE",{});var d5=o(Ol);j2=a(d5,"x_t"),d5.forEach(t),W2=a(Cf,"."),Cf.forEach(t),U2=c(Li),Al=n(Li,"P",{});var l5=o(Al);B2=a(l5,`See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix
is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.`),l5.forEach(t),Li.forEach(t),H2=c(z),U=n(z,"DIV",{class:!0});var Se=o(U);h(Hn.$$.fragment,Se),G2=c(Se),Gn=n(Se,"P",{});var kf=o(Gn);z2=a(kf,"Calculates the log probabilities for the predicted classes of the image at timestep "),Vl=n(kf,"CODE",{});var c5=o(Vl);Y2=a(c5,"t-1"),c5.forEach(t),J2=a(kf,". I.e. Equation (11)."),kf.forEach(t),X2=c(Se),Fl=n(Se,"P",{});var u5=o(Fl);Z2=a(u5,`Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only
forward probabilities.`),u5.forEach(t),e4=c(Se),Il=n(Se,"P",{});var f5=o(Il);t4=a(f5,"Equation (11) stated in terms of forward probabilities via Equation (5):"),f5.forEach(t),r4=c(Se),Nl=n(Se,"P",{});var p5=o(Nl);s4=a(p5,"Where:"),p5.forEach(t),n4=c(Se),Ll=n(Se,"UL",{});var h5=o(Ll);zn=n(h5,"LI",{});var Of=o(zn);o4=a(Of,"the sum is over x"),ql=n(Of,"EM",{});var m5=o(ql);i4=a(m5,"0 = {C_0 \u2026 C"),m5.forEach(t),a4=a(Of,"{k-1}} (classes for x_0)"),Of.forEach(t),h5.forEach(t),d4=c(Se),dt=n(Se,"P",{});var qi=o(dt);l4=a(qi,"p(x"),Kl=n(qi,"EM",{});var g5=o(Kl);c4=a(g5,"{t-1} | x_t) = sum( q(x_t | x"),g5.forEach(t),u4=a(qi,"{t-1}) "),Rl=n(qi,"EM",{});var _5=o(Rl);f4=a(_5,"q(x_{t-1} | x_0)"),_5.forEach(t),p4=a(qi," p(x_0) / q(x_t | x_0) )"),qi.forEach(t),Se.forEach(t),h4=c(z),Mr=n(z,"DIV",{class:!0});var Af=o(Mr);h(Yn.$$.fragment,Af),m4=c(Af),Ql=n(Af,"P",{});var v5=o(Ql);g4=a(v5,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),v5.forEach(t),Af.forEach(t),_4=c(z),Pr=n(z,"DIV",{class:!0});var Vf=o(Pr);h(Jn.$$.fragment,Vf),v4=c(Vf),Xn=n(Vf,"P",{});var Ff=o(Xn);b4=a(Ff,`Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the
docstring for `),jl=n(Ff,"CODE",{});var b5=o(jl);S4=a(b5,"self.q_posterior"),b5.forEach(t),D4=a(Ff," for more in depth docs on how Equation (11) is computed."),Ff.forEach(t),Vf.forEach(t),z.forEach(t),su=c(r),lt=n(r,"H4",{class:!0});var If=o(lt);Tr=n(If,"A",{id:!0,class:!0,href:!0});var S5=o(Tr);Wl=n(S5,"SPAN",{});var D5=o(Wl);h(Zn.$$.fragment,D5),D5.forEach(t),S5.forEach(t),x4=c(If),Ul=n(If,"SPAN",{});var x5=o(Ul);$4=a(x5,"RePaint scheduler"),x5.forEach(t),If.forEach(t),nu=c(r),Te=n(r,"P",{});var ao=o(Te);E4=a(ao,`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),$i=n(ao,"A",{href:!0});var $5=o($i);y4=a($5,"RePaintPipeline"),$5.forEach(t),w4=a(ao,`.
Based on the paper `),eo=n(ao,"A",{href:!0,rel:!0});var E5=o(eo);M4=a(E5,"RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),E5.forEach(t),P4=a(ao,`
and the original implementation by Andreas Lugmayr et al.: `),to=n(ao,"A",{href:!0,rel:!0});var y5=o(to);T4=a(y5,"https://github.com/andreas128/RePaint"),y5.forEach(t),ao.forEach(t),ou=c(r),X=n(r,"DIV",{class:!0});var we=o(X);h(ro.$$.fragment,we),C4=c(we),Bl=n(we,"P",{});var w5=o(Bl);k4=a(w5,"RePaint is a schedule for DDPM inpainting inside a given mask."),w5.forEach(t),O4=c(we),B=n(we,"P",{});var he=o(B);Ei=n(he,"A",{href:!0});var M5=o(Ei);A4=a(M5,"~ConfigMixin"),M5.forEach(t),V4=a(he," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Hl=n(he,"CODE",{});var P5=o(Hl);F4=a(P5,"__init__"),P5.forEach(t),I4=a(he,`
function, such as `),Gl=n(he,"CODE",{});var T5=o(Gl);N4=a(T5,"num_train_timesteps"),T5.forEach(t),L4=a(he,". They can be accessed via "),zl=n(he,"CODE",{});var C5=o(zl);q4=a(C5,"scheduler.config.num_train_timesteps"),C5.forEach(t),K4=a(he,`.
`),yi=n(he,"A",{href:!0});var k5=o(yi);R4=a(k5,"~ConfigMixin"),k5.forEach(t),Q4=a(he," also provides general loading and saving functionality via the "),wi=n(he,"A",{href:!0});var O5=o(wi);j4=a(O5,"save_config()"),O5.forEach(t),W4=a(he,` and
`),Mi=n(he,"A",{href:!0});var A5=o(Mi);U4=a(A5,"from_config()"),A5.forEach(t),B4=a(he," functions."),he.forEach(t),H4=c(we),Pi=n(we,"P",{});var hS=o(Pi);G4=a(hS,"For more details, see the original paper: "),so=n(hS,"A",{href:!0,rel:!0});var V5=o(so);z4=a(V5,"https://arxiv.org/pdf/2201.09865.pdf"),V5.forEach(t),hS.forEach(t),Y4=c(we),Cr=n(we,"DIV",{class:!0});var Nf=o(Cr);h(no.$$.fragment,Nf),J4=c(Nf),Yl=n(Nf,"P",{});var F5=o(Yl);X4=a(F5,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),F5.forEach(t),Nf.forEach(t),Z4=c(we),kr=n(we,"DIV",{class:!0});var Lf=o(kr);h(oo.$$.fragment,Lf),eS=c(Lf),Jl=n(Lf,"P",{});var I5=o(Jl);tS=a(I5,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),I5.forEach(t),Lf.forEach(t),we.forEach(t),this.h()},h(){d(Z,"name","hf:doc:metadata"),d(Z,"content",JSON.stringify(U5)),d(De,"id","schedulers"),d(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(De,"href","#schedulers"),d(ee,"class","relative group"),d(ut,"id","what-is-a-scheduler"),d(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ut,"href","#what-is-a-scheduler"),d(Le,"class","relative group"),d(ht,"id","discrete-versus-continuous-schedulers"),d(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ht,"href","#discrete-versus-continuous-schedulers"),d(Ke,"class","relative group"),d(uo,"href","/docs/diffusers/main/en/api/schedulers#diffusers.DDPMScheduler"),d(fo,"href","/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler"),d(po,"href","/docs/diffusers/main/en/api/schedulers#diffusers.ScoreSdeVeScheduler"),d(mt,"id","designing-reusable-schedulers"),d(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(mt,"href","#designing-reusable-schedulers"),d(Re,"class","relative group"),d(_t,"id","api"),d(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_t,"href","#api"),d(Qe,"class","relative group"),d(go,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(bt,"id","diffusers.SchedulerMixin"),d(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(bt,"href","#diffusers.SchedulerMixin"),d(je,"class","relative group"),d(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(St,"id","diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(St,"href","#diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(Ue,"class","relative group"),d(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Dt,"id","implemented-schedulers"),d(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Dt,"href","#implemented-schedulers"),d(He,"class","relative group"),d(xt,"id","diffusers.DDIMScheduler"),d(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xt,"href","#diffusers.DDIMScheduler"),d(Ge,"class","relative group"),d(vo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(bo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(So,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Do,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Jr,"href","https://arxiv.org/abs/2010.02502"),d(Jr,"rel","nofollow"),d($t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(wt,"id","diffusers.DDPMScheduler"),d(wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wt,"href","#diffusers.DDPMScheduler"),d(ze,"class","relative group"),d(rs,"href","https://arxiv.org/abs/2010.02502"),d(rs,"rel","nofollow"),d($o,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Eo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(yo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(wo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(ns,"href","https://arxiv.org/abs/2006.11239"),d(ns,"rel","nofollow"),d(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(kt,"id","diffusers.DPMSolverMultistepScheduler"),d(kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(kt,"href","#diffusers.DPMSolverMultistepScheduler"),d(Ye,"class","relative group"),d(ls,"href","https://arxiv.org/abs/2206.00927"),d(ls,"rel","nofollow"),d(cs,"href","https://arxiv.org/abs/2211.01095"),d(cs,"rel","nofollow"),d(us,"href","https://github.com/LuChengTHU/dpm-solver"),d(us,"rel","nofollow"),d(ps,"href","https://arxiv.org/abs/2206.00927"),d(ps,"rel","nofollow"),d(hs,"href","https://arxiv.org/abs/2211.01095"),d(hs,"rel","nofollow"),d(ms,"href","https://arxiv.org/abs/2205.11487"),d(ms,"rel","nofollow"),d(Po,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(To,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Co,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ko,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(bs,"href","https://arxiv.org/abs/2206.00927"),d(bs,"rel","nofollow"),d(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(It,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Lt,"id","diffusers.KarrasVeScheduler"),d(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Lt,"href","#diffusers.KarrasVeScheduler"),d(Xe,"class","relative group"),d(ws,"href","https://arxiv.org/abs/2006.11239"),d(ws,"rel","nofollow"),d(Ps,"href","https://arxiv.org/abs/2206.00364"),d(Ps,"rel","nofollow"),d(Ts,"href","https://arxiv.org/abs/2011.13456"),d(Ts,"rel","nofollow"),d(Oo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Ao,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Vo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Fo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(ks,"href","https://arxiv.org/abs/2206.00364"),d(ks,"rel","nofollow"),d(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ut,"id","diffusers.LMSDiscreteScheduler"),d(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ut,"href","#diffusers.LMSDiscreteScheduler"),d(Ze,"class","relative group"),d(Ls,"href","https://arxiv.org/abs/2206.00364"),d(Ls,"rel","nofollow"),d(Ks,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(Ks,"rel","nofollow"),d(No,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Lo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(qo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Ko,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Jt,"id","diffusers.PNDMScheduler"),d(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Jt,"href","#diffusers.PNDMScheduler"),d(et,"class","relative group"),d(Hs,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(Hs,"rel","nofollow"),d(Ro,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Qo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(jo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Wo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(zs,"href","https://arxiv.org/abs/2202.09778"),d(zs,"rel","nofollow"),d(Zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(er,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(sr,"id","diffusers.ScoreSdeVeScheduler"),d(sr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(sr,"href","#diffusers.ScoreSdeVeScheduler"),d(tt,"class","relative group"),d(rn,"href","https://arxiv.org/abs/2011.13456"),d(rn,"rel","nofollow"),d(nn,"href","https://arxiv.org/abs/2011.13456"),d(nn,"rel","nofollow"),d(Ho,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Go,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(zo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Yo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(or,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ir,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ar,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(lr,"id","diffusers.IPNDMScheduler"),d(lr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lr,"href","#diffusers.IPNDMScheduler"),d(st,"class","relative group"),d(fn,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(fn,"rel","nofollow"),d(hn,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(hn,"rel","nofollow"),d(Xo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Zo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ei,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ti,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(mn,"href","https://arxiv.org/abs/2202.09778"),d(mn,"rel","nofollow"),d(ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(hr,"id","diffusers.schedulers.ScoreSdeVpScheduler"),d(hr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(hr,"href","#diffusers.schedulers.ScoreSdeVpScheduler"),d(nt,"class","relative group"),d(Sn,"href","https://arxiv.org/abs/2011.13456"),d(Sn,"rel","nofollow"),d(si,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ni,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(oi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ii,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(xn,"href","https://arxiv.org/abs/2011.13456"),d(xn,"rel","nofollow"),d(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_r,"id","diffusers.EulerDiscreteScheduler"),d(_r,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_r,"href","#diffusers.EulerDiscreteScheduler"),d(ot,"class","relative group"),d(En,"href","https://arxiv.org/abs/2206.00364"),d(En,"rel","nofollow"),d(yn,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(yn,"rel","nofollow"),d(Mn,"href","https://arxiv.org/abs/2206.00364"),d(Mn,"rel","nofollow"),d(Pn,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(Pn,"rel","nofollow"),d(di,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(li,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ci,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ui,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(br,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(xr,"id","diffusers.EulerAncestralDiscreteScheduler"),d(xr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xr,"href","#diffusers.EulerAncestralDiscreteScheduler"),d(it,"class","relative group"),d(Fn,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),d(Fn,"rel","nofollow"),d(hi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(mi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(gi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(_i,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d($r,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Er,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(yr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(wr,"id","diffusers.VQDiffusionScheduler"),d(wr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wr,"href","#diffusers.VQDiffusionScheduler"),d(at,"class","relative group"),d(Qn,"href","https://arxiv.org/abs/2111.14822"),d(Qn,"rel","nofollow"),d(vi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(bi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Si,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Di,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Wn,"href","https://arxiv.org/abs/2111.14822"),d(Wn,"rel","nofollow"),d(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Tr,"id","diffusers.RePaintScheduler"),d(Tr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Tr,"href","#diffusers.RePaintScheduler"),d(lt,"class","relative group"),d($i,"href","/docs/diffusers/main/en/api/pipelines/repaint#diffusers.RePaintPipeline"),d(eo,"href","https://arxiv.org/abs/2201.09865"),d(eo,"rel","nofollow"),d(to,"href","https://github.com/andreas128/RePaint"),d(to,"rel","nofollow"),d(Ei,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(yi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(wi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Mi,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(so,"href","https://arxiv.org/pdf/2201.09865.pdf"),d(so,"rel","nofollow"),d(Cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(kr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,u){e(document.head,Z),f(r,ct,u),f(r,ee,u),e(ee,De),e(De,Ki),m(Ir,Ki,null),e(ee,qf),e(ee,Ri),e(Ri,Kf),f(r,rc,u),f(r,lo,u),e(lo,Rf),f(r,sc,u),f(r,Le,u),e(Le,ut),e(ut,Qi),m(Nr,Qi,null),e(Le,Qf),e(Le,ji),e(ji,jf),f(r,nc,u),f(r,ft,u),e(ft,Wf),e(ft,Wi),e(Wi,Uf),e(ft,Bf),f(r,oc,u),f(r,pt,u),e(pt,co),e(co,Hf),e(co,Lr),e(Lr,Ui),e(Ui,Gf),e(Lr,zf),e(Lr,Bi),e(Bi,Yf),e(pt,Jf),e(pt,qe),e(qe,Xf),e(qe,Hi),e(Hi,Zf),e(qe,ep),e(qe,Gi),e(Gi,tp),e(qe,rp),f(r,ic,u),f(r,Ke,u),e(Ke,ht),e(ht,zi),m(qr,zi,null),e(Ke,sp),e(Ke,Yi),e(Yi,np),f(r,ac,u),f(r,O,u),e(O,op),e(O,Ji),e(Ji,ip),e(O,ap),e(O,uo),e(uo,dp),e(O,lp),e(O,fo),e(fo,cp),e(O,up),e(O,Xi),e(Xi,fp),e(O,pp),e(O,po),e(po,hp),e(O,mp),e(O,Zi),e(Zi,gp),e(O,_p),f(r,dc,u),f(r,Re,u),e(Re,mt),e(mt,ea),m(Kr,ea,null),e(Re,vp),e(Re,ta),e(ta,bp),f(r,lc,u),f(r,ho,u),e(ho,Sp),f(r,cc,u),f(r,gt,u),e(gt,ra),e(ra,Dp),e(gt,xp),e(gt,sa),e(sa,$p),f(r,uc,u),f(r,Qe,u),e(Qe,_t),e(_t,na),m(Rr,na,null),e(Qe,Ep),e(Qe,oa),e(oa,yp),f(r,fc,u),f(r,mo,u),e(mo,wp),f(r,pc,u),f(r,Ce,u),e(Ce,Qr),e(Qr,Mp),e(Qr,ia),e(ia,Pp),e(Qr,Tp),e(Ce,Cp),e(Ce,jr),e(jr,kp),e(jr,aa),e(aa,Op),e(jr,Ap),e(Ce,Vp),e(Ce,da),e(da,Fp),f(r,hc,u),f(r,vt,u),e(vt,Ip),e(vt,go),e(go,Np),e(vt,Lp),f(r,mc,u),f(r,je,u),e(je,bt),e(bt,la),m(Wr,la,null),e(je,qp),e(je,ca),e(ca,Kp),f(r,gc,u),f(r,We,u),m(Ur,We,null),e(We,Rp),e(We,ua),e(ua,Qp),f(r,_c,u),f(r,Ue,u),e(Ue,St),e(St,fa),m(Br,fa,null),e(Ue,jp),e(Ue,pa),e(pa,Wp),f(r,vc,u),f(r,Be,u),m(Hr,Be,null),e(Be,Up),e(Be,ha),e(ha,Bp),f(r,bc,u),f(r,He,u),e(He,Dt),e(Dt,ma),m(Gr,ma,null),e(He,Hp),e(He,ga),e(ga,Gp),f(r,Sc,u),f(r,Ge,u),e(Ge,xt),e(xt,_a),m(zr,_a,null),e(Ge,zp),e(Ge,va),e(va,Yp),f(r,Dc,u),f(r,_o,u),e(_o,Jp),f(r,xc,u),f(r,P,u),m(Yr,P,null),e(P,Xp),e(P,ba),e(ba,Zp),e(P,eh),e(P,A),e(A,vo),e(vo,th),e(A,rh),e(A,Sa),e(Sa,sh),e(A,nh),e(A,Da),e(Da,oh),e(A,ih),e(A,xa),e(xa,ah),e(A,dh),e(A,bo),e(bo,lh),e(A,ch),e(A,So),e(So,uh),e(A,fh),e(A,Do),e(Do,ph),e(A,hh),e(P,mh),e(P,xo),e(xo,gh),e(xo,Jr),e(Jr,_h),e(P,vh),e(P,$t),m(Xr,$t,null),e($t,bh),e($t,$a),e($a,Sh),e(P,Dh),e(P,Et),m(Zr,Et,null),e(Et,xh),e(Et,Ea),e(Ea,$h),e(P,Eh),e(P,yt),m(es,yt,null),e(yt,yh),e(yt,ya),e(ya,wh),f(r,$c,u),f(r,ze,u),e(ze,wt),e(wt,wa),m(ts,wa,null),e(ze,Mh),e(ze,Ma),e(Ma,Ph),f(r,Ec,u),f(r,Mt,u),e(Mt,Th),e(Mt,rs),e(rs,Ch),e(Mt,kh),f(r,yc,u),f(r,T,u),m(ss,T,null),e(T,Oh),e(T,Pa),e(Pa,Ah),e(T,Vh),e(T,V),e(V,$o),e($o,Fh),e(V,Ih),e(V,Ta),e(Ta,Nh),e(V,Lh),e(V,Ca),e(Ca,qh),e(V,Kh),e(V,ka),e(ka,Rh),e(V,Qh),e(V,Eo),e(Eo,jh),e(V,Wh),e(V,yo),e(yo,Uh),e(V,Bh),e(V,wo),e(wo,Hh),e(V,Gh),e(T,zh),e(T,Mo),e(Mo,Yh),e(Mo,ns),e(ns,Jh),e(T,Xh),e(T,Pt),m(os,Pt,null),e(Pt,Zh),e(Pt,Oa),e(Oa,em),e(T,tm),e(T,Tt),m(is,Tt,null),e(Tt,rm),e(Tt,Aa),e(Aa,sm),e(T,nm),e(T,Ct),m(as,Ct,null),e(Ct,om),e(Ct,Va),e(Va,im),f(r,wc,u),f(r,Ye,u),e(Ye,kt),e(kt,Fa),m(ds,Fa,null),e(Ye,am),e(Ye,Ia),e(Ia,dm),f(r,Mc,u),f(r,xe,u),e(xe,lm),e(xe,ls),e(ls,cm),e(xe,um),e(xe,cs),e(cs,fm),e(xe,pm),e(xe,us),e(us,hm),e(xe,mm),f(r,Pc,u),f(r,S,u),m(fs,S,null),e(S,gm),e(S,Na),e(Na,_m),e(S,vm),e(S,Ot),e(Ot,bm),e(Ot,ps),e(ps,Sm),e(Ot,Dm),e(Ot,hs),e(hs,xm),e(S,$m),e(S,Je),e(Je,Em),e(Je,La),e(La,ym),e(Je,wm),e(Je,qa),e(qa,Mm),e(Je,Pm),e(S,Tm),e(S,Me),e(Me,Cm),e(Me,ms),e(ms,km),e(Me,Om),e(Me,Ka),e(Ka,Am),e(Me,Vm),e(Me,Ra),e(Ra,Fm),e(Me,Im),e(S,Nm),e(S,F),e(F,Po),e(Po,Lm),e(F,qm),e(F,Qa),e(Qa,Km),e(F,Rm),e(F,ja),e(ja,Qm),e(F,jm),e(F,Wa),e(Wa,Wm),e(F,Um),e(F,To),e(To,Bm),e(F,Hm),e(F,Co),e(Co,Gm),e(F,zm),e(F,ko),e(ko,Ym),e(F,Jm),e(S,Xm),e(S,$e),m(gs,$e,null),e($e,Zm),e($e,Ua),e(Ua,eg),e($e,tg),e($e,Ba),e(Ba,rg),e($e,sg),e($e,Ha),e(Ha,ng),e(S,og),e(S,ke),m(_s,ke,null),e(ke,ig),e(ke,Ga),e(Ga,ag),e(ke,dg),e(ke,vs),e(vs,lg),e(vs,bs),e(bs,cg),e(vs,ug),e(S,fg),e(S,At),m(Ss,At,null),e(At,pg),e(At,za),e(za,hg),e(S,mg),e(S,Vt),m(Ds,Vt,null),e(Vt,gg),e(Vt,Ya),e(Ya,_g),e(S,vg),e(S,Ft),m(xs,Ft,null),e(Ft,bg),e(Ft,Ja),e(Ja,Sg),e(S,Dg),e(S,It),m($s,It,null),e(It,xg),e(It,Xa),e(Xa,$g),e(S,Eg),e(S,Nt),m(Es,Nt,null),e(Nt,yg),e(Nt,Za),e(Za,wg),f(r,Tc,u),f(r,Xe,u),e(Xe,Lt),e(Lt,ed),m(ys,ed,null),e(Xe,Mg),e(Xe,td),e(td,Pg),f(r,Cc,u),f(r,qt,u),e(qt,Tg),e(qt,ws),e(ws,Cg),e(qt,kg),f(r,kc,u),f(r,x,u),m(Ms,x,null),e(x,Og),e(x,rd),e(rd,Ag),e(x,Vg),e(x,Kt),e(Kt,Fg),e(Kt,Ps),e(Ps,Ig),e(Kt,Ng),e(Kt,Ts),e(Ts,Lg),e(x,qg),e(x,I),e(I,Oo),e(Oo,Kg),e(I,Rg),e(I,sd),e(sd,Qg),e(I,jg),e(I,nd),e(nd,Wg),e(I,Ug),e(I,od),e(od,Bg),e(I,Hg),e(I,Ao),e(Ao,Gg),e(I,zg),e(I,Vo),e(Vo,Yg),e(I,Jg),e(I,Fo),e(Fo,Xg),e(I,Zg),e(x,e_),e(x,Cs),e(Cs,t_),e(Cs,ks),e(ks,r_),e(Cs,s_),e(x,n_),e(x,Oe),m(Os,Oe,null),e(Oe,o_),e(Oe,id),e(id,i_),e(Oe,a_),e(Oe,ad),e(ad,d_),e(x,l_),e(x,Rt),m(As,Rt,null),e(Rt,c_),e(Rt,dd),e(dd,u_),e(x,f_),e(x,Qt),m(Vs,Qt,null),e(Qt,p_),e(Qt,ld),e(ld,h_),e(x,m_),e(x,jt),m(Fs,jt,null),e(jt,g_),e(jt,cd),e(cd,__),e(x,v_),e(x,Wt),m(Is,Wt,null),e(Wt,b_),e(Wt,ud),e(ud,S_),f(r,Oc,u),f(r,Ze,u),e(Ze,Ut),e(Ut,fd),m(Ns,fd,null),e(Ze,D_),e(Ze,pd),e(pd,x_),f(r,Ac,u),f(r,Bt,u),e(Bt,$_),e(Bt,Ls),e(Ls,E_),e(Bt,y_),f(r,Vc,u),f(r,C,u),m(qs,C,null),e(C,w_),e(C,Io),e(Io,M_),e(Io,Ks),e(Ks,P_),e(C,T_),e(C,N),e(N,No),e(No,C_),e(N,k_),e(N,hd),e(hd,O_),e(N,A_),e(N,md),e(md,V_),e(N,F_),e(N,gd),e(gd,I_),e(N,N_),e(N,Lo),e(Lo,L_),e(N,q_),e(N,qo),e(qo,K_),e(N,R_),e(N,Ko),e(Ko,Q_),e(N,j_),e(C,W_),e(C,Ht),m(Rs,Ht,null),e(Ht,U_),e(Ht,_d),e(_d,B_),e(C,H_),e(C,Gt),m(Qs,Gt,null),e(Gt,G_),e(Gt,js),e(js,z_),e(js,vd),e(vd,Y_),e(js,J_),e(C,X_),e(C,zt),m(Ws,zt,null),e(zt,Z_),e(zt,bd),e(bd,ev),e(C,tv),e(C,Yt),m(Us,Yt,null),e(Yt,rv),e(Yt,Sd),e(Sd,sv),f(r,Fc,u),f(r,et,u),e(et,Jt),e(Jt,Dd),m(Bs,Dd,null),e(et,nv),e(et,xd),e(xd,ov),f(r,Ic,u),f(r,Xt,u),e(Xt,iv),e(Xt,Hs),e(Hs,av),e(Xt,dv),f(r,Nc,u),f(r,$,u),m(Gs,$,null),e($,lv),e($,$d),e($d,cv),e($,uv),e($,L),e(L,Ro),e(Ro,fv),e(L,pv),e(L,Ed),e(Ed,hv),e(L,mv),e(L,yd),e(yd,gv),e(L,_v),e(L,wd),e(wd,vv),e(L,bv),e(L,Qo),e(Qo,Sv),e(L,Dv),e(L,jo),e(jo,xv),e(L,$v),e(L,Wo),e(Wo,Ev),e(L,yv),e($,wv),e($,Uo),e(Uo,Mv),e(Uo,zs),e(zs,Pv),e($,Tv),e($,Zt),m(Ys,Zt,null),e(Zt,Cv),e(Zt,Md),e(Md,kv),e($,Ov),e($,er),m(Js,er,null),e(er,Av),e(er,Pd),e(Pd,Vv),e($,Fv),e($,Ae),m(Xs,Ae,null),e(Ae,Iv),e(Ae,Td),e(Td,Nv),e(Ae,Lv),e(Ae,Pe),e(Pe,qv),e(Pe,Cd),e(Cd,Kv),e(Pe,Rv),e(Pe,kd),e(kd,Qv),e(Pe,jv),e(Pe,Od),e(Od,Wv),e(Pe,Uv),e($,Bv),e($,tr),m(Zs,tr,null),e(tr,Hv),e(tr,Ad),e(Ad,Gv),e($,zv),e($,rr),m(en,rr,null),e(rr,Yv),e(rr,Vd),e(Vd,Jv),f(r,Lc,u),f(r,tt,u),e(tt,sr),e(sr,Fd),m(tn,Fd,null),e(tt,Xv),e(tt,Id),e(Id,Zv),f(r,qc,u),f(r,nr,u),e(nr,eb),e(nr,rn),e(rn,tb),e(nr,rb),f(r,Kc,u),f(r,E,u),m(sn,E,null),e(E,sb),e(E,Nd),e(Nd,nb),e(E,ob),e(E,Bo),e(Bo,ib),e(Bo,nn),e(nn,ab),e(E,db),e(E,q),e(q,Ho),e(Ho,lb),e(q,cb),e(q,Ld),e(Ld,ub),e(q,fb),e(q,qd),e(qd,pb),e(q,hb),e(q,Kd),e(Kd,mb),e(q,gb),e(q,Go),e(Go,_b),e(q,vb),e(q,zo),e(zo,bb),e(q,Sb),e(q,Yo),e(Yo,Db),e(q,xb),e(E,$b),e(E,or),m(on,or,null),e(or,Eb),e(or,Rd),e(Rd,yb),e(E,wb),e(E,Ve),m(an,Ve,null),e(Ve,Mb),e(Ve,Qd),e(Qd,Pb),e(Ve,Tb),e(Ve,rt),e(rt,Cb),e(rt,jd),e(jd,kb),e(rt,Ob),e(rt,Wd),e(Wd,Ab),e(rt,Vb),e(E,Fb),e(E,ir),m(dn,ir,null),e(ir,Ib),e(ir,Ud),e(Ud,Nb),e(E,Lb),e(E,ar),m(ln,ar,null),e(ar,qb),e(ar,Bd),e(Bd,Kb),e(E,Rb),e(E,dr),m(cn,dr,null),e(dr,Qb),e(dr,Hd),e(Hd,jb),f(r,Rc,u),f(r,st,u),e(st,lr),e(lr,Gd),m(un,Gd,null),e(st,Wb),e(st,zd),e(zd,Ub),f(r,Qc,u),f(r,cr,u),e(cr,Bb),e(cr,fn),e(fn,Hb),e(cr,Gb),f(r,jc,u),f(r,k,u),m(pn,k,null),e(k,zb),e(k,Jo),e(Jo,Yb),e(Jo,hn),e(hn,Jb),e(k,Xb),e(k,K),e(K,Xo),e(Xo,Zb),e(K,e1),e(K,Yd),e(Yd,t1),e(K,r1),e(K,Jd),e(Jd,s1),e(K,n1),e(K,Xd),e(Xd,o1),e(K,i1),e(K,Zo),e(Zo,a1),e(K,d1),e(K,ei),e(ei,l1),e(K,c1),e(K,ti),e(ti,u1),e(K,f1),e(k,p1),e(k,ri),e(ri,h1),e(ri,mn),e(mn,m1),e(k,g1),e(k,ur),m(gn,ur,null),e(ur,_1),e(ur,Zd),e(Zd,v1),e(k,b1),e(k,fr),m(_n,fr,null),e(fr,S1),e(fr,el),e(el,D1),e(k,x1),e(k,pr),m(vn,pr,null),e(pr,$1),e(pr,tl),e(tl,E1),f(r,Wc,u),f(r,nt,u),e(nt,hr),e(hr,rl),m(bn,rl,null),e(nt,y1),e(nt,sl),e(sl,w1),f(r,Uc,u),f(r,mr,u),e(mr,M1),e(mr,Sn),e(Sn,P1),e(mr,T1),f(r,Bc,u),m(gr,r,u),f(r,Hc,u),f(r,te,u),m(Dn,te,null),e(te,C1),e(te,nl),e(nl,k1),e(te,O1),e(te,R),e(R,si),e(si,A1),e(R,V1),e(R,ol),e(ol,F1),e(R,I1),e(R,il),e(il,N1),e(R,L1),e(R,al),e(al,q1),e(R,K1),e(R,ni),e(ni,R1),e(R,Q1),e(R,oi),e(oi,j1),e(R,W1),e(R,ii),e(ii,U1),e(R,B1),e(te,H1),e(te,ai),e(ai,G1),e(ai,xn),e(xn,z1),e(te,Y1),e(te,dl),e(dl,J1),f(r,Gc,u),f(r,ot,u),e(ot,_r),e(_r,ll),m($n,ll,null),e(ot,X1),e(ot,cl),e(cl,Z1),f(r,zc,u),f(r,Fe,u),e(Fe,e0),e(Fe,En),e(En,t0),e(Fe,r0),e(Fe,yn),e(yn,s0),e(Fe,n0),f(r,Yc,u),f(r,Y,u),m(wn,Y,null),e(Y,o0),e(Y,vr),e(vr,i0),e(vr,Mn),e(Mn,a0),e(vr,d0),e(vr,Pn),e(Pn,l0),e(Y,c0),e(Y,Q),e(Q,di),e(di,u0),e(Q,f0),e(Q,ul),e(ul,p0),e(Q,h0),e(Q,fl),e(fl,m0),e(Q,g0),e(Q,pl),e(pl,_0),e(Q,v0),e(Q,li),e(li,b0),e(Q,S0),e(Q,ci),e(ci,D0),e(Q,x0),e(Q,ui),e(ui,$0),e(Q,E0),e(Y,y0),e(Y,br),m(Tn,br,null),e(br,w0),e(br,Cn),e(Cn,M0),e(Cn,hl),e(hl,P0),e(Cn,T0),e(Y,C0),e(Y,Sr),m(kn,Sr,null),e(Sr,k0),e(Sr,ml),e(ml,O0),e(Y,A0),e(Y,Dr),m(On,Dr,null),e(Dr,V0),e(Dr,gl),e(gl,F0),f(r,Jc,u),f(r,it,u),e(it,xr),e(xr,_l),m(An,_l,null),e(it,I0),e(it,vl),e(vl,N0),f(r,Xc,u),f(r,fi,u),e(fi,L0),f(r,Zc,u),f(r,J,u),m(Vn,J,null),e(J,q0),e(J,pi),e(pi,K0),e(pi,Fn),e(Fn,R0),e(J,Q0),e(J,j),e(j,hi),e(hi,j0),e(j,W0),e(j,bl),e(bl,U0),e(j,B0),e(j,Sl),e(Sl,H0),e(j,G0),e(j,Dl),e(Dl,z0),e(j,Y0),e(j,mi),e(mi,J0),e(j,X0),e(j,gi),e(gi,Z0),e(j,e2),e(j,_i),e(_i,t2),e(j,r2),e(J,s2),e(J,$r),m(In,$r,null),e($r,n2),e($r,Nn),e(Nn,o2),e(Nn,xl),e(xl,i2),e(Nn,a2),e(J,d2),e(J,Er),m(Ln,Er,null),e(Er,l2),e(Er,$l),e($l,c2),e(J,u2),e(J,yr),m(qn,yr,null),e(yr,f2),e(yr,El),e(El,p2),f(r,eu,u),f(r,at,u),e(at,wr),e(wr,yl),m(Kn,yl,null),e(at,h2),e(at,wl),e(wl,m2),f(r,tu,u),f(r,Rn,u),e(Rn,g2),e(Rn,Qn),e(Qn,_2),f(r,ru,u),f(r,y,u),m(jn,y,null),e(y,v2),e(y,Ml),e(Ml,b2),e(y,S2),e(y,Pl),e(Pl,D2),e(y,x2),e(y,W),e(W,vi),e(vi,$2),e(W,E2),e(W,Tl),e(Tl,y2),e(W,w2),e(W,Cl),e(Cl,M2),e(W,P2),e(W,kl),e(kl,T2),e(W,C2),e(W,bi),e(bi,k2),e(W,O2),e(W,Si),e(Si,A2),e(W,V2),e(W,Di),e(Di,F2),e(W,I2),e(y,N2),e(y,xi),e(xi,L2),e(xi,Wn),e(Wn,q2),e(y,K2),e(y,Ie),m(Un,Ie,null),e(Ie,R2),e(Ie,Bn),e(Bn,Q2),e(Bn,Ol),e(Ol,j2),e(Bn,W2),e(Ie,U2),e(Ie,Al),e(Al,B2),e(y,H2),e(y,U),m(Hn,U,null),e(U,G2),e(U,Gn),e(Gn,z2),e(Gn,Vl),e(Vl,Y2),e(Gn,J2),e(U,X2),e(U,Fl),e(Fl,Z2),e(U,e4),e(U,Il),e(Il,t4),e(U,r4),e(U,Nl),e(Nl,s4),e(U,n4),e(U,Ll),e(Ll,zn),e(zn,o4),e(zn,ql),e(ql,i4),e(zn,a4),e(U,d4),e(U,dt),e(dt,l4),e(dt,Kl),e(Kl,c4),e(dt,u4),e(dt,Rl),e(Rl,f4),e(dt,p4),e(y,h4),e(y,Mr),m(Yn,Mr,null),e(Mr,m4),e(Mr,Ql),e(Ql,g4),e(y,_4),e(y,Pr),m(Jn,Pr,null),e(Pr,v4),e(Pr,Xn),e(Xn,b4),e(Xn,jl),e(jl,S4),e(Xn,D4),f(r,su,u),f(r,lt,u),e(lt,Tr),e(Tr,Wl),m(Zn,Wl,null),e(lt,x4),e(lt,Ul),e(Ul,$4),f(r,nu,u),f(r,Te,u),e(Te,E4),e(Te,$i),e($i,y4),e(Te,w4),e(Te,eo),e(eo,M4),e(Te,P4),e(Te,to),e(to,T4),f(r,ou,u),f(r,X,u),m(ro,X,null),e(X,C4),e(X,Bl),e(Bl,k4),e(X,O4),e(X,B),e(B,Ei),e(Ei,A4),e(B,V4),e(B,Hl),e(Hl,F4),e(B,I4),e(B,Gl),e(Gl,N4),e(B,L4),e(B,zl),e(zl,q4),e(B,K4),e(B,yi),e(yi,R4),e(B,Q4),e(B,wi),e(wi,j4),e(B,W4),e(B,Mi),e(Mi,U4),e(B,B4),e(X,H4),e(X,Pi),e(Pi,G4),e(Pi,so),e(so,z4),e(X,Y4),e(X,Cr),m(no,Cr,null),e(Cr,J4),e(Cr,Yl),e(Yl,X4),e(X,Z4),e(X,kr),m(oo,kr,null),e(kr,eS),e(kr,Jl),e(Jl,tS),iu=!0},p(r,[u]){const io={};u&2&&(io.$$scope={dirty:u,ctx:r}),gr.$set(io)},i(r){iu||(g(Ir.$$.fragment,r),g(Nr.$$.fragment,r),g(qr.$$.fragment,r),g(Kr.$$.fragment,r),g(Rr.$$.fragment,r),g(Wr.$$.fragment,r),g(Ur.$$.fragment,r),g(Br.$$.fragment,r),g(Hr.$$.fragment,r),g(Gr.$$.fragment,r),g(zr.$$.fragment,r),g(Yr.$$.fragment,r),g(Xr.$$.fragment,r),g(Zr.$$.fragment,r),g(es.$$.fragment,r),g(ts.$$.fragment,r),g(ss.$$.fragment,r),g(os.$$.fragment,r),g(is.$$.fragment,r),g(as.$$.fragment,r),g(ds.$$.fragment,r),g(fs.$$.fragment,r),g(gs.$$.fragment,r),g(_s.$$.fragment,r),g(Ss.$$.fragment,r),g(Ds.$$.fragment,r),g(xs.$$.fragment,r),g($s.$$.fragment,r),g(Es.$$.fragment,r),g(ys.$$.fragment,r),g(Ms.$$.fragment,r),g(Os.$$.fragment,r),g(As.$$.fragment,r),g(Vs.$$.fragment,r),g(Fs.$$.fragment,r),g(Is.$$.fragment,r),g(Ns.$$.fragment,r),g(qs.$$.fragment,r),g(Rs.$$.fragment,r),g(Qs.$$.fragment,r),g(Ws.$$.fragment,r),g(Us.$$.fragment,r),g(Bs.$$.fragment,r),g(Gs.$$.fragment,r),g(Ys.$$.fragment,r),g(Js.$$.fragment,r),g(Xs.$$.fragment,r),g(Zs.$$.fragment,r),g(en.$$.fragment,r),g(tn.$$.fragment,r),g(sn.$$.fragment,r),g(on.$$.fragment,r),g(an.$$.fragment,r),g(dn.$$.fragment,r),g(ln.$$.fragment,r),g(cn.$$.fragment,r),g(un.$$.fragment,r),g(pn.$$.fragment,r),g(gn.$$.fragment,r),g(_n.$$.fragment,r),g(vn.$$.fragment,r),g(bn.$$.fragment,r),g(gr.$$.fragment,r),g(Dn.$$.fragment,r),g($n.$$.fragment,r),g(wn.$$.fragment,r),g(Tn.$$.fragment,r),g(kn.$$.fragment,r),g(On.$$.fragment,r),g(An.$$.fragment,r),g(Vn.$$.fragment,r),g(In.$$.fragment,r),g(Ln.$$.fragment,r),g(qn.$$.fragment,r),g(Kn.$$.fragment,r),g(jn.$$.fragment,r),g(Un.$$.fragment,r),g(Hn.$$.fragment,r),g(Yn.$$.fragment,r),g(Jn.$$.fragment,r),g(Zn.$$.fragment,r),g(ro.$$.fragment,r),g(no.$$.fragment,r),g(oo.$$.fragment,r),iu=!0)},o(r){_(Ir.$$.fragment,r),_(Nr.$$.fragment,r),_(qr.$$.fragment,r),_(Kr.$$.fragment,r),_(Rr.$$.fragment,r),_(Wr.$$.fragment,r),_(Ur.$$.fragment,r),_(Br.$$.fragment,r),_(Hr.$$.fragment,r),_(Gr.$$.fragment,r),_(zr.$$.fragment,r),_(Yr.$$.fragment,r),_(Xr.$$.fragment,r),_(Zr.$$.fragment,r),_(es.$$.fragment,r),_(ts.$$.fragment,r),_(ss.$$.fragment,r),_(os.$$.fragment,r),_(is.$$.fragment,r),_(as.$$.fragment,r),_(ds.$$.fragment,r),_(fs.$$.fragment,r),_(gs.$$.fragment,r),_(_s.$$.fragment,r),_(Ss.$$.fragment,r),_(Ds.$$.fragment,r),_(xs.$$.fragment,r),_($s.$$.fragment,r),_(Es.$$.fragment,r),_(ys.$$.fragment,r),_(Ms.$$.fragment,r),_(Os.$$.fragment,r),_(As.$$.fragment,r),_(Vs.$$.fragment,r),_(Fs.$$.fragment,r),_(Is.$$.fragment,r),_(Ns.$$.fragment,r),_(qs.$$.fragment,r),_(Rs.$$.fragment,r),_(Qs.$$.fragment,r),_(Ws.$$.fragment,r),_(Us.$$.fragment,r),_(Bs.$$.fragment,r),_(Gs.$$.fragment,r),_(Ys.$$.fragment,r),_(Js.$$.fragment,r),_(Xs.$$.fragment,r),_(Zs.$$.fragment,r),_(en.$$.fragment,r),_(tn.$$.fragment,r),_(sn.$$.fragment,r),_(on.$$.fragment,r),_(an.$$.fragment,r),_(dn.$$.fragment,r),_(ln.$$.fragment,r),_(cn.$$.fragment,r),_(un.$$.fragment,r),_(pn.$$.fragment,r),_(gn.$$.fragment,r),_(_n.$$.fragment,r),_(vn.$$.fragment,r),_(bn.$$.fragment,r),_(gr.$$.fragment,r),_(Dn.$$.fragment,r),_($n.$$.fragment,r),_(wn.$$.fragment,r),_(Tn.$$.fragment,r),_(kn.$$.fragment,r),_(On.$$.fragment,r),_(An.$$.fragment,r),_(Vn.$$.fragment,r),_(In.$$.fragment,r),_(Ln.$$.fragment,r),_(qn.$$.fragment,r),_(Kn.$$.fragment,r),_(jn.$$.fragment,r),_(Un.$$.fragment,r),_(Hn.$$.fragment,r),_(Yn.$$.fragment,r),_(Jn.$$.fragment,r),_(Zn.$$.fragment,r),_(ro.$$.fragment,r),_(no.$$.fragment,r),_(oo.$$.fragment,r),iu=!1},d(r){t(Z),r&&t(ct),r&&t(ee),v(Ir),r&&t(rc),r&&t(lo),r&&t(sc),r&&t(Le),v(Nr),r&&t(nc),r&&t(ft),r&&t(oc),r&&t(pt),r&&t(ic),r&&t(Ke),v(qr),r&&t(ac),r&&t(O),r&&t(dc),r&&t(Re),v(Kr),r&&t(lc),r&&t(ho),r&&t(cc),r&&t(gt),r&&t(uc),r&&t(Qe),v(Rr),r&&t(fc),r&&t(mo),r&&t(pc),r&&t(Ce),r&&t(hc),r&&t(vt),r&&t(mc),r&&t(je),v(Wr),r&&t(gc),r&&t(We),v(Ur),r&&t(_c),r&&t(Ue),v(Br),r&&t(vc),r&&t(Be),v(Hr),r&&t(bc),r&&t(He),v(Gr),r&&t(Sc),r&&t(Ge),v(zr),r&&t(Dc),r&&t(_o),r&&t(xc),r&&t(P),v(Yr),v(Xr),v(Zr),v(es),r&&t($c),r&&t(ze),v(ts),r&&t(Ec),r&&t(Mt),r&&t(yc),r&&t(T),v(ss),v(os),v(is),v(as),r&&t(wc),r&&t(Ye),v(ds),r&&t(Mc),r&&t(xe),r&&t(Pc),r&&t(S),v(fs),v(gs),v(_s),v(Ss),v(Ds),v(xs),v($s),v(Es),r&&t(Tc),r&&t(Xe),v(ys),r&&t(Cc),r&&t(qt),r&&t(kc),r&&t(x),v(Ms),v(Os),v(As),v(Vs),v(Fs),v(Is),r&&t(Oc),r&&t(Ze),v(Ns),r&&t(Ac),r&&t(Bt),r&&t(Vc),r&&t(C),v(qs),v(Rs),v(Qs),v(Ws),v(Us),r&&t(Fc),r&&t(et),v(Bs),r&&t(Ic),r&&t(Xt),r&&t(Nc),r&&t($),v(Gs),v(Ys),v(Js),v(Xs),v(Zs),v(en),r&&t(Lc),r&&t(tt),v(tn),r&&t(qc),r&&t(nr),r&&t(Kc),r&&t(E),v(sn),v(on),v(an),v(dn),v(ln),v(cn),r&&t(Rc),r&&t(st),v(un),r&&t(Qc),r&&t(cr),r&&t(jc),r&&t(k),v(pn),v(gn),v(_n),v(vn),r&&t(Wc),r&&t(nt),v(bn),r&&t(Uc),r&&t(mr),r&&t(Bc),v(gr,r),r&&t(Hc),r&&t(te),v(Dn),r&&t(Gc),r&&t(ot),v($n),r&&t(zc),r&&t(Fe),r&&t(Yc),r&&t(Y),v(wn),v(Tn),v(kn),v(On),r&&t(Jc),r&&t(it),v(An),r&&t(Xc),r&&t(fi),r&&t(Zc),r&&t(J),v(Vn),v(In),v(Ln),v(qn),r&&t(eu),r&&t(at),v(Kn),r&&t(tu),r&&t(Rn),r&&t(ru),r&&t(y),v(jn),v(Un),v(Hn),v(Yn),v(Jn),r&&t(su),r&&t(lt),v(Zn),r&&t(nu),r&&t(Te),r&&t(ou),r&&t(X),v(ro),v(no),v(oo)}}}const U5={local:"schedulers",sections:[{local:"what-is-a-scheduler",sections:[{local:"discrete-versus-continuous-schedulers",title:"Discrete versus continuous schedulers"}],title:"What is a scheduler?"},{local:"designing-reusable-schedulers",title:"Designing Re-usable schedulers"},{local:"api",sections:[{local:"diffusers.SchedulerMixin",title:"SchedulerMixin"},{local:"diffusers.schedulers.scheduling_utils.SchedulerOutput",title:"SchedulerOutput"},{local:"implemented-schedulers",sections:[{local:"diffusers.DDIMScheduler",title:"Denoising diffusion implicit models (DDIM)"},{local:"diffusers.DDPMScheduler",title:"Denoising diffusion probabilistic models (DDPM)"},{local:"diffusers.DPMSolverMultistepScheduler",title:"Multistep DPM-Solver"},{local:"diffusers.KarrasVeScheduler",title:"Variance exploding, stochastic sampling from Karras et. al"},{local:"diffusers.LMSDiscreteScheduler",title:"Linear multistep scheduler for discrete beta schedules"},{local:"diffusers.PNDMScheduler",title:"Pseudo numerical methods for diffusion models (PNDM)"},{local:"diffusers.ScoreSdeVeScheduler",title:"variance exploding stochastic differential equation (VE-SDE) scheduler"},{local:"diffusers.IPNDMScheduler",title:"improved pseudo numerical methods for diffusion models (iPNDM)"},{local:"diffusers.schedulers.ScoreSdeVpScheduler",title:"variance preserving stochastic differential equation (VP-SDE) scheduler"},{local:"diffusers.EulerDiscreteScheduler",title:"Euler scheduler"},{local:"diffusers.EulerAncestralDiscreteScheduler",title:"Euler Ancestral scheduler"},{local:"diffusers.VQDiffusionScheduler",title:"VQDiffusionScheduler"},{local:"diffusers.RePaintScheduler",title:"RePaint scheduler"}],title:"Implemented Schedulers"}],title:"API"}],title:"Schedulers"};function B5(tc){return R5(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class J5 extends N5{constructor(Z){super();L5(this,Z,B5,W5,q5,{})}}export{J5 as default,U5 as metadata};
