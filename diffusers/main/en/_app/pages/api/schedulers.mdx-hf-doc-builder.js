import{S as Ny,i as Ly,s as qy,e as n,k as l,w as f,t as s,M as Ky,c as a,d as t,m as c,a as i,x as h,h as o,b as d,G as e,g as p,y as m,q as g,o as _,B as v,v as Ry}from"../../chunks/vendor-hf-doc-builder.js";import{T as ix}from"../../chunks/Tip-hf-doc-builder.js";import{D as b}from"../../chunks/Docstring-hf-doc-builder.js";import{I as k}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Qy($t){let S,oe,D,P,we,T,be,Pe;return{c(){S=n("p"),oe=s("It is required to be logged in ("),D=n("code"),P=s("huggingface-cli login"),we=s(") when you want to use private or "),T=n("a"),be=s(`gated
models`),Pe=s("."),this.h()},l(ze){S=a(ze,"P",{});var Me=i(S);oe=o(Me,"It is required to be logged in ("),D=a(Me,"CODE",{});var Je=i(D);P=o(Je,"huggingface-cli login"),Je.forEach(t),we=o(Me,") when you want to use private or "),T=a(Me,"A",{href:!0,rel:!0});var Mn=i(T);be=o(Mn,`gated
models`),Mn.forEach(t),Pe=o(Me,"."),Me.forEach(t),this.h()},h(){d(T,"href","https://huggingface.co/docs/hub/models-gated#gated-models"),d(T,"rel","nofollow")},m(ze,Me){p(ze,S,Me),e(S,oe),e(S,D),e(D,P),e(S,we),e(S,T),e(T,be),e(S,Pe)},d(ze){ze&&t(S)}}}function Wy($t){let S,oe,D,P,we;return{c(){S=n("p"),oe=s("Activate the special "),D=n("a"),P=s("\u201Coffline-mode\u201D"),we=s(` to
use this method in a firewalled environment.`),this.h()},l(T){S=a(T,"P",{});var be=i(S);oe=o(be,"Activate the special "),D=a(be,"A",{href:!0,rel:!0});var Pe=i(D);P=o(Pe,"\u201Coffline-mode\u201D"),Pe.forEach(t),we=o(be,` to
use this method in a firewalled environment.`),be.forEach(t),this.h()},h(){d(D,"href","https://huggingface.co/transformers/installation.html#offline-mode"),d(D,"rel","nofollow")},m(T,be){p(T,S,be),e(S,oe),e(S,D),e(D,P),e(S,we)},d(T){T&&t(S)}}}function jy($t){let S,oe;return{c(){S=n("p"),oe=s("Score SDE-VP is under construction.")},l(D){S=a(D,"P",{});var P=i(S);oe=o(P,"Score SDE-VP is under construction."),P.forEach(t)},m(D,P){p(D,S,P),e(S,oe)},d(D){D&&t(S)}}}function Uy($t){let S,oe,D,P,we,T,be,Pe,ze,Me,Je,Mn,kc,Ye,Et,ii,Xr,uf,di,pf,Oc,qe,ff,li,hf,mf,ci,gf,_f,Ac,yt,Pn,vf,Zr,ui,bf,Sf,pi,xf,Df,Xe,$f,fi,Ef,yf,hi,wf,Mf,Cc,Ze,wt,mi,es,Pf,gi,Tf,Vc,I,kf,_i,Of,Af,Tn,Cf,Vf,kn,Ff,If,vi,Nf,Lf,On,qf,Kf,bi,Rf,Qf,Fc,et,Mt,Si,ts,Wf,xi,jf,Ic,An,Uf,Nc,Pt,Di,Hf,Bf,$i,Gf,Lc,tt,Tt,Ei,rs,zf,yi,Jf,qc,Cn,Yf,Kc,Ke,ss,Xf,wi,Zf,eh,th,os,rh,Mi,sh,oh,nh,Pi,ah,Rc,kt,ih,Vn,dh,lh,Qc,rt,Ot,Ti,ns,ch,ki,uh,Wc,ee,as,ph,Oi,fh,hh,Ai,mh,gh,Ci,Re,Vi,_h,vh,Fi,bh,Sh,Ii,xh,Dh,$h,Te,is,Eh,Ni,yh,wh,At,Mh,Ct,Ph,Vt,ds,Th,st,kh,Li,Oh,Ah,Fn,Ch,Vh,jc,ot,Ft,qi,ls,Fh,Ki,Ih,Uc,nt,cs,Nh,Ri,Lh,Hc,at,It,Qi,us,qh,Wi,Kh,Bc,it,Nt,ji,ps,Rh,Ui,Qh,Gc,In,Wh,zc,A,fs,jh,Hi,Uh,Hh,N,Nn,Bh,Gh,Bi,zh,Jh,Gi,Yh,Xh,zi,Zh,em,Ln,tm,rm,qn,sm,om,Kn,nm,am,im,Rn,dm,hs,lm,cm,Lt,ms,um,Ji,pm,fm,qt,gs,hm,Yi,mm,gm,Kt,_s,_m,Xi,vm,Jc,dt,Rt,Zi,vs,bm,ed,Sm,Yc,Qt,xm,bs,Dm,$m,Xc,C,Ss,Em,td,ym,wm,L,Qn,Mm,Pm,rd,Tm,km,sd,Om,Am,od,Cm,Vm,Wn,Fm,Im,jn,Nm,Lm,Un,qm,Km,Rm,Hn,Qm,xs,Wm,jm,Wt,Ds,Um,nd,Hm,Bm,jt,$s,Gm,ad,zm,Jm,Ut,Es,Ym,id,Xm,Zc,lt,Ht,dd,ys,Zm,ld,eg,eu,ke,tg,ws,rg,sg,Ms,og,ng,Ps,ag,ig,tu,x,Ts,dg,cd,lg,cg,Bt,ug,ks,pg,fg,Os,hg,mg,ct,gg,ud,_g,vg,pd,bg,Sg,xg,Ie,Dg,As,$g,Eg,fd,yg,wg,hd,Mg,Pg,Tg,q,Bn,kg,Og,md,Ag,Cg,gd,Vg,Fg,_d,Ig,Ng,Gn,Lg,qg,zn,Kg,Rg,Jn,Qg,Wg,jg,Oe,Cs,Ug,vd,Hg,Bg,bd,Gg,zg,Sd,Jg,Yg,Qe,Vs,Xg,xd,Zg,e_,Fs,t_,Is,r_,s_,o_,Gt,Ns,n_,Dd,a_,i_,zt,Ls,d_,$d,l_,c_,Jt,qs,u_,Ed,p_,f_,Yt,Ks,h_,yd,m_,g_,Xt,Rs,__,wd,v_,ru,ut,Zt,Md,Qs,b_,Pd,S_,su,er,x_,Ws,D_,$_,ou,E,js,E_,Td,y_,w_,tr,M_,Us,P_,T_,Hs,k_,O_,K,Yn,A_,C_,kd,V_,F_,Od,I_,N_,Ad,L_,q_,Xn,K_,R_,Zn,Q_,W_,ea,j_,U_,H_,Bs,B_,Gs,G_,z_,J_,We,zs,Y_,Cd,X_,Z_,Vd,ev,tv,rr,Js,rv,Fd,sv,ov,sr,Ys,nv,Id,av,iv,or,Xs,dv,Nd,lv,cv,nr,Zs,uv,Ld,pv,nu,pt,ar,qd,eo,fv,Kd,hv,au,ir,mv,to,gv,_v,iu,V,ro,vv,ta,bv,so,Sv,xv,R,ra,Dv,$v,Rd,Ev,yv,Qd,wv,Mv,Wd,Pv,Tv,sa,kv,Ov,oa,Av,Cv,na,Vv,Fv,Iv,dr,oo,Nv,jd,Lv,qv,lr,no,Kv,ao,Rv,Ud,Qv,Wv,jv,cr,io,Uv,Hd,Hv,Bv,ur,lo,Gv,Bd,zv,du,ft,pr,Gd,co,Jv,zd,Yv,lu,fr,Xv,uo,Zv,eb,cu,y,po,tb,Jd,rb,sb,Q,aa,ob,nb,Yd,ab,ib,Xd,db,lb,Zd,cb,ub,ia,pb,fb,da,hb,mb,la,gb,_b,vb,ca,bb,fo,Sb,xb,hr,ho,Db,el,$b,Eb,mr,mo,yb,tl,wb,Mb,je,go,Pb,rl,Tb,kb,Ne,Ob,sl,Ab,Cb,ol,Vb,Fb,nl,Ib,Nb,Lb,gr,_o,qb,al,Kb,Rb,_r,vo,Qb,il,Wb,uu,ht,vr,dl,bo,jb,ll,Ub,pu,br,Hb,So,Bb,Gb,fu,w,xo,zb,cl,Jb,Yb,ua,Xb,Do,Zb,e1,W,pa,t1,r1,ul,s1,o1,pl,n1,a1,fl,i1,d1,fa,l1,c1,ha,u1,p1,ma,f1,h1,m1,Sr,$o,g1,hl,_1,v1,Ue,Eo,b1,ml,S1,x1,mt,D1,gl,$1,E1,_l,y1,w1,M1,xr,yo,P1,vl,T1,k1,Dr,wo,O1,bl,A1,C1,$r,Mo,V1,Sl,F1,hu,gt,Er,xl,Po,I1,Dl,N1,mu,yr,L1,To,q1,K1,gu,F,ko,R1,ga,Q1,Oo,W1,j1,j,_a,U1,H1,$l,B1,G1,El,z1,J1,yl,Y1,X1,va,Z1,e0,ba,t0,r0,Sa,s0,o0,n0,xa,a0,Ao,i0,d0,wr,Co,l0,wl,c0,u0,Mr,Vo,p0,Ml,f0,h0,Pr,Fo,m0,Pl,g0,_u,_t,Tr,Tl,Io,_0,kl,v0,vu,kr,b0,No,S0,x0,bu,Or,Su,ne,Lo,D0,Ol,$0,E0,U,Da,y0,w0,Al,M0,P0,Cl,T0,k0,Vl,O0,A0,$a,C0,V0,Ea,F0,I0,ya,N0,L0,q0,wa,K0,qo,R0,Q0,Fl,W0,xu,vt,Ar,Il,Ko,j0,Nl,U0,Du,He,H0,Ro,B0,G0,Qo,z0,J0,$u,te,Wo,Y0,Cr,X0,jo,Z0,e2,Uo,t2,r2,H,Ma,s2,o2,Ll,n2,a2,ql,i2,d2,Kl,l2,c2,Pa,u2,p2,Ta,f2,h2,ka,m2,g2,_2,Vr,Ho,v2,Bo,b2,Rl,S2,x2,D2,Fr,Go,$2,Ql,E2,y2,Ir,zo,w2,Wl,M2,Eu,bt,Nr,jl,Jo,P2,Ul,T2,yu,Oa,k2,wu,re,Yo,O2,Aa,A2,Xo,C2,V2,B,Ca,F2,I2,Hl,N2,L2,Bl,q2,K2,Gl,R2,Q2,Va,W2,j2,Fa,U2,H2,Ia,B2,G2,z2,Lr,Zo,J2,en,Y2,zl,X2,Z2,eS,qr,tn,tS,Jl,rS,sS,Kr,rn,oS,Yl,nS,Mu,St,Rr,Xl,sn,aS,Zl,iS,Pu,on,dS,nn,lS,Tu,M,an,cS,ec,uS,pS,tc,fS,hS,G,Na,mS,gS,rc,_S,vS,sc,bS,SS,oc,xS,DS,La,$S,ES,qa,yS,wS,Ka,MS,PS,TS,Ra,kS,dn,OS,AS,Be,ln,CS,cn,VS,nc,FS,IS,NS,ac,LS,qS,z,un,KS,pn,RS,ic,QS,WS,jS,dc,US,HS,lc,BS,GS,cc,zS,JS,uc,fn,YS,pc,XS,ZS,e4,xt,t4,fc,r4,s4,hc,o4,n4,a4,Qr,hn,i4,mc,d4,l4,Wr,mn,c4,gn,u4,gc,p4,f4,ku,Dt,jr,_c,_n,h4,vc,m4,Ou,Le,g4,Qa,_4,v4,vn,b4,S4,bn,x4,Au,se,Sn,D4,bc,$4,E4,J,Wa,y4,w4,Sc,M4,P4,xc,T4,k4,Dc,O4,A4,ja,C4,V4,Ua,F4,I4,Ha,N4,L4,q4,Ba,K4,xn,R4,Q4,Ur,Dn,W4,$c,j4,U4,Hr,$n,H4,Ec,B4,Cu;return T=new k({}),Xr=new k({}),es=new k({}),ts=new k({}),rs=new k({}),ns=new k({}),as=new b({props:{name:"class diffusers.SchedulerMixin",anchor:"diffusers.SchedulerMixin",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L41"}}),is=new b({props:{name:"from_pretrained",anchor:"diffusers.SchedulerMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Dict[str, typing.Any] = None"},{name:"subfolder",val:": typing.Optional[str] = None"},{name:"return_unused_kwargs",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.SchedulerMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a model repo on huggingface.co. Valid model ids should have an
organization name, like <code>google/ddpm-celebahq-256</code>.</li>
<li>A path to a <em>directory</em> containing the schedluer configurations saved using
<a href="/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"diffusers.SchedulerMixin.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo (either remote in
huggingface.co or downloaded locally), you can specify the folder name here.`,name:"subfolder"},{anchor:"diffusers.SchedulerMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether kwargs that are not consumed by the Python class should be returned or not.`,name:"return_unused_kwargs"},{anchor:"diffusers.SchedulerMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>Union[str, os.PathLike]</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"diffusers.SchedulerMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"diffusers.SchedulerMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"diffusers.SchedulerMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"diffusers.SchedulerMixin.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"diffusers.SchedulerMixin.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (i.e., do not try to download the model).`,name:"local_files_only(bool,"},{anchor:"diffusers.SchedulerMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"diffusers.SchedulerMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L55"}}),At=new ix({props:{$$slots:{default:[Qy]},$$scope:{ctx:$t}}}),Ct=new ix({props:{$$slots:{default:[Wy]},$$scope:{ctx:$t}}}),ds=new b({props:{name:"save_pretrained",anchor:"diffusers.SchedulerMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.SchedulerMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the configuration JSON file will be saved (will be created if it does not exist).`,name:"save_directory"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L126"}}),ls=new k({}),cs=new b({props:{name:"class diffusers.schedulers.scheduling_utils.SchedulerOutput",anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput",parameters:[{name:"prev_sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput.prev_sample",description:`<strong>prev_sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code> for images) &#x2014;
Computed sample (x_{t-1}) of previous timestep. <code>prev_sample</code> should be used as next model input in the
denoising loop.`,name:"prev_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L28"}}),us=new k({}),ps=new k({}),fs=new b({props:{name:"class diffusers.DDIMScheduler",anchor:"diffusers.DDIMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"},{name:"set_alpha_to_one",val:": bool = True"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDIMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDIMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDIMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDIMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDIMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"},{anchor:"diffusers.DDIMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.DDIMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L78"}}),ms=new b({props:{name:"scale_model_input",anchor:"diffusers.DDIMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDIMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L157",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),gs=new b({props:{name:"set_timesteps",anchor:"diffusers.DDIMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L181"}}),_s=new b({props:{name:"step",anchor:"diffusers.DDIMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"eta",val:": float = 0.0"},{name:"use_clipped_model_output",val:": bool = False"},{name:"generator",val:" = None"},{name:"variance_noise",val:": typing.Optional[torch.FloatTensor] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDIMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDIMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DDIMScheduler.step.eta",description:"<strong>eta</strong> (<code>float</code>) &#x2014; weight of noise for added noise in diffusion step.",name:"eta"},{anchor:"diffusers.DDIMScheduler.step.use_clipped_model_output",description:`<strong>use_clipped_model_output</strong> (<code>bool</code>) &#x2014; if <code>True</code>, compute &#x201C;corrected&#x201D; <code>model_output</code> from the clipped
predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when
<code>self.config.clip_sample</code> is <code>True</code>. If no clipping has happened, &#x201C;corrected&#x201D; <code>model_output</code> would
coincide with the one provided as input and <code>use_clipped_model_output</code> will have not effect.
generator &#x2014; random number generator.`,name:"use_clipped_model_output"},{anchor:"diffusers.DDIMScheduler.step.variance_noise",description:`<strong>variance_noise</strong> (<code>torch.FloatTensor</code>) &#x2014; instead of generating noise for the variance using <code>generator</code>, we
can directly provide the noise for the variance itself. This is useful for methods such as
CycleDiffusion. (<a href="https://arxiv.org/abs/2210.05559" rel="nofollow">https://arxiv.org/abs/2210.05559</a>)`,name:"variance_noise"},{anchor:"diffusers.DDIMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDIMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L197",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> or <code>tuple</code></p>
`}}),vs=new k({}),Ss=new b({props:{name:"class diffusers.DDPMScheduler",anchor:"diffusers.DDPMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"variance_type",val:": str = 'fixed_small'"},{name:"clip_sample",val:": bool = True"},{name:"predict_epsilon",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDPMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDPMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDPMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDPMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDPMScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.DDPMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"},{anchor:"diffusers.DDPMScheduler.predict_epsilon",description:`<strong>predict_epsilon</strong> (<code>bool</code>) &#x2014;
optional flag to use when the model predicts the noise (epsilon), or the samples instead of the noise.`,name:"predict_epsilon"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L76"}}),Ds=new b({props:{name:"scale_model_input",anchor:"diffusers.DDPMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDPMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L153",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),$s=new b({props:{name:"set_timesteps",anchor:"diffusers.DDPMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L167"}}),Es=new b({props:{name:"step",anchor:"diffusers.DDPMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"generator",val:" = None"},{name:"return_dict",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDPMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDPMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.DDPMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDPMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L216",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> or <code>tuple</code></p>
`}}),ys=new k({}),Ts=new b({props:{name:"class diffusers.DPMSolverMultistepScheduler",anchor:"diffusers.DPMSolverMultistepScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"solver_order",val:": int = 2"},{name:"predict_epsilon",val:": bool = True"},{name:"thresholding",val:": bool = False"},{name:"dynamic_thresholding_ratio",val:": float = 0.995"},{name:"sample_max_value",val:": float = 1.0"},{name:"algorithm_type",val:": str = 'dpmsolver++'"},{name:"solver_type",val:": str = 'midpoint'"},{name:"lower_order_final",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DPMSolverMultistepScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DPMSolverMultistepScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DPMSolverMultistepScheduler.solver_order",description:`<strong>solver_order</strong> (<code>int</code>, default <code>2</code>) &#x2014;
the order of DPM-Solver; can be <code>1</code> or <code>2</code> or <code>3</code>. We recommend to use <code>solver_order=2</code> for guided
sampling, and <code>solver_order=3</code> for unconditional sampling.`,name:"solver_order"},{anchor:"diffusers.DPMSolverMultistepScheduler.predict_epsilon",description:`<strong>predict_epsilon</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
we currently support both the noise prediction model and the data prediction model. If the model predicts
the noise / epsilon, set <code>predict_epsilon</code> to <code>True</code>. If the model predicts the data / x0 directly, set
<code>predict_epsilon</code> to <code>False</code>.`,name:"predict_epsilon"},{anchor:"diffusers.DPMSolverMultistepScheduler.thresholding",description:`<strong>thresholding</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
whether to use the &#x201C;dynamic thresholding&#x201D; method (introduced by Imagen, <a href="https://arxiv.org/abs/2205.11487" rel="nofollow">https://arxiv.org/abs/2205.11487</a>).
For pixel-space diffusion models, you can set both <code>algorithm_type=dpmsolver++</code> and <code>thresholding=True</code> to
use the dynamic thresholding. Note that the thresholding method is unsuitable for latent-space diffusion
models (such as stable-diffusion).`,name:"thresholding"},{anchor:"diffusers.DPMSolverMultistepScheduler.dynamic_thresholding_ratio",description:`<strong>dynamic_thresholding_ratio</strong> (<code>float</code>, default <code>0.995</code>) &#x2014;
the ratio for the dynamic thresholding method. Default is <code>0.995</code>, the same as Imagen
(<a href="https://arxiv.org/abs/2205.11487" rel="nofollow">https://arxiv.org/abs/2205.11487</a>).`,name:"dynamic_thresholding_ratio"},{anchor:"diffusers.DPMSolverMultistepScheduler.sample_max_value",description:`<strong>sample_max_value</strong> (<code>float</code>, default <code>1.0</code>) &#x2014;
the threshold value for dynamic thresholding. Valid only when <code>thresholding=True</code> and
<code>algorithm_type=&quot;dpmsolver++</code>.`,name:"sample_max_value"},{anchor:"diffusers.DPMSolverMultistepScheduler.algorithm_type",description:`<strong>algorithm_type</strong> (<code>str</code>, default <code>dpmsolver++</code>) &#x2014;
the algorithm type for the solver. Either <code>dpmsolver</code> or <code>dpmsolver++</code>. The <code>dpmsolver</code> type implements the
algorithms in <a href="https://arxiv.org/abs/2206.00927" rel="nofollow">https://arxiv.org/abs/2206.00927</a>, and the <code>dpmsolver++</code> type implements the algorithms in
<a href="https://arxiv.org/abs/2211.01095" rel="nofollow">https://arxiv.org/abs/2211.01095</a>. We recommend to use <code>dpmsolver++</code> with <code>solver_order=2</code> for guided
sampling (e.g. stable-diffusion).`,name:"algorithm_type"},{anchor:"diffusers.DPMSolverMultistepScheduler.solver_type",description:`<strong>solver_type</strong> (<code>str</code>, default <code>midpoint</code>) &#x2014;
the solver type for the second-order solver. Either <code>midpoint</code> or <code>heun</code>. The solver type slightly affects
the sample quality, especially for small number of steps. We empirically find that <code>midpoint</code> solvers are
slightly better, so we recommend to use the <code>midpoint</code> type.`,name:"solver_type"},{anchor:"diffusers.DPMSolverMultistepScheduler.lower_order_final",description:`<strong>lower_order_final</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
whether to use lower-order solvers in the final steps. Only valid for &lt; 15 inference steps. We empirically
find this trick can stabilize the sampling of DPM-Solver for steps &lt; 15, especially for steps &lt;= 10.`,name:"lower_order_final"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L57"}}),Cs=new b({props:{name:"convert_model_output",anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.convert_model_output.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L200",returnDescription:`
<p>the converted model output.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Vs=new b({props:{name:"dpm_solver_first_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.dpm_solver_first_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L249",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ns=new b({props:{name:"multistep_dpm_solver_second_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update",parameters:[{name:"model_output_list",val:": typing.List[torch.FloatTensor]"},{name:"timestep_list",val:": typing.List[int]"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.model_output_list",description:`<strong>model_output_list</strong> (<code>List[torch.FloatTensor]</code>) &#x2014;
direct outputs from learned diffusion model at current and latter timesteps.`,name:"model_output_list"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current and latter discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_second_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L281",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ls=new b({props:{name:"multistep_dpm_solver_third_order_update",anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update",parameters:[{name:"model_output_list",val:": typing.List[torch.FloatTensor]"},{name:"timestep_list",val:": typing.List[int]"},{name:"prev_timestep",val:": int"},{name:"sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.model_output_list",description:`<strong>model_output_list</strong> (<code>List[torch.FloatTensor]</code>) &#x2014;
direct outputs from learned diffusion model at current and latter timesteps.`,name:"model_output_list"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current and latter discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.prev_timestep",description:"<strong>prev_timestep</strong> (<code>int</code>) &#x2014; previous discrete timestep in the diffusion chain.",name:"prev_timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.multistep_dpm_solver_third_order_update.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L340",returnDescription:`
<p>the sample tensor at the previous timestep.</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),qs=new b({props:{name:"scale_model_input",anchor:"diffusers.DPMSolverMultistepScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L463",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ks=new b({props:{name:"set_timesteps",anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.DPMSolverMultistepScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L177"}}),Rs=new b({props:{name:"step",anchor:"diffusers.DPMSolverMultistepScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DPMSolverMultistepScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DPMSolverMultistepScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_dpmsolver_multistep.py#L395",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),Qs=new k({}),js=new b({props:{name:"class diffusers.KarrasVeScheduler",anchor:"diffusers.KarrasVeScheduler",parameters:[{name:"sigma_min",val:": float = 0.02"},{name:"sigma_max",val:": float = 100"},{name:"s_noise",val:": float = 1.007"},{name:"s_churn",val:": float = 80"},{name:"s_min",val:": float = 0.05"},{name:"s_max",val:": float = 50"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.sigma_min",description:"<strong>sigma_min</strong> (<code>float</code>) &#x2014; minimum noise magnitude",name:"sigma_min"},{anchor:"diffusers.KarrasVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum noise magnitude",name:"sigma_max"},{anchor:"diffusers.KarrasVeScheduler.s_noise",description:`<strong>s_noise</strong> (<code>float</code>) &#x2014; the amount of additional noise to counteract loss of detail during sampling.
A reasonable range is [1.000, 1.011].`,name:"s_noise"},{anchor:"diffusers.KarrasVeScheduler.s_churn",description:`<strong>s_churn</strong> (<code>float</code>) &#x2014; the parameter controlling the overall amount of stochasticity.
A reasonable range is [0, 100].`,name:"s_churn"},{anchor:"diffusers.KarrasVeScheduler.s_min",description:`<strong>s_min</strong> (<code>float</code>) &#x2014; the start value of the sigma range where we add noise (enable stochasticity).
A reasonable range is [0, 10].`,name:"s_min"},{anchor:"diffusers.KarrasVeScheduler.s_max",description:`<strong>s_max</strong> (<code>float</code>) &#x2014; the end value of the sigma range where we add noise.
A reasonable range is [0.2, 80].`,name:"s_max"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L48"}}),zs=new b({props:{name:"add_noise_to_input",anchor:"diffusers.KarrasVeScheduler.add_noise_to_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"sigma",val:": float"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L133"}}),Js=new b({props:{name:"scale_model_input",anchor:"diffusers.KarrasVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.KarrasVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L98",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Ys=new b({props:{name:"set_timesteps",anchor:"diffusers.KarrasVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L112"}}),Xs=new b({props:{name:"step",anchor:"diffusers.KarrasVeScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class</p>
<p>KarrasVeOutput &#x2014; updated sample in the diffusion chain and derivative (TODO double check).`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L154",returnDescription:`
<p><code>KarrasVeOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>KarrasVeOutput</code> or <code>tuple</code></p>
`}}),Zs=new b({props:{name:"step_correct",anchor:"diffusers.KarrasVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"sample_prev",val:": FloatTensor"},{name:"derivative",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_prev",description:"<strong>sample_prev</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.derivative",description:"<strong>derivative</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"derivative"},{anchor:"diffusers.KarrasVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L192",returnDescription:`
<p>updated sample in the diffusion chain. derivative (TODO): TODO</p>
`,returnType:`
<p>prev_sample (TODO)</p>
`}}),eo=new k({}),ro=new b({props:{name:"class diffusers.LMSDiscreteScheduler",anchor:"diffusers.LMSDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.LMSDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.LMSDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.LMSDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.LMSDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L47"}}),oo=new b({props:{name:"get_lms_coefficient",anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient",parameters:[{name:"order",val:""},{name:"t",val:""},{name:"current_order",val:""}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.order",description:"<strong>order</strong> (TODO) &#x2014;",name:"order"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.t",description:"<strong>t</strong> (TODO) &#x2014;",name:"t"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.current_order",description:"<strong>current_order</strong> (TODO) &#x2014;",name:"current_order"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L131"}}),no=new b({props:{name:"scale_model_input",anchor:"diffusers.LMSDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L110",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),io=new b({props:{name:"set_timesteps",anchor:"diffusers.LMSDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L153"}}),lo=new b({props:{name:"step",anchor:"diffusers.LMSDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"order",val:": int = 4"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.LMSDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.LMSDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
order &#x2014; coefficient for multi-step inference.`,name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than LMSDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L179",returnDescription:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),co=new k({}),po=new b({props:{name:"class diffusers.PNDMScheduler",anchor:"diffusers.PNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"skip_prk_steps",val:": bool = False"},{name:"set_alpha_to_one",val:": bool = False"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.PNDMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.PNDMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.PNDMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.PNDMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.PNDMScheduler.skip_prk_steps",description:`<strong>skip_prk_steps</strong> (<code>bool</code>) &#x2014;
allows the scheduler to skip the Runge-Kutta steps that are defined in the original paper as being required
before plms steps; defaults to <code>False</code>.`,name:"skip_prk_steps"},{anchor:"diffusers.PNDMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.PNDMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L57"}}),ho=new b({props:{name:"scale_model_input",anchor:"diffusers.PNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L339",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),mo=new b({props:{name:"set_timesteps",anchor:"diffusers.PNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L147"}}),go=new b({props:{name:"step",anchor:"diffusers.PNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L186",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> or <code>tuple</code></p>
`}}),_o=new b({props:{name:"step_plms",anchor:"diffusers.PNDMScheduler.step_plms",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_plms.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_plms.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_plms.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_plms.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L272",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),vo=new b({props:{name:"step_prk",anchor:"diffusers.PNDMScheduler.step_prk",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_prk.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_prk.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_prk.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_prk.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L217",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),bo=new k({}),xo=new b({props:{name:"class diffusers.ScoreSdeVeScheduler",anchor:"diffusers.ScoreSdeVeScheduler",parameters:[{name:"num_train_timesteps",val:": int = 2000"},{name:"snr",val:": float = 0.15"},{name:"sigma_min",val:": float = 0.01"},{name:"sigma_max",val:": float = 1348.0"},{name:"sampling_eps",val:": float = 1e-05"},{name:"correct_steps",val:": int = 1"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.ScoreSdeVeScheduler.snr",description:`<strong>snr</strong> (<code>float</code>) &#x2014;
coefficient weighting the step from the model_output sample (from the network) to the random noise.`,name:"snr"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>) &#x2014;
initial noise scale for sigma sequence in sampling procedure. The minimum sigma should mirror the
distribution of the data.`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum value used for the range of continuous timesteps passed into the model.",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.sampling_eps",description:`<strong>sampling_eps</strong> (<code>float</code>) &#x2014; the end value of sampling, where timesteps decrease progressively from 1 to
epsilon. &#x2014;`,name:"sampling_eps"},{anchor:"diffusers.ScoreSdeVeScheduler.correct_steps",description:"<strong>correct_steps</strong> (<code>int</code>) &#x2014; number of correction steps performed on a produced sample.",name:"correct_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L45"}}),$o=new b({props:{name:"scale_model_input",anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L87",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Eo=new b({props:{name:"set_sigmas",anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas",parameters:[{name:"num_inference_steps",val:": int"},{name:"sigma_min",val:": float = None"},{name:"sigma_max",val:": float = None"},{name:"sampling_eps",val:": float = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>, optional) &#x2014;
initial noise scale value (overrides value given at Scheduler instantiation).`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>, optional) &#x2014; final noise scale value (overrides value given at Scheduler instantiation).",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L117"}}),yo=new b({props:{name:"set_timesteps",anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"sampling_eps",val:": float = None"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L101"}}),wo=new b({props:{name:"step_correct",anchor:"diffusers.ScoreSdeVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L212",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),Mo=new b({props:{name:"step_pred",anchor:"diffusers.ScoreSdeVeScheduler.step_pred",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L151",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),Po=new k({}),ko=new b({props:{name:"class diffusers.IPNDMScheduler",anchor:"diffusers.IPNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L24"}}),Co=new b({props:{name:"scale_model_input",anchor:"diffusers.IPNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L126",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Vo=new b({props:{name:"set_timesteps",anchor:"diffusers.IPNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L56"}}),Fo=new b({props:{name:"step",anchor:"diffusers.IPNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.IPNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.IPNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.IPNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L76",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),Io=new k({}),Or=new ix({props:{warning:!0,$$slots:{default:[jy]},$$scope:{ctx:$t}}}),Lo=new b({props:{name:"class diffusers.schedulers.ScoreSdeVpScheduler",anchor:"diffusers.schedulers.ScoreSdeVpScheduler",parameters:[{name:"num_train_timesteps",val:" = 2000"},{name:"beta_min",val:" = 0.1"},{name:"beta_max",val:" = 20"},{name:"sampling_eps",val:" = 0.001"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_vp.py#L26"}}),Ko=new k({}),Wo=new b({props:{name:"class diffusers.EulerDiscreteScheduler",anchor:"diffusers.EulerDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L48"}}),Ho=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L110",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Go=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L131"}}),zo=new b({props:{name:"step",anchor:"diffusers.EulerDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"s_churn",val:": float = 0.0"},{name:"s_tmin",val:": float = 0.0"},{name:"s_tmax",val:": float = inf"},{name:"s_noise",val:": float = 1.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_churn",description:"<strong>s_churn</strong> (<code>float</code>) &#x2014;",name:"s_churn"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmin",description:"<strong>s_tmin</strong>  (<code>float</code>) &#x2014;",name:"s_tmin"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmax",description:"<strong>s_tmax</strong>  (<code>float</code>) &#x2014;",name:"s_tmax"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_noise",description:"<strong>s_noise</strong> (<code>float</code>) &#x2014;",name:"s_noise"},{anchor:"diffusers.EulerDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L154",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a
<code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),Jo=new k({}),Yo=new b({props:{name:"class diffusers.EulerAncestralDiscreteScheduler",anchor:"diffusers.EulerAncestralDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L48"}}),Zo=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L109",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),tn=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L130"}}),rn=new b({props:{name:"step",anchor:"diffusers.EulerAncestralDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerAncestralDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L153",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise
a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),sn=new k({}),an=new b({props:{name:"class diffusers.VQDiffusionScheduler",anchor:"diffusers.VQDiffusionScheduler",parameters:[{name:"num_vec_classes",val:": int"},{name:"num_train_timesteps",val:": int = 100"},{name:"alpha_cum_start",val:": float = 0.99999"},{name:"alpha_cum_end",val:": float = 9e-06"},{name:"gamma_cum_start",val:": float = 9e-06"},{name:"gamma_cum_end",val:": float = 0.99999"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.num_vec_classes",description:`<strong>num_vec_classes</strong> (<code>int</code>) &#x2014;
The number of classes of the vector embeddings of the latent pixels. Includes the class for the masked
latent pixel.`,name:"num_vec_classes"},{anchor:"diffusers.VQDiffusionScheduler.num_train_timesteps",description:`<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014;
Number of diffusion steps used to train the model.`,name:"num_train_timesteps"},{anchor:"diffusers.VQDiffusionScheduler.alpha_cum_start",description:`<strong>alpha_cum_start</strong> (<code>float</code>) &#x2014;
The starting cumulative alpha value.`,name:"alpha_cum_start"},{anchor:"diffusers.VQDiffusionScheduler.alpha_cum_end",description:`<strong>alpha_cum_end</strong> (<code>float</code>) &#x2014;
The ending cumulative alpha value.`,name:"alpha_cum_end"},{anchor:"diffusers.VQDiffusionScheduler.gamma_cum_start",description:`<strong>gamma_cum_start</strong> (<code>float</code>) &#x2014;
The starting cumulative gamma value.`,name:"gamma_cum_start"},{anchor:"diffusers.VQDiffusionScheduler.gamma_cum_end",description:`<strong>gamma_cum_end</strong> (<code>float</code>) &#x2014;
The ending cumulative gamma value.`,name:"gamma_cum_end"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L106"}}),ln=new b({props:{name:"log_Q_t_transitioning_to_known_class",anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class",parameters:[{name:"t",val:": torch.int32"},{name:"x_t",val:": LongTensor"},{name:"log_onehot_x_t",val:": FloatTensor"},{name:"cumulative",val:": bool"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.t",description:`<strong>t</strong> (torch.Long) &#x2014;
The timestep that determines which transition matrix is used.`,name:"t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.x_t",description:`<strong>x_t</strong> (<code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>) &#x2014;
The classes of each latent pixel at time <code>t</code>.`,name:"x_t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.log_onehot_x_t",description:`<strong>log_onehot_x_t</strong> (<code>torch.FloatTensor</code> of shape <code>(batch size, num classes, num latent pixels)</code>) &#x2014;
The log one-hot vectors of <code>x_t</code>`,name:"log_onehot_x_t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.cumulative",description:`<strong>cumulative</strong> (<code>bool</code>) &#x2014;
If cumulative is <code>False</code>, we use the single step transition matrix <code>t-1</code>-&gt;<code>t</code>. If cumulative is <code>True</code>,
we use the cumulative transition matrix <code>0</code>-&gt;<code>t</code>.`,name:"cumulative"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L377",returnDescription:`
<p>Each <em>column</em> of the returned matrix is a <em>row</em> of log probabilities of the complete probability
transition matrix.</p>
<p>When non cumulative, returns <code>self.num_classes - 1</code> rows because the initial latent pixel cannot be
masked.</p>
<p>Where:</p>
<ul>
<li><code>q_n</code> is the probability distribution for the forward process of the <code>n</code>th latent pixel.</li>
<li>C_0 is a class of a latent pixel embedding</li>
<li>C_k is the class of the masked latent pixel</li>
</ul>
<p>non-cumulative result (omitting logarithms):</p>

	<CodeBlock 
		code={\`q_0(x_t | x_{t-1\\} = C_0) ... q_n(x_t | x_{t-1\\} = C_0)
          .      .                     .
          .               .            .
          .                      .     .
q_0(x_t | x_{t-1\\} = C_k) ... q_n(x_t | x_{t-1\\} = C_k)\`}
		highlighted={\`q<span class="hljs-constructor">_0(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_0)</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_0)</span>
          .      .                     .
          .               .            .
          .                      .     .
q<span class="hljs-constructor">_0(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_k)</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_k)</span>\`}
	/>
<p>cumulative result (omitting logarithms):</p>

	<CodeBlock 
		code={\`q_0_cumulative(x_t | x_0 = C_0)    ...  q_n_cumulative(x_t | x_0 = C_0)
          .               .                          .
          .                        .                 .
          .                               .          .
q_0_cumulative(x_t | x_0 = C_{k-1\\}) ... q_n_cumulative(x_t | x_0 = C_{k-1\\})\`}
		highlighted={\`q<span class="hljs-constructor">_0_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_0)</span><span class="hljs-operator">    ...  </span>q<span class="hljs-constructor">_n_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_0)</span>
          .               .                          .
          .                        .                 .
          .                               .          .
q<span class="hljs-constructor">_0_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_{<span class="hljs-params">k</span>-1\\})</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_{<span class="hljs-params">k</span>-1\\})</span>\`}
	/>
`,returnType:`
<p><code>torch.FloatTensor</code> of shape <code>(batch size, num classes - 1, num latent pixels)</code></p>
`}}),un=new b({props:{name:"q_posterior",anchor:"diffusers.VQDiffusionScheduler.q_posterior",parameters:[{name:"log_p_x_0",val:""},{name:"x_t",val:""},{name:"t",val:""}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.q_posterior.t",description:`<strong>t</strong> (torch.Long) &#x2014;
The timestep that determines which transition matrix is used.`,name:"t"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L258",returnDescription:`
<p>The log probabilities for the predicted classes of the image at timestep <code>t-1</code>. I.e. Equation (11).</p>
`,returnType:`
<p><code>torch.FloatTensor</code> of shape <code>(batch size, num classes, num latent pixels)</code></p>
`}}),hn=new b({props:{name:"set_timesteps",anchor:"diffusers.VQDiffusionScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.VQDiffusionScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014;
device to place the timesteps and the diffusion process parameters (alpha, beta, gamma) on.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L188"}}),mn=new b({props:{name:"step",anchor:"diffusers.VQDiffusionScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": torch.int64"},{name:"sample",val:": LongTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.step.t",description:`<strong>t</strong> (<code>torch.long</code>) &#x2014;
The timestep that determines which transition matrices are used.</p>
<p>x_t &#x2014; (<code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>):
The classes of each latent pixel at time <code>t</code></p>
<p>generator &#x2014; (<code>torch.Generator</code> or None):
RNG for the noise applied to p(x_{t-1} | x_t) before it is sampled from.`,name:"t"},{anchor:"diffusers.VQDiffusionScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014;
option for returning tuple rather than VQDiffusionSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L210",returnDescription:`
<p><code>~schedulers.scheduling_utils.VQDiffusionSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.VQDiffusionSchedulerOutput</code> or <code>tuple</code></p>
`}}),_n=new k({}),Sn=new b({props:{name:"class diffusers.RePaintScheduler",anchor:"diffusers.RePaintScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"eta",val:": float = 0.0"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.RePaintScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.RePaintScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.RePaintScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.RePaintScheduler.eta",description:`<strong>eta</strong> (<code>float</code>) &#x2014;
The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 -0.0 is DDIM and
1.0 is DDPM scheduler respectively.`,name:"eta"},{anchor:"diffusers.RePaintScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.RePaintScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.RePaintScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L74"}}),Dn=new b({props:{name:"scale_model_input",anchor:"diffusers.RePaintScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.RePaintScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L150",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),$n=new b({props:{name:"step",anchor:"diffusers.RePaintScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"original_image",val:": FloatTensor"},{name:"mask",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.step.model_output",description:`<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned
diffusion model.`,name:"model_output"},{anchor:"diffusers.RePaintScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.RePaintScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.RePaintScheduler.step.original_image",description:`<strong>original_image</strong> (<code>torch.FloatTensor</code>) &#x2014;
the original image to inpaint on.`,name:"original_image"},{anchor:"diffusers.RePaintScheduler.step.mask",description:`<strong>mask</strong> (<code>torch.FloatTensor</code>) &#x2014;
the mask where 0.0 values define which part of the original image to inpaint (change).`,name:"mask"},{anchor:"diffusers.RePaintScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014; random number generator.",name:"generator"},{anchor:"diffusers.RePaintScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than
DDPMSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L213",returnDescription:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> or <code>tuple</code></p>
`}}),{c(){S=n("meta"),oe=l(),D=n("h1"),P=n("a"),we=n("span"),f(T.$$.fragment),be=l(),Pe=n("span"),ze=s("Schedulers"),Me=l(),Je=n("p"),Mn=s("Diffusers contains multiple pre-built schedule functions for the diffusion process."),kc=l(),Ye=n("h2"),Et=n("a"),ii=n("span"),f(Xr.$$.fragment),uf=l(),di=n("span"),pf=s("What is a scheduler?"),Oc=l(),qe=n("p"),ff=s("The schedule functions, denoted "),li=n("em"),hf=s("Schedulers"),mf=s(" in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample. That\u2019s why schedulers may also be called "),ci=n("em"),gf=s("Samplers"),_f=s(" in other diffusion models implementations."),Ac=l(),yt=n("ul"),Pn=n("li"),vf=s("Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),Zr=n("ul"),ui=n("li"),bf=s("adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),Sf=l(),pi=n("li"),xf=s("for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),Df=l(),Xe=n("li"),$f=s("Schedulers are often defined by a "),fi=n("em"),Ef=s("noise schedule"),yf=s(" and an "),hi=n("em"),wf=s("update rule"),Mf=s(" to solve the differential equation solution."),Cc=l(),Ze=n("h3"),wt=n("a"),mi=n("span"),f(es.$$.fragment),Pf=l(),gi=n("span"),Tf=s("Discrete versus continuous schedulers"),Vc=l(),I=n("p"),kf=s(`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),_i=n("code"),Of=s("int"),Af=s(" inputs), such as the "),Tn=n("a"),Cf=s("DDPMScheduler"),Vf=s(" or "),kn=n("a"),Ff=s("PNDMScheduler"),If=s(", and continuous (accepting "),vi=n("code"),Nf=s("float"),Lf=s(" inputs), such as the score-based schedulers "),On=n("a"),qf=s("ScoreSdeVeScheduler"),Kf=s(" or "),bi=n("code"),Rf=s("ScoreSdeVpScheduler"),Qf=s("."),Fc=l(),et=n("h2"),Mt=n("a"),Si=n("span"),f(ts.$$.fragment),Wf=l(),xi=n("span"),jf=s("Designing Re-usable schedulers"),Ic=l(),An=n("p"),Uf=s(`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),Nc=l(),Pt=n("ul"),Di=n("li"),Hf=s("Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),Bf=l(),$i=n("li"),Gf=s("Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),Lc=l(),tt=n("h2"),Tt=n("a"),Ei=n("span"),f(rs.$$.fragment),zf=l(),yi=n("span"),Jf=s("API"),qc=l(),Cn=n("p"),Yf=s("The core API for any new scheduler must follow a limited structure."),Kc=l(),Ke=n("ul"),ss=n("li"),Xf=s("Schedulers should provide one or more "),wi=n("code"),Zf=s("def step(...)"),eh=s(" functions that should be called to update the generated sample iteratively."),th=l(),os=n("li"),rh=s("Schedulers should provide a "),Mi=n("code"),sh=s("set_timesteps(...)"),oh=s(" method that configures the parameters of a schedule function for a specific inference task."),nh=l(),Pi=n("li"),ah=s("Schedulers should be framework-specific."),Rc=l(),kt=n("p"),ih=s("The base class "),Vn=n("a"),dh=s("SchedulerMixin"),lh=s(" implements low level utilities used by multiple schedulers."),Qc=l(),rt=n("h3"),Ot=n("a"),Ti=n("span"),f(ns.$$.fragment),ch=l(),ki=n("span"),uh=s("SchedulerMixin"),Wc=l(),ee=n("div"),f(as.$$.fragment),ph=l(),Oi=n("p"),fh=s("Mixin containing common functions for the schedulers."),hh=l(),Ai=n("p"),mh=s("Class attributes:"),gh=l(),Ci=n("ul"),Re=n("li"),Vi=n("strong"),_h=s("_compatibles"),vh=s(" ("),Fi=n("code"),bh=s("List[str]"),Sh=s(`) \u2014 A list of classes that are compatible with the parent class, so that
`),Ii=n("code"),xh=s("from_config"),Dh=s(` can be used from a class different than the one used to save the config (should be overridden
by parent class).`),$h=l(),Te=n("div"),f(is.$$.fragment),Eh=l(),Ni=n("p"),yh=s("Instantiate a Scheduler class from a pre-defined JSON configuration file inside a directory or Hub repo."),wh=l(),f(At.$$.fragment),Mh=l(),f(Ct.$$.fragment),Ph=l(),Vt=n("div"),f(ds.$$.fragment),Th=l(),st=n("p"),kh=s("Save a scheduler configuration object to the directory "),Li=n("code"),Oh=s("save_directory"),Ah=s(`, so that it can be re-loaded using the
`),Fn=n("a"),Ch=s("from_pretrained()"),Vh=s(" class method."),jc=l(),ot=n("h3"),Ft=n("a"),qi=n("span"),f(ls.$$.fragment),Fh=l(),Ki=n("span"),Ih=s("SchedulerOutput"),Uc=s("\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),nt=n("div"),f(cs.$$.fragment),Nh=l(),Ri=n("p"),Lh=s("Base class for the scheduler\u2019s step function output."),Hc=l(),at=n("h3"),It=n("a"),Qi=n("span"),f(us.$$.fragment),qh=l(),Wi=n("span"),Kh=s("Implemented Schedulers"),Bc=l(),it=n("h4"),Nt=n("a"),ji=n("span"),f(ps.$$.fragment),Rh=l(),Ui=n("span"),Qh=s("Denoising diffusion implicit models (DDIM)"),Gc=l(),In=n("p"),Wh=s("Original paper can be found here."),zc=l(),A=n("div"),f(fs.$$.fragment),jh=l(),Hi=n("p"),Uh=s(`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),Hh=l(),N=n("p"),Nn=n("a"),Bh=s("~ConfigMixin"),Gh=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Bi=n("code"),zh=s("__init__"),Jh=s(`
function, such as `),Gi=n("code"),Yh=s("num_train_timesteps"),Xh=s(". They can be accessed via "),zi=n("code"),Zh=s("scheduler.config.num_train_timesteps"),em=s(`.
`),Ln=n("a"),tm=s("SchedulerMixin"),rm=s(" provides general loading and saving functionality via the "),qn=n("a"),sm=s("SchedulerMixin.save_pretrained()"),om=s(` and
`),Kn=n("a"),nm=s("from_pretrained()"),am=s(" functions."),im=l(),Rn=n("p"),dm=s("For more details, see the original paper: "),hs=n("a"),lm=s("https://arxiv.org/abs/2010.02502"),cm=l(),Lt=n("div"),f(ms.$$.fragment),um=l(),Ji=n("p"),pm=s(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),fm=l(),qt=n("div"),f(gs.$$.fragment),hm=l(),Yi=n("p"),mm=s("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),gm=l(),Kt=n("div"),f(_s.$$.fragment),_m=l(),Xi=n("p"),vm=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Jc=l(),dt=n("h4"),Rt=n("a"),Zi=n("span"),f(vs.$$.fragment),bm=l(),ed=n("span"),Sm=s("Denoising diffusion probabilistic models (DDPM)"),Yc=l(),Qt=n("p"),xm=s("Original paper can be found "),bs=n("a"),Dm=s("here"),$m=s("."),Xc=l(),C=n("div"),f(Ss.$$.fragment),Em=l(),td=n("p"),ym=s(`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),wm=l(),L=n("p"),Qn=n("a"),Mm=s("~ConfigMixin"),Pm=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),rd=n("code"),Tm=s("__init__"),km=s(`
function, such as `),sd=n("code"),Om=s("num_train_timesteps"),Am=s(". They can be accessed via "),od=n("code"),Cm=s("scheduler.config.num_train_timesteps"),Vm=s(`.
`),Wn=n("a"),Fm=s("SchedulerMixin"),Im=s(" provides general loading and saving functionality via the "),jn=n("a"),Nm=s("SchedulerMixin.save_pretrained()"),Lm=s(` and
`),Un=n("a"),qm=s("from_pretrained()"),Km=s(" functions."),Rm=l(),Hn=n("p"),Qm=s("For more details, see the original paper: "),xs=n("a"),Wm=s("https://arxiv.org/abs/2006.11239"),jm=l(),Wt=n("div"),f(Ds.$$.fragment),Um=l(),nd=n("p"),Hm=s(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Bm=l(),jt=n("div"),f($s.$$.fragment),Gm=l(),ad=n("p"),zm=s("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Jm=l(),Ut=n("div"),f(Es.$$.fragment),Ym=l(),id=n("p"),Xm=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Zc=l(),lt=n("h4"),Ht=n("a"),dd=n("span"),f(ys.$$.fragment),Zm=l(),ld=n("span"),eg=s("Multistep DPM-Solver"),eu=l(),ke=n("p"),tg=s("Original paper can be found "),ws=n("a"),rg=s("here"),sg=s(" and the "),Ms=n("a"),og=s("improved version"),ng=s(". The original implementation can be found "),Ps=n("a"),ag=s("here"),ig=s("."),tu=l(),x=n("div"),f(Ts.$$.fragment),dg=l(),cd=n("p"),lg=s(`DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with
the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality
samples, and it can generate quite good samples even in only 10 steps.`),cg=l(),Bt=n("p"),ug=s("For more details, see the original paper: "),ks=n("a"),pg=s("https://arxiv.org/abs/2206.00927"),fg=s(" and "),Os=n("a"),hg=s("https://arxiv.org/abs/2211.01095"),mg=l(),ct=n("p"),gg=s(`Currently, we support the multistep DPM-Solver for both noise prediction models and data prediction models. We
recommend to use `),ud=n("code"),_g=s("solver_order=2"),vg=s(" for guided sampling, and "),pd=n("code"),bg=s("solver_order=3"),Sg=s(" for unconditional sampling."),xg=l(),Ie=n("p"),Dg=s("We also support the \u201Cdynamic thresholding\u201D method in Imagen ("),As=n("a"),$g=s("https://arxiv.org/abs/2205.11487"),Eg=s(`). For pixel-space
diffusion models, you can set both `),fd=n("code"),yg=s('algorithm_type="dpmsolver++"'),wg=s(" and "),hd=n("code"),Mg=s("thresholding=True"),Pg=s(` to use the dynamic
thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as
stable-diffusion).`),Tg=l(),q=n("p"),Bn=n("a"),kg=s("~ConfigMixin"),Og=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),md=n("code"),Ag=s("__init__"),Cg=s(`
function, such as `),gd=n("code"),Vg=s("num_train_timesteps"),Fg=s(". They can be accessed via "),_d=n("code"),Ig=s("scheduler.config.num_train_timesteps"),Ng=s(`.
`),Gn=n("a"),Lg=s("SchedulerMixin"),qg=s(" provides general loading and saving functionality via the "),zn=n("a"),Kg=s("SchedulerMixin.save_pretrained()"),Rg=s(` and
`),Jn=n("a"),Qg=s("from_pretrained()"),Wg=s(" functions."),jg=l(),Oe=n("div"),f(Cs.$$.fragment),Ug=l(),vd=n("p"),Hg=s("Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs."),Bg=l(),bd=n("p"),Gg=s(`DPM-Solver is designed to discretize an integral of the noise prediciton model, and DPM-Solver++ is designed to
discretize an integral of the data prediction model. So we need to first convert the model output to the
corresponding type to match the algorithm.`),zg=l(),Sd=n("p"),Jg=s(`Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or
DPM-Solver++ for both noise prediction model and data prediction model.`),Yg=l(),Qe=n("div"),f(Vs.$$.fragment),Xg=l(),xd=n("p"),Zg=s("One step for the first-order DPM-Solver (equivalent to DDIM)."),e_=l(),Fs=n("p"),t_=s("See "),Is=n("a"),r_=s("https://arxiv.org/abs/2206.00927"),s_=s(" for the detailed derivation."),o_=l(),Gt=n("div"),f(Ns.$$.fragment),n_=l(),Dd=n("p"),a_=s("One step for the second-order multistep DPM-Solver."),i_=l(),zt=n("div"),f(Ls.$$.fragment),d_=l(),$d=n("p"),l_=s("One step for the third-order multistep DPM-Solver."),c_=l(),Jt=n("div"),f(qs.$$.fragment),u_=l(),Ed=n("p"),p_=s(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),f_=l(),Yt=n("div"),f(Ks.$$.fragment),h_=l(),yd=n("p"),m_=s("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),g_=l(),Xt=n("div"),f(Rs.$$.fragment),__=l(),wd=n("p"),v_=s("Step function propagating the sample with the multistep DPM-Solver."),ru=l(),ut=n("h4"),Zt=n("a"),Md=n("span"),f(Qs.$$.fragment),b_=l(),Pd=n("span"),S_=s("Variance exploding, stochastic sampling from Karras et. al"),su=l(),er=n("p"),x_=s("Original paper can be found "),Ws=n("a"),D_=s("here"),$_=s("."),ou=l(),E=n("div"),f(js.$$.fragment),E_=l(),Td=n("p"),y_=s(`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),w_=l(),tr=n("p"),M_=s(`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Us=n("a"),P_=s("https://arxiv.org/abs/2206.00364"),T_=s(` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Hs=n("a"),k_=s("https://arxiv.org/abs/2011.13456"),O_=l(),K=n("p"),Yn=n("a"),A_=s("~ConfigMixin"),C_=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),kd=n("code"),V_=s("__init__"),F_=s(`
function, such as `),Od=n("code"),I_=s("num_train_timesteps"),N_=s(". They can be accessed via "),Ad=n("code"),L_=s("scheduler.config.num_train_timesteps"),q_=s(`.
`),Xn=n("a"),K_=s("SchedulerMixin"),R_=s(" provides general loading and saving functionality via the "),Zn=n("a"),Q_=s("SchedulerMixin.save_pretrained()"),W_=s(` and
`),ea=n("a"),j_=s("from_pretrained()"),U_=s(" functions."),H_=l(),Bs=n("p"),B_=s(`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),Gs=n("a"),G_=s("https://arxiv.org/abs/2206.00364"),z_=s(`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),J_=l(),We=n("div"),f(zs.$$.fragment),Y_=l(),Cd=n("p"),X_=s(`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),Z_=l(),Vd=n("p"),ev=s("TODO Args:"),tv=l(),rr=n("div"),f(Js.$$.fragment),rv=l(),Fd=n("p"),sv=s(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),ov=l(),sr=n("div"),f(Ys.$$.fragment),nv=l(),Id=n("p"),av=s("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),iv=l(),or=n("div"),f(Xs.$$.fragment),dv=l(),Nd=n("p"),lv=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),cv=l(),nr=n("div"),f(Zs.$$.fragment),uv=l(),Ld=n("p"),pv=s("Correct the predicted sample based on the output model_output of the network. TODO complete description"),nu=l(),pt=n("h4"),ar=n("a"),qd=n("span"),f(eo.$$.fragment),fv=l(),Kd=n("span"),hv=s("Linear multistep scheduler for discrete beta schedules"),au=l(),ir=n("p"),mv=s("Original implementation can be found "),to=n("a"),gv=s("here"),_v=s("."),iu=l(),V=n("div"),f(ro.$$.fragment),vv=l(),ta=n("p"),bv=s(`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),so=n("a"),Sv=s("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),xv=l(),R=n("p"),ra=n("a"),Dv=s("~ConfigMixin"),$v=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Rd=n("code"),Ev=s("__init__"),yv=s(`
function, such as `),Qd=n("code"),wv=s("num_train_timesteps"),Mv=s(". They can be accessed via "),Wd=n("code"),Pv=s("scheduler.config.num_train_timesteps"),Tv=s(`.
`),sa=n("a"),kv=s("SchedulerMixin"),Ov=s(" provides general loading and saving functionality via the "),oa=n("a"),Av=s("SchedulerMixin.save_pretrained()"),Cv=s(` and
`),na=n("a"),Vv=s("from_pretrained()"),Fv=s(" functions."),Iv=l(),dr=n("div"),f(oo.$$.fragment),Nv=l(),jd=n("p"),Lv=s("Compute a linear multistep coefficient."),qv=l(),lr=n("div"),f(no.$$.fragment),Kv=l(),ao=n("p"),Rv=s("Scales the denoising model input by "),Ud=n("code"),Qv=s("(sigma**2 + 1) ** 0.5"),Wv=s(" to match the K-LMS algorithm."),jv=l(),cr=n("div"),f(io.$$.fragment),Uv=l(),Hd=n("p"),Hv=s("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),Bv=l(),ur=n("div"),f(lo.$$.fragment),Gv=l(),Bd=n("p"),zv=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),du=l(),ft=n("h4"),pr=n("a"),Gd=n("span"),f(co.$$.fragment),Jv=l(),zd=n("span"),Yv=s("Pseudo numerical methods for diffusion models (PNDM)"),lu=l(),fr=n("p"),Xv=s("Original implementation can be found "),uo=n("a"),Zv=s("here"),eb=s("."),cu=l(),y=n("div"),f(po.$$.fragment),tb=l(),Jd=n("p"),rb=s(`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),sb=l(),Q=n("p"),aa=n("a"),ob=s("~ConfigMixin"),nb=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Yd=n("code"),ab=s("__init__"),ib=s(`
function, such as `),Xd=n("code"),db=s("num_train_timesteps"),lb=s(". They can be accessed via "),Zd=n("code"),cb=s("scheduler.config.num_train_timesteps"),ub=s(`.
`),ia=n("a"),pb=s("SchedulerMixin"),fb=s(" provides general loading and saving functionality via the "),da=n("a"),hb=s("SchedulerMixin.save_pretrained()"),mb=s(` and
`),la=n("a"),gb=s("from_pretrained()"),_b=s(" functions."),vb=l(),ca=n("p"),bb=s("For more details, see the original paper: "),fo=n("a"),Sb=s("https://arxiv.org/abs/2202.09778"),xb=l(),hr=n("div"),f(ho.$$.fragment),Db=l(),el=n("p"),$b=s(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Eb=l(),mr=n("div"),f(mo.$$.fragment),yb=l(),tl=n("p"),wb=s("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Mb=l(),je=n("div"),f(go.$$.fragment),Pb=l(),rl=n("p"),Tb=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),kb=l(),Ne=n("p"),Ob=s("This function calls "),sl=n("code"),Ab=s("step_prk()"),Cb=s(" or "),ol=n("code"),Vb=s("step_plms()"),Fb=s(" depending on the internal variable "),nl=n("code"),Ib=s("counter"),Nb=s("."),Lb=l(),gr=n("div"),f(_o.$$.fragment),qb=l(),al=n("p"),Kb=s(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),Rb=l(),_r=n("div"),f(vo.$$.fragment),Qb=l(),il=n("p"),Wb=s(`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),uu=l(),ht=n("h4"),vr=n("a"),dl=n("span"),f(bo.$$.fragment),jb=l(),ll=n("span"),Ub=s("variance exploding stochastic differential equation (VE-SDE) scheduler"),pu=l(),br=n("p"),Hb=s("Original paper can be found "),So=n("a"),Bb=s("here"),Gb=s("."),fu=l(),w=n("div"),f(xo.$$.fragment),zb=l(),cl=n("p"),Jb=s("The variance exploding stochastic differential equation (SDE) scheduler."),Yb=l(),ua=n("p"),Xb=s("For more information, see the original paper: "),Do=n("a"),Zb=s("https://arxiv.org/abs/2011.13456"),e1=l(),W=n("p"),pa=n("a"),t1=s("~ConfigMixin"),r1=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ul=n("code"),s1=s("__init__"),o1=s(`
function, such as `),pl=n("code"),n1=s("num_train_timesteps"),a1=s(". They can be accessed via "),fl=n("code"),i1=s("scheduler.config.num_train_timesteps"),d1=s(`.
`),fa=n("a"),l1=s("SchedulerMixin"),c1=s(" provides general loading and saving functionality via the "),ha=n("a"),u1=s("SchedulerMixin.save_pretrained()"),p1=s(` and
`),ma=n("a"),f1=s("from_pretrained()"),h1=s(" functions."),m1=l(),Sr=n("div"),f($o.$$.fragment),g1=l(),hl=n("p"),_1=s(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),v1=l(),Ue=n("div"),f(Eo.$$.fragment),b1=l(),ml=n("p"),S1=s("Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),x1=l(),mt=n("p"),D1=s("The sigmas control the weight of the "),gl=n("code"),$1=s("drift"),E1=s(" and "),_l=n("code"),y1=s("diffusion"),w1=s(" components of sample update."),M1=l(),xr=n("div"),f(yo.$$.fragment),P1=l(),vl=n("p"),T1=s("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),k1=l(),Dr=n("div"),f(wo.$$.fragment),O1=l(),bl=n("p"),A1=s(`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),C1=l(),$r=n("div"),f(Mo.$$.fragment),V1=l(),Sl=n("p"),F1=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),hu=l(),gt=n("h4"),Er=n("a"),xl=n("span"),f(Po.$$.fragment),I1=l(),Dl=n("span"),N1=s("improved pseudo numerical methods for diffusion models (iPNDM)"),mu=l(),yr=n("p"),L1=s("Original implementation can be found "),To=n("a"),q1=s("here"),K1=s("."),gu=l(),F=n("div"),f(ko.$$.fragment),R1=l(),ga=n("p"),Q1=s(`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),Oo=n("a"),W1=s("library"),j1=l(),j=n("p"),_a=n("a"),U1=s("~ConfigMixin"),H1=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),$l=n("code"),B1=s("__init__"),G1=s(`
function, such as `),El=n("code"),z1=s("num_train_timesteps"),J1=s(". They can be accessed via "),yl=n("code"),Y1=s("scheduler.config.num_train_timesteps"),X1=s(`.
`),va=n("a"),Z1=s("SchedulerMixin"),e0=s(" provides general loading and saving functionality via the "),ba=n("a"),t0=s("SchedulerMixin.save_pretrained()"),r0=s(` and
`),Sa=n("a"),s0=s("from_pretrained()"),o0=s(" functions."),n0=l(),xa=n("p"),a0=s("For more details, see the original paper: "),Ao=n("a"),i0=s("https://arxiv.org/abs/2202.09778"),d0=l(),wr=n("div"),f(Co.$$.fragment),l0=l(),wl=n("p"),c0=s(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),u0=l(),Mr=n("div"),f(Vo.$$.fragment),p0=l(),Ml=n("p"),f0=s("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),h0=l(),Pr=n("div"),f(Fo.$$.fragment),m0=l(),Pl=n("p"),g0=s(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),_u=l(),_t=n("h4"),Tr=n("a"),Tl=n("span"),f(Io.$$.fragment),_0=l(),kl=n("span"),v0=s("variance preserving stochastic differential equation (VP-SDE) scheduler"),vu=l(),kr=n("p"),b0=s("Original paper can be found "),No=n("a"),S0=s("here"),x0=s("."),bu=l(),f(Or.$$.fragment),Su=l(),ne=n("div"),f(Lo.$$.fragment),D0=l(),Ol=n("p"),$0=s("The variance preserving stochastic differential equation (SDE) scheduler."),E0=l(),U=n("p"),Da=n("a"),y0=s("~ConfigMixin"),w0=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Al=n("code"),M0=s("__init__"),P0=s(`
function, such as `),Cl=n("code"),T0=s("num_train_timesteps"),k0=s(". They can be accessed via "),Vl=n("code"),O0=s("scheduler.config.num_train_timesteps"),A0=s(`.
`),$a=n("a"),C0=s("SchedulerMixin"),V0=s(" provides general loading and saving functionality via the "),Ea=n("a"),F0=s("SchedulerMixin.save_pretrained()"),I0=s(` and
`),ya=n("a"),N0=s("from_pretrained()"),L0=s(" functions."),q0=l(),wa=n("p"),K0=s("For more information, see the original paper: "),qo=n("a"),R0=s("https://arxiv.org/abs/2011.13456"),Q0=l(),Fl=n("p"),W0=s("UNDER CONSTRUCTION"),xu=l(),vt=n("h4"),Ar=n("a"),Il=n("span"),f(Ko.$$.fragment),j0=l(),Nl=n("span"),U0=s("Euler scheduler"),Du=l(),He=n("p"),H0=s("Euler scheduler (Algorithm 2) from the paper "),Ro=n("a"),B0=s("Elucidating the Design Space of Diffusion-Based Generative Models"),G0=s(" by Karras et al. (2022). Based on the original "),Qo=n("a"),z0=s("k-diffusion"),J0=s(` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),$u=l(),te=n("div"),f(Wo.$$.fragment),Y0=l(),Cr=n("p"),X0=s("Euler scheduler (Algorithm 2) from Karras et al. (2022) "),jo=n("a"),Z0=s("https://arxiv.org/abs/2206.00364"),e2=s(`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),Uo=n("a"),t2=s("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),r2=l(),H=n("p"),Ma=n("a"),s2=s("~ConfigMixin"),o2=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ll=n("code"),n2=s("__init__"),a2=s(`
function, such as `),ql=n("code"),i2=s("num_train_timesteps"),d2=s(". They can be accessed via "),Kl=n("code"),l2=s("scheduler.config.num_train_timesteps"),c2=s(`.
`),Pa=n("a"),u2=s("SchedulerMixin"),p2=s(" provides general loading and saving functionality via the "),Ta=n("a"),f2=s("SchedulerMixin.save_pretrained()"),h2=s(` and
`),ka=n("a"),m2=s("from_pretrained()"),g2=s(" functions."),_2=l(),Vr=n("div"),f(Ho.$$.fragment),v2=l(),Bo=n("p"),b2=s("Scales the denoising model input by "),Rl=n("code"),S2=s("(sigma**2 + 1) ** 0.5"),x2=s(" to match the Euler algorithm."),D2=l(),Fr=n("div"),f(Go.$$.fragment),$2=l(),Ql=n("p"),E2=s("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),y2=l(),Ir=n("div"),f(zo.$$.fragment),w2=l(),Wl=n("p"),M2=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Eu=l(),bt=n("h4"),Nr=n("a"),jl=n("span"),f(Jo.$$.fragment),P2=l(),Ul=n("span"),T2=s("Euler Ancestral scheduler"),yu=l(),Oa=n("p"),k2=s(`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),wu=l(),re=n("div"),f(Yo.$$.fragment),O2=l(),Aa=n("p"),A2=s(`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Xo=n("a"),C2=s("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),V2=l(),B=n("p"),Ca=n("a"),F2=s("~ConfigMixin"),I2=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Hl=n("code"),N2=s("__init__"),L2=s(`
function, such as `),Bl=n("code"),q2=s("num_train_timesteps"),K2=s(". They can be accessed via "),Gl=n("code"),R2=s("scheduler.config.num_train_timesteps"),Q2=s(`.
`),Va=n("a"),W2=s("SchedulerMixin"),j2=s(" provides general loading and saving functionality via the "),Fa=n("a"),U2=s("SchedulerMixin.save_pretrained()"),H2=s(` and
`),Ia=n("a"),B2=s("from_pretrained()"),G2=s(" functions."),z2=l(),Lr=n("div"),f(Zo.$$.fragment),J2=l(),en=n("p"),Y2=s("Scales the denoising model input by "),zl=n("code"),X2=s("(sigma**2 + 1) ** 0.5"),Z2=s(" to match the Euler algorithm."),eS=l(),qr=n("div"),f(tn.$$.fragment),tS=l(),Jl=n("p"),rS=s("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),sS=l(),Kr=n("div"),f(rn.$$.fragment),oS=l(),Yl=n("p"),nS=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Mu=l(),St=n("h4"),Rr=n("a"),Xl=n("span"),f(sn.$$.fragment),aS=l(),Zl=n("span"),iS=s("VQDiffusionScheduler"),Pu=l(),on=n("p"),dS=s("Original paper can be found "),nn=n("a"),lS=s("here"),Tu=l(),M=n("div"),f(an.$$.fragment),cS=l(),ec=n("p"),uS=s("The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image."),pS=l(),tc=n("p"),fS=s(`The VQ-diffusion scheduler converts the transformer\u2019s output into a sample for the unnoised image at the previous
diffusion timestep.`),hS=l(),G=n("p"),Na=n("a"),mS=s("~ConfigMixin"),gS=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),rc=n("code"),_S=s("__init__"),vS=s(`
function, such as `),sc=n("code"),bS=s("num_train_timesteps"),SS=s(". They can be accessed via "),oc=n("code"),xS=s("scheduler.config.num_train_timesteps"),DS=s(`.
`),La=n("a"),$S=s("SchedulerMixin"),ES=s(" provides general loading and saving functionality via the "),qa=n("a"),yS=s("SchedulerMixin.save_pretrained()"),wS=s(` and
`),Ka=n("a"),MS=s("from_pretrained()"),PS=s(" functions."),TS=l(),Ra=n("p"),kS=s("For more details, see the original paper: "),dn=n("a"),OS=s("https://arxiv.org/abs/2111.14822"),AS=l(),Be=n("div"),f(ln.$$.fragment),CS=l(),cn=n("p"),VS=s(`Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each
latent pixel in `),nc=n("code"),FS=s("x_t"),IS=s("."),NS=l(),ac=n("p"),LS=s(`See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix
is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.`),qS=l(),z=n("div"),f(un.$$.fragment),KS=l(),pn=n("p"),RS=s("Calculates the log probabilities for the predicted classes of the image at timestep "),ic=n("code"),QS=s("t-1"),WS=s(". I.e. Equation (11)."),jS=l(),dc=n("p"),US=s(`Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only
forward probabilities.`),HS=l(),lc=n("p"),BS=s("Equation (11) stated in terms of forward probabilities via Equation (5):"),GS=l(),cc=n("p"),zS=s("Where:"),JS=l(),uc=n("ul"),fn=n("li"),YS=s("the sum is over x"),pc=n("em"),XS=s("0 = {C_0 \u2026 C"),ZS=s("{k-1}} (classes for x_0)"),e4=l(),xt=n("p"),t4=s("p(x"),fc=n("em"),r4=s("{t-1} | x_t) = sum( q(x_t | x"),s4=s("{t-1}) "),hc=n("em"),o4=s("q(x_{t-1} | x_0)"),n4=s(" p(x_0) / q(x_t | x_0) )"),a4=l(),Qr=n("div"),f(hn.$$.fragment),i4=l(),mc=n("p"),d4=s("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),l4=l(),Wr=n("div"),f(mn.$$.fragment),c4=l(),gn=n("p"),u4=s(`Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the
docstring for `),gc=n("code"),p4=s("self.q_posterior"),f4=s(" for more in depth docs on how Equation (11) is computed."),ku=l(),Dt=n("h4"),jr=n("a"),_c=n("span"),f(_n.$$.fragment),h4=l(),vc=n("span"),m4=s("RePaint scheduler"),Ou=l(),Le=n("p"),g4=s(`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),Qa=n("a"),_4=s("RePaintPipeline"),v4=s(`.
Based on the paper `),vn=n("a"),b4=s("RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),S4=s(`
and the original implementation by Andreas Lugmayr et al.: `),bn=n("a"),x4=s("https://github.com/andreas128/RePaint"),Au=l(),se=n("div"),f(Sn.$$.fragment),D4=l(),bc=n("p"),$4=s("RePaint is a schedule for DDPM inpainting inside a given mask."),E4=l(),J=n("p"),Wa=n("a"),y4=s("~ConfigMixin"),w4=s(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Sc=n("code"),M4=s("__init__"),P4=s(`
function, such as `),xc=n("code"),T4=s("num_train_timesteps"),k4=s(". They can be accessed via "),Dc=n("code"),O4=s("scheduler.config.num_train_timesteps"),A4=s(`.
`),ja=n("a"),C4=s("SchedulerMixin"),V4=s(" provides general loading and saving functionality via the "),Ua=n("a"),F4=s("SchedulerMixin.save_pretrained()"),I4=s(` and
`),Ha=n("a"),N4=s("from_pretrained()"),L4=s(" functions."),q4=l(),Ba=n("p"),K4=s("For more details, see the original paper: "),xn=n("a"),R4=s("https://arxiv.org/pdf/2201.09865.pdf"),Q4=l(),Ur=n("div"),f(Dn.$$.fragment),W4=l(),$c=n("p"),j4=s(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),U4=l(),Hr=n("div"),f($n.$$.fragment),H4=l(),Ec=n("p"),B4=s(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),this.h()},l(r){const u=Ky('[data-svelte="svelte-1phssyn"]',document.head);S=a(u,"META",{name:!0,content:!0}),u.forEach(t),oe=c(r),D=a(r,"H1",{class:!0});var En=i(D);P=a(En,"A",{id:!0,class:!0,href:!0});var yc=i(P);we=a(yc,"SPAN",{});var wc=i(we);h(T.$$.fragment,wc),wc.forEach(t),yc.forEach(t),be=c(En),Pe=a(En,"SPAN",{});var dx=i(Pe);ze=o(dx,"Schedulers"),dx.forEach(t),En.forEach(t),Me=c(r),Je=a(r,"P",{});var lx=i(Je);Mn=o(lx,"Diffusers contains multiple pre-built schedule functions for the diffusion process."),lx.forEach(t),kc=c(r),Ye=a(r,"H2",{class:!0});var Vu=i(Ye);Et=a(Vu,"A",{id:!0,class:!0,href:!0});var cx=i(Et);ii=a(cx,"SPAN",{});var ux=i(ii);h(Xr.$$.fragment,ux),ux.forEach(t),cx.forEach(t),uf=c(Vu),di=a(Vu,"SPAN",{});var px=i(di);pf=o(px,"What is a scheduler?"),px.forEach(t),Vu.forEach(t),Oc=c(r),qe=a(r,"P",{});var Ga=i(qe);ff=o(Ga,"The schedule functions, denoted "),li=a(Ga,"EM",{});var fx=i(li);hf=o(fx,"Schedulers"),fx.forEach(t),mf=o(Ga," in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample. That\u2019s why schedulers may also be called "),ci=a(Ga,"EM",{});var hx=i(ci);gf=o(hx,"Samplers"),hx.forEach(t),_f=o(Ga," in other diffusion models implementations."),Ga.forEach(t),Ac=c(r),yt=a(r,"UL",{});var Fu=i(yt);Pn=a(Fu,"LI",{});var G4=i(Pn);vf=o(G4,"Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),Zr=a(G4,"UL",{});var Iu=i(Zr);ui=a(Iu,"LI",{});var mx=i(ui);bf=o(mx,"adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),mx.forEach(t),Sf=c(Iu),pi=a(Iu,"LI",{});var gx=i(pi);xf=o(gx,"for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),gx.forEach(t),Iu.forEach(t),G4.forEach(t),Df=c(Fu),Xe=a(Fu,"LI",{});var za=i(Xe);$f=o(za,"Schedulers are often defined by a "),fi=a(za,"EM",{});var _x=i(fi);Ef=o(_x,"noise schedule"),_x.forEach(t),yf=o(za," and an "),hi=a(za,"EM",{});var vx=i(hi);wf=o(vx,"update rule"),vx.forEach(t),Mf=o(za," to solve the differential equation solution."),za.forEach(t),Fu.forEach(t),Cc=c(r),Ze=a(r,"H3",{class:!0});var Nu=i(Ze);wt=a(Nu,"A",{id:!0,class:!0,href:!0});var bx=i(wt);mi=a(bx,"SPAN",{});var Sx=i(mi);h(es.$$.fragment,Sx),Sx.forEach(t),bx.forEach(t),Pf=c(Nu),gi=a(Nu,"SPAN",{});var xx=i(gi);Tf=o(xx,"Discrete versus continuous schedulers"),xx.forEach(t),Nu.forEach(t),Vc=c(r),I=a(r,"P",{});var Se=i(I);kf=o(Se,`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),_i=a(Se,"CODE",{});var Dx=i(_i);Of=o(Dx,"int"),Dx.forEach(t),Af=o(Se," inputs), such as the "),Tn=a(Se,"A",{href:!0});var $x=i(Tn);Cf=o($x,"DDPMScheduler"),$x.forEach(t),Vf=o(Se," or "),kn=a(Se,"A",{href:!0});var Ex=i(kn);Ff=o(Ex,"PNDMScheduler"),Ex.forEach(t),If=o(Se,", and continuous (accepting "),vi=a(Se,"CODE",{});var yx=i(vi);Nf=o(yx,"float"),yx.forEach(t),Lf=o(Se," inputs), such as the score-based schedulers "),On=a(Se,"A",{href:!0});var wx=i(On);qf=o(wx,"ScoreSdeVeScheduler"),wx.forEach(t),Kf=o(Se," or "),bi=a(Se,"CODE",{});var Mx=i(bi);Rf=o(Mx,"ScoreSdeVpScheduler"),Mx.forEach(t),Qf=o(Se,"."),Se.forEach(t),Fc=c(r),et=a(r,"H2",{class:!0});var Lu=i(et);Mt=a(Lu,"A",{id:!0,class:!0,href:!0});var Px=i(Mt);Si=a(Px,"SPAN",{});var Tx=i(Si);h(ts.$$.fragment,Tx),Tx.forEach(t),Px.forEach(t),Wf=c(Lu),xi=a(Lu,"SPAN",{});var kx=i(xi);jf=o(kx,"Designing Re-usable schedulers"),kx.forEach(t),Lu.forEach(t),Ic=c(r),An=a(r,"P",{});var Ox=i(An);Uf=o(Ox,`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),Ox.forEach(t),Nc=c(r),Pt=a(r,"UL",{});var qu=i(Pt);Di=a(qu,"LI",{});var Ax=i(Di);Hf=o(Ax,"Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),Ax.forEach(t),Bf=c(qu),$i=a(qu,"LI",{});var Cx=i($i);Gf=o(Cx,"Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),Cx.forEach(t),qu.forEach(t),Lc=c(r),tt=a(r,"H2",{class:!0});var Ku=i(tt);Tt=a(Ku,"A",{id:!0,class:!0,href:!0});var Vx=i(Tt);Ei=a(Vx,"SPAN",{});var Fx=i(Ei);h(rs.$$.fragment,Fx),Fx.forEach(t),Vx.forEach(t),zf=c(Ku),yi=a(Ku,"SPAN",{});var Ix=i(yi);Jf=o(Ix,"API"),Ix.forEach(t),Ku.forEach(t),qc=c(r),Cn=a(r,"P",{});var Nx=i(Cn);Yf=o(Nx,"The core API for any new scheduler must follow a limited structure."),Nx.forEach(t),Kc=c(r),Ke=a(r,"UL",{});var Ja=i(Ke);ss=a(Ja,"LI",{});var Ru=i(ss);Xf=o(Ru,"Schedulers should provide one or more "),wi=a(Ru,"CODE",{});var Lx=i(wi);Zf=o(Lx,"def step(...)"),Lx.forEach(t),eh=o(Ru," functions that should be called to update the generated sample iteratively."),Ru.forEach(t),th=c(Ja),os=a(Ja,"LI",{});var Qu=i(os);rh=o(Qu,"Schedulers should provide a "),Mi=a(Qu,"CODE",{});var qx=i(Mi);sh=o(qx,"set_timesteps(...)"),qx.forEach(t),oh=o(Qu," method that configures the parameters of a schedule function for a specific inference task."),Qu.forEach(t),nh=c(Ja),Pi=a(Ja,"LI",{});var Kx=i(Pi);ah=o(Kx,"Schedulers should be framework-specific."),Kx.forEach(t),Ja.forEach(t),Rc=c(r),kt=a(r,"P",{});var Wu=i(kt);ih=o(Wu,"The base class "),Vn=a(Wu,"A",{href:!0});var Rx=i(Vn);dh=o(Rx,"SchedulerMixin"),Rx.forEach(t),lh=o(Wu," implements low level utilities used by multiple schedulers."),Wu.forEach(t),Qc=c(r),rt=a(r,"H3",{class:!0});var ju=i(rt);Ot=a(ju,"A",{id:!0,class:!0,href:!0});var Qx=i(Ot);Ti=a(Qx,"SPAN",{});var Wx=i(Ti);h(ns.$$.fragment,Wx),Wx.forEach(t),Qx.forEach(t),ch=c(ju),ki=a(ju,"SPAN",{});var jx=i(ki);uh=o(jx,"SchedulerMixin"),jx.forEach(t),ju.forEach(t),Wc=c(r),ee=a(r,"DIV",{class:!0});var Ae=i(ee);h(as.$$.fragment,Ae),ph=c(Ae),Oi=a(Ae,"P",{});var Ux=i(Oi);fh=o(Ux,"Mixin containing common functions for the schedulers."),Ux.forEach(t),hh=c(Ae),Ai=a(Ae,"P",{});var Hx=i(Ai);mh=o(Hx,"Class attributes:"),Hx.forEach(t),gh=c(Ae),Ci=a(Ae,"UL",{});var Bx=i(Ci);Re=a(Bx,"LI",{});var yn=i(Re);Vi=a(yn,"STRONG",{});var Gx=i(Vi);_h=o(Gx,"_compatibles"),Gx.forEach(t),vh=o(yn," ("),Fi=a(yn,"CODE",{});var zx=i(Fi);bh=o(zx,"List[str]"),zx.forEach(t),Sh=o(yn,`) \u2014 A list of classes that are compatible with the parent class, so that
`),Ii=a(yn,"CODE",{});var Jx=i(Ii);xh=o(Jx,"from_config"),Jx.forEach(t),Dh=o(yn,` can be used from a class different than the one used to save the config (should be overridden
by parent class).`),yn.forEach(t),Bx.forEach(t),$h=c(Ae),Te=a(Ae,"DIV",{class:!0});var Br=i(Te);h(is.$$.fragment,Br),Eh=c(Br),Ni=a(Br,"P",{});var Yx=i(Ni);yh=o(Yx,"Instantiate a Scheduler class from a pre-defined JSON configuration file inside a directory or Hub repo."),Yx.forEach(t),wh=c(Br),h(At.$$.fragment,Br),Mh=c(Br),h(Ct.$$.fragment,Br),Br.forEach(t),Ph=c(Ae),Vt=a(Ae,"DIV",{class:!0});var Uu=i(Vt);h(ds.$$.fragment,Uu),Th=c(Uu),st=a(Uu,"P",{});var Ya=i(st);kh=o(Ya,"Save a scheduler configuration object to the directory "),Li=a(Ya,"CODE",{});var Xx=i(Li);Oh=o(Xx,"save_directory"),Xx.forEach(t),Ah=o(Ya,`, so that it can be re-loaded using the
`),Fn=a(Ya,"A",{href:!0});var Zx=i(Fn);Ch=o(Zx,"from_pretrained()"),Zx.forEach(t),Vh=o(Ya," class method."),Ya.forEach(t),Uu.forEach(t),Ae.forEach(t),jc=c(r),ot=a(r,"H3",{class:!0});var Hu=i(ot);Ft=a(Hu,"A",{id:!0,class:!0,href:!0});var eD=i(Ft);qi=a(eD,"SPAN",{});var tD=i(qi);h(ls.$$.fragment,tD),tD.forEach(t),eD.forEach(t),Fh=c(Hu),Ki=a(Hu,"SPAN",{});var rD=i(Ki);Ih=o(rD,"SchedulerOutput"),rD.forEach(t),Hu.forEach(t),Uc=o(r,"\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),nt=a(r,"DIV",{class:!0});var Bu=i(nt);h(cs.$$.fragment,Bu),Nh=c(Bu),Ri=a(Bu,"P",{});var sD=i(Ri);Lh=o(sD,"Base class for the scheduler\u2019s step function output."),sD.forEach(t),Bu.forEach(t),Hc=c(r),at=a(r,"H3",{class:!0});var Gu=i(at);It=a(Gu,"A",{id:!0,class:!0,href:!0});var oD=i(It);Qi=a(oD,"SPAN",{});var nD=i(Qi);h(us.$$.fragment,nD),nD.forEach(t),oD.forEach(t),qh=c(Gu),Wi=a(Gu,"SPAN",{});var aD=i(Wi);Kh=o(aD,"Implemented Schedulers"),aD.forEach(t),Gu.forEach(t),Bc=c(r),it=a(r,"H4",{class:!0});var zu=i(it);Nt=a(zu,"A",{id:!0,class:!0,href:!0});var iD=i(Nt);ji=a(iD,"SPAN",{});var dD=i(ji);h(ps.$$.fragment,dD),dD.forEach(t),iD.forEach(t),Rh=c(zu),Ui=a(zu,"SPAN",{});var lD=i(Ui);Qh=o(lD,"Denoising diffusion implicit models (DDIM)"),lD.forEach(t),zu.forEach(t),Gc=c(r),In=a(r,"P",{});var cD=i(In);Wh=o(cD,"Original paper can be found here."),cD.forEach(t),zc=c(r),A=a(r,"DIV",{class:!0});var xe=i(A);h(fs.$$.fragment,xe),jh=c(xe),Hi=a(xe,"P",{});var uD=i(Hi);Uh=o(uD,`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),uD.forEach(t),Hh=c(xe),N=a(xe,"P",{});var ae=i(N);Nn=a(ae,"A",{href:!0});var pD=i(Nn);Bh=o(pD,"~ConfigMixin"),pD.forEach(t),Gh=o(ae," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Bi=a(ae,"CODE",{});var fD=i(Bi);zh=o(fD,"__init__"),fD.forEach(t),Jh=o(ae,`
function, such as `),Gi=a(ae,"CODE",{});var hD=i(Gi);Yh=o(hD,"num_train_timesteps"),hD.forEach(t),Xh=o(ae,". They can be accessed via "),zi=a(ae,"CODE",{});var mD=i(zi);Zh=o(mD,"scheduler.config.num_train_timesteps"),mD.forEach(t),em=o(ae,`.
`),Ln=a(ae,"A",{href:!0});var gD=i(Ln);tm=o(gD,"SchedulerMixin"),gD.forEach(t),rm=o(ae," provides general loading and saving functionality via the "),qn=a(ae,"A",{href:!0});var _D=i(qn);sm=o(_D,"SchedulerMixin.save_pretrained()"),_D.forEach(t),om=o(ae,` and
`),Kn=a(ae,"A",{href:!0});var vD=i(Kn);nm=o(vD,"from_pretrained()"),vD.forEach(t),am=o(ae," functions."),ae.forEach(t),im=c(xe),Rn=a(xe,"P",{});var z4=i(Rn);dm=o(z4,"For more details, see the original paper: "),hs=a(z4,"A",{href:!0,rel:!0});var bD=i(hs);lm=o(bD,"https://arxiv.org/abs/2010.02502"),bD.forEach(t),z4.forEach(t),cm=c(xe),Lt=a(xe,"DIV",{class:!0});var Ju=i(Lt);h(ms.$$.fragment,Ju),um=c(Ju),Ji=a(Ju,"P",{});var SD=i(Ji);pm=o(SD,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),SD.forEach(t),Ju.forEach(t),fm=c(xe),qt=a(xe,"DIV",{class:!0});var Yu=i(qt);h(gs.$$.fragment,Yu),hm=c(Yu),Yi=a(Yu,"P",{});var xD=i(Yi);mm=o(xD,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),xD.forEach(t),Yu.forEach(t),gm=c(xe),Kt=a(xe,"DIV",{class:!0});var Xu=i(Kt);h(_s.$$.fragment,Xu),_m=c(Xu),Xi=a(Xu,"P",{});var DD=i(Xi);vm=o(DD,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),DD.forEach(t),Xu.forEach(t),xe.forEach(t),Jc=c(r),dt=a(r,"H4",{class:!0});var Zu=i(dt);Rt=a(Zu,"A",{id:!0,class:!0,href:!0});var $D=i(Rt);Zi=a($D,"SPAN",{});var ED=i(Zi);h(vs.$$.fragment,ED),ED.forEach(t),$D.forEach(t),bm=c(Zu),ed=a(Zu,"SPAN",{});var yD=i(ed);Sm=o(yD,"Denoising diffusion probabilistic models (DDPM)"),yD.forEach(t),Zu.forEach(t),Yc=c(r),Qt=a(r,"P",{});var ep=i(Qt);xm=o(ep,"Original paper can be found "),bs=a(ep,"A",{href:!0,rel:!0});var wD=i(bs);Dm=o(wD,"here"),wD.forEach(t),$m=o(ep,"."),ep.forEach(t),Xc=c(r),C=a(r,"DIV",{class:!0});var De=i(C);h(Ss.$$.fragment,De),Em=c(De),td=a(De,"P",{});var MD=i(td);ym=o(MD,`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),MD.forEach(t),wm=c(De),L=a(De,"P",{});var ie=i(L);Qn=a(ie,"A",{href:!0});var PD=i(Qn);Mm=o(PD,"~ConfigMixin"),PD.forEach(t),Pm=o(ie," takes care of storing all config attributes that are passed in the scheduler\u2019s "),rd=a(ie,"CODE",{});var TD=i(rd);Tm=o(TD,"__init__"),TD.forEach(t),km=o(ie,`
function, such as `),sd=a(ie,"CODE",{});var kD=i(sd);Om=o(kD,"num_train_timesteps"),kD.forEach(t),Am=o(ie,". They can be accessed via "),od=a(ie,"CODE",{});var OD=i(od);Cm=o(OD,"scheduler.config.num_train_timesteps"),OD.forEach(t),Vm=o(ie,`.
`),Wn=a(ie,"A",{href:!0});var AD=i(Wn);Fm=o(AD,"SchedulerMixin"),AD.forEach(t),Im=o(ie," provides general loading and saving functionality via the "),jn=a(ie,"A",{href:!0});var CD=i(jn);Nm=o(CD,"SchedulerMixin.save_pretrained()"),CD.forEach(t),Lm=o(ie,` and
`),Un=a(ie,"A",{href:!0});var VD=i(Un);qm=o(VD,"from_pretrained()"),VD.forEach(t),Km=o(ie," functions."),ie.forEach(t),Rm=c(De),Hn=a(De,"P",{});var J4=i(Hn);Qm=o(J4,"For more details, see the original paper: "),xs=a(J4,"A",{href:!0,rel:!0});var FD=i(xs);Wm=o(FD,"https://arxiv.org/abs/2006.11239"),FD.forEach(t),J4.forEach(t),jm=c(De),Wt=a(De,"DIV",{class:!0});var tp=i(Wt);h(Ds.$$.fragment,tp),Um=c(tp),nd=a(tp,"P",{});var ID=i(nd);Hm=o(ID,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),ID.forEach(t),tp.forEach(t),Bm=c(De),jt=a(De,"DIV",{class:!0});var rp=i(jt);h($s.$$.fragment,rp),Gm=c(rp),ad=a(rp,"P",{});var ND=i(ad);zm=o(ND,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),ND.forEach(t),rp.forEach(t),Jm=c(De),Ut=a(De,"DIV",{class:!0});var sp=i(Ut);h(Es.$$.fragment,sp),Ym=c(sp),id=a(sp,"P",{});var LD=i(id);Xm=o(LD,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),LD.forEach(t),sp.forEach(t),De.forEach(t),Zc=c(r),lt=a(r,"H4",{class:!0});var op=i(lt);Ht=a(op,"A",{id:!0,class:!0,href:!0});var qD=i(Ht);dd=a(qD,"SPAN",{});var KD=i(dd);h(ys.$$.fragment,KD),KD.forEach(t),qD.forEach(t),Zm=c(op),ld=a(op,"SPAN",{});var RD=i(ld);eg=o(RD,"Multistep DPM-Solver"),RD.forEach(t),op.forEach(t),eu=c(r),ke=a(r,"P",{});var Gr=i(ke);tg=o(Gr,"Original paper can be found "),ws=a(Gr,"A",{href:!0,rel:!0});var QD=i(ws);rg=o(QD,"here"),QD.forEach(t),sg=o(Gr," and the "),Ms=a(Gr,"A",{href:!0,rel:!0});var WD=i(Ms);og=o(WD,"improved version"),WD.forEach(t),ng=o(Gr,". The original implementation can be found "),Ps=a(Gr,"A",{href:!0,rel:!0});var jD=i(Ps);ag=o(jD,"here"),jD.forEach(t),ig=o(Gr,"."),Gr.forEach(t),tu=c(r),x=a(r,"DIV",{class:!0});var $=i(x);h(Ts.$$.fragment,$),dg=c($),cd=a($,"P",{});var UD=i(cd);lg=o(UD,`DPM-Solver (and the improved version DPM-Solver++) is a fast dedicated high-order solver for diffusion ODEs with
the convergence order guarantee. Empirically, sampling by DPM-Solver with only 20 steps can generate high-quality
samples, and it can generate quite good samples even in only 10 steps.`),UD.forEach(t),cg=c($),Bt=a($,"P",{});var Mc=i(Bt);ug=o(Mc,"For more details, see the original paper: "),ks=a(Mc,"A",{href:!0,rel:!0});var HD=i(ks);pg=o(HD,"https://arxiv.org/abs/2206.00927"),HD.forEach(t),fg=o(Mc," and "),Os=a(Mc,"A",{href:!0,rel:!0});var BD=i(Os);hg=o(BD,"https://arxiv.org/abs/2211.01095"),BD.forEach(t),Mc.forEach(t),mg=c($),ct=a($,"P",{});var Xa=i(ct);gg=o(Xa,`Currently, we support the multistep DPM-Solver for both noise prediction models and data prediction models. We
recommend to use `),ud=a(Xa,"CODE",{});var GD=i(ud);_g=o(GD,"solver_order=2"),GD.forEach(t),vg=o(Xa," for guided sampling, and "),pd=a(Xa,"CODE",{});var zD=i(pd);bg=o(zD,"solver_order=3"),zD.forEach(t),Sg=o(Xa," for unconditional sampling."),Xa.forEach(t),xg=c($),Ie=a($,"P",{});var zr=i(Ie);Dg=o(zr,"We also support the \u201Cdynamic thresholding\u201D method in Imagen ("),As=a(zr,"A",{href:!0,rel:!0});var JD=i(As);$g=o(JD,"https://arxiv.org/abs/2205.11487"),JD.forEach(t),Eg=o(zr,`). For pixel-space
diffusion models, you can set both `),fd=a(zr,"CODE",{});var YD=i(fd);yg=o(YD,'algorithm_type="dpmsolver++"'),YD.forEach(t),wg=o(zr," and "),hd=a(zr,"CODE",{});var XD=i(hd);Mg=o(XD,"thresholding=True"),XD.forEach(t),Pg=o(zr,` to use the dynamic
thresholding. Note that the thresholding method is unsuitable for latent-space diffusion models (such as
stable-diffusion).`),zr.forEach(t),Tg=c($),q=a($,"P",{});var de=i(q);Bn=a(de,"A",{href:!0});var ZD=i(Bn);kg=o(ZD,"~ConfigMixin"),ZD.forEach(t),Og=o(de," takes care of storing all config attributes that are passed in the scheduler\u2019s "),md=a(de,"CODE",{});var e$=i(md);Ag=o(e$,"__init__"),e$.forEach(t),Cg=o(de,`
function, such as `),gd=a(de,"CODE",{});var t$=i(gd);Vg=o(t$,"num_train_timesteps"),t$.forEach(t),Fg=o(de,". They can be accessed via "),_d=a(de,"CODE",{});var r$=i(_d);Ig=o(r$,"scheduler.config.num_train_timesteps"),r$.forEach(t),Ng=o(de,`.
`),Gn=a(de,"A",{href:!0});var s$=i(Gn);Lg=o(s$,"SchedulerMixin"),s$.forEach(t),qg=o(de," provides general loading and saving functionality via the "),zn=a(de,"A",{href:!0});var o$=i(zn);Kg=o(o$,"SchedulerMixin.save_pretrained()"),o$.forEach(t),Rg=o(de,` and
`),Jn=a(de,"A",{href:!0});var n$=i(Jn);Qg=o(n$,"from_pretrained()"),n$.forEach(t),Wg=o(de," functions."),de.forEach(t),jg=c($),Oe=a($,"DIV",{class:!0});var Jr=i(Oe);h(Cs.$$.fragment,Jr),Ug=c(Jr),vd=a(Jr,"P",{});var a$=i(vd);Hg=o(a$,"Convert the model output to the corresponding type that the algorithm (DPM-Solver / DPM-Solver++) needs."),a$.forEach(t),Bg=c(Jr),bd=a(Jr,"P",{});var i$=i(bd);Gg=o(i$,`DPM-Solver is designed to discretize an integral of the noise prediciton model, and DPM-Solver++ is designed to
discretize an integral of the data prediction model. So we need to first convert the model output to the
corresponding type to match the algorithm.`),i$.forEach(t),zg=c(Jr),Sd=a(Jr,"P",{});var d$=i(Sd);Jg=o(d$,`Note that the algorithm type and the model type is decoupled. That is to say, we can use either DPM-Solver or
DPM-Solver++ for both noise prediction model and data prediction model.`),d$.forEach(t),Jr.forEach(t),Yg=c($),Qe=a($,"DIV",{class:!0});var Za=i(Qe);h(Vs.$$.fragment,Za),Xg=c(Za),xd=a(Za,"P",{});var l$=i(xd);Zg=o(l$,"One step for the first-order DPM-Solver (equivalent to DDIM)."),l$.forEach(t),e_=c(Za),Fs=a(Za,"P",{});var np=i(Fs);t_=o(np,"See "),Is=a(np,"A",{href:!0,rel:!0});var c$=i(Is);r_=o(c$,"https://arxiv.org/abs/2206.00927"),c$.forEach(t),s_=o(np," for the detailed derivation."),np.forEach(t),Za.forEach(t),o_=c($),Gt=a($,"DIV",{class:!0});var ap=i(Gt);h(Ns.$$.fragment,ap),n_=c(ap),Dd=a(ap,"P",{});var u$=i(Dd);a_=o(u$,"One step for the second-order multistep DPM-Solver."),u$.forEach(t),ap.forEach(t),i_=c($),zt=a($,"DIV",{class:!0});var ip=i(zt);h(Ls.$$.fragment,ip),d_=c(ip),$d=a(ip,"P",{});var p$=i($d);l_=o(p$,"One step for the third-order multistep DPM-Solver."),p$.forEach(t),ip.forEach(t),c_=c($),Jt=a($,"DIV",{class:!0});var dp=i(Jt);h(qs.$$.fragment,dp),u_=c(dp),Ed=a(dp,"P",{});var f$=i(Ed);p_=o(f$,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),f$.forEach(t),dp.forEach(t),f_=c($),Yt=a($,"DIV",{class:!0});var lp=i(Yt);h(Ks.$$.fragment,lp),h_=c(lp),yd=a(lp,"P",{});var h$=i(yd);m_=o(h$,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),h$.forEach(t),lp.forEach(t),g_=c($),Xt=a($,"DIV",{class:!0});var cp=i(Xt);h(Rs.$$.fragment,cp),__=c(cp),wd=a(cp,"P",{});var m$=i(wd);v_=o(m$,"Step function propagating the sample with the multistep DPM-Solver."),m$.forEach(t),cp.forEach(t),$.forEach(t),ru=c(r),ut=a(r,"H4",{class:!0});var up=i(ut);Zt=a(up,"A",{id:!0,class:!0,href:!0});var g$=i(Zt);Md=a(g$,"SPAN",{});var _$=i(Md);h(Qs.$$.fragment,_$),_$.forEach(t),g$.forEach(t),b_=c(up),Pd=a(up,"SPAN",{});var v$=i(Pd);S_=o(v$,"Variance exploding, stochastic sampling from Karras et. al"),v$.forEach(t),up.forEach(t),su=c(r),er=a(r,"P",{});var pp=i(er);x_=o(pp,"Original paper can be found "),Ws=a(pp,"A",{href:!0,rel:!0});var b$=i(Ws);D_=o(b$,"here"),b$.forEach(t),$_=o(pp,"."),pp.forEach(t),ou=c(r),E=a(r,"DIV",{class:!0});var O=i(E);h(js.$$.fragment,O),E_=c(O),Td=a(O,"P",{});var S$=i(Td);y_=o(S$,`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),S$.forEach(t),w_=c(O),tr=a(O,"P",{});var Pc=i(tr);M_=o(Pc,`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Us=a(Pc,"A",{href:!0,rel:!0});var x$=i(Us);P_=o(x$,"https://arxiv.org/abs/2206.00364"),x$.forEach(t),T_=o(Pc,` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Hs=a(Pc,"A",{href:!0,rel:!0});var D$=i(Hs);k_=o(D$,"https://arxiv.org/abs/2011.13456"),D$.forEach(t),Pc.forEach(t),O_=c(O),K=a(O,"P",{});var le=i(K);Yn=a(le,"A",{href:!0});var $$=i(Yn);A_=o($$,"~ConfigMixin"),$$.forEach(t),C_=o(le," takes care of storing all config attributes that are passed in the scheduler\u2019s "),kd=a(le,"CODE",{});var E$=i(kd);V_=o(E$,"__init__"),E$.forEach(t),F_=o(le,`
function, such as `),Od=a(le,"CODE",{});var y$=i(Od);I_=o(y$,"num_train_timesteps"),y$.forEach(t),N_=o(le,". They can be accessed via "),Ad=a(le,"CODE",{});var w$=i(Ad);L_=o(w$,"scheduler.config.num_train_timesteps"),w$.forEach(t),q_=o(le,`.
`),Xn=a(le,"A",{href:!0});var M$=i(Xn);K_=o(M$,"SchedulerMixin"),M$.forEach(t),R_=o(le," provides general loading and saving functionality via the "),Zn=a(le,"A",{href:!0});var P$=i(Zn);Q_=o(P$,"SchedulerMixin.save_pretrained()"),P$.forEach(t),W_=o(le,` and
`),ea=a(le,"A",{href:!0});var T$=i(ea);j_=o(T$,"from_pretrained()"),T$.forEach(t),U_=o(le," functions."),le.forEach(t),H_=c(O),Bs=a(O,"P",{});var fp=i(Bs);B_=o(fp,`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),Gs=a(fp,"A",{href:!0,rel:!0});var k$=i(Gs);G_=o(k$,"https://arxiv.org/abs/2206.00364"),k$.forEach(t),z_=o(fp,`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),fp.forEach(t),J_=c(O),We=a(O,"DIV",{class:!0});var ei=i(We);h(zs.$$.fragment,ei),Y_=c(ei),Cd=a(ei,"P",{});var O$=i(Cd);X_=o(O$,`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),O$.forEach(t),Z_=c(ei),Vd=a(ei,"P",{});var A$=i(Vd);ev=o(A$,"TODO Args:"),A$.forEach(t),ei.forEach(t),tv=c(O),rr=a(O,"DIV",{class:!0});var hp=i(rr);h(Js.$$.fragment,hp),rv=c(hp),Fd=a(hp,"P",{});var C$=i(Fd);sv=o(C$,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),C$.forEach(t),hp.forEach(t),ov=c(O),sr=a(O,"DIV",{class:!0});var mp=i(sr);h(Ys.$$.fragment,mp),nv=c(mp),Id=a(mp,"P",{});var V$=i(Id);av=o(V$,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),V$.forEach(t),mp.forEach(t),iv=c(O),or=a(O,"DIV",{class:!0});var gp=i(or);h(Xs.$$.fragment,gp),dv=c(gp),Nd=a(gp,"P",{});var F$=i(Nd);lv=o(F$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),F$.forEach(t),gp.forEach(t),cv=c(O),nr=a(O,"DIV",{class:!0});var _p=i(nr);h(Zs.$$.fragment,_p),uv=c(_p),Ld=a(_p,"P",{});var I$=i(Ld);pv=o(I$,"Correct the predicted sample based on the output model_output of the network. TODO complete description"),I$.forEach(t),_p.forEach(t),O.forEach(t),nu=c(r),pt=a(r,"H4",{class:!0});var vp=i(pt);ar=a(vp,"A",{id:!0,class:!0,href:!0});var N$=i(ar);qd=a(N$,"SPAN",{});var L$=i(qd);h(eo.$$.fragment,L$),L$.forEach(t),N$.forEach(t),fv=c(vp),Kd=a(vp,"SPAN",{});var q$=i(Kd);hv=o(q$,"Linear multistep scheduler for discrete beta schedules"),q$.forEach(t),vp.forEach(t),au=c(r),ir=a(r,"P",{});var bp=i(ir);mv=o(bp,"Original implementation can be found "),to=a(bp,"A",{href:!0,rel:!0});var K$=i(to);gv=o(K$,"here"),K$.forEach(t),_v=o(bp,"."),bp.forEach(t),iu=c(r),V=a(r,"DIV",{class:!0});var $e=i(V);h(ro.$$.fragment,$e),vv=c($e),ta=a($e,"P",{});var Y4=i(ta);bv=o(Y4,`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),so=a(Y4,"A",{href:!0,rel:!0});var R$=i(so);Sv=o(R$,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),R$.forEach(t),Y4.forEach(t),xv=c($e),R=a($e,"P",{});var ce=i(R);ra=a(ce,"A",{href:!0});var Q$=i(ra);Dv=o(Q$,"~ConfigMixin"),Q$.forEach(t),$v=o(ce," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Rd=a(ce,"CODE",{});var W$=i(Rd);Ev=o(W$,"__init__"),W$.forEach(t),yv=o(ce,`
function, such as `),Qd=a(ce,"CODE",{});var j$=i(Qd);wv=o(j$,"num_train_timesteps"),j$.forEach(t),Mv=o(ce,". They can be accessed via "),Wd=a(ce,"CODE",{});var U$=i(Wd);Pv=o(U$,"scheduler.config.num_train_timesteps"),U$.forEach(t),Tv=o(ce,`.
`),sa=a(ce,"A",{href:!0});var H$=i(sa);kv=o(H$,"SchedulerMixin"),H$.forEach(t),Ov=o(ce," provides general loading and saving functionality via the "),oa=a(ce,"A",{href:!0});var B$=i(oa);Av=o(B$,"SchedulerMixin.save_pretrained()"),B$.forEach(t),Cv=o(ce,` and
`),na=a(ce,"A",{href:!0});var G$=i(na);Vv=o(G$,"from_pretrained()"),G$.forEach(t),Fv=o(ce," functions."),ce.forEach(t),Iv=c($e),dr=a($e,"DIV",{class:!0});var Sp=i(dr);h(oo.$$.fragment,Sp),Nv=c(Sp),jd=a(Sp,"P",{});var z$=i(jd);Lv=o(z$,"Compute a linear multistep coefficient."),z$.forEach(t),Sp.forEach(t),qv=c($e),lr=a($e,"DIV",{class:!0});var xp=i(lr);h(no.$$.fragment,xp),Kv=c(xp),ao=a(xp,"P",{});var Dp=i(ao);Rv=o(Dp,"Scales the denoising model input by "),Ud=a(Dp,"CODE",{});var J$=i(Ud);Qv=o(J$,"(sigma**2 + 1) ** 0.5"),J$.forEach(t),Wv=o(Dp," to match the K-LMS algorithm."),Dp.forEach(t),xp.forEach(t),jv=c($e),cr=a($e,"DIV",{class:!0});var $p=i(cr);h(io.$$.fragment,$p),Uv=c($p),Hd=a($p,"P",{});var Y$=i(Hd);Hv=o(Y$,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),Y$.forEach(t),$p.forEach(t),Bv=c($e),ur=a($e,"DIV",{class:!0});var Ep=i(ur);h(lo.$$.fragment,Ep),Gv=c(Ep),Bd=a(Ep,"P",{});var X$=i(Bd);zv=o(X$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),X$.forEach(t),Ep.forEach(t),$e.forEach(t),du=c(r),ft=a(r,"H4",{class:!0});var yp=i(ft);pr=a(yp,"A",{id:!0,class:!0,href:!0});var Z$=i(pr);Gd=a(Z$,"SPAN",{});var eE=i(Gd);h(co.$$.fragment,eE),eE.forEach(t),Z$.forEach(t),Jv=c(yp),zd=a(yp,"SPAN",{});var tE=i(zd);Yv=o(tE,"Pseudo numerical methods for diffusion models (PNDM)"),tE.forEach(t),yp.forEach(t),lu=c(r),fr=a(r,"P",{});var wp=i(fr);Xv=o(wp,"Original implementation can be found "),uo=a(wp,"A",{href:!0,rel:!0});var rE=i(uo);Zv=o(rE,"here"),rE.forEach(t),eb=o(wp,"."),wp.forEach(t),cu=c(r),y=a(r,"DIV",{class:!0});var Y=i(y);h(po.$$.fragment,Y),tb=c(Y),Jd=a(Y,"P",{});var sE=i(Jd);rb=o(sE,`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),sE.forEach(t),sb=c(Y),Q=a(Y,"P",{});var ue=i(Q);aa=a(ue,"A",{href:!0});var oE=i(aa);ob=o(oE,"~ConfigMixin"),oE.forEach(t),nb=o(ue," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Yd=a(ue,"CODE",{});var nE=i(Yd);ab=o(nE,"__init__"),nE.forEach(t),ib=o(ue,`
function, such as `),Xd=a(ue,"CODE",{});var aE=i(Xd);db=o(aE,"num_train_timesteps"),aE.forEach(t),lb=o(ue,". They can be accessed via "),Zd=a(ue,"CODE",{});var iE=i(Zd);cb=o(iE,"scheduler.config.num_train_timesteps"),iE.forEach(t),ub=o(ue,`.
`),ia=a(ue,"A",{href:!0});var dE=i(ia);pb=o(dE,"SchedulerMixin"),dE.forEach(t),fb=o(ue," provides general loading and saving functionality via the "),da=a(ue,"A",{href:!0});var lE=i(da);hb=o(lE,"SchedulerMixin.save_pretrained()"),lE.forEach(t),mb=o(ue,` and
`),la=a(ue,"A",{href:!0});var cE=i(la);gb=o(cE,"from_pretrained()"),cE.forEach(t),_b=o(ue," functions."),ue.forEach(t),vb=c(Y),ca=a(Y,"P",{});var X4=i(ca);bb=o(X4,"For more details, see the original paper: "),fo=a(X4,"A",{href:!0,rel:!0});var uE=i(fo);Sb=o(uE,"https://arxiv.org/abs/2202.09778"),uE.forEach(t),X4.forEach(t),xb=c(Y),hr=a(Y,"DIV",{class:!0});var Mp=i(hr);h(ho.$$.fragment,Mp),Db=c(Mp),el=a(Mp,"P",{});var pE=i(el);$b=o(pE,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),pE.forEach(t),Mp.forEach(t),Eb=c(Y),mr=a(Y,"DIV",{class:!0});var Pp=i(mr);h(mo.$$.fragment,Pp),yb=c(Pp),tl=a(Pp,"P",{});var fE=i(tl);wb=o(fE,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),fE.forEach(t),Pp.forEach(t),Mb=c(Y),je=a(Y,"DIV",{class:!0});var ti=i(je);h(go.$$.fragment,ti),Pb=c(ti),rl=a(ti,"P",{});var hE=i(rl);Tb=o(hE,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),hE.forEach(t),kb=c(ti),Ne=a(ti,"P",{});var Yr=i(Ne);Ob=o(Yr,"This function calls "),sl=a(Yr,"CODE",{});var mE=i(sl);Ab=o(mE,"step_prk()"),mE.forEach(t),Cb=o(Yr," or "),ol=a(Yr,"CODE",{});var gE=i(ol);Vb=o(gE,"step_plms()"),gE.forEach(t),Fb=o(Yr," depending on the internal variable "),nl=a(Yr,"CODE",{});var _E=i(nl);Ib=o(_E,"counter"),_E.forEach(t),Nb=o(Yr,"."),Yr.forEach(t),ti.forEach(t),Lb=c(Y),gr=a(Y,"DIV",{class:!0});var Tp=i(gr);h(_o.$$.fragment,Tp),qb=c(Tp),al=a(Tp,"P",{});var vE=i(al);Kb=o(vE,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),vE.forEach(t),Tp.forEach(t),Rb=c(Y),_r=a(Y,"DIV",{class:!0});var kp=i(_r);h(vo.$$.fragment,kp),Qb=c(kp),il=a(kp,"P",{});var bE=i(il);Wb=o(bE,`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),bE.forEach(t),kp.forEach(t),Y.forEach(t),uu=c(r),ht=a(r,"H4",{class:!0});var Op=i(ht);vr=a(Op,"A",{id:!0,class:!0,href:!0});var SE=i(vr);dl=a(SE,"SPAN",{});var xE=i(dl);h(bo.$$.fragment,xE),xE.forEach(t),SE.forEach(t),jb=c(Op),ll=a(Op,"SPAN",{});var DE=i(ll);Ub=o(DE,"variance exploding stochastic differential equation (VE-SDE) scheduler"),DE.forEach(t),Op.forEach(t),pu=c(r),br=a(r,"P",{});var Ap=i(br);Hb=o(Ap,"Original paper can be found "),So=a(Ap,"A",{href:!0,rel:!0});var $E=i(So);Bb=o($E,"here"),$E.forEach(t),Gb=o(Ap,"."),Ap.forEach(t),fu=c(r),w=a(r,"DIV",{class:!0});var X=i(w);h(xo.$$.fragment,X),zb=c(X),cl=a(X,"P",{});var EE=i(cl);Jb=o(EE,"The variance exploding stochastic differential equation (SDE) scheduler."),EE.forEach(t),Yb=c(X),ua=a(X,"P",{});var Z4=i(ua);Xb=o(Z4,"For more information, see the original paper: "),Do=a(Z4,"A",{href:!0,rel:!0});var yE=i(Do);Zb=o(yE,"https://arxiv.org/abs/2011.13456"),yE.forEach(t),Z4.forEach(t),e1=c(X),W=a(X,"P",{});var pe=i(W);pa=a(pe,"A",{href:!0});var wE=i(pa);t1=o(wE,"~ConfigMixin"),wE.forEach(t),r1=o(pe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ul=a(pe,"CODE",{});var ME=i(ul);s1=o(ME,"__init__"),ME.forEach(t),o1=o(pe,`
function, such as `),pl=a(pe,"CODE",{});var PE=i(pl);n1=o(PE,"num_train_timesteps"),PE.forEach(t),a1=o(pe,". They can be accessed via "),fl=a(pe,"CODE",{});var TE=i(fl);i1=o(TE,"scheduler.config.num_train_timesteps"),TE.forEach(t),d1=o(pe,`.
`),fa=a(pe,"A",{href:!0});var kE=i(fa);l1=o(kE,"SchedulerMixin"),kE.forEach(t),c1=o(pe," provides general loading and saving functionality via the "),ha=a(pe,"A",{href:!0});var OE=i(ha);u1=o(OE,"SchedulerMixin.save_pretrained()"),OE.forEach(t),p1=o(pe,` and
`),ma=a(pe,"A",{href:!0});var AE=i(ma);f1=o(AE,"from_pretrained()"),AE.forEach(t),h1=o(pe," functions."),pe.forEach(t),m1=c(X),Sr=a(X,"DIV",{class:!0});var Cp=i(Sr);h($o.$$.fragment,Cp),g1=c(Cp),hl=a(Cp,"P",{});var CE=i(hl);_1=o(CE,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),CE.forEach(t),Cp.forEach(t),v1=c(X),Ue=a(X,"DIV",{class:!0});var ri=i(Ue);h(Eo.$$.fragment,ri),b1=c(ri),ml=a(ri,"P",{});var VE=i(ml);S1=o(VE,"Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),VE.forEach(t),x1=c(ri),mt=a(ri,"P",{});var si=i(mt);D1=o(si,"The sigmas control the weight of the "),gl=a(si,"CODE",{});var FE=i(gl);$1=o(FE,"drift"),FE.forEach(t),E1=o(si," and "),_l=a(si,"CODE",{});var IE=i(_l);y1=o(IE,"diffusion"),IE.forEach(t),w1=o(si," components of sample update."),si.forEach(t),ri.forEach(t),M1=c(X),xr=a(X,"DIV",{class:!0});var Vp=i(xr);h(yo.$$.fragment,Vp),P1=c(Vp),vl=a(Vp,"P",{});var NE=i(vl);T1=o(NE,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),NE.forEach(t),Vp.forEach(t),k1=c(X),Dr=a(X,"DIV",{class:!0});var Fp=i(Dr);h(wo.$$.fragment,Fp),O1=c(Fp),bl=a(Fp,"P",{});var LE=i(bl);A1=o(LE,`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),LE.forEach(t),Fp.forEach(t),C1=c(X),$r=a(X,"DIV",{class:!0});var Ip=i($r);h(Mo.$$.fragment,Ip),V1=c(Ip),Sl=a(Ip,"P",{});var qE=i(Sl);F1=o(qE,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),qE.forEach(t),Ip.forEach(t),X.forEach(t),hu=c(r),gt=a(r,"H4",{class:!0});var Np=i(gt);Er=a(Np,"A",{id:!0,class:!0,href:!0});var KE=i(Er);xl=a(KE,"SPAN",{});var RE=i(xl);h(Po.$$.fragment,RE),RE.forEach(t),KE.forEach(t),I1=c(Np),Dl=a(Np,"SPAN",{});var QE=i(Dl);N1=o(QE,"improved pseudo numerical methods for diffusion models (iPNDM)"),QE.forEach(t),Np.forEach(t),mu=c(r),yr=a(r,"P",{});var Lp=i(yr);L1=o(Lp,"Original implementation can be found "),To=a(Lp,"A",{href:!0,rel:!0});var WE=i(To);q1=o(WE,"here"),WE.forEach(t),K1=o(Lp,"."),Lp.forEach(t),gu=c(r),F=a(r,"DIV",{class:!0});var Ee=i(F);h(ko.$$.fragment,Ee),R1=c(Ee),ga=a(Ee,"P",{});var ex=i(ga);Q1=o(ex,`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),Oo=a(ex,"A",{href:!0,rel:!0});var jE=i(Oo);W1=o(jE,"library"),jE.forEach(t),ex.forEach(t),j1=c(Ee),j=a(Ee,"P",{});var fe=i(j);_a=a(fe,"A",{href:!0});var UE=i(_a);U1=o(UE,"~ConfigMixin"),UE.forEach(t),H1=o(fe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),$l=a(fe,"CODE",{});var HE=i($l);B1=o(HE,"__init__"),HE.forEach(t),G1=o(fe,`
function, such as `),El=a(fe,"CODE",{});var BE=i(El);z1=o(BE,"num_train_timesteps"),BE.forEach(t),J1=o(fe,". They can be accessed via "),yl=a(fe,"CODE",{});var GE=i(yl);Y1=o(GE,"scheduler.config.num_train_timesteps"),GE.forEach(t),X1=o(fe,`.
`),va=a(fe,"A",{href:!0});var zE=i(va);Z1=o(zE,"SchedulerMixin"),zE.forEach(t),e0=o(fe," provides general loading and saving functionality via the "),ba=a(fe,"A",{href:!0});var JE=i(ba);t0=o(JE,"SchedulerMixin.save_pretrained()"),JE.forEach(t),r0=o(fe,` and
`),Sa=a(fe,"A",{href:!0});var YE=i(Sa);s0=o(YE,"from_pretrained()"),YE.forEach(t),o0=o(fe," functions."),fe.forEach(t),n0=c(Ee),xa=a(Ee,"P",{});var tx=i(xa);a0=o(tx,"For more details, see the original paper: "),Ao=a(tx,"A",{href:!0,rel:!0});var XE=i(Ao);i0=o(XE,"https://arxiv.org/abs/2202.09778"),XE.forEach(t),tx.forEach(t),d0=c(Ee),wr=a(Ee,"DIV",{class:!0});var qp=i(wr);h(Co.$$.fragment,qp),l0=c(qp),wl=a(qp,"P",{});var ZE=i(wl);c0=o(ZE,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),ZE.forEach(t),qp.forEach(t),u0=c(Ee),Mr=a(Ee,"DIV",{class:!0});var Kp=i(Mr);h(Vo.$$.fragment,Kp),p0=c(Kp),Ml=a(Kp,"P",{});var e5=i(Ml);f0=o(e5,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),e5.forEach(t),Kp.forEach(t),h0=c(Ee),Pr=a(Ee,"DIV",{class:!0});var Rp=i(Pr);h(Fo.$$.fragment,Rp),m0=c(Rp),Pl=a(Rp,"P",{});var t5=i(Pl);g0=o(t5,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),t5.forEach(t),Rp.forEach(t),Ee.forEach(t),_u=c(r),_t=a(r,"H4",{class:!0});var Qp=i(_t);Tr=a(Qp,"A",{id:!0,class:!0,href:!0});var r5=i(Tr);Tl=a(r5,"SPAN",{});var s5=i(Tl);h(Io.$$.fragment,s5),s5.forEach(t),r5.forEach(t),_0=c(Qp),kl=a(Qp,"SPAN",{});var o5=i(kl);v0=o(o5,"variance preserving stochastic differential equation (VP-SDE) scheduler"),o5.forEach(t),Qp.forEach(t),vu=c(r),kr=a(r,"P",{});var Wp=i(kr);b0=o(Wp,"Original paper can be found "),No=a(Wp,"A",{href:!0,rel:!0});var n5=i(No);S0=o(n5,"here"),n5.forEach(t),x0=o(Wp,"."),Wp.forEach(t),bu=c(r),h(Or.$$.fragment,r),Su=c(r),ne=a(r,"DIV",{class:!0});var Ge=i(ne);h(Lo.$$.fragment,Ge),D0=c(Ge),Ol=a(Ge,"P",{});var a5=i(Ol);$0=o(a5,"The variance preserving stochastic differential equation (SDE) scheduler."),a5.forEach(t),E0=c(Ge),U=a(Ge,"P",{});var he=i(U);Da=a(he,"A",{href:!0});var i5=i(Da);y0=o(i5,"~ConfigMixin"),i5.forEach(t),w0=o(he," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Al=a(he,"CODE",{});var d5=i(Al);M0=o(d5,"__init__"),d5.forEach(t),P0=o(he,`
function, such as `),Cl=a(he,"CODE",{});var l5=i(Cl);T0=o(l5,"num_train_timesteps"),l5.forEach(t),k0=o(he,". They can be accessed via "),Vl=a(he,"CODE",{});var c5=i(Vl);O0=o(c5,"scheduler.config.num_train_timesteps"),c5.forEach(t),A0=o(he,`.
`),$a=a(he,"A",{href:!0});var u5=i($a);C0=o(u5,"SchedulerMixin"),u5.forEach(t),V0=o(he," provides general loading and saving functionality via the "),Ea=a(he,"A",{href:!0});var p5=i(Ea);F0=o(p5,"SchedulerMixin.save_pretrained()"),p5.forEach(t),I0=o(he,` and
`),ya=a(he,"A",{href:!0});var f5=i(ya);N0=o(f5,"from_pretrained()"),f5.forEach(t),L0=o(he," functions."),he.forEach(t),q0=c(Ge),wa=a(Ge,"P",{});var rx=i(wa);K0=o(rx,"For more information, see the original paper: "),qo=a(rx,"A",{href:!0,rel:!0});var h5=i(qo);R0=o(h5,"https://arxiv.org/abs/2011.13456"),h5.forEach(t),rx.forEach(t),Q0=c(Ge),Fl=a(Ge,"P",{});var m5=i(Fl);W0=o(m5,"UNDER CONSTRUCTION"),m5.forEach(t),Ge.forEach(t),xu=c(r),vt=a(r,"H4",{class:!0});var jp=i(vt);Ar=a(jp,"A",{id:!0,class:!0,href:!0});var g5=i(Ar);Il=a(g5,"SPAN",{});var _5=i(Il);h(Ko.$$.fragment,_5),_5.forEach(t),g5.forEach(t),j0=c(jp),Nl=a(jp,"SPAN",{});var v5=i(Nl);U0=o(v5,"Euler scheduler"),v5.forEach(t),jp.forEach(t),Du=c(r),He=a(r,"P",{});var oi=i(He);H0=o(oi,"Euler scheduler (Algorithm 2) from the paper "),Ro=a(oi,"A",{href:!0,rel:!0});var b5=i(Ro);B0=o(b5,"Elucidating the Design Space of Diffusion-Based Generative Models"),b5.forEach(t),G0=o(oi," by Karras et al. (2022). Based on the original "),Qo=a(oi,"A",{href:!0,rel:!0});var S5=i(Qo);z0=o(S5,"k-diffusion"),S5.forEach(t),J0=o(oi,` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),oi.forEach(t),$u=c(r),te=a(r,"DIV",{class:!0});var Ce=i(te);h(Wo.$$.fragment,Ce),Y0=c(Ce),Cr=a(Ce,"P",{});var Tc=i(Cr);X0=o(Tc,"Euler scheduler (Algorithm 2) from Karras et al. (2022) "),jo=a(Tc,"A",{href:!0,rel:!0});var x5=i(jo);Z0=o(x5,"https://arxiv.org/abs/2206.00364"),x5.forEach(t),e2=o(Tc,`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),Uo=a(Tc,"A",{href:!0,rel:!0});var D5=i(Uo);t2=o(D5,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),D5.forEach(t),Tc.forEach(t),r2=c(Ce),H=a(Ce,"P",{});var me=i(H);Ma=a(me,"A",{href:!0});var $5=i(Ma);s2=o($5,"~ConfigMixin"),$5.forEach(t),o2=o(me," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ll=a(me,"CODE",{});var E5=i(Ll);n2=o(E5,"__init__"),E5.forEach(t),a2=o(me,`
function, such as `),ql=a(me,"CODE",{});var y5=i(ql);i2=o(y5,"num_train_timesteps"),y5.forEach(t),d2=o(me,". They can be accessed via "),Kl=a(me,"CODE",{});var w5=i(Kl);l2=o(w5,"scheduler.config.num_train_timesteps"),w5.forEach(t),c2=o(me,`.
`),Pa=a(me,"A",{href:!0});var M5=i(Pa);u2=o(M5,"SchedulerMixin"),M5.forEach(t),p2=o(me," provides general loading and saving functionality via the "),Ta=a(me,"A",{href:!0});var P5=i(Ta);f2=o(P5,"SchedulerMixin.save_pretrained()"),P5.forEach(t),h2=o(me,` and
`),ka=a(me,"A",{href:!0});var T5=i(ka);m2=o(T5,"from_pretrained()"),T5.forEach(t),g2=o(me," functions."),me.forEach(t),_2=c(Ce),Vr=a(Ce,"DIV",{class:!0});var Up=i(Vr);h(Ho.$$.fragment,Up),v2=c(Up),Bo=a(Up,"P",{});var Hp=i(Bo);b2=o(Hp,"Scales the denoising model input by "),Rl=a(Hp,"CODE",{});var k5=i(Rl);S2=o(k5,"(sigma**2 + 1) ** 0.5"),k5.forEach(t),x2=o(Hp," to match the Euler algorithm."),Hp.forEach(t),Up.forEach(t),D2=c(Ce),Fr=a(Ce,"DIV",{class:!0});var Bp=i(Fr);h(Go.$$.fragment,Bp),$2=c(Bp),Ql=a(Bp,"P",{});var O5=i(Ql);E2=o(O5,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),O5.forEach(t),Bp.forEach(t),y2=c(Ce),Ir=a(Ce,"DIV",{class:!0});var Gp=i(Ir);h(zo.$$.fragment,Gp),w2=c(Gp),Wl=a(Gp,"P",{});var A5=i(Wl);M2=o(A5,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),A5.forEach(t),Gp.forEach(t),Ce.forEach(t),Eu=c(r),bt=a(r,"H4",{class:!0});var zp=i(bt);Nr=a(zp,"A",{id:!0,class:!0,href:!0});var C5=i(Nr);jl=a(C5,"SPAN",{});var V5=i(jl);h(Jo.$$.fragment,V5),V5.forEach(t),C5.forEach(t),P2=c(zp),Ul=a(zp,"SPAN",{});var F5=i(Ul);T2=o(F5,"Euler Ancestral scheduler"),F5.forEach(t),zp.forEach(t),yu=c(r),Oa=a(r,"P",{});var I5=i(Oa);k2=o(I5,`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),I5.forEach(t),wu=c(r),re=a(r,"DIV",{class:!0});var Ve=i(re);h(Yo.$$.fragment,Ve),O2=c(Ve),Aa=a(Ve,"P",{});var sx=i(Aa);A2=o(sx,`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Xo=a(sx,"A",{href:!0,rel:!0});var N5=i(Xo);C2=o(N5,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),N5.forEach(t),sx.forEach(t),V2=c(Ve),B=a(Ve,"P",{});var ge=i(B);Ca=a(ge,"A",{href:!0});var L5=i(Ca);F2=o(L5,"~ConfigMixin"),L5.forEach(t),I2=o(ge," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Hl=a(ge,"CODE",{});var q5=i(Hl);N2=o(q5,"__init__"),q5.forEach(t),L2=o(ge,`
function, such as `),Bl=a(ge,"CODE",{});var K5=i(Bl);q2=o(K5,"num_train_timesteps"),K5.forEach(t),K2=o(ge,". They can be accessed via "),Gl=a(ge,"CODE",{});var R5=i(Gl);R2=o(R5,"scheduler.config.num_train_timesteps"),R5.forEach(t),Q2=o(ge,`.
`),Va=a(ge,"A",{href:!0});var Q5=i(Va);W2=o(Q5,"SchedulerMixin"),Q5.forEach(t),j2=o(ge," provides general loading and saving functionality via the "),Fa=a(ge,"A",{href:!0});var W5=i(Fa);U2=o(W5,"SchedulerMixin.save_pretrained()"),W5.forEach(t),H2=o(ge,` and
`),Ia=a(ge,"A",{href:!0});var j5=i(Ia);B2=o(j5,"from_pretrained()"),j5.forEach(t),G2=o(ge," functions."),ge.forEach(t),z2=c(Ve),Lr=a(Ve,"DIV",{class:!0});var Jp=i(Lr);h(Zo.$$.fragment,Jp),J2=c(Jp),en=a(Jp,"P",{});var Yp=i(en);Y2=o(Yp,"Scales the denoising model input by "),zl=a(Yp,"CODE",{});var U5=i(zl);X2=o(U5,"(sigma**2 + 1) ** 0.5"),U5.forEach(t),Z2=o(Yp," to match the Euler algorithm."),Yp.forEach(t),Jp.forEach(t),eS=c(Ve),qr=a(Ve,"DIV",{class:!0});var Xp=i(qr);h(tn.$$.fragment,Xp),tS=c(Xp),Jl=a(Xp,"P",{});var H5=i(Jl);rS=o(H5,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),H5.forEach(t),Xp.forEach(t),sS=c(Ve),Kr=a(Ve,"DIV",{class:!0});var Zp=i(Kr);h(rn.$$.fragment,Zp),oS=c(Zp),Yl=a(Zp,"P",{});var B5=i(Yl);nS=o(B5,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),B5.forEach(t),Zp.forEach(t),Ve.forEach(t),Mu=c(r),St=a(r,"H4",{class:!0});var ef=i(St);Rr=a(ef,"A",{id:!0,class:!0,href:!0});var G5=i(Rr);Xl=a(G5,"SPAN",{});var z5=i(Xl);h(sn.$$.fragment,z5),z5.forEach(t),G5.forEach(t),aS=c(ef),Zl=a(ef,"SPAN",{});var J5=i(Zl);iS=o(J5,"VQDiffusionScheduler"),J5.forEach(t),ef.forEach(t),Pu=c(r),on=a(r,"P",{});var ox=i(on);dS=o(ox,"Original paper can be found "),nn=a(ox,"A",{href:!0,rel:!0});var Y5=i(nn);lS=o(Y5,"here"),Y5.forEach(t),ox.forEach(t),Tu=c(r),M=a(r,"DIV",{class:!0});var Z=i(M);h(an.$$.fragment,Z),cS=c(Z),ec=a(Z,"P",{});var X5=i(ec);uS=o(X5,"The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image."),X5.forEach(t),pS=c(Z),tc=a(Z,"P",{});var Z5=i(tc);fS=o(Z5,`The VQ-diffusion scheduler converts the transformer\u2019s output into a sample for the unnoised image at the previous
diffusion timestep.`),Z5.forEach(t),hS=c(Z),G=a(Z,"P",{});var _e=i(G);Na=a(_e,"A",{href:!0});var ey=i(Na);mS=o(ey,"~ConfigMixin"),ey.forEach(t),gS=o(_e," takes care of storing all config attributes that are passed in the scheduler\u2019s "),rc=a(_e,"CODE",{});var ty=i(rc);_S=o(ty,"__init__"),ty.forEach(t),vS=o(_e,`
function, such as `),sc=a(_e,"CODE",{});var ry=i(sc);bS=o(ry,"num_train_timesteps"),ry.forEach(t),SS=o(_e,". They can be accessed via "),oc=a(_e,"CODE",{});var sy=i(oc);xS=o(sy,"scheduler.config.num_train_timesteps"),sy.forEach(t),DS=o(_e,`.
`),La=a(_e,"A",{href:!0});var oy=i(La);$S=o(oy,"SchedulerMixin"),oy.forEach(t),ES=o(_e," provides general loading and saving functionality via the "),qa=a(_e,"A",{href:!0});var ny=i(qa);yS=o(ny,"SchedulerMixin.save_pretrained()"),ny.forEach(t),wS=o(_e,` and
`),Ka=a(_e,"A",{href:!0});var ay=i(Ka);MS=o(ay,"from_pretrained()"),ay.forEach(t),PS=o(_e," functions."),_e.forEach(t),TS=c(Z),Ra=a(Z,"P",{});var nx=i(Ra);kS=o(nx,"For more details, see the original paper: "),dn=a(nx,"A",{href:!0,rel:!0});var iy=i(dn);OS=o(iy,"https://arxiv.org/abs/2111.14822"),iy.forEach(t),nx.forEach(t),AS=c(Z),Be=a(Z,"DIV",{class:!0});var ni=i(Be);h(ln.$$.fragment,ni),CS=c(ni),cn=a(ni,"P",{});var tf=i(cn);VS=o(tf,`Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each
latent pixel in `),nc=a(tf,"CODE",{});var dy=i(nc);FS=o(dy,"x_t"),dy.forEach(t),IS=o(tf,"."),tf.forEach(t),NS=c(ni),ac=a(ni,"P",{});var ly=i(ac);LS=o(ly,`See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix
is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.`),ly.forEach(t),ni.forEach(t),qS=c(Z),z=a(Z,"DIV",{class:!0});var ye=i(z);h(un.$$.fragment,ye),KS=c(ye),pn=a(ye,"P",{});var rf=i(pn);RS=o(rf,"Calculates the log probabilities for the predicted classes of the image at timestep "),ic=a(rf,"CODE",{});var cy=i(ic);QS=o(cy,"t-1"),cy.forEach(t),WS=o(rf,". I.e. Equation (11)."),rf.forEach(t),jS=c(ye),dc=a(ye,"P",{});var uy=i(dc);US=o(uy,`Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only
forward probabilities.`),uy.forEach(t),HS=c(ye),lc=a(ye,"P",{});var py=i(lc);BS=o(py,"Equation (11) stated in terms of forward probabilities via Equation (5):"),py.forEach(t),GS=c(ye),cc=a(ye,"P",{});var fy=i(cc);zS=o(fy,"Where:"),fy.forEach(t),JS=c(ye),uc=a(ye,"UL",{});var hy=i(uc);fn=a(hy,"LI",{});var sf=i(fn);YS=o(sf,"the sum is over x"),pc=a(sf,"EM",{});var my=i(pc);XS=o(my,"0 = {C_0 \u2026 C"),my.forEach(t),ZS=o(sf,"{k-1}} (classes for x_0)"),sf.forEach(t),hy.forEach(t),e4=c(ye),xt=a(ye,"P",{});var ai=i(xt);t4=o(ai,"p(x"),fc=a(ai,"EM",{});var gy=i(fc);r4=o(gy,"{t-1} | x_t) = sum( q(x_t | x"),gy.forEach(t),s4=o(ai,"{t-1}) "),hc=a(ai,"EM",{});var _y=i(hc);o4=o(_y,"q(x_{t-1} | x_0)"),_y.forEach(t),n4=o(ai," p(x_0) / q(x_t | x_0) )"),ai.forEach(t),ye.forEach(t),a4=c(Z),Qr=a(Z,"DIV",{class:!0});var of=i(Qr);h(hn.$$.fragment,of),i4=c(of),mc=a(of,"P",{});var vy=i(mc);d4=o(vy,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),vy.forEach(t),of.forEach(t),l4=c(Z),Wr=a(Z,"DIV",{class:!0});var nf=i(Wr);h(mn.$$.fragment,nf),c4=c(nf),gn=a(nf,"P",{});var af=i(gn);u4=o(af,`Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the
docstring for `),gc=a(af,"CODE",{});var by=i(gc);p4=o(by,"self.q_posterior"),by.forEach(t),f4=o(af," for more in depth docs on how Equation (11) is computed."),af.forEach(t),nf.forEach(t),Z.forEach(t),ku=c(r),Dt=a(r,"H4",{class:!0});var df=i(Dt);jr=a(df,"A",{id:!0,class:!0,href:!0});var Sy=i(jr);_c=a(Sy,"SPAN",{});var xy=i(_c);h(_n.$$.fragment,xy),xy.forEach(t),Sy.forEach(t),h4=c(df),vc=a(df,"SPAN",{});var Dy=i(vc);m4=o(Dy,"RePaint scheduler"),Dy.forEach(t),df.forEach(t),Ou=c(r),Le=a(r,"P",{});var wn=i(Le);g4=o(wn,`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),Qa=a(wn,"A",{href:!0});var $y=i(Qa);_4=o($y,"RePaintPipeline"),$y.forEach(t),v4=o(wn,`.
Based on the paper `),vn=a(wn,"A",{href:!0,rel:!0});var Ey=i(vn);b4=o(Ey,"RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),Ey.forEach(t),S4=o(wn,`
and the original implementation by Andreas Lugmayr et al.: `),bn=a(wn,"A",{href:!0,rel:!0});var yy=i(bn);x4=o(yy,"https://github.com/andreas128/RePaint"),yy.forEach(t),wn.forEach(t),Au=c(r),se=a(r,"DIV",{class:!0});var Fe=i(se);h(Sn.$$.fragment,Fe),D4=c(Fe),bc=a(Fe,"P",{});var wy=i(bc);$4=o(wy,"RePaint is a schedule for DDPM inpainting inside a given mask."),wy.forEach(t),E4=c(Fe),J=a(Fe,"P",{});var ve=i(J);Wa=a(ve,"A",{href:!0});var My=i(Wa);y4=o(My,"~ConfigMixin"),My.forEach(t),w4=o(ve," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Sc=a(ve,"CODE",{});var Py=i(Sc);M4=o(Py,"__init__"),Py.forEach(t),P4=o(ve,`
function, such as `),xc=a(ve,"CODE",{});var Ty=i(xc);T4=o(Ty,"num_train_timesteps"),Ty.forEach(t),k4=o(ve,". They can be accessed via "),Dc=a(ve,"CODE",{});var ky=i(Dc);O4=o(ky,"scheduler.config.num_train_timesteps"),ky.forEach(t),A4=o(ve,`.
`),ja=a(ve,"A",{href:!0});var Oy=i(ja);C4=o(Oy,"SchedulerMixin"),Oy.forEach(t),V4=o(ve," provides general loading and saving functionality via the "),Ua=a(ve,"A",{href:!0});var Ay=i(Ua);F4=o(Ay,"SchedulerMixin.save_pretrained()"),Ay.forEach(t),I4=o(ve,` and
`),Ha=a(ve,"A",{href:!0});var Cy=i(Ha);N4=o(Cy,"from_pretrained()"),Cy.forEach(t),L4=o(ve," functions."),ve.forEach(t),q4=c(Fe),Ba=a(Fe,"P",{});var ax=i(Ba);K4=o(ax,"For more details, see the original paper: "),xn=a(ax,"A",{href:!0,rel:!0});var Vy=i(xn);R4=o(Vy,"https://arxiv.org/pdf/2201.09865.pdf"),Vy.forEach(t),ax.forEach(t),Q4=c(Fe),Ur=a(Fe,"DIV",{class:!0});var lf=i(Ur);h(Dn.$$.fragment,lf),W4=c(lf),$c=a(lf,"P",{});var Fy=i($c);j4=o(Fy,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Fy.forEach(t),lf.forEach(t),U4=c(Fe),Hr=a(Fe,"DIV",{class:!0});var cf=i(Hr);h($n.$$.fragment,cf),H4=c(cf),Ec=a(cf,"P",{});var Iy=i(Ec);B4=o(Iy,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Iy.forEach(t),cf.forEach(t),Fe.forEach(t),this.h()},h(){d(S,"name","hf:doc:metadata"),d(S,"content",JSON.stringify(Hy)),d(P,"id","schedulers"),d(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P,"href","#schedulers"),d(D,"class","relative group"),d(Et,"id","what-is-a-scheduler"),d(Et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Et,"href","#what-is-a-scheduler"),d(Ye,"class","relative group"),d(wt,"id","discrete-versus-continuous-schedulers"),d(wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wt,"href","#discrete-versus-continuous-schedulers"),d(Ze,"class","relative group"),d(Tn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.DDPMScheduler"),d(kn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler"),d(On,"href","/docs/diffusers/main/en/api/schedulers#diffusers.ScoreSdeVeScheduler"),d(Mt,"id","designing-reusable-schedulers"),d(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Mt,"href","#designing-reusable-schedulers"),d(et,"class","relative group"),d(Tt,"id","api"),d(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Tt,"href","#api"),d(tt,"class","relative group"),d(Vn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(Ot,"id","diffusers.SchedulerMixin"),d(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ot,"href","#diffusers.SchedulerMixin"),d(rt,"class","relative group"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(Vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ft,"id","diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ft,"href","#diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(ot,"class","relative group"),d(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(It,"id","implemented-schedulers"),d(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(It,"href","#implemented-schedulers"),d(at,"class","relative group"),d(Nt,"id","diffusers.DDIMScheduler"),d(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Nt,"href","#diffusers.DDIMScheduler"),d(it,"class","relative group"),d(Nn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Ln,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(qn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(Kn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(hs,"href","https://arxiv.org/abs/2010.02502"),d(hs,"rel","nofollow"),d(Lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Rt,"id","diffusers.DDPMScheduler"),d(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Rt,"href","#diffusers.DDPMScheduler"),d(dt,"class","relative group"),d(bs,"href","https://arxiv.org/abs/2010.02502"),d(bs,"rel","nofollow"),d(Qn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Wn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(jn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(Un,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(xs,"href","https://arxiv.org/abs/2006.11239"),d(xs,"rel","nofollow"),d(Wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ht,"id","diffusers.DPMSolverMultistepScheduler"),d(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ht,"href","#diffusers.DPMSolverMultistepScheduler"),d(lt,"class","relative group"),d(ws,"href","https://arxiv.org/abs/2206.00927"),d(ws,"rel","nofollow"),d(Ms,"href","https://arxiv.org/abs/2211.01095"),d(Ms,"rel","nofollow"),d(Ps,"href","https://github.com/LuChengTHU/dpm-solver"),d(Ps,"rel","nofollow"),d(ks,"href","https://arxiv.org/abs/2206.00927"),d(ks,"rel","nofollow"),d(Os,"href","https://arxiv.org/abs/2211.01095"),d(Os,"rel","nofollow"),d(As,"href","https://arxiv.org/abs/2205.11487"),d(As,"rel","nofollow"),d(Bn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Gn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(zn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(Jn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Is,"href","https://arxiv.org/abs/2206.00927"),d(Is,"rel","nofollow"),d(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Zt,"id","diffusers.KarrasVeScheduler"),d(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Zt,"href","#diffusers.KarrasVeScheduler"),d(ut,"class","relative group"),d(Ws,"href","https://arxiv.org/abs/2006.11239"),d(Ws,"rel","nofollow"),d(Us,"href","https://arxiv.org/abs/2206.00364"),d(Us,"rel","nofollow"),d(Hs,"href","https://arxiv.org/abs/2011.13456"),d(Hs,"rel","nofollow"),d(Yn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Xn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(Zn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(ea,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(Gs,"href","https://arxiv.org/abs/2206.00364"),d(Gs,"rel","nofollow"),d(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(or,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(nr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ar,"id","diffusers.LMSDiscreteScheduler"),d(ar,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ar,"href","#diffusers.LMSDiscreteScheduler"),d(pt,"class","relative group"),d(to,"href","https://arxiv.org/abs/2206.00364"),d(to,"rel","nofollow"),d(so,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(so,"rel","nofollow"),d(ra,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(sa,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(oa,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(na,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(lr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(pr,"id","diffusers.PNDMScheduler"),d(pr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(pr,"href","#diffusers.PNDMScheduler"),d(ft,"class","relative group"),d(uo,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(uo,"rel","nofollow"),d(aa,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ia,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(da,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(la,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(fo,"href","https://arxiv.org/abs/2202.09778"),d(fo,"rel","nofollow"),d(hr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(gr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_r,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(vr,"id","diffusers.ScoreSdeVeScheduler"),d(vr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(vr,"href","#diffusers.ScoreSdeVeScheduler"),d(ht,"class","relative group"),d(So,"href","https://arxiv.org/abs/2011.13456"),d(So,"rel","nofollow"),d(Do,"href","https://arxiv.org/abs/2011.13456"),d(Do,"rel","nofollow"),d(pa,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(fa,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(ha,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(ma,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(Sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(xr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($r,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Er,"id","diffusers.IPNDMScheduler"),d(Er,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Er,"href","#diffusers.IPNDMScheduler"),d(gt,"class","relative group"),d(To,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(To,"rel","nofollow"),d(Oo,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(Oo,"rel","nofollow"),d(_a,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(va,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(ba,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(Sa,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(Ao,"href","https://arxiv.org/abs/2202.09778"),d(Ao,"rel","nofollow"),d(wr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Tr,"id","diffusers.schedulers.ScoreSdeVpScheduler"),d(Tr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Tr,"href","#diffusers.schedulers.ScoreSdeVpScheduler"),d(_t,"class","relative group"),d(No,"href","https://arxiv.org/abs/2011.13456"),d(No,"rel","nofollow"),d(Da,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d($a,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(Ea,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(ya,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(qo,"href","https://arxiv.org/abs/2011.13456"),d(qo,"rel","nofollow"),d(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ar,"id","diffusers.EulerDiscreteScheduler"),d(Ar,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ar,"href","#diffusers.EulerDiscreteScheduler"),d(vt,"class","relative group"),d(Ro,"href","https://arxiv.org/abs/2206.00364"),d(Ro,"rel","nofollow"),d(Qo,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(Qo,"rel","nofollow"),d(jo,"href","https://arxiv.org/abs/2206.00364"),d(jo,"rel","nofollow"),d(Uo,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(Uo,"rel","nofollow"),d(Ma,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Pa,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(Ta,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(ka,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(Vr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ir,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Nr,"id","diffusers.EulerAncestralDiscreteScheduler"),d(Nr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Nr,"href","#diffusers.EulerAncestralDiscreteScheduler"),d(bt,"class","relative group"),d(Xo,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),d(Xo,"rel","nofollow"),d(Ca,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Va,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(Fa,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(Ia,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(Lr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Kr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Rr,"id","diffusers.VQDiffusionScheduler"),d(Rr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Rr,"href","#diffusers.VQDiffusionScheduler"),d(St,"class","relative group"),d(nn,"href","https://arxiv.org/abs/2111.14822"),d(nn,"rel","nofollow"),d(Na,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(La,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(qa,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(Ka,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(dn,"href","https://arxiv.org/abs/2111.14822"),d(dn,"rel","nofollow"),d(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Qr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Wr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(jr,"id","diffusers.RePaintScheduler"),d(jr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(jr,"href","#diffusers.RePaintScheduler"),d(Dt,"class","relative group"),d(Qa,"href","/docs/diffusers/main/en/api/pipelines/repaint#diffusers.RePaintPipeline"),d(vn,"href","https://arxiv.org/abs/2201.09865"),d(vn,"rel","nofollow"),d(bn,"href","https://github.com/andreas128/RePaint"),d(bn,"rel","nofollow"),d(Wa,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ja,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(Ua,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.save_pretrained"),d(Ha,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin.from_pretrained"),d(xn,"href","https://arxiv.org/pdf/2201.09865.pdf"),d(xn,"rel","nofollow"),d(Ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Hr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,u){e(document.head,S),p(r,oe,u),p(r,D,u),e(D,P),e(P,we),m(T,we,null),e(D,be),e(D,Pe),e(Pe,ze),p(r,Me,u),p(r,Je,u),e(Je,Mn),p(r,kc,u),p(r,Ye,u),e(Ye,Et),e(Et,ii),m(Xr,ii,null),e(Ye,uf),e(Ye,di),e(di,pf),p(r,Oc,u),p(r,qe,u),e(qe,ff),e(qe,li),e(li,hf),e(qe,mf),e(qe,ci),e(ci,gf),e(qe,_f),p(r,Ac,u),p(r,yt,u),e(yt,Pn),e(Pn,vf),e(Pn,Zr),e(Zr,ui),e(ui,bf),e(Zr,Sf),e(Zr,pi),e(pi,xf),e(yt,Df),e(yt,Xe),e(Xe,$f),e(Xe,fi),e(fi,Ef),e(Xe,yf),e(Xe,hi),e(hi,wf),e(Xe,Mf),p(r,Cc,u),p(r,Ze,u),e(Ze,wt),e(wt,mi),m(es,mi,null),e(Ze,Pf),e(Ze,gi),e(gi,Tf),p(r,Vc,u),p(r,I,u),e(I,kf),e(I,_i),e(_i,Of),e(I,Af),e(I,Tn),e(Tn,Cf),e(I,Vf),e(I,kn),e(kn,Ff),e(I,If),e(I,vi),e(vi,Nf),e(I,Lf),e(I,On),e(On,qf),e(I,Kf),e(I,bi),e(bi,Rf),e(I,Qf),p(r,Fc,u),p(r,et,u),e(et,Mt),e(Mt,Si),m(ts,Si,null),e(et,Wf),e(et,xi),e(xi,jf),p(r,Ic,u),p(r,An,u),e(An,Uf),p(r,Nc,u),p(r,Pt,u),e(Pt,Di),e(Di,Hf),e(Pt,Bf),e(Pt,$i),e($i,Gf),p(r,Lc,u),p(r,tt,u),e(tt,Tt),e(Tt,Ei),m(rs,Ei,null),e(tt,zf),e(tt,yi),e(yi,Jf),p(r,qc,u),p(r,Cn,u),e(Cn,Yf),p(r,Kc,u),p(r,Ke,u),e(Ke,ss),e(ss,Xf),e(ss,wi),e(wi,Zf),e(ss,eh),e(Ke,th),e(Ke,os),e(os,rh),e(os,Mi),e(Mi,sh),e(os,oh),e(Ke,nh),e(Ke,Pi),e(Pi,ah),p(r,Rc,u),p(r,kt,u),e(kt,ih),e(kt,Vn),e(Vn,dh),e(kt,lh),p(r,Qc,u),p(r,rt,u),e(rt,Ot),e(Ot,Ti),m(ns,Ti,null),e(rt,ch),e(rt,ki),e(ki,uh),p(r,Wc,u),p(r,ee,u),m(as,ee,null),e(ee,ph),e(ee,Oi),e(Oi,fh),e(ee,hh),e(ee,Ai),e(Ai,mh),e(ee,gh),e(ee,Ci),e(Ci,Re),e(Re,Vi),e(Vi,_h),e(Re,vh),e(Re,Fi),e(Fi,bh),e(Re,Sh),e(Re,Ii),e(Ii,xh),e(Re,Dh),e(ee,$h),e(ee,Te),m(is,Te,null),e(Te,Eh),e(Te,Ni),e(Ni,yh),e(Te,wh),m(At,Te,null),e(Te,Mh),m(Ct,Te,null),e(ee,Ph),e(ee,Vt),m(ds,Vt,null),e(Vt,Th),e(Vt,st),e(st,kh),e(st,Li),e(Li,Oh),e(st,Ah),e(st,Fn),e(Fn,Ch),e(st,Vh),p(r,jc,u),p(r,ot,u),e(ot,Ft),e(Ft,qi),m(ls,qi,null),e(ot,Fh),e(ot,Ki),e(Ki,Ih),p(r,Uc,u),p(r,nt,u),m(cs,nt,null),e(nt,Nh),e(nt,Ri),e(Ri,Lh),p(r,Hc,u),p(r,at,u),e(at,It),e(It,Qi),m(us,Qi,null),e(at,qh),e(at,Wi),e(Wi,Kh),p(r,Bc,u),p(r,it,u),e(it,Nt),e(Nt,ji),m(ps,ji,null),e(it,Rh),e(it,Ui),e(Ui,Qh),p(r,Gc,u),p(r,In,u),e(In,Wh),p(r,zc,u),p(r,A,u),m(fs,A,null),e(A,jh),e(A,Hi),e(Hi,Uh),e(A,Hh),e(A,N),e(N,Nn),e(Nn,Bh),e(N,Gh),e(N,Bi),e(Bi,zh),e(N,Jh),e(N,Gi),e(Gi,Yh),e(N,Xh),e(N,zi),e(zi,Zh),e(N,em),e(N,Ln),e(Ln,tm),e(N,rm),e(N,qn),e(qn,sm),e(N,om),e(N,Kn),e(Kn,nm),e(N,am),e(A,im),e(A,Rn),e(Rn,dm),e(Rn,hs),e(hs,lm),e(A,cm),e(A,Lt),m(ms,Lt,null),e(Lt,um),e(Lt,Ji),e(Ji,pm),e(A,fm),e(A,qt),m(gs,qt,null),e(qt,hm),e(qt,Yi),e(Yi,mm),e(A,gm),e(A,Kt),m(_s,Kt,null),e(Kt,_m),e(Kt,Xi),e(Xi,vm),p(r,Jc,u),p(r,dt,u),e(dt,Rt),e(Rt,Zi),m(vs,Zi,null),e(dt,bm),e(dt,ed),e(ed,Sm),p(r,Yc,u),p(r,Qt,u),e(Qt,xm),e(Qt,bs),e(bs,Dm),e(Qt,$m),p(r,Xc,u),p(r,C,u),m(Ss,C,null),e(C,Em),e(C,td),e(td,ym),e(C,wm),e(C,L),e(L,Qn),e(Qn,Mm),e(L,Pm),e(L,rd),e(rd,Tm),e(L,km),e(L,sd),e(sd,Om),e(L,Am),e(L,od),e(od,Cm),e(L,Vm),e(L,Wn),e(Wn,Fm),e(L,Im),e(L,jn),e(jn,Nm),e(L,Lm),e(L,Un),e(Un,qm),e(L,Km),e(C,Rm),e(C,Hn),e(Hn,Qm),e(Hn,xs),e(xs,Wm),e(C,jm),e(C,Wt),m(Ds,Wt,null),e(Wt,Um),e(Wt,nd),e(nd,Hm),e(C,Bm),e(C,jt),m($s,jt,null),e(jt,Gm),e(jt,ad),e(ad,zm),e(C,Jm),e(C,Ut),m(Es,Ut,null),e(Ut,Ym),e(Ut,id),e(id,Xm),p(r,Zc,u),p(r,lt,u),e(lt,Ht),e(Ht,dd),m(ys,dd,null),e(lt,Zm),e(lt,ld),e(ld,eg),p(r,eu,u),p(r,ke,u),e(ke,tg),e(ke,ws),e(ws,rg),e(ke,sg),e(ke,Ms),e(Ms,og),e(ke,ng),e(ke,Ps),e(Ps,ag),e(ke,ig),p(r,tu,u),p(r,x,u),m(Ts,x,null),e(x,dg),e(x,cd),e(cd,lg),e(x,cg),e(x,Bt),e(Bt,ug),e(Bt,ks),e(ks,pg),e(Bt,fg),e(Bt,Os),e(Os,hg),e(x,mg),e(x,ct),e(ct,gg),e(ct,ud),e(ud,_g),e(ct,vg),e(ct,pd),e(pd,bg),e(ct,Sg),e(x,xg),e(x,Ie),e(Ie,Dg),e(Ie,As),e(As,$g),e(Ie,Eg),e(Ie,fd),e(fd,yg),e(Ie,wg),e(Ie,hd),e(hd,Mg),e(Ie,Pg),e(x,Tg),e(x,q),e(q,Bn),e(Bn,kg),e(q,Og),e(q,md),e(md,Ag),e(q,Cg),e(q,gd),e(gd,Vg),e(q,Fg),e(q,_d),e(_d,Ig),e(q,Ng),e(q,Gn),e(Gn,Lg),e(q,qg),e(q,zn),e(zn,Kg),e(q,Rg),e(q,Jn),e(Jn,Qg),e(q,Wg),e(x,jg),e(x,Oe),m(Cs,Oe,null),e(Oe,Ug),e(Oe,vd),e(vd,Hg),e(Oe,Bg),e(Oe,bd),e(bd,Gg),e(Oe,zg),e(Oe,Sd),e(Sd,Jg),e(x,Yg),e(x,Qe),m(Vs,Qe,null),e(Qe,Xg),e(Qe,xd),e(xd,Zg),e(Qe,e_),e(Qe,Fs),e(Fs,t_),e(Fs,Is),e(Is,r_),e(Fs,s_),e(x,o_),e(x,Gt),m(Ns,Gt,null),e(Gt,n_),e(Gt,Dd),e(Dd,a_),e(x,i_),e(x,zt),m(Ls,zt,null),e(zt,d_),e(zt,$d),e($d,l_),e(x,c_),e(x,Jt),m(qs,Jt,null),e(Jt,u_),e(Jt,Ed),e(Ed,p_),e(x,f_),e(x,Yt),m(Ks,Yt,null),e(Yt,h_),e(Yt,yd),e(yd,m_),e(x,g_),e(x,Xt),m(Rs,Xt,null),e(Xt,__),e(Xt,wd),e(wd,v_),p(r,ru,u),p(r,ut,u),e(ut,Zt),e(Zt,Md),m(Qs,Md,null),e(ut,b_),e(ut,Pd),e(Pd,S_),p(r,su,u),p(r,er,u),e(er,x_),e(er,Ws),e(Ws,D_),e(er,$_),p(r,ou,u),p(r,E,u),m(js,E,null),e(E,E_),e(E,Td),e(Td,y_),e(E,w_),e(E,tr),e(tr,M_),e(tr,Us),e(Us,P_),e(tr,T_),e(tr,Hs),e(Hs,k_),e(E,O_),e(E,K),e(K,Yn),e(Yn,A_),e(K,C_),e(K,kd),e(kd,V_),e(K,F_),e(K,Od),e(Od,I_),e(K,N_),e(K,Ad),e(Ad,L_),e(K,q_),e(K,Xn),e(Xn,K_),e(K,R_),e(K,Zn),e(Zn,Q_),e(K,W_),e(K,ea),e(ea,j_),e(K,U_),e(E,H_),e(E,Bs),e(Bs,B_),e(Bs,Gs),e(Gs,G_),e(Bs,z_),e(E,J_),e(E,We),m(zs,We,null),e(We,Y_),e(We,Cd),e(Cd,X_),e(We,Z_),e(We,Vd),e(Vd,ev),e(E,tv),e(E,rr),m(Js,rr,null),e(rr,rv),e(rr,Fd),e(Fd,sv),e(E,ov),e(E,sr),m(Ys,sr,null),e(sr,nv),e(sr,Id),e(Id,av),e(E,iv),e(E,or),m(Xs,or,null),e(or,dv),e(or,Nd),e(Nd,lv),e(E,cv),e(E,nr),m(Zs,nr,null),e(nr,uv),e(nr,Ld),e(Ld,pv),p(r,nu,u),p(r,pt,u),e(pt,ar),e(ar,qd),m(eo,qd,null),e(pt,fv),e(pt,Kd),e(Kd,hv),p(r,au,u),p(r,ir,u),e(ir,mv),e(ir,to),e(to,gv),e(ir,_v),p(r,iu,u),p(r,V,u),m(ro,V,null),e(V,vv),e(V,ta),e(ta,bv),e(ta,so),e(so,Sv),e(V,xv),e(V,R),e(R,ra),e(ra,Dv),e(R,$v),e(R,Rd),e(Rd,Ev),e(R,yv),e(R,Qd),e(Qd,wv),e(R,Mv),e(R,Wd),e(Wd,Pv),e(R,Tv),e(R,sa),e(sa,kv),e(R,Ov),e(R,oa),e(oa,Av),e(R,Cv),e(R,na),e(na,Vv),e(R,Fv),e(V,Iv),e(V,dr),m(oo,dr,null),e(dr,Nv),e(dr,jd),e(jd,Lv),e(V,qv),e(V,lr),m(no,lr,null),e(lr,Kv),e(lr,ao),e(ao,Rv),e(ao,Ud),e(Ud,Qv),e(ao,Wv),e(V,jv),e(V,cr),m(io,cr,null),e(cr,Uv),e(cr,Hd),e(Hd,Hv),e(V,Bv),e(V,ur),m(lo,ur,null),e(ur,Gv),e(ur,Bd),e(Bd,zv),p(r,du,u),p(r,ft,u),e(ft,pr),e(pr,Gd),m(co,Gd,null),e(ft,Jv),e(ft,zd),e(zd,Yv),p(r,lu,u),p(r,fr,u),e(fr,Xv),e(fr,uo),e(uo,Zv),e(fr,eb),p(r,cu,u),p(r,y,u),m(po,y,null),e(y,tb),e(y,Jd),e(Jd,rb),e(y,sb),e(y,Q),e(Q,aa),e(aa,ob),e(Q,nb),e(Q,Yd),e(Yd,ab),e(Q,ib),e(Q,Xd),e(Xd,db),e(Q,lb),e(Q,Zd),e(Zd,cb),e(Q,ub),e(Q,ia),e(ia,pb),e(Q,fb),e(Q,da),e(da,hb),e(Q,mb),e(Q,la),e(la,gb),e(Q,_b),e(y,vb),e(y,ca),e(ca,bb),e(ca,fo),e(fo,Sb),e(y,xb),e(y,hr),m(ho,hr,null),e(hr,Db),e(hr,el),e(el,$b),e(y,Eb),e(y,mr),m(mo,mr,null),e(mr,yb),e(mr,tl),e(tl,wb),e(y,Mb),e(y,je),m(go,je,null),e(je,Pb),e(je,rl),e(rl,Tb),e(je,kb),e(je,Ne),e(Ne,Ob),e(Ne,sl),e(sl,Ab),e(Ne,Cb),e(Ne,ol),e(ol,Vb),e(Ne,Fb),e(Ne,nl),e(nl,Ib),e(Ne,Nb),e(y,Lb),e(y,gr),m(_o,gr,null),e(gr,qb),e(gr,al),e(al,Kb),e(y,Rb),e(y,_r),m(vo,_r,null),e(_r,Qb),e(_r,il),e(il,Wb),p(r,uu,u),p(r,ht,u),e(ht,vr),e(vr,dl),m(bo,dl,null),e(ht,jb),e(ht,ll),e(ll,Ub),p(r,pu,u),p(r,br,u),e(br,Hb),e(br,So),e(So,Bb),e(br,Gb),p(r,fu,u),p(r,w,u),m(xo,w,null),e(w,zb),e(w,cl),e(cl,Jb),e(w,Yb),e(w,ua),e(ua,Xb),e(ua,Do),e(Do,Zb),e(w,e1),e(w,W),e(W,pa),e(pa,t1),e(W,r1),e(W,ul),e(ul,s1),e(W,o1),e(W,pl),e(pl,n1),e(W,a1),e(W,fl),e(fl,i1),e(W,d1),e(W,fa),e(fa,l1),e(W,c1),e(W,ha),e(ha,u1),e(W,p1),e(W,ma),e(ma,f1),e(W,h1),e(w,m1),e(w,Sr),m($o,Sr,null),e(Sr,g1),e(Sr,hl),e(hl,_1),e(w,v1),e(w,Ue),m(Eo,Ue,null),e(Ue,b1),e(Ue,ml),e(ml,S1),e(Ue,x1),e(Ue,mt),e(mt,D1),e(mt,gl),e(gl,$1),e(mt,E1),e(mt,_l),e(_l,y1),e(mt,w1),e(w,M1),e(w,xr),m(yo,xr,null),e(xr,P1),e(xr,vl),e(vl,T1),e(w,k1),e(w,Dr),m(wo,Dr,null),e(Dr,O1),e(Dr,bl),e(bl,A1),e(w,C1),e(w,$r),m(Mo,$r,null),e($r,V1),e($r,Sl),e(Sl,F1),p(r,hu,u),p(r,gt,u),e(gt,Er),e(Er,xl),m(Po,xl,null),e(gt,I1),e(gt,Dl),e(Dl,N1),p(r,mu,u),p(r,yr,u),e(yr,L1),e(yr,To),e(To,q1),e(yr,K1),p(r,gu,u),p(r,F,u),m(ko,F,null),e(F,R1),e(F,ga),e(ga,Q1),e(ga,Oo),e(Oo,W1),e(F,j1),e(F,j),e(j,_a),e(_a,U1),e(j,H1),e(j,$l),e($l,B1),e(j,G1),e(j,El),e(El,z1),e(j,J1),e(j,yl),e(yl,Y1),e(j,X1),e(j,va),e(va,Z1),e(j,e0),e(j,ba),e(ba,t0),e(j,r0),e(j,Sa),e(Sa,s0),e(j,o0),e(F,n0),e(F,xa),e(xa,a0),e(xa,Ao),e(Ao,i0),e(F,d0),e(F,wr),m(Co,wr,null),e(wr,l0),e(wr,wl),e(wl,c0),e(F,u0),e(F,Mr),m(Vo,Mr,null),e(Mr,p0),e(Mr,Ml),e(Ml,f0),e(F,h0),e(F,Pr),m(Fo,Pr,null),e(Pr,m0),e(Pr,Pl),e(Pl,g0),p(r,_u,u),p(r,_t,u),e(_t,Tr),e(Tr,Tl),m(Io,Tl,null),e(_t,_0),e(_t,kl),e(kl,v0),p(r,vu,u),p(r,kr,u),e(kr,b0),e(kr,No),e(No,S0),e(kr,x0),p(r,bu,u),m(Or,r,u),p(r,Su,u),p(r,ne,u),m(Lo,ne,null),e(ne,D0),e(ne,Ol),e(Ol,$0),e(ne,E0),e(ne,U),e(U,Da),e(Da,y0),e(U,w0),e(U,Al),e(Al,M0),e(U,P0),e(U,Cl),e(Cl,T0),e(U,k0),e(U,Vl),e(Vl,O0),e(U,A0),e(U,$a),e($a,C0),e(U,V0),e(U,Ea),e(Ea,F0),e(U,I0),e(U,ya),e(ya,N0),e(U,L0),e(ne,q0),e(ne,wa),e(wa,K0),e(wa,qo),e(qo,R0),e(ne,Q0),e(ne,Fl),e(Fl,W0),p(r,xu,u),p(r,vt,u),e(vt,Ar),e(Ar,Il),m(Ko,Il,null),e(vt,j0),e(vt,Nl),e(Nl,U0),p(r,Du,u),p(r,He,u),e(He,H0),e(He,Ro),e(Ro,B0),e(He,G0),e(He,Qo),e(Qo,z0),e(He,J0),p(r,$u,u),p(r,te,u),m(Wo,te,null),e(te,Y0),e(te,Cr),e(Cr,X0),e(Cr,jo),e(jo,Z0),e(Cr,e2),e(Cr,Uo),e(Uo,t2),e(te,r2),e(te,H),e(H,Ma),e(Ma,s2),e(H,o2),e(H,Ll),e(Ll,n2),e(H,a2),e(H,ql),e(ql,i2),e(H,d2),e(H,Kl),e(Kl,l2),e(H,c2),e(H,Pa),e(Pa,u2),e(H,p2),e(H,Ta),e(Ta,f2),e(H,h2),e(H,ka),e(ka,m2),e(H,g2),e(te,_2),e(te,Vr),m(Ho,Vr,null),e(Vr,v2),e(Vr,Bo),e(Bo,b2),e(Bo,Rl),e(Rl,S2),e(Bo,x2),e(te,D2),e(te,Fr),m(Go,Fr,null),e(Fr,$2),e(Fr,Ql),e(Ql,E2),e(te,y2),e(te,Ir),m(zo,Ir,null),e(Ir,w2),e(Ir,Wl),e(Wl,M2),p(r,Eu,u),p(r,bt,u),e(bt,Nr),e(Nr,jl),m(Jo,jl,null),e(bt,P2),e(bt,Ul),e(Ul,T2),p(r,yu,u),p(r,Oa,u),e(Oa,k2),p(r,wu,u),p(r,re,u),m(Yo,re,null),e(re,O2),e(re,Aa),e(Aa,A2),e(Aa,Xo),e(Xo,C2),e(re,V2),e(re,B),e(B,Ca),e(Ca,F2),e(B,I2),e(B,Hl),e(Hl,N2),e(B,L2),e(B,Bl),e(Bl,q2),e(B,K2),e(B,Gl),e(Gl,R2),e(B,Q2),e(B,Va),e(Va,W2),e(B,j2),e(B,Fa),e(Fa,U2),e(B,H2),e(B,Ia),e(Ia,B2),e(B,G2),e(re,z2),e(re,Lr),m(Zo,Lr,null),e(Lr,J2),e(Lr,en),e(en,Y2),e(en,zl),e(zl,X2),e(en,Z2),e(re,eS),e(re,qr),m(tn,qr,null),e(qr,tS),e(qr,Jl),e(Jl,rS),e(re,sS),e(re,Kr),m(rn,Kr,null),e(Kr,oS),e(Kr,Yl),e(Yl,nS),p(r,Mu,u),p(r,St,u),e(St,Rr),e(Rr,Xl),m(sn,Xl,null),e(St,aS),e(St,Zl),e(Zl,iS),p(r,Pu,u),p(r,on,u),e(on,dS),e(on,nn),e(nn,lS),p(r,Tu,u),p(r,M,u),m(an,M,null),e(M,cS),e(M,ec),e(ec,uS),e(M,pS),e(M,tc),e(tc,fS),e(M,hS),e(M,G),e(G,Na),e(Na,mS),e(G,gS),e(G,rc),e(rc,_S),e(G,vS),e(G,sc),e(sc,bS),e(G,SS),e(G,oc),e(oc,xS),e(G,DS),e(G,La),e(La,$S),e(G,ES),e(G,qa),e(qa,yS),e(G,wS),e(G,Ka),e(Ka,MS),e(G,PS),e(M,TS),e(M,Ra),e(Ra,kS),e(Ra,dn),e(dn,OS),e(M,AS),e(M,Be),m(ln,Be,null),e(Be,CS),e(Be,cn),e(cn,VS),e(cn,nc),e(nc,FS),e(cn,IS),e(Be,NS),e(Be,ac),e(ac,LS),e(M,qS),e(M,z),m(un,z,null),e(z,KS),e(z,pn),e(pn,RS),e(pn,ic),e(ic,QS),e(pn,WS),e(z,jS),e(z,dc),e(dc,US),e(z,HS),e(z,lc),e(lc,BS),e(z,GS),e(z,cc),e(cc,zS),e(z,JS),e(z,uc),e(uc,fn),e(fn,YS),e(fn,pc),e(pc,XS),e(fn,ZS),e(z,e4),e(z,xt),e(xt,t4),e(xt,fc),e(fc,r4),e(xt,s4),e(xt,hc),e(hc,o4),e(xt,n4),e(M,a4),e(M,Qr),m(hn,Qr,null),e(Qr,i4),e(Qr,mc),e(mc,d4),e(M,l4),e(M,Wr),m(mn,Wr,null),e(Wr,c4),e(Wr,gn),e(gn,u4),e(gn,gc),e(gc,p4),e(gn,f4),p(r,ku,u),p(r,Dt,u),e(Dt,jr),e(jr,_c),m(_n,_c,null),e(Dt,h4),e(Dt,vc),e(vc,m4),p(r,Ou,u),p(r,Le,u),e(Le,g4),e(Le,Qa),e(Qa,_4),e(Le,v4),e(Le,vn),e(vn,b4),e(Le,S4),e(Le,bn),e(bn,x4),p(r,Au,u),p(r,se,u),m(Sn,se,null),e(se,D4),e(se,bc),e(bc,$4),e(se,E4),e(se,J),e(J,Wa),e(Wa,y4),e(J,w4),e(J,Sc),e(Sc,M4),e(J,P4),e(J,xc),e(xc,T4),e(J,k4),e(J,Dc),e(Dc,O4),e(J,A4),e(J,ja),e(ja,C4),e(J,V4),e(J,Ua),e(Ua,F4),e(J,I4),e(J,Ha),e(Ha,N4),e(J,L4),e(se,q4),e(se,Ba),e(Ba,K4),e(Ba,xn),e(xn,R4),e(se,Q4),e(se,Ur),m(Dn,Ur,null),e(Ur,W4),e(Ur,$c),e($c,j4),e(se,U4),e(se,Hr),m($n,Hr,null),e(Hr,H4),e(Hr,Ec),e(Ec,B4),Cu=!0},p(r,[u]){const En={};u&2&&(En.$$scope={dirty:u,ctx:r}),At.$set(En);const yc={};u&2&&(yc.$$scope={dirty:u,ctx:r}),Ct.$set(yc);const wc={};u&2&&(wc.$$scope={dirty:u,ctx:r}),Or.$set(wc)},i(r){Cu||(g(T.$$.fragment,r),g(Xr.$$.fragment,r),g(es.$$.fragment,r),g(ts.$$.fragment,r),g(rs.$$.fragment,r),g(ns.$$.fragment,r),g(as.$$.fragment,r),g(is.$$.fragment,r),g(At.$$.fragment,r),g(Ct.$$.fragment,r),g(ds.$$.fragment,r),g(ls.$$.fragment,r),g(cs.$$.fragment,r),g(us.$$.fragment,r),g(ps.$$.fragment,r),g(fs.$$.fragment,r),g(ms.$$.fragment,r),g(gs.$$.fragment,r),g(_s.$$.fragment,r),g(vs.$$.fragment,r),g(Ss.$$.fragment,r),g(Ds.$$.fragment,r),g($s.$$.fragment,r),g(Es.$$.fragment,r),g(ys.$$.fragment,r),g(Ts.$$.fragment,r),g(Cs.$$.fragment,r),g(Vs.$$.fragment,r),g(Ns.$$.fragment,r),g(Ls.$$.fragment,r),g(qs.$$.fragment,r),g(Ks.$$.fragment,r),g(Rs.$$.fragment,r),g(Qs.$$.fragment,r),g(js.$$.fragment,r),g(zs.$$.fragment,r),g(Js.$$.fragment,r),g(Ys.$$.fragment,r),g(Xs.$$.fragment,r),g(Zs.$$.fragment,r),g(eo.$$.fragment,r),g(ro.$$.fragment,r),g(oo.$$.fragment,r),g(no.$$.fragment,r),g(io.$$.fragment,r),g(lo.$$.fragment,r),g(co.$$.fragment,r),g(po.$$.fragment,r),g(ho.$$.fragment,r),g(mo.$$.fragment,r),g(go.$$.fragment,r),g(_o.$$.fragment,r),g(vo.$$.fragment,r),g(bo.$$.fragment,r),g(xo.$$.fragment,r),g($o.$$.fragment,r),g(Eo.$$.fragment,r),g(yo.$$.fragment,r),g(wo.$$.fragment,r),g(Mo.$$.fragment,r),g(Po.$$.fragment,r),g(ko.$$.fragment,r),g(Co.$$.fragment,r),g(Vo.$$.fragment,r),g(Fo.$$.fragment,r),g(Io.$$.fragment,r),g(Or.$$.fragment,r),g(Lo.$$.fragment,r),g(Ko.$$.fragment,r),g(Wo.$$.fragment,r),g(Ho.$$.fragment,r),g(Go.$$.fragment,r),g(zo.$$.fragment,r),g(Jo.$$.fragment,r),g(Yo.$$.fragment,r),g(Zo.$$.fragment,r),g(tn.$$.fragment,r),g(rn.$$.fragment,r),g(sn.$$.fragment,r),g(an.$$.fragment,r),g(ln.$$.fragment,r),g(un.$$.fragment,r),g(hn.$$.fragment,r),g(mn.$$.fragment,r),g(_n.$$.fragment,r),g(Sn.$$.fragment,r),g(Dn.$$.fragment,r),g($n.$$.fragment,r),Cu=!0)},o(r){_(T.$$.fragment,r),_(Xr.$$.fragment,r),_(es.$$.fragment,r),_(ts.$$.fragment,r),_(rs.$$.fragment,r),_(ns.$$.fragment,r),_(as.$$.fragment,r),_(is.$$.fragment,r),_(At.$$.fragment,r),_(Ct.$$.fragment,r),_(ds.$$.fragment,r),_(ls.$$.fragment,r),_(cs.$$.fragment,r),_(us.$$.fragment,r),_(ps.$$.fragment,r),_(fs.$$.fragment,r),_(ms.$$.fragment,r),_(gs.$$.fragment,r),_(_s.$$.fragment,r),_(vs.$$.fragment,r),_(Ss.$$.fragment,r),_(Ds.$$.fragment,r),_($s.$$.fragment,r),_(Es.$$.fragment,r),_(ys.$$.fragment,r),_(Ts.$$.fragment,r),_(Cs.$$.fragment,r),_(Vs.$$.fragment,r),_(Ns.$$.fragment,r),_(Ls.$$.fragment,r),_(qs.$$.fragment,r),_(Ks.$$.fragment,r),_(Rs.$$.fragment,r),_(Qs.$$.fragment,r),_(js.$$.fragment,r),_(zs.$$.fragment,r),_(Js.$$.fragment,r),_(Ys.$$.fragment,r),_(Xs.$$.fragment,r),_(Zs.$$.fragment,r),_(eo.$$.fragment,r),_(ro.$$.fragment,r),_(oo.$$.fragment,r),_(no.$$.fragment,r),_(io.$$.fragment,r),_(lo.$$.fragment,r),_(co.$$.fragment,r),_(po.$$.fragment,r),_(ho.$$.fragment,r),_(mo.$$.fragment,r),_(go.$$.fragment,r),_(_o.$$.fragment,r),_(vo.$$.fragment,r),_(bo.$$.fragment,r),_(xo.$$.fragment,r),_($o.$$.fragment,r),_(Eo.$$.fragment,r),_(yo.$$.fragment,r),_(wo.$$.fragment,r),_(Mo.$$.fragment,r),_(Po.$$.fragment,r),_(ko.$$.fragment,r),_(Co.$$.fragment,r),_(Vo.$$.fragment,r),_(Fo.$$.fragment,r),_(Io.$$.fragment,r),_(Or.$$.fragment,r),_(Lo.$$.fragment,r),_(Ko.$$.fragment,r),_(Wo.$$.fragment,r),_(Ho.$$.fragment,r),_(Go.$$.fragment,r),_(zo.$$.fragment,r),_(Jo.$$.fragment,r),_(Yo.$$.fragment,r),_(Zo.$$.fragment,r),_(tn.$$.fragment,r),_(rn.$$.fragment,r),_(sn.$$.fragment,r),_(an.$$.fragment,r),_(ln.$$.fragment,r),_(un.$$.fragment,r),_(hn.$$.fragment,r),_(mn.$$.fragment,r),_(_n.$$.fragment,r),_(Sn.$$.fragment,r),_(Dn.$$.fragment,r),_($n.$$.fragment,r),Cu=!1},d(r){t(S),r&&t(oe),r&&t(D),v(T),r&&t(Me),r&&t(Je),r&&t(kc),r&&t(Ye),v(Xr),r&&t(Oc),r&&t(qe),r&&t(Ac),r&&t(yt),r&&t(Cc),r&&t(Ze),v(es),r&&t(Vc),r&&t(I),r&&t(Fc),r&&t(et),v(ts),r&&t(Ic),r&&t(An),r&&t(Nc),r&&t(Pt),r&&t(Lc),r&&t(tt),v(rs),r&&t(qc),r&&t(Cn),r&&t(Kc),r&&t(Ke),r&&t(Rc),r&&t(kt),r&&t(Qc),r&&t(rt),v(ns),r&&t(Wc),r&&t(ee),v(as),v(is),v(At),v(Ct),v(ds),r&&t(jc),r&&t(ot),v(ls),r&&t(Uc),r&&t(nt),v(cs),r&&t(Hc),r&&t(at),v(us),r&&t(Bc),r&&t(it),v(ps),r&&t(Gc),r&&t(In),r&&t(zc),r&&t(A),v(fs),v(ms),v(gs),v(_s),r&&t(Jc),r&&t(dt),v(vs),r&&t(Yc),r&&t(Qt),r&&t(Xc),r&&t(C),v(Ss),v(Ds),v($s),v(Es),r&&t(Zc),r&&t(lt),v(ys),r&&t(eu),r&&t(ke),r&&t(tu),r&&t(x),v(Ts),v(Cs),v(Vs),v(Ns),v(Ls),v(qs),v(Ks),v(Rs),r&&t(ru),r&&t(ut),v(Qs),r&&t(su),r&&t(er),r&&t(ou),r&&t(E),v(js),v(zs),v(Js),v(Ys),v(Xs),v(Zs),r&&t(nu),r&&t(pt),v(eo),r&&t(au),r&&t(ir),r&&t(iu),r&&t(V),v(ro),v(oo),v(no),v(io),v(lo),r&&t(du),r&&t(ft),v(co),r&&t(lu),r&&t(fr),r&&t(cu),r&&t(y),v(po),v(ho),v(mo),v(go),v(_o),v(vo),r&&t(uu),r&&t(ht),v(bo),r&&t(pu),r&&t(br),r&&t(fu),r&&t(w),v(xo),v($o),v(Eo),v(yo),v(wo),v(Mo),r&&t(hu),r&&t(gt),v(Po),r&&t(mu),r&&t(yr),r&&t(gu),r&&t(F),v(ko),v(Co),v(Vo),v(Fo),r&&t(_u),r&&t(_t),v(Io),r&&t(vu),r&&t(kr),r&&t(bu),v(Or,r),r&&t(Su),r&&t(ne),v(Lo),r&&t(xu),r&&t(vt),v(Ko),r&&t(Du),r&&t(He),r&&t($u),r&&t(te),v(Wo),v(Ho),v(Go),v(zo),r&&t(Eu),r&&t(bt),v(Jo),r&&t(yu),r&&t(Oa),r&&t(wu),r&&t(re),v(Yo),v(Zo),v(tn),v(rn),r&&t(Mu),r&&t(St),v(sn),r&&t(Pu),r&&t(on),r&&t(Tu),r&&t(M),v(an),v(ln),v(un),v(hn),v(mn),r&&t(ku),r&&t(Dt),v(_n),r&&t(Ou),r&&t(Le),r&&t(Au),r&&t(se),v(Sn),v(Dn),v($n)}}}const Hy={local:"schedulers",sections:[{local:"what-is-a-scheduler",sections:[{local:"discrete-versus-continuous-schedulers",title:"Discrete versus continuous schedulers"}],title:"What is a scheduler?"},{local:"designing-reusable-schedulers",title:"Designing Re-usable schedulers"},{local:"api",sections:[{local:"diffusers.SchedulerMixin",title:"SchedulerMixin"},{local:"diffusers.schedulers.scheduling_utils.SchedulerOutput",title:"SchedulerOutput"},{local:"implemented-schedulers",sections:[{local:"diffusers.DDIMScheduler",title:"Denoising diffusion implicit models (DDIM)"},{local:"diffusers.DDPMScheduler",title:"Denoising diffusion probabilistic models (DDPM)"},{local:"diffusers.DPMSolverMultistepScheduler",title:"Multistep DPM-Solver"},{local:"diffusers.KarrasVeScheduler",title:"Variance exploding, stochastic sampling from Karras et. al"},{local:"diffusers.LMSDiscreteScheduler",title:"Linear multistep scheduler for discrete beta schedules"},{local:"diffusers.PNDMScheduler",title:"Pseudo numerical methods for diffusion models (PNDM)"},{local:"diffusers.ScoreSdeVeScheduler",title:"variance exploding stochastic differential equation (VE-SDE) scheduler"},{local:"diffusers.IPNDMScheduler",title:"improved pseudo numerical methods for diffusion models (iPNDM)"},{local:"diffusers.schedulers.ScoreSdeVpScheduler",title:"variance preserving stochastic differential equation (VP-SDE) scheduler"},{local:"diffusers.EulerDiscreteScheduler",title:"Euler scheduler"},{local:"diffusers.EulerAncestralDiscreteScheduler",title:"Euler Ancestral scheduler"},{local:"diffusers.VQDiffusionScheduler",title:"VQDiffusionScheduler"},{local:"diffusers.RePaintScheduler",title:"RePaint scheduler"}],title:"Implemented Schedulers"}],title:"API"}],title:"Schedulers"};function By($t){return Ry(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Xy extends Ny{constructor(S){super();Ly(this,S,By,Uy,qy,{})}}export{Xy as default,Hy as metadata};
