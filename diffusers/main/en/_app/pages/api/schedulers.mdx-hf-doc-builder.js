import{S as $E,i as EE,s as xE,e as s,k as l,w as p,t as i,M as DE,c as n,d as t,m as c,a as o,x as h,h as a,b as d,G as e,g as f,y as m,q as g,o as _,B as v,v as yE}from"../../chunks/vendor-hf-doc-builder.js";import{T as wE}from"../../chunks/Tip-hf-doc-builder.js";import{D as b}from"../../chunks/Docstring-hf-doc-builder.js";import{I as y}from"../../chunks/IconCopyLink-hf-doc-builder.js";function PE(Wd){let Y,et;return{c(){Y=s("p"),et=i("Score SDE-VP is under construction.")},l(J){Y=n(J,"P",{});var _e=o(Y);et=a(_e,"Score SDE-VP is under construction."),_e.forEach(t)},m(J,_e){f(J,Y,_e),e(Y,et)},d(J){J&&t(Y)}}}function TE(Wd){let Y,et,J,_e,Jo,gr,bu,Xo,Su,Hd,yn,$u,Gd,Ce,tt,Zo,_r,Eu,ei,xu,zd,rt,Du,ti,yu,wu,Yd,st,wn,Pu,vr,ri,Tu,Mu,si,Cu,ku,ke,Au,ni,Ou,Vu,oi,Nu,Fu,Jd,Ae,nt,ii,br,Iu,ai,Lu,Xd,C,qu,di,Ku,Ru,Pn,Qu,ju,Tn,Uu,Bu,li,Wu,Hu,Mn,Gu,zu,ci,Yu,Ju,Zd,Oe,ot,ui,Sr,Xu,fi,Zu,el,Cn,ef,tl,it,pi,tf,rf,hi,sf,rl,Ve,at,mi,$r,nf,gi,of,sl,kn,af,nl,xe,Er,df,_i,lf,cf,uf,xr,ff,vi,pf,hf,mf,bi,gf,ol,dt,_f,An,vf,bf,il,Ne,lt,Si,Dr,Sf,$i,$f,al,Fe,yr,Ef,Ei,xf,dl,Ie,ct,xi,wr,Df,Di,yf,ll,Le,Pr,wf,yi,Pf,cl,qe,ut,wi,Tr,Tf,Pi,Mf,ul,Ke,ft,Ti,Mr,Cf,Mi,kf,fl,On,Af,pl,w,Cr,Of,Ci,Vf,Nf,k,Vn,Ff,If,ki,Lf,qf,Ai,Kf,Rf,Oi,Qf,jf,Nn,Uf,Bf,Fn,Wf,Hf,In,Gf,zf,Yf,Ln,Jf,kr,Xf,Zf,pt,Ar,ep,Vi,tp,rp,ht,Or,sp,Ni,np,op,mt,Vr,ip,Fi,ap,hl,Re,gt,Ii,Nr,dp,Li,lp,ml,_t,cp,Fr,up,fp,gl,P,Ir,pp,qi,hp,mp,A,qn,gp,_p,Ki,vp,bp,Ri,Sp,$p,Qi,Ep,xp,Kn,Dp,yp,Rn,wp,Pp,Qn,Tp,Mp,Cp,jn,kp,Lr,Ap,Op,vt,qr,Vp,ji,Np,Fp,bt,Kr,Ip,Ui,Lp,qp,St,Rr,Kp,Bi,Rp,_l,Qe,$t,Wi,Qr,Qp,Hi,jp,vl,Et,Up,jr,Bp,Wp,bl,S,Ur,Hp,Gi,Gp,zp,xt,Yp,Br,Jp,Xp,Wr,Zp,eh,O,Un,th,rh,zi,sh,nh,Yi,oh,ih,Ji,ah,dh,Bn,lh,ch,Wn,uh,fh,Hn,ph,hh,mh,Hr,gh,Gr,_h,vh,bh,De,zr,Sh,Xi,$h,Eh,Zi,xh,Dh,Dt,Yr,yh,ea,wh,Ph,yt,Jr,Th,ta,Mh,Ch,wt,Xr,kh,ra,Ah,Oh,Pt,Zr,Vh,sa,Nh,Sl,je,Tt,na,es,Fh,oa,Ih,$l,Mt,Lh,ts,qh,Kh,El,T,rs,Rh,Gn,Qh,ss,jh,Uh,V,zn,Bh,Wh,ia,Hh,Gh,aa,zh,Yh,da,Jh,Xh,Yn,Zh,em,Jn,tm,rm,Xn,sm,nm,om,Ct,ns,im,la,am,dm,kt,os,lm,is,cm,ca,um,fm,pm,At,as,hm,ua,mm,gm,Ot,ds,_m,fa,vm,xl,Ue,Vt,pa,ls,bm,ha,Sm,Dl,Nt,$m,cs,Em,xm,yl,$,us,Dm,ma,ym,wm,N,Zn,Pm,Tm,ga,Mm,Cm,_a,km,Am,va,Om,Vm,eo,Nm,Fm,to,Im,Lm,ro,qm,Km,Rm,so,Qm,fs,jm,Um,Ft,ps,Bm,ba,Wm,Hm,It,hs,Gm,Sa,zm,Ym,ye,ms,Jm,$a,Xm,Zm,$e,eg,Ea,tg,rg,xa,sg,ng,Da,og,ig,ag,Lt,gs,dg,ya,lg,cg,qt,_s,ug,wa,fg,wl,Be,Kt,Pa,vs,pg,Ta,hg,Pl,Rt,mg,bs,gg,_g,Tl,E,Ss,vg,Ma,bg,Sg,no,$g,$s,Eg,xg,F,oo,Dg,yg,Ca,wg,Pg,ka,Tg,Mg,Aa,Cg,kg,io,Ag,Og,ao,Vg,Ng,lo,Fg,Ig,Lg,Qt,Es,qg,Oa,Kg,Rg,we,xs,Qg,Va,jg,Ug,We,Bg,Na,Wg,Hg,Fa,Gg,zg,Yg,jt,Ds,Jg,Ia,Xg,Zg,Ut,ys,e_,La,t_,r_,Bt,ws,s_,qa,n_,Ml,He,Wt,Ka,Ps,o_,Ra,i_,Cl,Ht,a_,Ts,d_,l_,kl,M,Ms,c_,co,u_,Cs,f_,p_,I,uo,h_,m_,Qa,g_,__,ja,v_,b_,Ua,S_,$_,fo,E_,x_,po,D_,y_,ho,w_,P_,T_,mo,M_,ks,C_,k_,Gt,As,A_,Ba,O_,V_,zt,Os,N_,Wa,F_,I_,Yt,Vs,L_,Ha,q_,Al,Ge,Jt,Ga,Ns,K_,za,R_,Ol,Xt,Q_,Fs,j_,U_,Vl,Zt,Nl,X,Is,B_,Ya,W_,H_,L,go,G_,z_,Ja,Y_,J_,Xa,X_,Z_,Za,ev,tv,_o,rv,sv,vo,nv,ov,bo,iv,av,dv,So,lv,Ls,cv,uv,ed,fv,Fl,ze,er,td,qs,pv,rd,hv,Il,Pe,mv,Ks,gv,_v,Rs,vv,bv,Ll,H,Qs,Sv,tr,$v,js,Ev,xv,Us,Dv,yv,q,$o,wv,Pv,sd,Tv,Mv,nd,Cv,kv,od,Av,Ov,Eo,Vv,Nv,xo,Fv,Iv,Do,Lv,qv,Kv,rr,Bs,Rv,Ws,Qv,id,jv,Uv,Bv,sr,Hs,Wv,ad,Hv,Gv,nr,Gs,zv,dd,Yv,ql,Ye,or,ld,zs,Jv,cd,Xv,Kl,yo,Zv,Rl,G,Ys,eb,wo,tb,Js,rb,sb,K,Po,nb,ob,ud,ib,ab,fd,db,lb,pd,cb,ub,To,fb,pb,Mo,hb,mb,Co,gb,_b,vb,ir,Xs,bb,Zs,Sb,hd,$b,Eb,xb,ar,en,Db,md,yb,wb,dr,tn,Pb,gd,Tb,Ql,Je,lr,_d,rn,Mb,vd,Cb,jl,sn,kb,nn,Ab,Ul,x,on,Ob,bd,Vb,Nb,Sd,Fb,Ib,R,ko,Lb,qb,$d,Kb,Rb,Ed,Qb,jb,xd,Ub,Bb,Ao,Wb,Hb,Oo,Gb,zb,Vo,Yb,Jb,Xb,No,Zb,an,e1,t1,Te,dn,r1,ln,s1,Dd,n1,o1,i1,yd,a1,d1,Q,cn,l1,un,c1,wd,u1,f1,p1,Pd,h1,m1,Td,g1,_1,Md,v1,b1,Cd,fn,S1,kd,$1,E1,x1,Xe,D1,Ad,y1,w1,Od,P1,T1,M1,cr,pn,C1,Vd,k1,A1,ur,hn,O1,mn,V1,Nd,N1,F1,Bl,Ze,fr,Fd,gn,I1,Id,L1,Wl,Ee,q1,Fo,K1,R1,_n,Q1,j1,vn,U1,Hl,z,bn,B1,Ld,W1,H1,j,Io,G1,z1,qd,Y1,J1,Kd,X1,Z1,Rd,e0,t0,Lo,r0,s0,qo,n0,o0,Ko,i0,a0,d0,Ro,l0,Sn,c0,u0,pr,$n,f0,Qd,p0,h0,hr,En,m0,jd,g0,Gl;return gr=new y({}),_r=new y({}),br=new y({}),Sr=new y({}),$r=new y({}),Dr=new y({}),yr=new b({props:{name:"class diffusers.SchedulerMixin",anchor:"diffusers.SchedulerMixin",parameters:[],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L38"}}),wr=new y({}),Pr=new b({props:{name:"class diffusers.schedulers.scheduling_utils.SchedulerOutput",anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput",parameters:[{name:"prev_sample",val:": FloatTensor"}],parametersDescription:[{anchor:"diffusers.schedulers.scheduling_utils.SchedulerOutput.prev_sample",description:`<strong>prev_sample</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code> for images) &#x2014;
Computed sample (x_{t-1}) of previous timestep. <code>prev_sample</code> should be used as next model input in the
denoising loop.`,name:"prev_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_utils.py#L25"}}),Tr=new y({}),Mr=new y({}),Cr=new b({props:{name:"class diffusers.DDIMScheduler",anchor:"diffusers.DDIMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"},{name:"set_alpha_to_one",val:": bool = True"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDIMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDIMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDIMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDIMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDIMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"},{anchor:"diffusers.DDIMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.DDIMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L78"}}),Ar=new b({props:{name:"scale_model_input",anchor:"diffusers.DDIMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDIMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L163",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Or=new b({props:{name:"set_timesteps",anchor:"diffusers.DDIMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L187"}}),Vr=new b({props:{name:"step",anchor:"diffusers.DDIMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"eta",val:": float = 0.0"},{name:"use_clipped_model_output",val:": bool = False"},{name:"generator",val:" = None"},{name:"variance_noise",val:": typing.Optional[torch.FloatTensor] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDIMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDIMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDIMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DDIMScheduler.step.eta",description:"<strong>eta</strong> (<code>float</code>) &#x2014; weight of noise for added noise in diffusion step.",name:"eta"},{anchor:"diffusers.DDIMScheduler.step.use_clipped_model_output",description:`<strong>use_clipped_model_output</strong> (<code>bool</code>) &#x2014; if <code>True</code>, compute &#x201C;corrected&#x201D; <code>model_output</code> from the clipped
predicted original sample. Necessary because predicted original sample is clipped to [-1, 1] when
<code>self.config.clip_sample</code> is <code>True</code>. If no clipping has happened, &#x201C;corrected&#x201D; <code>model_output</code> would
coincide with the one provided as input and <code>use_clipped_model_output</code> will have not effect.
generator &#x2014; random number generator.`,name:"use_clipped_model_output"},{anchor:"diffusers.DDIMScheduler.step.variance_noise",description:`<strong>variance_noise</strong> (<code>torch.FloatTensor</code>) &#x2014; instead of generating noise for the variance using <code>generator</code>, we
can directly provide the noise for the variance itself. This is useful for methods such as
CycleDiffusion. (<a href="https://arxiv.org/abs/2210.05559" rel="nofollow">https://arxiv.org/abs/2210.05559</a>)`,name:"variance_noise"},{anchor:"diffusers.DDIMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDIMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L203",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDIMSchedulerOutput</code> or <code>tuple</code></p>
`}}),Nr=new y({}),Ir=new b({props:{name:"class diffusers.DDPMScheduler",anchor:"diffusers.DDPMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"variance_type",val:": str = 'fixed_small'"},{name:"clip_sample",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.DDPMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.DDPMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.DDPMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.DDPMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.DDPMScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.DDPMScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L76"}}),qr=new b({props:{name:"scale_model_input",anchor:"diffusers.DDPMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.DDPMScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L156",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Kr=new b({props:{name:"set_timesteps",anchor:"diffusers.DDPMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L170"}}),Rr=new b({props:{name:"step",anchor:"diffusers.DDPMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"predict_epsilon",val:" = True"},{name:"generator",val:" = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.DDPMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.DDPMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.DDPMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.DDPMScheduler.step.predict_epsilon",description:`<strong>predict_epsilon</strong> (<code>bool</code>) &#x2014;
optional flag to use when model predicts the samples directly instead of the noise, epsilon.
generator &#x2014; random number generator.`,name:"predict_epsilon"},{anchor:"diffusers.DDPMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than DDPMSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddpm.py#L218",returnDescription:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.DDPMSchedulerOutput</code> or <code>tuple</code></p>
`}}),Qr=new y({}),Ur=new b({props:{name:"class diffusers.KarrasVeScheduler",anchor:"diffusers.KarrasVeScheduler",parameters:[{name:"sigma_min",val:": float = 0.02"},{name:"sigma_max",val:": float = 100"},{name:"s_noise",val:": float = 1.007"},{name:"s_churn",val:": float = 80"},{name:"s_min",val:": float = 0.05"},{name:"s_max",val:": float = 50"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.sigma_min",description:"<strong>sigma_min</strong> (<code>float</code>) &#x2014; minimum noise magnitude",name:"sigma_min"},{anchor:"diffusers.KarrasVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum noise magnitude",name:"sigma_max"},{anchor:"diffusers.KarrasVeScheduler.s_noise",description:`<strong>s_noise</strong> (<code>float</code>) &#x2014; the amount of additional noise to counteract loss of detail during sampling.
A reasonable range is [1.000, 1.011].`,name:"s_noise"},{anchor:"diffusers.KarrasVeScheduler.s_churn",description:`<strong>s_churn</strong> (<code>float</code>) &#x2014; the parameter controlling the overall amount of stochasticity.
A reasonable range is [0, 100].`,name:"s_churn"},{anchor:"diffusers.KarrasVeScheduler.s_min",description:`<strong>s_min</strong> (<code>float</code>) &#x2014; the start value of the sigma range where we add noise (enable stochasticity).
A reasonable range is [0, 10].`,name:"s_min"},{anchor:"diffusers.KarrasVeScheduler.s_max",description:`<strong>s_max</strong> (<code>float</code>) &#x2014; the end value of the sigma range where we add noise.
A reasonable range is [0.2, 80].`,name:"s_max"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L48"}}),zr=new b({props:{name:"add_noise_to_input",anchor:"diffusers.KarrasVeScheduler.add_noise_to_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"sigma",val:": float"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L133"}}),Yr=new b({props:{name:"scale_model_input",anchor:"diffusers.KarrasVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.KarrasVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L98",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Jr=new b({props:{name:"set_timesteps",anchor:"diffusers.KarrasVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L112"}}),Xr=new b({props:{name:"step",anchor:"diffusers.KarrasVeScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class</p>
<p>KarrasVeOutput &#x2014; updated sample in the diffusion chain and derivative (TODO double check).`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L154",returnDescription:`
<p><code>KarrasVeOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>KarrasVeOutput</code> or <code>tuple</code></p>
`}}),Zr=new b({props:{name:"step_correct",anchor:"diffusers.KarrasVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sigma_hat",val:": float"},{name:"sigma_prev",val:": float"},{name:"sample_hat",val:": FloatTensor"},{name:"sample_prev",val:": FloatTensor"},{name:"derivative",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.KarrasVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_hat",description:"<strong>sigma_hat</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sigma_prev",description:"<strong>sigma_prev</strong> (<code>float</code>) &#x2014; TODO",name:"sigma_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_hat",description:"<strong>sample_hat</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_hat"},{anchor:"diffusers.KarrasVeScheduler.step_correct.sample_prev",description:"<strong>sample_prev</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"sample_prev"},{anchor:"diffusers.KarrasVeScheduler.step_correct.derivative",description:"<strong>derivative</strong> (<code>torch.FloatTensor</code>) &#x2014; TODO",name:"derivative"},{anchor:"diffusers.KarrasVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than KarrasVeOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_karras_ve.py#L192",returnDescription:`
<p>updated sample in the diffusion chain. derivative (TODO): TODO</p>
`,returnType:`
<p>prev_sample (TODO)</p>
`}}),es=new y({}),rs=new b({props:{name:"class diffusers.LMSDiscreteScheduler",anchor:"diffusers.LMSDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.LMSDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.LMSDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.LMSDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.LMSDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L47"}}),ns=new b({props:{name:"get_lms_coefficient",anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient",parameters:[{name:"order",val:""},{name:"t",val:""},{name:"current_order",val:""}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.order",description:"<strong>order</strong> (TODO) &#x2014;",name:"order"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.t",description:"<strong>t</strong> (TODO) &#x2014;",name:"t"},{anchor:"diffusers.LMSDiscreteScheduler.get_lms_coefficient.current_order",description:"<strong>current_order</strong> (TODO) &#x2014;",name:"current_order"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L137"}}),os=new b({props:{name:"scale_model_input",anchor:"diffusers.LMSDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L116",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),as=new b({props:{name:"set_timesteps",anchor:"diffusers.LMSDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.LMSDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L159"}}),ds=new b({props:{name:"step",anchor:"diffusers.LMSDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"order",val:": int = 4"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.LMSDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.LMSDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.LMSDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
order &#x2014; coefficient for multi-step inference.`,name:"sample"},{anchor:"diffusers.LMSDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than LMSDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py#L180",returnDescription:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.LMSDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),ls=new y({}),us=new b({props:{name:"class diffusers.PNDMScheduler",anchor:"diffusers.PNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"skip_prk_steps",val:": bool = False"},{name:"set_alpha_to_one",val:": bool = False"},{name:"steps_offset",val:": int = 0"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.PNDMScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.PNDMScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.PNDMScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.PNDMScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.PNDMScheduler.skip_prk_steps",description:`<strong>skip_prk_steps</strong> (<code>bool</code>) &#x2014;
allows the scheduler to skip the Runge-Kutta steps that are defined in the original paper as being required
before plms steps; defaults to <code>False</code>.`,name:"skip_prk_steps"},{anchor:"diffusers.PNDMScheduler.set_alpha_to_one",description:`<strong>set_alpha_to_one</strong> (<code>bool</code>, default <code>False</code>) &#x2014;
each diffusion step uses the value of alphas product at that step and at the previous one. For the final
step there is no previous alpha. When this option is <code>True</code> the previous alpha product is fixed to <code>1</code>,
otherwise it uses the value of alpha at step 0.`,name:"set_alpha_to_one"},{anchor:"diffusers.PNDMScheduler.steps_offset",description:`<strong>steps_offset</strong> (<code>int</code>, default <code>0</code>) &#x2014;
an offset added to the inference steps. You can use a combination of <code>offset=1</code> and
<code>set_alpha_to_one=False</code>, to make the last step use step 0 for the previous alpha product, as done in
stable diffusion.`,name:"steps_offset"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L56"}}),ps=new b({props:{name:"scale_model_input",anchor:"diffusers.PNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L344",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),hs=new b({props:{name:"set_timesteps",anchor:"diffusers.PNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L152"}}),ms=new b({props:{name:"step",anchor:"diffusers.PNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L191",returnDescription:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><a
  href="/docs/diffusers/main/en/api/schedulers#diffusers.schedulers.scheduling_utils.SchedulerOutput"
>SchedulerOutput</a> or <code>tuple</code></p>
`}}),gs=new b({props:{name:"step_plms",anchor:"diffusers.PNDMScheduler.step_plms",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_plms.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_plms.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_plms.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_plms.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L277",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),_s=new b({props:{name:"step_prk",anchor:"diffusers.PNDMScheduler.step_prk",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.PNDMScheduler.step_prk.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.PNDMScheduler.step_prk.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.PNDMScheduler.step_prk.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.PNDMScheduler.step_prk.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py#L222",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),vs=new y({}),Ss=new b({props:{name:"class diffusers.ScoreSdeVeScheduler",anchor:"diffusers.ScoreSdeVeScheduler",parameters:[{name:"num_train_timesteps",val:": int = 2000"},{name:"snr",val:": float = 0.15"},{name:"sigma_min",val:": float = 0.01"},{name:"sigma_max",val:": float = 1348.0"},{name:"sampling_eps",val:": float = 1e-05"},{name:"correct_steps",val:": int = 1"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.ScoreSdeVeScheduler.snr",description:`<strong>snr</strong> (<code>float</code>) &#x2014;
coefficient weighting the step from the model_output sample (from the network) to the random noise.`,name:"snr"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>) &#x2014;
initial noise scale for sigma sequence in sampling procedure. The minimum sigma should mirror the
distribution of the data.`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>) &#x2014; maximum value used for the range of continuous timesteps passed into the model.",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.sampling_eps",description:`<strong>sampling_eps</strong> (<code>float</code>) &#x2014; the end value of sampling, where timesteps decrease progressively from 1 to
epsilon. &#x2014;`,name:"sampling_eps"},{anchor:"diffusers.ScoreSdeVeScheduler.correct_steps",description:"<strong>correct_steps</strong> (<code>int</code>) &#x2014; number of correction steps performed on a produced sample.",name:"correct_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L45"}}),Es=new b({props:{name:"scale_model_input",anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L87",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),xs=new b({props:{name:"set_sigmas",anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas",parameters:[{name:"num_inference_steps",val:": int"},{name:"sigma_min",val:": float = None"},{name:"sigma_max",val:": float = None"},{name:"sampling_eps",val:": float = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_min",description:`<strong>sigma_min</strong> (<code>float</code>, optional) &#x2014;
initial noise scale value (overrides value given at Scheduler instantiation).`,name:"sigma_min"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sigma_max",description:"<strong>sigma_max</strong> (<code>float</code>, optional) &#x2014; final noise scale value (overrides value given at Scheduler instantiation).",name:"sigma_max"},{anchor:"diffusers.ScoreSdeVeScheduler.set_sigmas.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L117"}}),Ds=new b({props:{name:"set_timesteps",anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"sampling_eps",val:": float = None"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.ScoreSdeVeScheduler.set_timesteps.sampling_eps",description:"<strong>sampling_eps</strong> (<code>float</code>, optional) &#x2014; final timestep value (overrides value given at Scheduler instantiation).",name:"sampling_eps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L101"}}),ys=new b({props:{name:"step_correct",anchor:"diffusers.ScoreSdeVeScheduler.step_correct",parameters:[{name:"model_output",val:": FloatTensor"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_correct.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L212",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),ws=new b({props:{name:"step_pred",anchor:"diffusers.ScoreSdeVeScheduler.step_pred",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.
generator &#x2014; random number generator.`,name:"sample"},{anchor:"diffusers.ScoreSdeVeScheduler.step_pred.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_ve.py#L151",returnDescription:`
<p><code>SdeVeOutput</code> if
<code>return_dict</code> is True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>SdeVeOutput</code> or <code>tuple</code></p>
`}}),Ps=new y({}),Ms=new b({props:{name:"class diffusers.IPNDMScheduler",anchor:"diffusers.IPNDMScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L24"}}),As=new b({props:{name:"scale_model_input",anchor:"diffusers.IPNDMScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L126",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Os=new b({props:{name:"set_timesteps",anchor:"diffusers.IPNDMScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L56"}}),Vs=new b({props:{name:"step",anchor:"diffusers.IPNDMScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.IPNDMScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.IPNDMScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.IPNDMScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.IPNDMScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than SchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ipndm.py#L76",returnDescription:`
<p><code>~scheduling_utils.SchedulerOutput</code> if <code>return_dict</code> is
True, otherwise a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~scheduling_utils.SchedulerOutput</code> or <code>tuple</code></p>
`}}),Ns=new y({}),Zt=new wE({props:{warning:!0,$$slots:{default:[PE]},$$scope:{ctx:Wd}}}),Is=new b({props:{name:"class diffusers.schedulers.ScoreSdeVpScheduler",anchor:"diffusers.schedulers.ScoreSdeVpScheduler",parameters:[{name:"num_train_timesteps",val:" = 2000"},{name:"beta_min",val:" = 0.1"},{name:"beta_max",val:" = 20"},{name:"sampling_eps",val:" = 0.001"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_sde_vp.py#L26"}}),qs=new y({}),Qs=new b({props:{name:"class diffusers.EulerDiscreteScheduler",anchor:"diffusers.EulerDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L48"}}),Bs=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L116",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),Hs=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L137"}}),Gs=new b({props:{name:"step",anchor:"diffusers.EulerDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"s_churn",val:": float = 0.0"},{name:"s_tmin",val:": float = 0.0"},{name:"s_tmax",val:": float = inf"},{name:"s_noise",val:": float = 1.0"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_churn",description:"<strong>s_churn</strong> (<code>float</code>) &#x2014;",name:"s_churn"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmin",description:"<strong>s_tmin</strong>  (<code>float</code>) &#x2014;",name:"s_tmin"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_tmax",description:"<strong>s_tmax</strong>  (<code>float</code>) &#x2014;",name:"s_tmax"},{anchor:"diffusers.EulerDiscreteScheduler.step.s_noise",description:"<strong>s_noise</strong> (<code>float</code>) &#x2014;",name:"s_noise"},{anchor:"diffusers.EulerDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_discrete.py#L156",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a
<code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),zs=new y({}),Ys=new b({props:{name:"class diffusers.EulerAncestralDiscreteScheduler",anchor:"diffusers.EulerAncestralDiscreteScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code> or <code>scaled_linear</code>.`,name:"beta_schedule"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L48"}}),Xs=new b({props:{name:"scale_model_input",anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>float</code> or <code>torch.FloatTensor</code>) &#x2014; the current timestep in the diffusion chain",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L115",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),en=new b({props:{name:"set_timesteps",anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>, optional) &#x2014;
the device to which the timesteps should be moved to. If <code>None</code>, the timesteps are not moved.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L136"}}),tn=new b({props:{name:"step",anchor:"diffusers.EulerAncestralDiscreteScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": typing.Union[float, torch.FloatTensor]"},{name:"sample",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.model_output",description:"<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned diffusion model.",name:"model_output"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.timestep",description:"<strong>timestep</strong> (<code>float</code>) &#x2014; current timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, optional) &#x2014; Random number generator.",name:"generator"},{anchor:"diffusers.EulerAncestralDiscreteScheduler.step.return_dict",description:"<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than EulerAncestralDiscreteSchedulerOutput class",name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py#L155",returnDescription:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> if <code>return_dict</code> is True, otherwise
a <code>tuple</code>. When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.EulerAncestralDiscreteSchedulerOutput</code> or <code>tuple</code></p>
`}}),rn=new y({}),on=new b({props:{name:"class diffusers.VQDiffusionScheduler",anchor:"diffusers.VQDiffusionScheduler",parameters:[{name:"num_vec_classes",val:": int"},{name:"num_train_timesteps",val:": int = 100"},{name:"alpha_cum_start",val:": float = 0.99999"},{name:"alpha_cum_end",val:": float = 9e-06"},{name:"gamma_cum_start",val:": float = 9e-06"},{name:"gamma_cum_end",val:": float = 0.99999"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.num_vec_classes",description:`<strong>num_vec_classes</strong> (<code>int</code>) &#x2014;
The number of classes of the vector embeddings of the latent pixels. Includes the class for the masked
latent pixel.`,name:"num_vec_classes"},{anchor:"diffusers.VQDiffusionScheduler.num_train_timesteps",description:`<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014;
Number of diffusion steps used to train the model.`,name:"num_train_timesteps"},{anchor:"diffusers.VQDiffusionScheduler.alpha_cum_start",description:`<strong>alpha_cum_start</strong> (<code>float</code>) &#x2014;
The starting cumulative alpha value.`,name:"alpha_cum_start"},{anchor:"diffusers.VQDiffusionScheduler.alpha_cum_end",description:`<strong>alpha_cum_end</strong> (<code>float</code>) &#x2014;
The ending cumulative alpha value.`,name:"alpha_cum_end"},{anchor:"diffusers.VQDiffusionScheduler.gamma_cum_start",description:`<strong>gamma_cum_start</strong> (<code>float</code>) &#x2014;
The starting cumulative gamma value.`,name:"gamma_cum_start"},{anchor:"diffusers.VQDiffusionScheduler.gamma_cum_end",description:`<strong>gamma_cum_end</strong> (<code>float</code>) &#x2014;
The ending cumulative gamma value.`,name:"gamma_cum_end"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L106"}}),dn=new b({props:{name:"log_Q_t_transitioning_to_known_class",anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class",parameters:[{name:"t",val:": torch.int32"},{name:"x_t",val:": LongTensor"},{name:"log_onehot_x_t",val:": FloatTensor"},{name:"cumulative",val:": bool"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.t",description:`<strong>t</strong> (torch.Long) &#x2014;
The timestep that determines which transition matrix is used.`,name:"t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.x_t",description:`<strong>x_t</strong> (<code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>) &#x2014;
The classes of each latent pixel at time <code>t</code>.`,name:"x_t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.log_onehot_x_t",description:`<strong>log_onehot_x_t</strong> (<code>torch.FloatTensor</code> of shape <code>(batch size, num classes, num latent pixels)</code>) &#x2014;
The log one-hot vectors of <code>x_t</code>`,name:"log_onehot_x_t"},{anchor:"diffusers.VQDiffusionScheduler.log_Q_t_transitioning_to_known_class.cumulative",description:`<strong>cumulative</strong> (<code>bool</code>) &#x2014;
If cumulative is <code>False</code>, we use the single step transition matrix <code>t-1</code>-&gt;<code>t</code>. If cumulative is <code>True</code>,
we use the cumulative transition matrix <code>0</code>-&gt;<code>t</code>.`,name:"cumulative"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L377",returnDescription:`
<p>Each <em>column</em> of the returned matrix is a <em>row</em> of log probabilities of the complete probability
transition matrix.</p>
<p>When non cumulative, returns <code>self.num_classes - 1</code> rows because the initial latent pixel cannot be
masked.</p>
<p>Where:</p>
<ul>
<li><code>q_n</code> is the probability distribution for the forward process of the <code>n</code>th latent pixel.</li>
<li>C_0 is a class of a latent pixel embedding</li>
<li>C_k is the class of the masked latent pixel</li>
</ul>
<p>non-cumulative result (omitting logarithms):</p>

	<CodeBlock 
		code={\`q_0(x_t | x_{t-1\\} = C_0) ... q_n(x_t | x_{t-1\\} = C_0)
          .      .                     .
          .               .            .
          .                      .     .
q_0(x_t | x_{t-1\\} = C_k) ... q_n(x_t | x_{t-1\\} = C_k)\`}
		highlighted={\`q<span class="hljs-constructor">_0(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_0)</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_0)</span>
          .      .                     .
          .               .            .
          .                      .     .
q<span class="hljs-constructor">_0(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_k)</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_</span>{<span class="hljs-params">t</span>-1\\} = C_k)</span>\`}
	/>
<p>cumulative result (omitting logarithms):</p>

	<CodeBlock 
		code={\`q_0_cumulative(x_t | x_0 = C_0)    ...  q_n_cumulative(x_t | x_0 = C_0)
          .               .                          .
          .                        .                 .
          .                               .          .
q_0_cumulative(x_t | x_0 = C_{k-1\\}) ... q_n_cumulative(x_t | x_0 = C_{k-1\\})\`}
		highlighted={\`q<span class="hljs-constructor">_0_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_0)</span><span class="hljs-operator">    ...  </span>q<span class="hljs-constructor">_n_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_0)</span>
          .               .                          .
          .                        .                 .
          .                               .          .
q<span class="hljs-constructor">_0_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_{<span class="hljs-params">k</span>-1\\})</span><span class="hljs-operator"> ... </span>q<span class="hljs-constructor">_n_cumulative(<span class="hljs-params">x_t</span> | <span class="hljs-params">x_0</span> = C_{<span class="hljs-params">k</span>-1\\})</span>\`}
	/>
`,returnType:`
<p><code>torch.FloatTensor</code> of shape <code>(batch size, num classes - 1, num latent pixels)</code></p>
`}}),cn=new b({props:{name:"q_posterior",anchor:"diffusers.VQDiffusionScheduler.q_posterior",parameters:[{name:"log_p_x_0",val:""},{name:"x_t",val:""},{name:"t",val:""}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.q_posterior.t",description:`<strong>t</strong> (torch.Long) &#x2014;
The timestep that determines which transition matrix is used.`,name:"t"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L258",returnDescription:`
<p>The log probabilities for the predicted classes of the image at timestep <code>t-1</code>. I.e. Equation (11).</p>
`,returnType:`
<p><code>torch.FloatTensor</code> of shape <code>(batch size, num classes, num latent pixels)</code></p>
`}}),pn=new b({props:{name:"set_timesteps",anchor:"diffusers.VQDiffusionScheduler.set_timesteps",parameters:[{name:"num_inference_steps",val:": int"},{name:"device",val:": typing.Union[str, torch.device] = None"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.set_timesteps.num_inference_steps",description:`<strong>num_inference_steps</strong> (<code>int</code>) &#x2014;
the number of diffusion steps used when generating samples with a pre-trained model.`,name:"num_inference_steps"},{anchor:"diffusers.VQDiffusionScheduler.set_timesteps.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014;
device to place the timesteps and the diffusion process parameters (alpha, beta, gamma) on.`,name:"device"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L188"}}),hn=new b({props:{name:"step",anchor:"diffusers.VQDiffusionScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": torch.int64"},{name:"sample",val:": LongTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.VQDiffusionScheduler.step.t",description:`<strong>t</strong> (<code>torch.long</code>) &#x2014;
The timestep that determines which transition matrices are used.</p>
<p>x_t &#x2014; (<code>torch.LongTensor</code> of shape <code>(batch size, num latent pixels)</code>):
The classes of each latent pixel at time <code>t</code></p>
<p>generator &#x2014; (<code>torch.Generator</code> or None):
RNG for the noise applied to p(x_{t-1} | x_t) before it is sampled from.`,name:"t"},{anchor:"diffusers.VQDiffusionScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014;
option for returning tuple rather than VQDiffusionSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_vq_diffusion.py#L210",returnDescription:`
<p><code>~schedulers.scheduling_utils.VQDiffusionSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>.
When returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.VQDiffusionSchedulerOutput</code> or <code>tuple</code></p>
`}}),gn=new y({}),bn=new b({props:{name:"class diffusers.RePaintScheduler",anchor:"diffusers.RePaintScheduler",parameters:[{name:"num_train_timesteps",val:": int = 1000"},{name:"beta_start",val:": float = 0.0001"},{name:"beta_end",val:": float = 0.02"},{name:"beta_schedule",val:": str = 'linear'"},{name:"eta",val:": float = 0.0"},{name:"trained_betas",val:": typing.Optional[numpy.ndarray] = None"},{name:"clip_sample",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.num_train_timesteps",description:"<strong>num_train_timesteps</strong> (<code>int</code>) &#x2014; number of diffusion steps used to train the model.",name:"num_train_timesteps"},{anchor:"diffusers.RePaintScheduler.beta_start",description:"<strong>beta_start</strong> (<code>float</code>) &#x2014; the starting <code>beta</code> value of inference.",name:"beta_start"},{anchor:"diffusers.RePaintScheduler.beta_end",description:"<strong>beta_end</strong> (<code>float</code>) &#x2014; the final <code>beta</code> value.",name:"beta_end"},{anchor:"diffusers.RePaintScheduler.beta_schedule",description:`<strong>beta_schedule</strong> (<code>str</code>) &#x2014;
the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model. Choose from
<code>linear</code>, <code>scaled_linear</code>, or <code>squaredcos_cap_v2</code>.`,name:"beta_schedule"},{anchor:"diffusers.RePaintScheduler.eta",description:`<strong>eta</strong> (<code>float</code>) &#x2014;
The weight of noise for added noise in a diffusion step. Its value is between 0.0 and 1.0 -0.0 is DDIM and
1.0 is DDPM scheduler respectively.`,name:"eta"},{anchor:"diffusers.RePaintScheduler.trained_betas",description:`<strong>trained_betas</strong> (<code>np.ndarray</code>, optional) &#x2014;
option to pass an array of betas directly to the constructor to bypass <code>beta_start</code>, <code>beta_end</code> etc.`,name:"trained_betas"},{anchor:"diffusers.RePaintScheduler.variance_type",description:`<strong>variance_type</strong> (<code>str</code>) &#x2014;
options to clip the variance used when adding noise to the denoised sample. Choose from <code>fixed_small</code>,
<code>fixed_small_log</code>, <code>fixed_large</code>, <code>fixed_large_log</code>, <code>learned</code> or <code>learned_range</code>.`,name:"variance_type"},{anchor:"diffusers.RePaintScheduler.clip_sample",description:`<strong>clip_sample</strong> (<code>bool</code>, default <code>True</code>) &#x2014;
option to clip predicted sample between -1 and 1 for numerical stability.`,name:"clip_sample"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L74"}}),$n=new b({props:{name:"scale_model_input",anchor:"diffusers.RePaintScheduler.scale_model_input",parameters:[{name:"sample",val:": FloatTensor"},{name:"timestep",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.scale_model_input.sample",description:"<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014; input sample",name:"sample"},{anchor:"diffusers.RePaintScheduler.scale_model_input.timestep",description:"<strong>timestep</strong> (<code>int</code>, optional) &#x2014; current timestep",name:"timestep"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L150",returnDescription:`
<p>scaled input sample</p>
`,returnType:`
<p><code>torch.FloatTensor</code></p>
`}}),En=new b({props:{name:"step",anchor:"diffusers.RePaintScheduler.step",parameters:[{name:"model_output",val:": FloatTensor"},{name:"timestep",val:": int"},{name:"sample",val:": FloatTensor"},{name:"original_image",val:": FloatTensor"},{name:"mask",val:": FloatTensor"},{name:"generator",val:": typing.Optional[torch._C.Generator] = None"},{name:"return_dict",val:": bool = True"}],parametersDescription:[{anchor:"diffusers.RePaintScheduler.step.model_output",description:`<strong>model_output</strong> (<code>torch.FloatTensor</code>) &#x2014; direct output from learned
diffusion model.`,name:"model_output"},{anchor:"diffusers.RePaintScheduler.step.timestep",description:"<strong>timestep</strong> (<code>int</code>) &#x2014; current discrete timestep in the diffusion chain.",name:"timestep"},{anchor:"diffusers.RePaintScheduler.step.sample",description:`<strong>sample</strong> (<code>torch.FloatTensor</code>) &#x2014;
current instance of sample being created by diffusion process.`,name:"sample"},{anchor:"diffusers.RePaintScheduler.step.original_image",description:`<strong>original_image</strong> (<code>torch.FloatTensor</code>) &#x2014;
the original image to inpaint on.`,name:"original_image"},{anchor:"diffusers.RePaintScheduler.step.mask",description:`<strong>mask</strong> (<code>torch.FloatTensor</code>) &#x2014;
the mask where 0.0 values define which part of the original image to inpaint (change).`,name:"mask"},{anchor:"diffusers.RePaintScheduler.step.generator",description:"<strong>generator</strong> (<code>torch.Generator</code>, <em>optional</em>) &#x2014; random number generator.",name:"generator"},{anchor:"diffusers.RePaintScheduler.step.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>) &#x2014; option for returning tuple rather than
DDPMSchedulerOutput class`,name:"return_dict"}],source:"https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_repaint.py#L213",returnDescription:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> if <code>return_dict</code> is True, otherwise a <code>tuple</code>. When
returning a tuple, the first element is the sample tensor.</p>
`,returnType:`
<p><code>~schedulers.scheduling_utils.RePaintSchedulerOutput</code> or <code>tuple</code></p>
`}}),{c(){Y=s("meta"),et=l(),J=s("h1"),_e=s("a"),Jo=s("span"),p(gr.$$.fragment),bu=l(),Xo=s("span"),Su=i("Schedulers"),Hd=l(),yn=s("p"),$u=i("Diffusers contains multiple pre-built schedule functions for the diffusion process."),Gd=l(),Ce=s("h2"),tt=s("a"),Zo=s("span"),p(_r.$$.fragment),Eu=l(),ei=s("span"),xu=i("What is a scheduler?"),zd=l(),rt=s("p"),Du=i("The schedule functions, denoted "),ti=s("em"),yu=i("Schedulers"),wu=i(" in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample."),Yd=l(),st=s("ul"),wn=s("li"),Pu=i("Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),vr=s("ul"),ri=s("li"),Tu=i("adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),Mu=l(),si=s("li"),Cu=i("for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),ku=l(),ke=s("li"),Au=i("Schedulers are often defined by a "),ni=s("em"),Ou=i("noise schedule"),Vu=i(" and an "),oi=s("em"),Nu=i("update rule"),Fu=i(" to solve the differential equation solution."),Jd=l(),Ae=s("h3"),nt=s("a"),ii=s("span"),p(br.$$.fragment),Iu=l(),ai=s("span"),Lu=i("Discrete versus continuous schedulers"),Xd=l(),C=s("p"),qu=i(`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),di=s("code"),Ku=i("int"),Ru=i(" inputs), such as the "),Pn=s("a"),Qu=i("DDPMScheduler"),ju=i(" or "),Tn=s("a"),Uu=i("PNDMScheduler"),Bu=i(", and continuous (accepting "),li=s("code"),Wu=i("float"),Hu=i(" inputs), such as the score-based schedulers "),Mn=s("a"),Gu=i("ScoreSdeVeScheduler"),zu=i(" or "),ci=s("code"),Yu=i("ScoreSdeVpScheduler"),Ju=i("."),Zd=l(),Oe=s("h2"),ot=s("a"),ui=s("span"),p(Sr.$$.fragment),Xu=l(),fi=s("span"),Zu=i("Designing Re-usable schedulers"),el=l(),Cn=s("p"),ef=i(`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),tl=l(),it=s("ul"),pi=s("li"),tf=i("Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),rf=l(),hi=s("li"),sf=i("Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),rl=l(),Ve=s("h2"),at=s("a"),mi=s("span"),p($r.$$.fragment),nf=l(),gi=s("span"),of=i("API"),sl=l(),kn=s("p"),af=i("The core API for any new scheduler must follow a limited structure."),nl=l(),xe=s("ul"),Er=s("li"),df=i("Schedulers should provide one or more "),_i=s("code"),lf=i("def step(...)"),cf=i(" functions that should be called to update the generated sample iteratively."),uf=l(),xr=s("li"),ff=i("Schedulers should provide a "),vi=s("code"),pf=i("set_timesteps(...)"),hf=i(" method that configures the parameters of a schedule function for a specific inference task."),mf=l(),bi=s("li"),gf=i("Schedulers should be framework-specific."),ol=l(),dt=s("p"),_f=i("The base class "),An=s("a"),vf=i("SchedulerMixin"),bf=i(" implements low level utilities used by multiple schedulers."),il=l(),Ne=s("h3"),lt=s("a"),Si=s("span"),p(Dr.$$.fragment),Sf=l(),$i=s("span"),$f=i("SchedulerMixin"),al=l(),Fe=s("div"),p(yr.$$.fragment),Ef=l(),Ei=s("p"),xf=i("Mixin containing common functions for the schedulers."),dl=l(),Ie=s("h3"),ct=s("a"),xi=s("span"),p(wr.$$.fragment),Df=l(),Di=s("span"),yf=i("SchedulerOutput"),ll=i("\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),Le=s("div"),p(Pr.$$.fragment),wf=l(),yi=s("p"),Pf=i("Base class for the scheduler\u2019s step function output."),cl=l(),qe=s("h3"),ut=s("a"),wi=s("span"),p(Tr.$$.fragment),Tf=l(),Pi=s("span"),Mf=i("Implemented Schedulers"),ul=l(),Ke=s("h4"),ft=s("a"),Ti=s("span"),p(Mr.$$.fragment),Cf=l(),Mi=s("span"),kf=i("Denoising diffusion implicit models (DDIM)"),fl=l(),On=s("p"),Af=i("Original paper can be found here."),pl=l(),w=s("div"),p(Cr.$$.fragment),Of=l(),Ci=s("p"),Vf=i(`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),Nf=l(),k=s("p"),Vn=s("a"),Ff=i("~ConfigMixin"),If=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ki=s("code"),Lf=i("__init__"),qf=i(`
function, such as `),Ai=s("code"),Kf=i("num_train_timesteps"),Rf=i(". They can be accessed via "),Oi=s("code"),Qf=i("scheduler.config.num_train_timesteps"),jf=i(`.
`),Nn=s("a"),Uf=i("~ConfigMixin"),Bf=i(" also provides general loading and saving functionality via the "),Fn=s("a"),Wf=i("save_config()"),Hf=i(` and
`),In=s("a"),Gf=i("from_config()"),zf=i(" functions."),Yf=l(),Ln=s("p"),Jf=i("For more details, see the original paper: "),kr=s("a"),Xf=i("https://arxiv.org/abs/2010.02502"),Zf=l(),pt=s("div"),p(Ar.$$.fragment),ep=l(),Vi=s("p"),tp=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),rp=l(),ht=s("div"),p(Or.$$.fragment),sp=l(),Ni=s("p"),np=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),op=l(),mt=s("div"),p(Vr.$$.fragment),ip=l(),Fi=s("p"),ap=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),hl=l(),Re=s("h4"),gt=s("a"),Ii=s("span"),p(Nr.$$.fragment),dp=l(),Li=s("span"),lp=i("Denoising diffusion probabilistic models (DDPM)"),ml=l(),_t=s("p"),cp=i("Original paper can be found "),Fr=s("a"),up=i("here"),fp=i("."),gl=l(),P=s("div"),p(Ir.$$.fragment),pp=l(),qi=s("p"),hp=i(`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),mp=l(),A=s("p"),qn=s("a"),gp=i("~ConfigMixin"),_p=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ki=s("code"),vp=i("__init__"),bp=i(`
function, such as `),Ri=s("code"),Sp=i("num_train_timesteps"),$p=i(". They can be accessed via "),Qi=s("code"),Ep=i("scheduler.config.num_train_timesteps"),xp=i(`.
`),Kn=s("a"),Dp=i("~ConfigMixin"),yp=i(" also provides general loading and saving functionality via the "),Rn=s("a"),wp=i("save_config()"),Pp=i(` and
`),Qn=s("a"),Tp=i("from_config()"),Mp=i(" functions."),Cp=l(),jn=s("p"),kp=i("For more details, see the original paper: "),Lr=s("a"),Ap=i("https://arxiv.org/abs/2006.11239"),Op=l(),vt=s("div"),p(qr.$$.fragment),Vp=l(),ji=s("p"),Np=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Fp=l(),bt=s("div"),p(Kr.$$.fragment),Ip=l(),Ui=s("p"),Lp=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),qp=l(),St=s("div"),p(Rr.$$.fragment),Kp=l(),Bi=s("p"),Rp=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),_l=l(),Qe=s("h4"),$t=s("a"),Wi=s("span"),p(Qr.$$.fragment),Qp=l(),Hi=s("span"),jp=i("Variance exploding, stochastic sampling from Karras et. al"),vl=l(),Et=s("p"),Up=i("Original paper can be found "),jr=s("a"),Bp=i("here"),Wp=i("."),bl=l(),S=s("div"),p(Ur.$$.fragment),Hp=l(),Gi=s("p"),Gp=i(`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),zp=l(),xt=s("p"),Yp=i(`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Br=s("a"),Jp=i("https://arxiv.org/abs/2206.00364"),Xp=i(` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Wr=s("a"),Zp=i("https://arxiv.org/abs/2011.13456"),eh=l(),O=s("p"),Un=s("a"),th=i("~ConfigMixin"),rh=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),zi=s("code"),sh=i("__init__"),nh=i(`
function, such as `),Yi=s("code"),oh=i("num_train_timesteps"),ih=i(". They can be accessed via "),Ji=s("code"),ah=i("scheduler.config.num_train_timesteps"),dh=i(`.
`),Bn=s("a"),lh=i("~ConfigMixin"),ch=i(" also provides general loading and saving functionality via the "),Wn=s("a"),uh=i("save_config()"),fh=i(` and
`),Hn=s("a"),ph=i("from_config()"),hh=i(" functions."),mh=l(),Hr=s("p"),gh=i(`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),Gr=s("a"),_h=i("https://arxiv.org/abs/2206.00364"),vh=i(`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),bh=l(),De=s("div"),p(zr.$$.fragment),Sh=l(),Xi=s("p"),$h=i(`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),Eh=l(),Zi=s("p"),xh=i("TODO Args:"),Dh=l(),Dt=s("div"),p(Yr.$$.fragment),yh=l(),ea=s("p"),wh=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Ph=l(),yt=s("div"),p(Jr.$$.fragment),Th=l(),ta=s("p"),Mh=i("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),Ch=l(),wt=s("div"),p(Xr.$$.fragment),kh=l(),ra=s("p"),Ah=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Oh=l(),Pt=s("div"),p(Zr.$$.fragment),Vh=l(),sa=s("p"),Nh=i("Correct the predicted sample based on the output model_output of the network. TODO complete description"),Sl=l(),je=s("h4"),Tt=s("a"),na=s("span"),p(es.$$.fragment),Fh=l(),oa=s("span"),Ih=i("Linear multistep scheduler for discrete beta schedules"),$l=l(),Mt=s("p"),Lh=i("Original implementation can be found "),ts=s("a"),qh=i("here"),Kh=i("."),El=l(),T=s("div"),p(rs.$$.fragment),Rh=l(),Gn=s("p"),Qh=i(`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),ss=s("a"),jh=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),Uh=l(),V=s("p"),zn=s("a"),Bh=i("~ConfigMixin"),Wh=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ia=s("code"),Hh=i("__init__"),Gh=i(`
function, such as `),aa=s("code"),zh=i("num_train_timesteps"),Yh=i(". They can be accessed via "),da=s("code"),Jh=i("scheduler.config.num_train_timesteps"),Xh=i(`.
`),Yn=s("a"),Zh=i("~ConfigMixin"),em=i(" also provides general loading and saving functionality via the "),Jn=s("a"),tm=i("save_config()"),rm=i(` and
`),Xn=s("a"),sm=i("from_config()"),nm=i(" functions."),om=l(),Ct=s("div"),p(ns.$$.fragment),im=l(),la=s("p"),am=i("Compute a linear multistep coefficient."),dm=l(),kt=s("div"),p(os.$$.fragment),lm=l(),is=s("p"),cm=i("Scales the denoising model input by "),ca=s("code"),um=i("(sigma**2 + 1) ** 0.5"),fm=i(" to match the K-LMS algorithm."),pm=l(),At=s("div"),p(as.$$.fragment),hm=l(),ua=s("p"),mm=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),gm=l(),Ot=s("div"),p(ds.$$.fragment),_m=l(),fa=s("p"),vm=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),xl=l(),Ue=s("h4"),Vt=s("a"),pa=s("span"),p(ls.$$.fragment),bm=l(),ha=s("span"),Sm=i("Pseudo numerical methods for diffusion models (PNDM)"),Dl=l(),Nt=s("p"),$m=i("Original implementation can be found "),cs=s("a"),Em=i("here"),xm=i("."),yl=l(),$=s("div"),p(us.$$.fragment),Dm=l(),ma=s("p"),ym=i(`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),wm=l(),N=s("p"),Zn=s("a"),Pm=i("~ConfigMixin"),Tm=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ga=s("code"),Mm=i("__init__"),Cm=i(`
function, such as `),_a=s("code"),km=i("num_train_timesteps"),Am=i(". They can be accessed via "),va=s("code"),Om=i("scheduler.config.num_train_timesteps"),Vm=i(`.
`),eo=s("a"),Nm=i("~ConfigMixin"),Fm=i(" also provides general loading and saving functionality via the "),to=s("a"),Im=i("save_config()"),Lm=i(` and
`),ro=s("a"),qm=i("from_config()"),Km=i(" functions."),Rm=l(),so=s("p"),Qm=i("For more details, see the original paper: "),fs=s("a"),jm=i("https://arxiv.org/abs/2202.09778"),Um=l(),Ft=s("div"),p(ps.$$.fragment),Bm=l(),ba=s("p"),Wm=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Hm=l(),It=s("div"),p(hs.$$.fragment),Gm=l(),Sa=s("p"),zm=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),Ym=l(),ye=s("div"),p(ms.$$.fragment),Jm=l(),$a=s("p"),Xm=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Zm=l(),$e=s("p"),eg=i("This function calls "),Ea=s("code"),tg=i("step_prk()"),rg=i(" or "),xa=s("code"),sg=i("step_plms()"),ng=i(" depending on the internal variable "),Da=s("code"),og=i("counter"),ig=i("."),ag=l(),Lt=s("div"),p(gs.$$.fragment),dg=l(),ya=s("p"),lg=i(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),cg=l(),qt=s("div"),p(_s.$$.fragment),ug=l(),wa=s("p"),fg=i(`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),wl=l(),Be=s("h4"),Kt=s("a"),Pa=s("span"),p(vs.$$.fragment),pg=l(),Ta=s("span"),hg=i("variance exploding stochastic differential equation (VE-SDE) scheduler"),Pl=l(),Rt=s("p"),mg=i("Original paper can be found "),bs=s("a"),gg=i("here"),_g=i("."),Tl=l(),E=s("div"),p(Ss.$$.fragment),vg=l(),Ma=s("p"),bg=i("The variance exploding stochastic differential equation (SDE) scheduler."),Sg=l(),no=s("p"),$g=i("For more information, see the original paper: "),$s=s("a"),Eg=i("https://arxiv.org/abs/2011.13456"),xg=l(),F=s("p"),oo=s("a"),Dg=i("~ConfigMixin"),yg=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ca=s("code"),wg=i("__init__"),Pg=i(`
function, such as `),ka=s("code"),Tg=i("num_train_timesteps"),Mg=i(". They can be accessed via "),Aa=s("code"),Cg=i("scheduler.config.num_train_timesteps"),kg=i(`.
`),io=s("a"),Ag=i("~ConfigMixin"),Og=i(" also provides general loading and saving functionality via the "),ao=s("a"),Vg=i("save_config()"),Ng=i(` and
`),lo=s("a"),Fg=i("from_config()"),Ig=i(" functions."),Lg=l(),Qt=s("div"),p(Es.$$.fragment),qg=l(),Oa=s("p"),Kg=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Rg=l(),we=s("div"),p(xs.$$.fragment),Qg=l(),Va=s("p"),jg=i("Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),Ug=l(),We=s("p"),Bg=i("The sigmas control the weight of the "),Na=s("code"),Wg=i("drift"),Hg=i(" and "),Fa=s("code"),Gg=i("diffusion"),zg=i(" components of sample update."),Yg=l(),jt=s("div"),p(Ds.$$.fragment),Jg=l(),Ia=s("p"),Xg=i("Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),Zg=l(),Ut=s("div"),p(ys.$$.fragment),e_=l(),La=s("p"),t_=i(`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),r_=l(),Bt=s("div"),p(ws.$$.fragment),s_=l(),qa=s("p"),n_=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Ml=l(),He=s("h4"),Wt=s("a"),Ka=s("span"),p(Ps.$$.fragment),o_=l(),Ra=s("span"),i_=i("improved pseudo numerical methods for diffusion models (iPNDM)"),Cl=l(),Ht=s("p"),a_=i("Original implementation can be found "),Ts=s("a"),d_=i("here"),l_=i("."),kl=l(),M=s("div"),p(Ms.$$.fragment),c_=l(),co=s("p"),u_=i(`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),Cs=s("a"),f_=i("library"),p_=l(),I=s("p"),uo=s("a"),h_=i("~ConfigMixin"),m_=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Qa=s("code"),g_=i("__init__"),__=i(`
function, such as `),ja=s("code"),v_=i("num_train_timesteps"),b_=i(". They can be accessed via "),Ua=s("code"),S_=i("scheduler.config.num_train_timesteps"),$_=i(`.
`),fo=s("a"),E_=i("~ConfigMixin"),x_=i(" also provides general loading and saving functionality via the "),po=s("a"),D_=i("save_config()"),y_=i(` and
`),ho=s("a"),w_=i("from_config()"),P_=i(" functions."),T_=l(),mo=s("p"),M_=i("For more details, see the original paper: "),ks=s("a"),C_=i("https://arxiv.org/abs/2202.09778"),k_=l(),Gt=s("div"),p(As.$$.fragment),A_=l(),Ba=s("p"),O_=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),V_=l(),zt=s("div"),p(Os.$$.fragment),N_=l(),Wa=s("p"),F_=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),I_=l(),Yt=s("div"),p(Vs.$$.fragment),L_=l(),Ha=s("p"),q_=i(`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),Al=l(),Ge=s("h4"),Jt=s("a"),Ga=s("span"),p(Ns.$$.fragment),K_=l(),za=s("span"),R_=i("variance preserving stochastic differential equation (VP-SDE) scheduler"),Ol=l(),Xt=s("p"),Q_=i("Original paper can be found "),Fs=s("a"),j_=i("here"),U_=i("."),Vl=l(),p(Zt.$$.fragment),Nl=l(),X=s("div"),p(Is.$$.fragment),B_=l(),Ya=s("p"),W_=i("The variance preserving stochastic differential equation (SDE) scheduler."),H_=l(),L=s("p"),go=s("a"),G_=i("~ConfigMixin"),z_=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ja=s("code"),Y_=i("__init__"),J_=i(`
function, such as `),Xa=s("code"),X_=i("num_train_timesteps"),Z_=i(". They can be accessed via "),Za=s("code"),ev=i("scheduler.config.num_train_timesteps"),tv=i(`.
`),_o=s("a"),rv=i("~ConfigMixin"),sv=i(" also provides general loading and saving functionality via the "),vo=s("a"),nv=i("save_config()"),ov=i(` and
`),bo=s("a"),iv=i("from_config()"),av=i(" functions."),dv=l(),So=s("p"),lv=i("For more information, see the original paper: "),Ls=s("a"),cv=i("https://arxiv.org/abs/2011.13456"),uv=l(),ed=s("p"),fv=i("UNDER CONSTRUCTION"),Fl=l(),ze=s("h4"),er=s("a"),td=s("span"),p(qs.$$.fragment),pv=l(),rd=s("span"),hv=i("Euler scheduler"),Il=l(),Pe=s("p"),mv=i("Euler scheduler (Algorithm 2) from the paper "),Ks=s("a"),gv=i("Elucidating the Design Space of Diffusion-Based Generative Models"),_v=i(" by Karras et al. (2022). Based on the original "),Rs=s("a"),vv=i("k-diffusion"),bv=i(` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Ll=l(),H=s("div"),p(Qs.$$.fragment),Sv=l(),tr=s("p"),$v=i("Euler scheduler (Algorithm 2) from Karras et al. (2022) "),js=s("a"),Ev=i("https://arxiv.org/abs/2206.00364"),xv=i(`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),Us=s("a"),Dv=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),yv=l(),q=s("p"),$o=s("a"),wv=i("~ConfigMixin"),Pv=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),sd=s("code"),Tv=i("__init__"),Mv=i(`
function, such as `),nd=s("code"),Cv=i("num_train_timesteps"),kv=i(". They can be accessed via "),od=s("code"),Av=i("scheduler.config.num_train_timesteps"),Ov=i(`.
`),Eo=s("a"),Vv=i("~ConfigMixin"),Nv=i(" also provides general loading and saving functionality via the "),xo=s("a"),Fv=i("save_config()"),Iv=i(` and
`),Do=s("a"),Lv=i("from_config()"),qv=i(" functions."),Kv=l(),rr=s("div"),p(Bs.$$.fragment),Rv=l(),Ws=s("p"),Qv=i("Scales the denoising model input by "),id=s("code"),jv=i("(sigma**2 + 1) ** 0.5"),Uv=i(" to match the Euler algorithm."),Bv=l(),sr=s("div"),p(Hs.$$.fragment),Wv=l(),ad=s("p"),Hv=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),Gv=l(),nr=s("div"),p(Gs.$$.fragment),zv=l(),dd=s("p"),Yv=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),ql=l(),Ye=s("h4"),or=s("a"),ld=s("span"),p(zs.$$.fragment),Jv=l(),cd=s("span"),Xv=i("Euler Ancestral scheduler"),Kl=l(),yo=s("p"),Zv=i(`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Rl=l(),G=s("div"),p(Ys.$$.fragment),eb=l(),wo=s("p"),tb=i(`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Js=s("a"),rb=i("https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),sb=l(),K=s("p"),Po=s("a"),nb=i("~ConfigMixin"),ob=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),ud=s("code"),ib=i("__init__"),ab=i(`
function, such as `),fd=s("code"),db=i("num_train_timesteps"),lb=i(". They can be accessed via "),pd=s("code"),cb=i("scheduler.config.num_train_timesteps"),ub=i(`.
`),To=s("a"),fb=i("~ConfigMixin"),pb=i(" also provides general loading and saving functionality via the "),Mo=s("a"),hb=i("save_config()"),mb=i(` and
`),Co=s("a"),gb=i("from_config()"),_b=i(" functions."),vb=l(),ir=s("div"),p(Xs.$$.fragment),bb=l(),Zs=s("p"),Sb=i("Scales the denoising model input by "),hd=s("code"),$b=i("(sigma**2 + 1) ** 0.5"),Eb=i(" to match the Euler algorithm."),xb=l(),ar=s("div"),p(en.$$.fragment),Db=l(),md=s("p"),yb=i("Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),wb=l(),dr=s("div"),p(tn.$$.fragment),Pb=l(),gd=s("p"),Tb=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Ql=l(),Je=s("h4"),lr=s("a"),_d=s("span"),p(rn.$$.fragment),Mb=l(),vd=s("span"),Cb=i("VQDiffusionScheduler"),jl=l(),sn=s("p"),kb=i("Original paper can be found "),nn=s("a"),Ab=i("here"),Ul=l(),x=s("div"),p(on.$$.fragment),Ob=l(),bd=s("p"),Vb=i("The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image."),Nb=l(),Sd=s("p"),Fb=i(`The VQ-diffusion scheduler converts the transformer\u2019s output into a sample for the unnoised image at the previous
diffusion timestep.`),Ib=l(),R=s("p"),ko=s("a"),Lb=i("~ConfigMixin"),qb=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),$d=s("code"),Kb=i("__init__"),Rb=i(`
function, such as `),Ed=s("code"),Qb=i("num_train_timesteps"),jb=i(". They can be accessed via "),xd=s("code"),Ub=i("scheduler.config.num_train_timesteps"),Bb=i(`.
`),Ao=s("a"),Wb=i("~ConfigMixin"),Hb=i(" also provides general loading and saving functionality via the "),Oo=s("a"),Gb=i("save_config()"),zb=i(` and
`),Vo=s("a"),Yb=i("from_config()"),Jb=i(" functions."),Xb=l(),No=s("p"),Zb=i("For more details, see the original paper: "),an=s("a"),e1=i("https://arxiv.org/abs/2111.14822"),t1=l(),Te=s("div"),p(dn.$$.fragment),r1=l(),ln=s("p"),s1=i(`Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each
latent pixel in `),Dd=s("code"),n1=i("x_t"),o1=i("."),i1=l(),yd=s("p"),a1=i(`See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix
is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.`),d1=l(),Q=s("div"),p(cn.$$.fragment),l1=l(),un=s("p"),c1=i("Calculates the log probabilities for the predicted classes of the image at timestep "),wd=s("code"),u1=i("t-1"),f1=i(". I.e. Equation (11)."),p1=l(),Pd=s("p"),h1=i(`Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only
forward probabilities.`),m1=l(),Td=s("p"),g1=i("Equation (11) stated in terms of forward probabilities via Equation (5):"),_1=l(),Md=s("p"),v1=i("Where:"),b1=l(),Cd=s("ul"),fn=s("li"),S1=i("the sum is over x"),kd=s("em"),$1=i("0 = {C_0 \u2026 C"),E1=i("{k-1}} (classes for x_0)"),x1=l(),Xe=s("p"),D1=i("p(x"),Ad=s("em"),y1=i("{t-1} | x_t) = sum( q(x_t | x"),w1=i("{t-1}) "),Od=s("em"),P1=i("q(x_{t-1} | x_0)"),T1=i(" p(x_0) / q(x_t | x_0) )"),M1=l(),cr=s("div"),p(pn.$$.fragment),C1=l(),Vd=s("p"),k1=i("Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),A1=l(),ur=s("div"),p(hn.$$.fragment),O1=l(),mn=s("p"),V1=i(`Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the
docstring for `),Nd=s("code"),N1=i("self.q_posterior"),F1=i(" for more in depth docs on how Equation (11) is computed."),Bl=l(),Ze=s("h4"),fr=s("a"),Fd=s("span"),p(gn.$$.fragment),I1=l(),Id=s("span"),L1=i("RePaint scheduler"),Wl=l(),Ee=s("p"),q1=i(`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),Fo=s("a"),K1=i("RePaintPipeline"),R1=i(`.
Based on the paper `),_n=s("a"),Q1=i("RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),j1=i(`
and the original implementation by Andreas Lugmayr et al.: `),vn=s("a"),U1=i("https://github.com/andreas128/RePaint"),Hl=l(),z=s("div"),p(bn.$$.fragment),B1=l(),Ld=s("p"),W1=i("RePaint is a schedule for DDPM inpainting inside a given mask."),H1=l(),j=s("p"),Io=s("a"),G1=i("~ConfigMixin"),z1=i(" takes care of storing all config attributes that are passed in the scheduler\u2019s "),qd=s("code"),Y1=i("__init__"),J1=i(`
function, such as `),Kd=s("code"),X1=i("num_train_timesteps"),Z1=i(". They can be accessed via "),Rd=s("code"),e0=i("scheduler.config.num_train_timesteps"),t0=i(`.
`),Lo=s("a"),r0=i("~ConfigMixin"),s0=i(" also provides general loading and saving functionality via the "),qo=s("a"),n0=i("save_config()"),o0=i(` and
`),Ko=s("a"),i0=i("from_config()"),a0=i(" functions."),d0=l(),Ro=s("p"),l0=i("For more details, see the original paper: "),Sn=s("a"),c0=i("https://arxiv.org/pdf/2201.09865.pdf"),u0=l(),pr=s("div"),p($n.$$.fragment),f0=l(),Qd=s("p"),p0=i(`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),h0=l(),hr=s("div"),p(En.$$.fragment),m0=l(),jd=s("p"),g0=i(`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),this.h()},l(r){const u=DE('[data-svelte="svelte-1phssyn"]',document.head);Y=n(u,"META",{name:!0,content:!0}),u.forEach(t),et=c(r),J=n(r,"H1",{class:!0});var xn=o(J);_e=n(xn,"A",{id:!0,class:!0,href:!0});var C0=o(_e);Jo=n(C0,"SPAN",{});var k0=o(Jo);h(gr.$$.fragment,k0),k0.forEach(t),C0.forEach(t),bu=c(xn),Xo=n(xn,"SPAN",{});var A0=o(Xo);Su=a(A0,"Schedulers"),A0.forEach(t),xn.forEach(t),Hd=c(r),yn=n(r,"P",{});var O0=o(yn);$u=a(O0,"Diffusers contains multiple pre-built schedule functions for the diffusion process."),O0.forEach(t),Gd=c(r),Ce=n(r,"H2",{class:!0});var zl=o(Ce);tt=n(zl,"A",{id:!0,class:!0,href:!0});var V0=o(tt);Zo=n(V0,"SPAN",{});var N0=o(Zo);h(_r.$$.fragment,N0),N0.forEach(t),V0.forEach(t),Eu=c(zl),ei=n(zl,"SPAN",{});var F0=o(ei);xu=a(F0,"What is a scheduler?"),F0.forEach(t),zl.forEach(t),zd=c(r),rt=n(r,"P",{});var Yl=o(rt);Du=a(Yl,"The schedule functions, denoted "),ti=n(Yl,"EM",{});var I0=o(ti);yu=a(I0,"Schedulers"),I0.forEach(t),wu=a(Yl," in the library take in the output of a trained model, a sample which the diffusion process is iterating on, and a timestep to return a denoised sample."),Yl.forEach(t),Yd=c(r),st=n(r,"UL",{});var Jl=o(st);wn=n(Jl,"LI",{});var _0=o(wn);Pu=a(_0,"Schedulers define the methodology for iteratively adding noise to an image or for updating a sample based on model outputs."),vr=n(_0,"UL",{});var Xl=o(vr);ri=n(Xl,"LI",{});var L0=o(ri);Tu=a(L0,"adding noise in different manners represent the algorithmic processes to train a diffusion model by adding noise to images."),L0.forEach(t),Mu=c(Xl),si=n(Xl,"LI",{});var q0=o(si);Cu=a(q0,"for inference, the scheduler defines how to update a sample based on an output from a pretrained model."),q0.forEach(t),Xl.forEach(t),_0.forEach(t),ku=c(Jl),ke=n(Jl,"LI",{});var Qo=o(ke);Au=a(Qo,"Schedulers are often defined by a "),ni=n(Qo,"EM",{});var K0=o(ni);Ou=a(K0,"noise schedule"),K0.forEach(t),Vu=a(Qo," and an "),oi=n(Qo,"EM",{});var R0=o(oi);Nu=a(R0,"update rule"),R0.forEach(t),Fu=a(Qo," to solve the differential equation solution."),Qo.forEach(t),Jl.forEach(t),Jd=c(r),Ae=n(r,"H3",{class:!0});var Zl=o(Ae);nt=n(Zl,"A",{id:!0,class:!0,href:!0});var Q0=o(nt);ii=n(Q0,"SPAN",{});var j0=o(ii);h(br.$$.fragment,j0),j0.forEach(t),Q0.forEach(t),Iu=c(Zl),ai=n(Zl,"SPAN",{});var U0=o(ai);Lu=a(U0,"Discrete versus continuous schedulers"),U0.forEach(t),Zl.forEach(t),Xd=c(r),C=n(r,"P",{});var ue=o(C);qu=a(ue,`All schedulers take in a timestep to predict the updated version of the sample being diffused.
The timesteps dictate where in the diffusion process the step is, where data is generated by iterating forward in time and inference is executed by propagating backwards through timesteps.
Different algorithms use timesteps that both discrete (accepting `),di=n(ue,"CODE",{});var B0=o(di);Ku=a(B0,"int"),B0.forEach(t),Ru=a(ue," inputs), such as the "),Pn=n(ue,"A",{href:!0});var W0=o(Pn);Qu=a(W0,"DDPMScheduler"),W0.forEach(t),ju=a(ue," or "),Tn=n(ue,"A",{href:!0});var H0=o(Tn);Uu=a(H0,"PNDMScheduler"),H0.forEach(t),Bu=a(ue,", and continuous (accepting "),li=n(ue,"CODE",{});var G0=o(li);Wu=a(G0,"float"),G0.forEach(t),Hu=a(ue," inputs), such as the score-based schedulers "),Mn=n(ue,"A",{href:!0});var z0=o(Mn);Gu=a(z0,"ScoreSdeVeScheduler"),z0.forEach(t),zu=a(ue," or "),ci=n(ue,"CODE",{});var Y0=o(ci);Yu=a(Y0,"ScoreSdeVpScheduler"),Y0.forEach(t),Ju=a(ue,"."),ue.forEach(t),Zd=c(r),Oe=n(r,"H2",{class:!0});var ec=o(Oe);ot=n(ec,"A",{id:!0,class:!0,href:!0});var J0=o(ot);ui=n(J0,"SPAN",{});var X0=o(ui);h(Sr.$$.fragment,X0),X0.forEach(t),J0.forEach(t),Xu=c(ec),fi=n(ec,"SPAN",{});var Z0=o(fi);Zu=a(Z0,"Designing Re-usable schedulers"),Z0.forEach(t),ec.forEach(t),el=c(r),Cn=n(r,"P",{});var e2=o(Cn);ef=a(e2,`The core design principle between the schedule functions is to be model, system, and framework independent.
This allows for rapid experimentation and cleaner abstractions in the code, where the model prediction is separated from the sample update.
To this end, the design of schedulers is such that:`),e2.forEach(t),tl=c(r),it=n(r,"UL",{});var tc=o(it);pi=n(tc,"LI",{});var t2=o(pi);tf=a(t2,"Schedulers can be used interchangeably between diffusion models in inference to find the preferred trade-off between speed and generation quality."),t2.forEach(t),rf=c(tc),hi=n(tc,"LI",{});var r2=o(hi);sf=a(r2,"Schedulers are currently by default in PyTorch, but are designed to be framework independent (partial Jax support currently exists)."),r2.forEach(t),tc.forEach(t),rl=c(r),Ve=n(r,"H2",{class:!0});var rc=o(Ve);at=n(rc,"A",{id:!0,class:!0,href:!0});var s2=o(at);mi=n(s2,"SPAN",{});var n2=o(mi);h($r.$$.fragment,n2),n2.forEach(t),s2.forEach(t),nf=c(rc),gi=n(rc,"SPAN",{});var o2=o(gi);of=a(o2,"API"),o2.forEach(t),rc.forEach(t),sl=c(r),kn=n(r,"P",{});var i2=o(kn);af=a(i2,"The core API for any new scheduler must follow a limited structure."),i2.forEach(t),nl=c(r),xe=n(r,"UL",{});var jo=o(xe);Er=n(jo,"LI",{});var sc=o(Er);df=a(sc,"Schedulers should provide one or more "),_i=n(sc,"CODE",{});var a2=o(_i);lf=a(a2,"def step(...)"),a2.forEach(t),cf=a(sc," functions that should be called to update the generated sample iteratively."),sc.forEach(t),uf=c(jo),xr=n(jo,"LI",{});var nc=o(xr);ff=a(nc,"Schedulers should provide a "),vi=n(nc,"CODE",{});var d2=o(vi);pf=a(d2,"set_timesteps(...)"),d2.forEach(t),hf=a(nc," method that configures the parameters of a schedule function for a specific inference task."),nc.forEach(t),mf=c(jo),bi=n(jo,"LI",{});var l2=o(bi);gf=a(l2,"Schedulers should be framework-specific."),l2.forEach(t),jo.forEach(t),ol=c(r),dt=n(r,"P",{});var oc=o(dt);_f=a(oc,"The base class "),An=n(oc,"A",{href:!0});var c2=o(An);vf=a(c2,"SchedulerMixin"),c2.forEach(t),bf=a(oc," implements low level utilities used by multiple schedulers."),oc.forEach(t),il=c(r),Ne=n(r,"H3",{class:!0});var ic=o(Ne);lt=n(ic,"A",{id:!0,class:!0,href:!0});var u2=o(lt);Si=n(u2,"SPAN",{});var f2=o(Si);h(Dr.$$.fragment,f2),f2.forEach(t),u2.forEach(t),Sf=c(ic),$i=n(ic,"SPAN",{});var p2=o($i);$f=a(p2,"SchedulerMixin"),p2.forEach(t),ic.forEach(t),al=c(r),Fe=n(r,"DIV",{class:!0});var ac=o(Fe);h(yr.$$.fragment,ac),Ef=c(ac),Ei=n(ac,"P",{});var h2=o(Ei);xf=a(h2,"Mixin containing common functions for the schedulers."),h2.forEach(t),ac.forEach(t),dl=c(r),Ie=n(r,"H3",{class:!0});var dc=o(Ie);ct=n(dc,"A",{id:!0,class:!0,href:!0});var m2=o(ct);xi=n(m2,"SPAN",{});var g2=o(xi);h(wr.$$.fragment,g2),g2.forEach(t),m2.forEach(t),Df=c(dc),Di=n(dc,"SPAN",{});var _2=o(Di);yf=a(_2,"SchedulerOutput"),_2.forEach(t),dc.forEach(t),ll=a(r,"\n\nThe class `SchedulerOutput` contains the outputs from any schedulers `step(...)` call.\n"),Le=n(r,"DIV",{class:!0});var lc=o(Le);h(Pr.$$.fragment,lc),wf=c(lc),yi=n(lc,"P",{});var v2=o(yi);Pf=a(v2,"Base class for the scheduler\u2019s step function output."),v2.forEach(t),lc.forEach(t),cl=c(r),qe=n(r,"H3",{class:!0});var cc=o(qe);ut=n(cc,"A",{id:!0,class:!0,href:!0});var b2=o(ut);wi=n(b2,"SPAN",{});var S2=o(wi);h(Tr.$$.fragment,S2),S2.forEach(t),b2.forEach(t),Tf=c(cc),Pi=n(cc,"SPAN",{});var $2=o(Pi);Mf=a($2,"Implemented Schedulers"),$2.forEach(t),cc.forEach(t),ul=c(r),Ke=n(r,"H4",{class:!0});var uc=o(Ke);ft=n(uc,"A",{id:!0,class:!0,href:!0});var E2=o(ft);Ti=n(E2,"SPAN",{});var x2=o(Ti);h(Mr.$$.fragment,x2),x2.forEach(t),E2.forEach(t),Cf=c(uc),Mi=n(uc,"SPAN",{});var D2=o(Mi);kf=a(D2,"Denoising diffusion implicit models (DDIM)"),D2.forEach(t),uc.forEach(t),fl=c(r),On=n(r,"P",{});var y2=o(On);Af=a(y2,"Original paper can be found here."),y2.forEach(t),pl=c(r),w=n(r,"DIV",{class:!0});var fe=o(w);h(Cr.$$.fragment,fe),Of=c(fe),Ci=n(fe,"P",{});var w2=o(Ci);Vf=a(w2,`Denoising diffusion implicit models is a scheduler that extends the denoising procedure introduced in denoising
diffusion probabilistic models (DDPMs) with non-Markovian guidance.`),w2.forEach(t),Nf=c(fe),k=n(fe,"P",{});var Z=o(k);Vn=n(Z,"A",{href:!0});var P2=o(Vn);Ff=a(P2,"~ConfigMixin"),P2.forEach(t),If=a(Z," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ki=n(Z,"CODE",{});var T2=o(ki);Lf=a(T2,"__init__"),T2.forEach(t),qf=a(Z,`
function, such as `),Ai=n(Z,"CODE",{});var M2=o(Ai);Kf=a(M2,"num_train_timesteps"),M2.forEach(t),Rf=a(Z,". They can be accessed via "),Oi=n(Z,"CODE",{});var C2=o(Oi);Qf=a(C2,"scheduler.config.num_train_timesteps"),C2.forEach(t),jf=a(Z,`.
`),Nn=n(Z,"A",{href:!0});var k2=o(Nn);Uf=a(k2,"~ConfigMixin"),k2.forEach(t),Bf=a(Z," also provides general loading and saving functionality via the "),Fn=n(Z,"A",{href:!0});var A2=o(Fn);Wf=a(A2,"save_config()"),A2.forEach(t),Hf=a(Z,` and
`),In=n(Z,"A",{href:!0});var O2=o(In);Gf=a(O2,"from_config()"),O2.forEach(t),zf=a(Z," functions."),Z.forEach(t),Yf=c(fe),Ln=n(fe,"P",{});var v0=o(Ln);Jf=a(v0,"For more details, see the original paper: "),kr=n(v0,"A",{href:!0,rel:!0});var V2=o(kr);Xf=a(V2,"https://arxiv.org/abs/2010.02502"),V2.forEach(t),v0.forEach(t),Zf=c(fe),pt=n(fe,"DIV",{class:!0});var fc=o(pt);h(Ar.$$.fragment,fc),ep=c(fc),Vi=n(fc,"P",{});var N2=o(Vi);tp=a(N2,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),N2.forEach(t),fc.forEach(t),rp=c(fe),ht=n(fe,"DIV",{class:!0});var pc=o(ht);h(Or.$$.fragment,pc),sp=c(pc),Ni=n(pc,"P",{});var F2=o(Ni);np=a(F2,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),F2.forEach(t),pc.forEach(t),op=c(fe),mt=n(fe,"DIV",{class:!0});var hc=o(mt);h(Vr.$$.fragment,hc),ip=c(hc),Fi=n(hc,"P",{});var I2=o(Fi);ap=a(I2,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),I2.forEach(t),hc.forEach(t),fe.forEach(t),hl=c(r),Re=n(r,"H4",{class:!0});var mc=o(Re);gt=n(mc,"A",{id:!0,class:!0,href:!0});var L2=o(gt);Ii=n(L2,"SPAN",{});var q2=o(Ii);h(Nr.$$.fragment,q2),q2.forEach(t),L2.forEach(t),dp=c(mc),Li=n(mc,"SPAN",{});var K2=o(Li);lp=a(K2,"Denoising diffusion probabilistic models (DDPM)"),K2.forEach(t),mc.forEach(t),ml=c(r),_t=n(r,"P",{});var gc=o(_t);cp=a(gc,"Original paper can be found "),Fr=n(gc,"A",{href:!0,rel:!0});var R2=o(Fr);up=a(R2,"here"),R2.forEach(t),fp=a(gc,"."),gc.forEach(t),gl=c(r),P=n(r,"DIV",{class:!0});var pe=o(P);h(Ir.$$.fragment,pe),pp=c(pe),qi=n(pe,"P",{});var Q2=o(qi);hp=a(Q2,`Denoising diffusion probabilistic models (DDPMs) explores the connections between denoising score matching and
Langevin dynamics sampling.`),Q2.forEach(t),mp=c(pe),A=n(pe,"P",{});var ee=o(A);qn=n(ee,"A",{href:!0});var j2=o(qn);gp=a(j2,"~ConfigMixin"),j2.forEach(t),_p=a(ee," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ki=n(ee,"CODE",{});var U2=o(Ki);vp=a(U2,"__init__"),U2.forEach(t),bp=a(ee,`
function, such as `),Ri=n(ee,"CODE",{});var B2=o(Ri);Sp=a(B2,"num_train_timesteps"),B2.forEach(t),$p=a(ee,". They can be accessed via "),Qi=n(ee,"CODE",{});var W2=o(Qi);Ep=a(W2,"scheduler.config.num_train_timesteps"),W2.forEach(t),xp=a(ee,`.
`),Kn=n(ee,"A",{href:!0});var H2=o(Kn);Dp=a(H2,"~ConfigMixin"),H2.forEach(t),yp=a(ee," also provides general loading and saving functionality via the "),Rn=n(ee,"A",{href:!0});var G2=o(Rn);wp=a(G2,"save_config()"),G2.forEach(t),Pp=a(ee,` and
`),Qn=n(ee,"A",{href:!0});var z2=o(Qn);Tp=a(z2,"from_config()"),z2.forEach(t),Mp=a(ee," functions."),ee.forEach(t),Cp=c(pe),jn=n(pe,"P",{});var b0=o(jn);kp=a(b0,"For more details, see the original paper: "),Lr=n(b0,"A",{href:!0,rel:!0});var Y2=o(Lr);Ap=a(Y2,"https://arxiv.org/abs/2006.11239"),Y2.forEach(t),b0.forEach(t),Op=c(pe),vt=n(pe,"DIV",{class:!0});var _c=o(vt);h(qr.$$.fragment,_c),Vp=c(_c),ji=n(_c,"P",{});var J2=o(ji);Np=a(J2,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),J2.forEach(t),_c.forEach(t),Fp=c(pe),bt=n(pe,"DIV",{class:!0});var vc=o(bt);h(Kr.$$.fragment,vc),Ip=c(vc),Ui=n(vc,"P",{});var X2=o(Ui);Lp=a(X2,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),X2.forEach(t),vc.forEach(t),qp=c(pe),St=n(pe,"DIV",{class:!0});var bc=o(St);h(Rr.$$.fragment,bc),Kp=c(bc),Bi=n(bc,"P",{});var Z2=o(Bi);Rp=a(Z2,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),Z2.forEach(t),bc.forEach(t),pe.forEach(t),_l=c(r),Qe=n(r,"H4",{class:!0});var Sc=o(Qe);$t=n(Sc,"A",{id:!0,class:!0,href:!0});var e4=o($t);Wi=n(e4,"SPAN",{});var t4=o(Wi);h(Qr.$$.fragment,t4),t4.forEach(t),e4.forEach(t),Qp=c(Sc),Hi=n(Sc,"SPAN",{});var r4=o(Hi);jp=a(r4,"Variance exploding, stochastic sampling from Karras et. al"),r4.forEach(t),Sc.forEach(t),vl=c(r),Et=n(r,"P",{});var $c=o(Et);Up=a($c,"Original paper can be found "),jr=n($c,"A",{href:!0,rel:!0});var s4=o(jr);Bp=a(s4,"here"),s4.forEach(t),Wp=a($c,"."),$c.forEach(t),bl=c(r),S=n(r,"DIV",{class:!0});var D=o(S);h(Ur.$$.fragment,D),Hp=c(D),Gi=n(D,"P",{});var n4=o(Gi);Gp=a(n4,`Stochastic sampling from Karras et al. [1] tailored to the Variance-Expanding (VE) models [2]. Use Algorithm 2 and
the VE column of Table 1 from [1] for reference.`),n4.forEach(t),zp=c(D),xt=n(D,"P",{});var Ud=o(xt);Yp=a(Ud,`[1] Karras, Tero, et al. \u201CElucidating the Design Space of Diffusion-Based Generative Models.\u201D
`),Br=n(Ud,"A",{href:!0,rel:!0});var o4=o(Br);Jp=a(o4,"https://arxiv.org/abs/2206.00364"),o4.forEach(t),Xp=a(Ud,` [2] Song, Yang, et al. \u201CScore-based generative modeling through stochastic
differential equations.\u201D `),Wr=n(Ud,"A",{href:!0,rel:!0});var i4=o(Wr);Zp=a(i4,"https://arxiv.org/abs/2011.13456"),i4.forEach(t),Ud.forEach(t),eh=c(D),O=n(D,"P",{});var te=o(O);Un=n(te,"A",{href:!0});var a4=o(Un);th=a(a4,"~ConfigMixin"),a4.forEach(t),rh=a(te," takes care of storing all config attributes that are passed in the scheduler\u2019s "),zi=n(te,"CODE",{});var d4=o(zi);sh=a(d4,"__init__"),d4.forEach(t),nh=a(te,`
function, such as `),Yi=n(te,"CODE",{});var l4=o(Yi);oh=a(l4,"num_train_timesteps"),l4.forEach(t),ih=a(te,". They can be accessed via "),Ji=n(te,"CODE",{});var c4=o(Ji);ah=a(c4,"scheduler.config.num_train_timesteps"),c4.forEach(t),dh=a(te,`.
`),Bn=n(te,"A",{href:!0});var u4=o(Bn);lh=a(u4,"~ConfigMixin"),u4.forEach(t),ch=a(te," also provides general loading and saving functionality via the "),Wn=n(te,"A",{href:!0});var f4=o(Wn);uh=a(f4,"save_config()"),f4.forEach(t),fh=a(te,` and
`),Hn=n(te,"A",{href:!0});var p4=o(Hn);ph=a(p4,"from_config()"),p4.forEach(t),hh=a(te," functions."),te.forEach(t),mh=c(D),Hr=n(D,"P",{});var Ec=o(Hr);gh=a(Ec,`For more details on the parameters, see the original paper\u2019s Appendix E.: \u201CElucidating the Design Space of
Diffusion-Based Generative Models.\u201D `),Gr=n(Ec,"A",{href:!0,rel:!0});var h4=o(Gr);_h=a(h4,"https://arxiv.org/abs/2206.00364"),h4.forEach(t),vh=a(Ec,`. The grid search values used to find the
optimal {s_noise, s_churn, s_min, s_max} for a specific model are described in Table 5 of the paper.`),Ec.forEach(t),bh=c(D),De=n(D,"DIV",{class:!0});var Uo=o(De);h(zr.$$.fragment,Uo),Sh=c(Uo),Xi=n(Uo,"P",{});var m4=o(Xi);$h=a(m4,`Explicit Langevin-like \u201Cchurn\u201D step of adding noise to the sample according to a factor gamma_i \u2265 0 to reach a
higher noise level sigma_hat = sigma_i + gamma_i*sigma_i.`),m4.forEach(t),Eh=c(Uo),Zi=n(Uo,"P",{});var g4=o(Zi);xh=a(g4,"TODO Args:"),g4.forEach(t),Uo.forEach(t),Dh=c(D),Dt=n(D,"DIV",{class:!0});var xc=o(Dt);h(Yr.$$.fragment,xc),yh=c(xc),ea=n(xc,"P",{});var _4=o(ea);wh=a(_4,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),_4.forEach(t),xc.forEach(t),Ph=c(D),yt=n(D,"DIV",{class:!0});var Dc=o(yt);h(Jr.$$.fragment,Dc),Th=c(Dc),ta=n(Dc,"P",{});var v4=o(ta);Mh=a(v4,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),v4.forEach(t),Dc.forEach(t),Ch=c(D),wt=n(D,"DIV",{class:!0});var yc=o(wt);h(Xr.$$.fragment,yc),kh=c(yc),ra=n(yc,"P",{});var b4=o(ra);Ah=a(b4,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),b4.forEach(t),yc.forEach(t),Oh=c(D),Pt=n(D,"DIV",{class:!0});var wc=o(Pt);h(Zr.$$.fragment,wc),Vh=c(wc),sa=n(wc,"P",{});var S4=o(sa);Nh=a(S4,"Correct the predicted sample based on the output model_output of the network. TODO complete description"),S4.forEach(t),wc.forEach(t),D.forEach(t),Sl=c(r),je=n(r,"H4",{class:!0});var Pc=o(je);Tt=n(Pc,"A",{id:!0,class:!0,href:!0});var $4=o(Tt);na=n($4,"SPAN",{});var E4=o(na);h(es.$$.fragment,E4),E4.forEach(t),$4.forEach(t),Fh=c(Pc),oa=n(Pc,"SPAN",{});var x4=o(oa);Ih=a(x4,"Linear multistep scheduler for discrete beta schedules"),x4.forEach(t),Pc.forEach(t),$l=c(r),Mt=n(r,"P",{});var Tc=o(Mt);Lh=a(Tc,"Original implementation can be found "),ts=n(Tc,"A",{href:!0,rel:!0});var D4=o(ts);qh=a(D4,"here"),D4.forEach(t),Kh=a(Tc,"."),Tc.forEach(t),El=c(r),T=n(r,"DIV",{class:!0});var he=o(T);h(rs.$$.fragment,he),Rh=c(he),Gn=n(he,"P",{});var S0=o(Gn);Qh=a(S0,`Linear Multistep Scheduler for discrete beta schedules. Based on the original k-diffusion implementation by
Katherine Crowson:
`),ss=n(S0,"A",{href:!0,rel:!0});var y4=o(ss);jh=a(y4,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),y4.forEach(t),S0.forEach(t),Uh=c(he),V=n(he,"P",{});var re=o(V);zn=n(re,"A",{href:!0});var w4=o(zn);Bh=a(w4,"~ConfigMixin"),w4.forEach(t),Wh=a(re," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ia=n(re,"CODE",{});var P4=o(ia);Hh=a(P4,"__init__"),P4.forEach(t),Gh=a(re,`
function, such as `),aa=n(re,"CODE",{});var T4=o(aa);zh=a(T4,"num_train_timesteps"),T4.forEach(t),Yh=a(re,". They can be accessed via "),da=n(re,"CODE",{});var M4=o(da);Jh=a(M4,"scheduler.config.num_train_timesteps"),M4.forEach(t),Xh=a(re,`.
`),Yn=n(re,"A",{href:!0});var C4=o(Yn);Zh=a(C4,"~ConfigMixin"),C4.forEach(t),em=a(re," also provides general loading and saving functionality via the "),Jn=n(re,"A",{href:!0});var k4=o(Jn);tm=a(k4,"save_config()"),k4.forEach(t),rm=a(re,` and
`),Xn=n(re,"A",{href:!0});var A4=o(Xn);sm=a(A4,"from_config()"),A4.forEach(t),nm=a(re," functions."),re.forEach(t),om=c(he),Ct=n(he,"DIV",{class:!0});var Mc=o(Ct);h(ns.$$.fragment,Mc),im=c(Mc),la=n(Mc,"P",{});var O4=o(la);am=a(O4,"Compute a linear multistep coefficient."),O4.forEach(t),Mc.forEach(t),dm=c(he),kt=n(he,"DIV",{class:!0});var Cc=o(kt);h(os.$$.fragment,Cc),lm=c(Cc),is=n(Cc,"P",{});var kc=o(is);cm=a(kc,"Scales the denoising model input by "),ca=n(kc,"CODE",{});var V4=o(ca);um=a(V4,"(sigma**2 + 1) ** 0.5"),V4.forEach(t),fm=a(kc," to match the K-LMS algorithm."),kc.forEach(t),Cc.forEach(t),pm=c(he),At=n(he,"DIV",{class:!0});var Ac=o(At);h(as.$$.fragment,Ac),hm=c(Ac),ua=n(Ac,"P",{});var N4=o(ua);mm=a(N4,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),N4.forEach(t),Ac.forEach(t),gm=c(he),Ot=n(he,"DIV",{class:!0});var Oc=o(Ot);h(ds.$$.fragment,Oc),_m=c(Oc),fa=n(Oc,"P",{});var F4=o(fa);vm=a(F4,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),F4.forEach(t),Oc.forEach(t),he.forEach(t),xl=c(r),Ue=n(r,"H4",{class:!0});var Vc=o(Ue);Vt=n(Vc,"A",{id:!0,class:!0,href:!0});var I4=o(Vt);pa=n(I4,"SPAN",{});var L4=o(pa);h(ls.$$.fragment,L4),L4.forEach(t),I4.forEach(t),bm=c(Vc),ha=n(Vc,"SPAN",{});var q4=o(ha);Sm=a(q4,"Pseudo numerical methods for diffusion models (PNDM)"),q4.forEach(t),Vc.forEach(t),Dl=c(r),Nt=n(r,"P",{});var Nc=o(Nt);$m=a(Nc,"Original implementation can be found "),cs=n(Nc,"A",{href:!0,rel:!0});var K4=o(cs);Em=a(K4,"here"),K4.forEach(t),xm=a(Nc,"."),Nc.forEach(t),yl=c(r),$=n(r,"DIV",{class:!0});var U=o($);h(us.$$.fragment,U),Dm=c(U),ma=n(U,"P",{});var R4=o(ma);ym=a(R4,`Pseudo numerical methods for diffusion models (PNDM) proposes using more advanced ODE integration techniques,
namely Runge-Kutta method and a linear multi-step method.`),R4.forEach(t),wm=c(U),N=n(U,"P",{});var se=o(N);Zn=n(se,"A",{href:!0});var Q4=o(Zn);Pm=a(Q4,"~ConfigMixin"),Q4.forEach(t),Tm=a(se," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ga=n(se,"CODE",{});var j4=o(ga);Mm=a(j4,"__init__"),j4.forEach(t),Cm=a(se,`
function, such as `),_a=n(se,"CODE",{});var U4=o(_a);km=a(U4,"num_train_timesteps"),U4.forEach(t),Am=a(se,". They can be accessed via "),va=n(se,"CODE",{});var B4=o(va);Om=a(B4,"scheduler.config.num_train_timesteps"),B4.forEach(t),Vm=a(se,`.
`),eo=n(se,"A",{href:!0});var W4=o(eo);Nm=a(W4,"~ConfigMixin"),W4.forEach(t),Fm=a(se," also provides general loading and saving functionality via the "),to=n(se,"A",{href:!0});var H4=o(to);Im=a(H4,"save_config()"),H4.forEach(t),Lm=a(se,` and
`),ro=n(se,"A",{href:!0});var G4=o(ro);qm=a(G4,"from_config()"),G4.forEach(t),Km=a(se," functions."),se.forEach(t),Rm=c(U),so=n(U,"P",{});var $0=o(so);Qm=a($0,"For more details, see the original paper: "),fs=n($0,"A",{href:!0,rel:!0});var z4=o(fs);jm=a(z4,"https://arxiv.org/abs/2202.09778"),z4.forEach(t),$0.forEach(t),Um=c(U),Ft=n(U,"DIV",{class:!0});var Fc=o(Ft);h(ps.$$.fragment,Fc),Bm=c(Fc),ba=n(Fc,"P",{});var Y4=o(ba);Wm=a(Y4,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),Y4.forEach(t),Fc.forEach(t),Hm=c(U),It=n(U,"DIV",{class:!0});var Ic=o(It);h(hs.$$.fragment,Ic),Gm=c(Ic),Sa=n(Ic,"P",{});var J4=o(Sa);zm=a(J4,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),J4.forEach(t),Ic.forEach(t),Ym=c(U),ye=n(U,"DIV",{class:!0});var Bo=o(ye);h(ms.$$.fragment,Bo),Jm=c(Bo),$a=n(Bo,"P",{});var X4=o($a);Xm=a(X4,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),X4.forEach(t),Zm=c(Bo),$e=n(Bo,"P",{});var mr=o($e);eg=a(mr,"This function calls "),Ea=n(mr,"CODE",{});var Z4=o(Ea);tg=a(Z4,"step_prk()"),Z4.forEach(t),rg=a(mr," or "),xa=n(mr,"CODE",{});var eS=o(xa);sg=a(eS,"step_plms()"),eS.forEach(t),ng=a(mr," depending on the internal variable "),Da=n(mr,"CODE",{});var tS=o(Da);og=a(tS,"counter"),tS.forEach(t),ig=a(mr,"."),mr.forEach(t),Bo.forEach(t),ag=c(U),Lt=n(U,"DIV",{class:!0});var Lc=o(Lt);h(gs.$$.fragment,Lc),dg=c(Lc),ya=n(Lc,"P",{});var rS=o(ya);lg=a(rS,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),rS.forEach(t),Lc.forEach(t),cg=c(U),qt=n(U,"DIV",{class:!0});var qc=o(qt);h(_s.$$.fragment,qc),ug=c(qc),wa=n(qc,"P",{});var sS=o(wa);fg=a(sS,`Step function propagating the sample with the Runge-Kutta method. RK takes 4 forward passes to approximate the
solution to the differential equation.`),sS.forEach(t),qc.forEach(t),U.forEach(t),wl=c(r),Be=n(r,"H4",{class:!0});var Kc=o(Be);Kt=n(Kc,"A",{id:!0,class:!0,href:!0});var nS=o(Kt);Pa=n(nS,"SPAN",{});var oS=o(Pa);h(vs.$$.fragment,oS),oS.forEach(t),nS.forEach(t),pg=c(Kc),Ta=n(Kc,"SPAN",{});var iS=o(Ta);hg=a(iS,"variance exploding stochastic differential equation (VE-SDE) scheduler"),iS.forEach(t),Kc.forEach(t),Pl=c(r),Rt=n(r,"P",{});var Rc=o(Rt);mg=a(Rc,"Original paper can be found "),bs=n(Rc,"A",{href:!0,rel:!0});var aS=o(bs);gg=a(aS,"here"),aS.forEach(t),_g=a(Rc,"."),Rc.forEach(t),Tl=c(r),E=n(r,"DIV",{class:!0});var B=o(E);h(Ss.$$.fragment,B),vg=c(B),Ma=n(B,"P",{});var dS=o(Ma);bg=a(dS,"The variance exploding stochastic differential equation (SDE) scheduler."),dS.forEach(t),Sg=c(B),no=n(B,"P",{});var E0=o(no);$g=a(E0,"For more information, see the original paper: "),$s=n(E0,"A",{href:!0,rel:!0});var lS=o($s);Eg=a(lS,"https://arxiv.org/abs/2011.13456"),lS.forEach(t),E0.forEach(t),xg=c(B),F=n(B,"P",{});var ne=o(F);oo=n(ne,"A",{href:!0});var cS=o(oo);Dg=a(cS,"~ConfigMixin"),cS.forEach(t),yg=a(ne," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ca=n(ne,"CODE",{});var uS=o(Ca);wg=a(uS,"__init__"),uS.forEach(t),Pg=a(ne,`
function, such as `),ka=n(ne,"CODE",{});var fS=o(ka);Tg=a(fS,"num_train_timesteps"),fS.forEach(t),Mg=a(ne,". They can be accessed via "),Aa=n(ne,"CODE",{});var pS=o(Aa);Cg=a(pS,"scheduler.config.num_train_timesteps"),pS.forEach(t),kg=a(ne,`.
`),io=n(ne,"A",{href:!0});var hS=o(io);Ag=a(hS,"~ConfigMixin"),hS.forEach(t),Og=a(ne," also provides general loading and saving functionality via the "),ao=n(ne,"A",{href:!0});var mS=o(ao);Vg=a(mS,"save_config()"),mS.forEach(t),Ng=a(ne,` and
`),lo=n(ne,"A",{href:!0});var gS=o(lo);Fg=a(gS,"from_config()"),gS.forEach(t),Ig=a(ne," functions."),ne.forEach(t),Lg=c(B),Qt=n(B,"DIV",{class:!0});var Qc=o(Qt);h(Es.$$.fragment,Qc),qg=c(Qc),Oa=n(Qc,"P",{});var _S=o(Oa);Kg=a(_S,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),_S.forEach(t),Qc.forEach(t),Rg=c(B),we=n(B,"DIV",{class:!0});var Wo=o(we);h(xs.$$.fragment,Wo),Qg=c(Wo),Va=n(Wo,"P",{});var vS=o(Va);jg=a(vS,"Sets the noise scales used for the diffusion chain. Supporting function to be run before inference."),vS.forEach(t),Ug=c(Wo),We=n(Wo,"P",{});var Ho=o(We);Bg=a(Ho,"The sigmas control the weight of the "),Na=n(Ho,"CODE",{});var bS=o(Na);Wg=a(bS,"drift"),bS.forEach(t),Hg=a(Ho," and "),Fa=n(Ho,"CODE",{});var SS=o(Fa);Gg=a(SS,"diffusion"),SS.forEach(t),zg=a(Ho," components of sample update."),Ho.forEach(t),Wo.forEach(t),Yg=c(B),jt=n(B,"DIV",{class:!0});var jc=o(jt);h(Ds.$$.fragment,jc),Jg=c(jc),Ia=n(jc,"P",{});var $S=o(Ia);Xg=a($S,"Sets the continuous timesteps used for the diffusion chain. Supporting function to be run before inference."),$S.forEach(t),jc.forEach(t),Zg=c(B),Ut=n(B,"DIV",{class:!0});var Uc=o(Ut);h(ys.$$.fragment,Uc),e_=c(Uc),La=n(Uc,"P",{});var ES=o(La);t_=a(ES,`Correct the predicted sample based on the output model_output of the network. This is often run repeatedly
after making the prediction for the previous timestep.`),ES.forEach(t),Uc.forEach(t),r_=c(B),Bt=n(B,"DIV",{class:!0});var Bc=o(Bt);h(ws.$$.fragment,Bc),s_=c(Bc),qa=n(Bc,"P",{});var xS=o(qa);n_=a(xS,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),xS.forEach(t),Bc.forEach(t),B.forEach(t),Ml=c(r),He=n(r,"H4",{class:!0});var Wc=o(He);Wt=n(Wc,"A",{id:!0,class:!0,href:!0});var DS=o(Wt);Ka=n(DS,"SPAN",{});var yS=o(Ka);h(Ps.$$.fragment,yS),yS.forEach(t),DS.forEach(t),o_=c(Wc),Ra=n(Wc,"SPAN",{});var wS=o(Ra);i_=a(wS,"improved pseudo numerical methods for diffusion models (iPNDM)"),wS.forEach(t),Wc.forEach(t),Cl=c(r),Ht=n(r,"P",{});var Hc=o(Ht);a_=a(Hc,"Original implementation can be found "),Ts=n(Hc,"A",{href:!0,rel:!0});var PS=o(Ts);d_=a(PS,"here"),PS.forEach(t),l_=a(Hc,"."),Hc.forEach(t),kl=c(r),M=n(r,"DIV",{class:!0});var me=o(M);h(Ms.$$.fragment,me),c_=c(me),co=n(me,"P",{});var x0=o(co);u_=a(x0,`Improved Pseudo numerical methods for diffusion models (iPNDM) ported from @crowsonkb\u2019s amazing k-diffusion
`),Cs=n(x0,"A",{href:!0,rel:!0});var TS=o(Cs);f_=a(TS,"library"),TS.forEach(t),x0.forEach(t),p_=c(me),I=n(me,"P",{});var oe=o(I);uo=n(oe,"A",{href:!0});var MS=o(uo);h_=a(MS,"~ConfigMixin"),MS.forEach(t),m_=a(oe," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Qa=n(oe,"CODE",{});var CS=o(Qa);g_=a(CS,"__init__"),CS.forEach(t),__=a(oe,`
function, such as `),ja=n(oe,"CODE",{});var kS=o(ja);v_=a(kS,"num_train_timesteps"),kS.forEach(t),b_=a(oe,". They can be accessed via "),Ua=n(oe,"CODE",{});var AS=o(Ua);S_=a(AS,"scheduler.config.num_train_timesteps"),AS.forEach(t),$_=a(oe,`.
`),fo=n(oe,"A",{href:!0});var OS=o(fo);E_=a(OS,"~ConfigMixin"),OS.forEach(t),x_=a(oe," also provides general loading and saving functionality via the "),po=n(oe,"A",{href:!0});var VS=o(po);D_=a(VS,"save_config()"),VS.forEach(t),y_=a(oe,` and
`),ho=n(oe,"A",{href:!0});var NS=o(ho);w_=a(NS,"from_config()"),NS.forEach(t),P_=a(oe," functions."),oe.forEach(t),T_=c(me),mo=n(me,"P",{});var D0=o(mo);M_=a(D0,"For more details, see the original paper: "),ks=n(D0,"A",{href:!0,rel:!0});var FS=o(ks);C_=a(FS,"https://arxiv.org/abs/2202.09778"),FS.forEach(t),D0.forEach(t),k_=c(me),Gt=n(me,"DIV",{class:!0});var Gc=o(Gt);h(As.$$.fragment,Gc),A_=c(Gc),Ba=n(Gc,"P",{});var IS=o(Ba);O_=a(IS,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),IS.forEach(t),Gc.forEach(t),V_=c(me),zt=n(me,"DIV",{class:!0});var zc=o(zt);h(Os.$$.fragment,zc),N_=c(zc),Wa=n(zc,"P",{});var LS=o(Wa);F_=a(LS,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),LS.forEach(t),zc.forEach(t),I_=c(me),Yt=n(me,"DIV",{class:!0});var Yc=o(Yt);h(Vs.$$.fragment,Yc),L_=c(Yc),Ha=n(Yc,"P",{});var qS=o(Ha);q_=a(qS,`Step function propagating the sample with the linear multi-step method. This has one forward pass with multiple
times to approximate the solution.`),qS.forEach(t),Yc.forEach(t),me.forEach(t),Al=c(r),Ge=n(r,"H4",{class:!0});var Jc=o(Ge);Jt=n(Jc,"A",{id:!0,class:!0,href:!0});var KS=o(Jt);Ga=n(KS,"SPAN",{});var RS=o(Ga);h(Ns.$$.fragment,RS),RS.forEach(t),KS.forEach(t),K_=c(Jc),za=n(Jc,"SPAN",{});var QS=o(za);R_=a(QS,"variance preserving stochastic differential equation (VP-SDE) scheduler"),QS.forEach(t),Jc.forEach(t),Ol=c(r),Xt=n(r,"P",{});var Xc=o(Xt);Q_=a(Xc,"Original paper can be found "),Fs=n(Xc,"A",{href:!0,rel:!0});var jS=o(Fs);j_=a(jS,"here"),jS.forEach(t),U_=a(Xc,"."),Xc.forEach(t),Vl=c(r),h(Zt.$$.fragment,r),Nl=c(r),X=n(r,"DIV",{class:!0});var Me=o(X);h(Is.$$.fragment,Me),B_=c(Me),Ya=n(Me,"P",{});var US=o(Ya);W_=a(US,"The variance preserving stochastic differential equation (SDE) scheduler."),US.forEach(t),H_=c(Me),L=n(Me,"P",{});var ie=o(L);go=n(ie,"A",{href:!0});var BS=o(go);G_=a(BS,"~ConfigMixin"),BS.forEach(t),z_=a(ie," takes care of storing all config attributes that are passed in the scheduler\u2019s "),Ja=n(ie,"CODE",{});var WS=o(Ja);Y_=a(WS,"__init__"),WS.forEach(t),J_=a(ie,`
function, such as `),Xa=n(ie,"CODE",{});var HS=o(Xa);X_=a(HS,"num_train_timesteps"),HS.forEach(t),Z_=a(ie,". They can be accessed via "),Za=n(ie,"CODE",{});var GS=o(Za);ev=a(GS,"scheduler.config.num_train_timesteps"),GS.forEach(t),tv=a(ie,`.
`),_o=n(ie,"A",{href:!0});var zS=o(_o);rv=a(zS,"~ConfigMixin"),zS.forEach(t),sv=a(ie," also provides general loading and saving functionality via the "),vo=n(ie,"A",{href:!0});var YS=o(vo);nv=a(YS,"save_config()"),YS.forEach(t),ov=a(ie,` and
`),bo=n(ie,"A",{href:!0});var JS=o(bo);iv=a(JS,"from_config()"),JS.forEach(t),av=a(ie," functions."),ie.forEach(t),dv=c(Me),So=n(Me,"P",{});var y0=o(So);lv=a(y0,"For more information, see the original paper: "),Ls=n(y0,"A",{href:!0,rel:!0});var XS=o(Ls);cv=a(XS,"https://arxiv.org/abs/2011.13456"),XS.forEach(t),y0.forEach(t),uv=c(Me),ed=n(Me,"P",{});var ZS=o(ed);fv=a(ZS,"UNDER CONSTRUCTION"),ZS.forEach(t),Me.forEach(t),Fl=c(r),ze=n(r,"H4",{class:!0});var Zc=o(ze);er=n(Zc,"A",{id:!0,class:!0,href:!0});var e$=o(er);td=n(e$,"SPAN",{});var t$=o(td);h(qs.$$.fragment,t$),t$.forEach(t),e$.forEach(t),pv=c(Zc),rd=n(Zc,"SPAN",{});var r$=o(rd);hv=a(r$,"Euler scheduler"),r$.forEach(t),Zc.forEach(t),Il=c(r),Pe=n(r,"P",{});var Go=o(Pe);mv=a(Go,"Euler scheduler (Algorithm 2) from the paper "),Ks=n(Go,"A",{href:!0,rel:!0});var s$=o(Ks);gv=a(s$,"Elucidating the Design Space of Diffusion-Based Generative Models"),s$.forEach(t),_v=a(Go," by Karras et al. (2022). Based on the original "),Rs=n(Go,"A",{href:!0,rel:!0});var n$=o(Rs);vv=a(n$,"k-diffusion"),n$.forEach(t),bv=a(Go,` implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),Go.forEach(t),Ll=c(r),H=n(r,"DIV",{class:!0});var ve=o(H);h(Qs.$$.fragment,ve),Sv=c(ve),tr=n(ve,"P",{});var Bd=o(tr);$v=a(Bd,"Euler scheduler (Algorithm 2) from Karras et al. (2022) "),js=n(Bd,"A",{href:!0,rel:!0});var o$=o(js);Ev=a(o$,"https://arxiv.org/abs/2206.00364"),o$.forEach(t),xv=a(Bd,`. . Based on the original
k-diffusion implementation by Katherine Crowson:
`),Us=n(Bd,"A",{href:!0,rel:!0});var i$=o(Us);Dv=a(i$,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),i$.forEach(t),Bd.forEach(t),yv=c(ve),q=n(ve,"P",{});var ae=o(q);$o=n(ae,"A",{href:!0});var a$=o($o);wv=a(a$,"~ConfigMixin"),a$.forEach(t),Pv=a(ae," takes care of storing all config attributes that are passed in the scheduler\u2019s "),sd=n(ae,"CODE",{});var d$=o(sd);Tv=a(d$,"__init__"),d$.forEach(t),Mv=a(ae,`
function, such as `),nd=n(ae,"CODE",{});var l$=o(nd);Cv=a(l$,"num_train_timesteps"),l$.forEach(t),kv=a(ae,". They can be accessed via "),od=n(ae,"CODE",{});var c$=o(od);Av=a(c$,"scheduler.config.num_train_timesteps"),c$.forEach(t),Ov=a(ae,`.
`),Eo=n(ae,"A",{href:!0});var u$=o(Eo);Vv=a(u$,"~ConfigMixin"),u$.forEach(t),Nv=a(ae," also provides general loading and saving functionality via the "),xo=n(ae,"A",{href:!0});var f$=o(xo);Fv=a(f$,"save_config()"),f$.forEach(t),Iv=a(ae,` and
`),Do=n(ae,"A",{href:!0});var p$=o(Do);Lv=a(p$,"from_config()"),p$.forEach(t),qv=a(ae," functions."),ae.forEach(t),Kv=c(ve),rr=n(ve,"DIV",{class:!0});var eu=o(rr);h(Bs.$$.fragment,eu),Rv=c(eu),Ws=n(eu,"P",{});var tu=o(Ws);Qv=a(tu,"Scales the denoising model input by "),id=n(tu,"CODE",{});var h$=o(id);jv=a(h$,"(sigma**2 + 1) ** 0.5"),h$.forEach(t),Uv=a(tu," to match the Euler algorithm."),tu.forEach(t),eu.forEach(t),Bv=c(ve),sr=n(ve,"DIV",{class:!0});var ru=o(sr);h(Hs.$$.fragment,ru),Wv=c(ru),ad=n(ru,"P",{});var m$=o(ad);Hv=a(m$,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),m$.forEach(t),ru.forEach(t),Gv=c(ve),nr=n(ve,"DIV",{class:!0});var su=o(nr);h(Gs.$$.fragment,su),zv=c(su),dd=n(su,"P",{});var g$=o(dd);Yv=a(g$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),g$.forEach(t),su.forEach(t),ve.forEach(t),ql=c(r),Ye=n(r,"H4",{class:!0});var nu=o(Ye);or=n(nu,"A",{id:!0,class:!0,href:!0});var _$=o(or);ld=n(_$,"SPAN",{});var v$=o(ld);h(zs.$$.fragment,v$),v$.forEach(t),_$.forEach(t),Jv=c(nu),cd=n(nu,"SPAN",{});var b$=o(cd);Xv=a(b$,"Euler Ancestral scheduler"),b$.forEach(t),nu.forEach(t),Kl=c(r),yo=n(r,"P",{});var S$=o(yo);Zv=a(S$,`Ancestral sampling with Euler method steps. Based on the original (k-diffusion)[https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72] implementation by Katherine Crowson.
Fast scheduler which often times generates good outputs with 20-30 steps.`),S$.forEach(t),Rl=c(r),G=n(r,"DIV",{class:!0});var be=o(G);h(Ys.$$.fragment,be),eb=c(be),wo=n(be,"P",{});var w0=o(wo);tb=a(w0,`Ancestral sampling with Euler method steps. Based on the original k-diffusion implementation by Katherine Crowson:
`),Js=n(w0,"A",{href:!0,rel:!0});var $$=o(Js);rb=a($$,"https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),$$.forEach(t),w0.forEach(t),sb=c(be),K=n(be,"P",{});var de=o(K);Po=n(de,"A",{href:!0});var E$=o(Po);nb=a(E$,"~ConfigMixin"),E$.forEach(t),ob=a(de," takes care of storing all config attributes that are passed in the scheduler\u2019s "),ud=n(de,"CODE",{});var x$=o(ud);ib=a(x$,"__init__"),x$.forEach(t),ab=a(de,`
function, such as `),fd=n(de,"CODE",{});var D$=o(fd);db=a(D$,"num_train_timesteps"),D$.forEach(t),lb=a(de,". They can be accessed via "),pd=n(de,"CODE",{});var y$=o(pd);cb=a(y$,"scheduler.config.num_train_timesteps"),y$.forEach(t),ub=a(de,`.
`),To=n(de,"A",{href:!0});var w$=o(To);fb=a(w$,"~ConfigMixin"),w$.forEach(t),pb=a(de," also provides general loading and saving functionality via the "),Mo=n(de,"A",{href:!0});var P$=o(Mo);hb=a(P$,"save_config()"),P$.forEach(t),mb=a(de,` and
`),Co=n(de,"A",{href:!0});var T$=o(Co);gb=a(T$,"from_config()"),T$.forEach(t),_b=a(de," functions."),de.forEach(t),vb=c(be),ir=n(be,"DIV",{class:!0});var ou=o(ir);h(Xs.$$.fragment,ou),bb=c(ou),Zs=n(ou,"P",{});var iu=o(Zs);Sb=a(iu,"Scales the denoising model input by "),hd=n(iu,"CODE",{});var M$=o(hd);$b=a(M$,"(sigma**2 + 1) ** 0.5"),M$.forEach(t),Eb=a(iu," to match the Euler algorithm."),iu.forEach(t),ou.forEach(t),xb=c(be),ar=n(be,"DIV",{class:!0});var au=o(ar);h(en.$$.fragment,au),Db=c(au),md=n(au,"P",{});var C$=o(md);yb=a(C$,"Sets the timesteps used for the diffusion chain. Supporting function to be run before inference."),C$.forEach(t),au.forEach(t),wb=c(be),dr=n(be,"DIV",{class:!0});var du=o(dr);h(tn.$$.fragment,du),Pb=c(du),gd=n(du,"P",{});var k$=o(gd);Tb=a(k$,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),k$.forEach(t),du.forEach(t),be.forEach(t),Ql=c(r),Je=n(r,"H4",{class:!0});var lu=o(Je);lr=n(lu,"A",{id:!0,class:!0,href:!0});var A$=o(lr);_d=n(A$,"SPAN",{});var O$=o(_d);h(rn.$$.fragment,O$),O$.forEach(t),A$.forEach(t),Mb=c(lu),vd=n(lu,"SPAN",{});var V$=o(vd);Cb=a(V$,"VQDiffusionScheduler"),V$.forEach(t),lu.forEach(t),jl=c(r),sn=n(r,"P",{});var P0=o(sn);kb=a(P0,"Original paper can be found "),nn=n(P0,"A",{href:!0,rel:!0});var N$=o(nn);Ab=a(N$,"here"),N$.forEach(t),P0.forEach(t),Ul=c(r),x=n(r,"DIV",{class:!0});var W=o(x);h(on.$$.fragment,W),Ob=c(W),bd=n(W,"P",{});var F$=o(bd);Vb=a(F$,"The VQ-diffusion transformer outputs predicted probabilities of the initial unnoised image."),F$.forEach(t),Nb=c(W),Sd=n(W,"P",{});var I$=o(Sd);Fb=a(I$,`The VQ-diffusion scheduler converts the transformer\u2019s output into a sample for the unnoised image at the previous
diffusion timestep.`),I$.forEach(t),Ib=c(W),R=n(W,"P",{});var le=o(R);ko=n(le,"A",{href:!0});var L$=o(ko);Lb=a(L$,"~ConfigMixin"),L$.forEach(t),qb=a(le," takes care of storing all config attributes that are passed in the scheduler\u2019s "),$d=n(le,"CODE",{});var q$=o($d);Kb=a(q$,"__init__"),q$.forEach(t),Rb=a(le,`
function, such as `),Ed=n(le,"CODE",{});var K$=o(Ed);Qb=a(K$,"num_train_timesteps"),K$.forEach(t),jb=a(le,". They can be accessed via "),xd=n(le,"CODE",{});var R$=o(xd);Ub=a(R$,"scheduler.config.num_train_timesteps"),R$.forEach(t),Bb=a(le,`.
`),Ao=n(le,"A",{href:!0});var Q$=o(Ao);Wb=a(Q$,"~ConfigMixin"),Q$.forEach(t),Hb=a(le," also provides general loading and saving functionality via the "),Oo=n(le,"A",{href:!0});var j$=o(Oo);Gb=a(j$,"save_config()"),j$.forEach(t),zb=a(le,` and
`),Vo=n(le,"A",{href:!0});var U$=o(Vo);Yb=a(U$,"from_config()"),U$.forEach(t),Jb=a(le," functions."),le.forEach(t),Xb=c(W),No=n(W,"P",{});var T0=o(No);Zb=a(T0,"For more details, see the original paper: "),an=n(T0,"A",{href:!0,rel:!0});var B$=o(an);e1=a(B$,"https://arxiv.org/abs/2111.14822"),B$.forEach(t),T0.forEach(t),t1=c(W),Te=n(W,"DIV",{class:!0});var zo=o(Te);h(dn.$$.fragment,zo),r1=c(zo),ln=n(zo,"P",{});var cu=o(ln);s1=a(cu,`Returns the log probabilities of the rows from the (cumulative or non-cumulative) transition matrix for each
latent pixel in `),Dd=n(cu,"CODE",{});var W$=o(Dd);n1=a(W$,"x_t"),W$.forEach(t),o1=a(cu,"."),cu.forEach(t),i1=c(zo),yd=n(zo,"P",{});var H$=o(yd);a1=a(H$,`See equation (7) for the complete non-cumulative transition matrix. The complete cumulative transition matrix
is the same structure except the parameters (alpha, beta, gamma) are the cumulative analogs.`),H$.forEach(t),zo.forEach(t),d1=c(W),Q=n(W,"DIV",{class:!0});var ge=o(Q);h(cn.$$.fragment,ge),l1=c(ge),un=n(ge,"P",{});var uu=o(un);c1=a(uu,"Calculates the log probabilities for the predicted classes of the image at timestep "),wd=n(uu,"CODE",{});var G$=o(wd);u1=a(G$,"t-1"),G$.forEach(t),f1=a(uu,". I.e. Equation (11)."),uu.forEach(t),p1=c(ge),Pd=n(ge,"P",{});var z$=o(Pd);h1=a(z$,`Instead of directly computing equation (11), we use Equation (5) to restate Equation (11) in terms of only
forward probabilities.`),z$.forEach(t),m1=c(ge),Td=n(ge,"P",{});var Y$=o(Td);g1=a(Y$,"Equation (11) stated in terms of forward probabilities via Equation (5):"),Y$.forEach(t),_1=c(ge),Md=n(ge,"P",{});var J$=o(Md);v1=a(J$,"Where:"),J$.forEach(t),b1=c(ge),Cd=n(ge,"UL",{});var X$=o(Cd);fn=n(X$,"LI",{});var fu=o(fn);S1=a(fu,"the sum is over x"),kd=n(fu,"EM",{});var Z$=o(kd);$1=a(Z$,"0 = {C_0 \u2026 C"),Z$.forEach(t),E1=a(fu,"{k-1}} (classes for x_0)"),fu.forEach(t),X$.forEach(t),x1=c(ge),Xe=n(ge,"P",{});var Yo=o(Xe);D1=a(Yo,"p(x"),Ad=n(Yo,"EM",{});var eE=o(Ad);y1=a(eE,"{t-1} | x_t) = sum( q(x_t | x"),eE.forEach(t),w1=a(Yo,"{t-1}) "),Od=n(Yo,"EM",{});var tE=o(Od);P1=a(tE,"q(x_{t-1} | x_0)"),tE.forEach(t),T1=a(Yo," p(x_0) / q(x_t | x_0) )"),Yo.forEach(t),ge.forEach(t),M1=c(W),cr=n(W,"DIV",{class:!0});var pu=o(cr);h(pn.$$.fragment,pu),C1=c(pu),Vd=n(pu,"P",{});var rE=o(Vd);k1=a(rE,"Sets the discrete timesteps used for the diffusion chain. Supporting function to be run before inference."),rE.forEach(t),pu.forEach(t),A1=c(W),ur=n(W,"DIV",{class:!0});var hu=o(ur);h(hn.$$.fragment,hu),O1=c(hu),mn=n(hu,"P",{});var mu=o(mn);V1=a(mu,`Predict the sample at the previous timestep via the reverse transition distribution i.e. Equation (11). See the
docstring for `),Nd=n(mu,"CODE",{});var sE=o(Nd);N1=a(sE,"self.q_posterior"),sE.forEach(t),F1=a(mu," for more in depth docs on how Equation (11) is computed."),mu.forEach(t),hu.forEach(t),W.forEach(t),Bl=c(r),Ze=n(r,"H4",{class:!0});var gu=o(Ze);fr=n(gu,"A",{id:!0,class:!0,href:!0});var nE=o(fr);Fd=n(nE,"SPAN",{});var oE=o(Fd);h(gn.$$.fragment,oE),oE.forEach(t),nE.forEach(t),I1=c(gu),Id=n(gu,"SPAN",{});var iE=o(Id);L1=a(iE,"RePaint scheduler"),iE.forEach(t),gu.forEach(t),Wl=c(r),Ee=n(r,"P",{});var Dn=o(Ee);q1=a(Dn,`DDPM-based inpainting scheduler for unsupervised inpainting with extreme masks.
Intended for use with `),Fo=n(Dn,"A",{href:!0});var aE=o(Fo);K1=a(aE,"RePaintPipeline"),aE.forEach(t),R1=a(Dn,`.
Based on the paper `),_n=n(Dn,"A",{href:!0,rel:!0});var dE=o(_n);Q1=a(dE,"RePaint: Inpainting using Denoising Diffusion Probabilistic Models"),dE.forEach(t),j1=a(Dn,`
and the original implementation by Andreas Lugmayr et al.: `),vn=n(Dn,"A",{href:!0,rel:!0});var lE=o(vn);U1=a(lE,"https://github.com/andreas128/RePaint"),lE.forEach(t),Dn.forEach(t),Hl=c(r),z=n(r,"DIV",{class:!0});var Se=o(z);h(bn.$$.fragment,Se),B1=c(Se),Ld=n(Se,"P",{});var cE=o(Ld);W1=a(cE,"RePaint is a schedule for DDPM inpainting inside a given mask."),cE.forEach(t),H1=c(Se),j=n(Se,"P",{});var ce=o(j);Io=n(ce,"A",{href:!0});var uE=o(Io);G1=a(uE,"~ConfigMixin"),uE.forEach(t),z1=a(ce," takes care of storing all config attributes that are passed in the scheduler\u2019s "),qd=n(ce,"CODE",{});var fE=o(qd);Y1=a(fE,"__init__"),fE.forEach(t),J1=a(ce,`
function, such as `),Kd=n(ce,"CODE",{});var pE=o(Kd);X1=a(pE,"num_train_timesteps"),pE.forEach(t),Z1=a(ce,". They can be accessed via "),Rd=n(ce,"CODE",{});var hE=o(Rd);e0=a(hE,"scheduler.config.num_train_timesteps"),hE.forEach(t),t0=a(ce,`.
`),Lo=n(ce,"A",{href:!0});var mE=o(Lo);r0=a(mE,"~ConfigMixin"),mE.forEach(t),s0=a(ce," also provides general loading and saving functionality via the "),qo=n(ce,"A",{href:!0});var gE=o(qo);n0=a(gE,"save_config()"),gE.forEach(t),o0=a(ce,` and
`),Ko=n(ce,"A",{href:!0});var _E=o(Ko);i0=a(_E,"from_config()"),_E.forEach(t),a0=a(ce," functions."),ce.forEach(t),d0=c(Se),Ro=n(Se,"P",{});var M0=o(Ro);l0=a(M0,"For more details, see the original paper: "),Sn=n(M0,"A",{href:!0,rel:!0});var vE=o(Sn);c0=a(vE,"https://arxiv.org/pdf/2201.09865.pdf"),vE.forEach(t),M0.forEach(t),u0=c(Se),pr=n(Se,"DIV",{class:!0});var _u=o(pr);h($n.$$.fragment,_u),f0=c(_u),Qd=n(_u,"P",{});var bE=o(Qd);p0=a(bE,`Ensures interchangeability with schedulers that need to scale the denoising model input depending on the
current timestep.`),bE.forEach(t),_u.forEach(t),h0=c(Se),hr=n(Se,"DIV",{class:!0});var vu=o(hr);h(En.$$.fragment,vu),m0=c(vu),jd=n(vu,"P",{});var SE=o(jd);g0=a(SE,`Predict the sample at the previous timestep by reversing the SDE. Core function to propagate the diffusion
process from the learned model outputs (most often the predicted noise).`),SE.forEach(t),vu.forEach(t),Se.forEach(t),this.h()},h(){d(Y,"name","hf:doc:metadata"),d(Y,"content",JSON.stringify(ME)),d(_e,"id","schedulers"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#schedulers"),d(J,"class","relative group"),d(tt,"id","what-is-a-scheduler"),d(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(tt,"href","#what-is-a-scheduler"),d(Ce,"class","relative group"),d(nt,"id","discrete-versus-continuous-schedulers"),d(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(nt,"href","#discrete-versus-continuous-schedulers"),d(Ae,"class","relative group"),d(Pn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.DDPMScheduler"),d(Tn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.PNDMScheduler"),d(Mn,"href","/docs/diffusers/main/en/api/schedulers#diffusers.ScoreSdeVeScheduler"),d(ot,"id","designing-reusable-schedulers"),d(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ot,"href","#designing-reusable-schedulers"),d(Oe,"class","relative group"),d(at,"id","api"),d(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(at,"href","#api"),d(Ve,"class","relative group"),d(An,"href","/docs/diffusers/main/en/api/schedulers#diffusers.SchedulerMixin"),d(lt,"id","diffusers.SchedulerMixin"),d(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lt,"href","#diffusers.SchedulerMixin"),d(Ne,"class","relative group"),d(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ct,"id","diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ct,"href","#diffusers.schedulers.scheduling_utils.SchedulerOutput"),d(Ie,"class","relative group"),d(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ut,"id","implemented-schedulers"),d(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ut,"href","#implemented-schedulers"),d(qe,"class","relative group"),d(ft,"id","diffusers.DDIMScheduler"),d(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ft,"href","#diffusers.DDIMScheduler"),d(Ke,"class","relative group"),d(Vn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Nn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Fn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(In,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(kr,"href","https://arxiv.org/abs/2010.02502"),d(kr,"rel","nofollow"),d(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(gt,"id","diffusers.DDPMScheduler"),d(gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(gt,"href","#diffusers.DDPMScheduler"),d(Re,"class","relative group"),d(Fr,"href","https://arxiv.org/abs/2010.02502"),d(Fr,"rel","nofollow"),d(qn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Kn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Rn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Qn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Lr,"href","https://arxiv.org/abs/2006.11239"),d(Lr,"rel","nofollow"),d(vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(St,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($t,"id","diffusers.KarrasVeScheduler"),d($t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($t,"href","#diffusers.KarrasVeScheduler"),d(Qe,"class","relative group"),d(jr,"href","https://arxiv.org/abs/2006.11239"),d(jr,"rel","nofollow"),d(Br,"href","https://arxiv.org/abs/2206.00364"),d(Br,"rel","nofollow"),d(Wr,"href","https://arxiv.org/abs/2011.13456"),d(Wr,"rel","nofollow"),d(Un,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Bn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Wn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Hn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Gr,"href","https://arxiv.org/abs/2206.00364"),d(Gr,"rel","nofollow"),d(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Tt,"id","diffusers.LMSDiscreteScheduler"),d(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Tt,"href","#diffusers.LMSDiscreteScheduler"),d(je,"class","relative group"),d(ts,"href","https://arxiv.org/abs/2206.00364"),d(ts,"rel","nofollow"),d(ss,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(ss,"rel","nofollow"),d(zn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Yn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Jn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Xn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Vt,"id","diffusers.PNDMScheduler"),d(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Vt,"href","#diffusers.PNDMScheduler"),d(Ue,"class","relative group"),d(cs,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L181"),d(cs,"rel","nofollow"),d(Zn,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(eo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(to,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ro,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(fs,"href","https://arxiv.org/abs/2202.09778"),d(fs,"rel","nofollow"),d(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(It,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Kt,"id","diffusers.ScoreSdeVeScheduler"),d(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Kt,"href","#diffusers.ScoreSdeVeScheduler"),d(Be,"class","relative group"),d(bs,"href","https://arxiv.org/abs/2011.13456"),d(bs,"rel","nofollow"),d($s,"href","https://arxiv.org/abs/2011.13456"),d($s,"rel","nofollow"),d(oo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(io,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(ao,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(lo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Wt,"id","diffusers.IPNDMScheduler"),d(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Wt,"href","#diffusers.IPNDMScheduler"),d(He,"class","relative group"),d(Ts,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(Ts,"rel","nofollow"),d(Cs,"href","https://github.com/crowsonkb/v-diffusion-pytorch/blob/987f8985e38208345c1959b0ea767a625831cc9b/diffusion/sampling.py#L296"),d(Cs,"rel","nofollow"),d(uo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(fo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(po,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(ho,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(ks,"href","https://arxiv.org/abs/2202.09778"),d(ks,"rel","nofollow"),d(Gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Jt,"id","diffusers.schedulers.ScoreSdeVpScheduler"),d(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Jt,"href","#diffusers.schedulers.ScoreSdeVpScheduler"),d(Ge,"class","relative group"),d(Fs,"href","https://arxiv.org/abs/2011.13456"),d(Fs,"rel","nofollow"),d(go,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(_o,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(vo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(bo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Ls,"href","https://arxiv.org/abs/2011.13456"),d(Ls,"rel","nofollow"),d(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(er,"id","diffusers.EulerDiscreteScheduler"),d(er,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(er,"href","#diffusers.EulerDiscreteScheduler"),d(ze,"class","relative group"),d(Ks,"href","https://arxiv.org/abs/2206.00364"),d(Ks,"rel","nofollow"),d(Rs,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(Rs,"rel","nofollow"),d(js,"href","https://arxiv.org/abs/2206.00364"),d(js,"rel","nofollow"),d(Us,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L51"),d(Us,"rel","nofollow"),d($o,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Eo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(xo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Do,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(nr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(or,"id","diffusers.EulerAncestralDiscreteScheduler"),d(or,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(or,"href","#diffusers.EulerAncestralDiscreteScheduler"),d(Ye,"class","relative group"),d(Js,"href","https://github.com/crowsonkb/k-diffusion/blob/481677d114f6ea445aa009cf5bd7a9cdee909e47/k_diffusion/sampling.py#L72"),d(Js,"rel","nofollow"),d(Po,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(To,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Mo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Co,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(ir,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ar,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(lr,"id","diffusers.VQDiffusionScheduler"),d(lr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lr,"href","#diffusers.VQDiffusionScheduler"),d(Je,"class","relative group"),d(nn,"href","https://arxiv.org/abs/2111.14822"),d(nn,"rel","nofollow"),d(ko,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Ao,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Oo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Vo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(an,"href","https://arxiv.org/abs/2111.14822"),d(an,"rel","nofollow"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(fr,"id","diffusers.RePaintScheduler"),d(fr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fr,"href","#diffusers.RePaintScheduler"),d(Ze,"class","relative group"),d(Fo,"href","/docs/diffusers/main/en/api/pipelines/repaint#diffusers.RePaintPipeline"),d(_n,"href","https://arxiv.org/abs/2201.09865"),d(_n,"rel","nofollow"),d(vn,"href","https://github.com/andreas128/RePaint"),d(vn,"rel","nofollow"),d(Io,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(Lo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin"),d(qo,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.save_config"),d(Ko,"href","/docs/diffusers/main/en/using-diffusers/configuration#diffusers.ConfigMixin.from_config"),d(Sn,"href","https://arxiv.org/pdf/2201.09865.pdf"),d(Sn,"rel","nofollow"),d(pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(hr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,u){e(document.head,Y),f(r,et,u),f(r,J,u),e(J,_e),e(_e,Jo),m(gr,Jo,null),e(J,bu),e(J,Xo),e(Xo,Su),f(r,Hd,u),f(r,yn,u),e(yn,$u),f(r,Gd,u),f(r,Ce,u),e(Ce,tt),e(tt,Zo),m(_r,Zo,null),e(Ce,Eu),e(Ce,ei),e(ei,xu),f(r,zd,u),f(r,rt,u),e(rt,Du),e(rt,ti),e(ti,yu),e(rt,wu),f(r,Yd,u),f(r,st,u),e(st,wn),e(wn,Pu),e(wn,vr),e(vr,ri),e(ri,Tu),e(vr,Mu),e(vr,si),e(si,Cu),e(st,ku),e(st,ke),e(ke,Au),e(ke,ni),e(ni,Ou),e(ke,Vu),e(ke,oi),e(oi,Nu),e(ke,Fu),f(r,Jd,u),f(r,Ae,u),e(Ae,nt),e(nt,ii),m(br,ii,null),e(Ae,Iu),e(Ae,ai),e(ai,Lu),f(r,Xd,u),f(r,C,u),e(C,qu),e(C,di),e(di,Ku),e(C,Ru),e(C,Pn),e(Pn,Qu),e(C,ju),e(C,Tn),e(Tn,Uu),e(C,Bu),e(C,li),e(li,Wu),e(C,Hu),e(C,Mn),e(Mn,Gu),e(C,zu),e(C,ci),e(ci,Yu),e(C,Ju),f(r,Zd,u),f(r,Oe,u),e(Oe,ot),e(ot,ui),m(Sr,ui,null),e(Oe,Xu),e(Oe,fi),e(fi,Zu),f(r,el,u),f(r,Cn,u),e(Cn,ef),f(r,tl,u),f(r,it,u),e(it,pi),e(pi,tf),e(it,rf),e(it,hi),e(hi,sf),f(r,rl,u),f(r,Ve,u),e(Ve,at),e(at,mi),m($r,mi,null),e(Ve,nf),e(Ve,gi),e(gi,of),f(r,sl,u),f(r,kn,u),e(kn,af),f(r,nl,u),f(r,xe,u),e(xe,Er),e(Er,df),e(Er,_i),e(_i,lf),e(Er,cf),e(xe,uf),e(xe,xr),e(xr,ff),e(xr,vi),e(vi,pf),e(xr,hf),e(xe,mf),e(xe,bi),e(bi,gf),f(r,ol,u),f(r,dt,u),e(dt,_f),e(dt,An),e(An,vf),e(dt,bf),f(r,il,u),f(r,Ne,u),e(Ne,lt),e(lt,Si),m(Dr,Si,null),e(Ne,Sf),e(Ne,$i),e($i,$f),f(r,al,u),f(r,Fe,u),m(yr,Fe,null),e(Fe,Ef),e(Fe,Ei),e(Ei,xf),f(r,dl,u),f(r,Ie,u),e(Ie,ct),e(ct,xi),m(wr,xi,null),e(Ie,Df),e(Ie,Di),e(Di,yf),f(r,ll,u),f(r,Le,u),m(Pr,Le,null),e(Le,wf),e(Le,yi),e(yi,Pf),f(r,cl,u),f(r,qe,u),e(qe,ut),e(ut,wi),m(Tr,wi,null),e(qe,Tf),e(qe,Pi),e(Pi,Mf),f(r,ul,u),f(r,Ke,u),e(Ke,ft),e(ft,Ti),m(Mr,Ti,null),e(Ke,Cf),e(Ke,Mi),e(Mi,kf),f(r,fl,u),f(r,On,u),e(On,Af),f(r,pl,u),f(r,w,u),m(Cr,w,null),e(w,Of),e(w,Ci),e(Ci,Vf),e(w,Nf),e(w,k),e(k,Vn),e(Vn,Ff),e(k,If),e(k,ki),e(ki,Lf),e(k,qf),e(k,Ai),e(Ai,Kf),e(k,Rf),e(k,Oi),e(Oi,Qf),e(k,jf),e(k,Nn),e(Nn,Uf),e(k,Bf),e(k,Fn),e(Fn,Wf),e(k,Hf),e(k,In),e(In,Gf),e(k,zf),e(w,Yf),e(w,Ln),e(Ln,Jf),e(Ln,kr),e(kr,Xf),e(w,Zf),e(w,pt),m(Ar,pt,null),e(pt,ep),e(pt,Vi),e(Vi,tp),e(w,rp),e(w,ht),m(Or,ht,null),e(ht,sp),e(ht,Ni),e(Ni,np),e(w,op),e(w,mt),m(Vr,mt,null),e(mt,ip),e(mt,Fi),e(Fi,ap),f(r,hl,u),f(r,Re,u),e(Re,gt),e(gt,Ii),m(Nr,Ii,null),e(Re,dp),e(Re,Li),e(Li,lp),f(r,ml,u),f(r,_t,u),e(_t,cp),e(_t,Fr),e(Fr,up),e(_t,fp),f(r,gl,u),f(r,P,u),m(Ir,P,null),e(P,pp),e(P,qi),e(qi,hp),e(P,mp),e(P,A),e(A,qn),e(qn,gp),e(A,_p),e(A,Ki),e(Ki,vp),e(A,bp),e(A,Ri),e(Ri,Sp),e(A,$p),e(A,Qi),e(Qi,Ep),e(A,xp),e(A,Kn),e(Kn,Dp),e(A,yp),e(A,Rn),e(Rn,wp),e(A,Pp),e(A,Qn),e(Qn,Tp),e(A,Mp),e(P,Cp),e(P,jn),e(jn,kp),e(jn,Lr),e(Lr,Ap),e(P,Op),e(P,vt),m(qr,vt,null),e(vt,Vp),e(vt,ji),e(ji,Np),e(P,Fp),e(P,bt),m(Kr,bt,null),e(bt,Ip),e(bt,Ui),e(Ui,Lp),e(P,qp),e(P,St),m(Rr,St,null),e(St,Kp),e(St,Bi),e(Bi,Rp),f(r,_l,u),f(r,Qe,u),e(Qe,$t),e($t,Wi),m(Qr,Wi,null),e(Qe,Qp),e(Qe,Hi),e(Hi,jp),f(r,vl,u),f(r,Et,u),e(Et,Up),e(Et,jr),e(jr,Bp),e(Et,Wp),f(r,bl,u),f(r,S,u),m(Ur,S,null),e(S,Hp),e(S,Gi),e(Gi,Gp),e(S,zp),e(S,xt),e(xt,Yp),e(xt,Br),e(Br,Jp),e(xt,Xp),e(xt,Wr),e(Wr,Zp),e(S,eh),e(S,O),e(O,Un),e(Un,th),e(O,rh),e(O,zi),e(zi,sh),e(O,nh),e(O,Yi),e(Yi,oh),e(O,ih),e(O,Ji),e(Ji,ah),e(O,dh),e(O,Bn),e(Bn,lh),e(O,ch),e(O,Wn),e(Wn,uh),e(O,fh),e(O,Hn),e(Hn,ph),e(O,hh),e(S,mh),e(S,Hr),e(Hr,gh),e(Hr,Gr),e(Gr,_h),e(Hr,vh),e(S,bh),e(S,De),m(zr,De,null),e(De,Sh),e(De,Xi),e(Xi,$h),e(De,Eh),e(De,Zi),e(Zi,xh),e(S,Dh),e(S,Dt),m(Yr,Dt,null),e(Dt,yh),e(Dt,ea),e(ea,wh),e(S,Ph),e(S,yt),m(Jr,yt,null),e(yt,Th),e(yt,ta),e(ta,Mh),e(S,Ch),e(S,wt),m(Xr,wt,null),e(wt,kh),e(wt,ra),e(ra,Ah),e(S,Oh),e(S,Pt),m(Zr,Pt,null),e(Pt,Vh),e(Pt,sa),e(sa,Nh),f(r,Sl,u),f(r,je,u),e(je,Tt),e(Tt,na),m(es,na,null),e(je,Fh),e(je,oa),e(oa,Ih),f(r,$l,u),f(r,Mt,u),e(Mt,Lh),e(Mt,ts),e(ts,qh),e(Mt,Kh),f(r,El,u),f(r,T,u),m(rs,T,null),e(T,Rh),e(T,Gn),e(Gn,Qh),e(Gn,ss),e(ss,jh),e(T,Uh),e(T,V),e(V,zn),e(zn,Bh),e(V,Wh),e(V,ia),e(ia,Hh),e(V,Gh),e(V,aa),e(aa,zh),e(V,Yh),e(V,da),e(da,Jh),e(V,Xh),e(V,Yn),e(Yn,Zh),e(V,em),e(V,Jn),e(Jn,tm),e(V,rm),e(V,Xn),e(Xn,sm),e(V,nm),e(T,om),e(T,Ct),m(ns,Ct,null),e(Ct,im),e(Ct,la),e(la,am),e(T,dm),e(T,kt),m(os,kt,null),e(kt,lm),e(kt,is),e(is,cm),e(is,ca),e(ca,um),e(is,fm),e(T,pm),e(T,At),m(as,At,null),e(At,hm),e(At,ua),e(ua,mm),e(T,gm),e(T,Ot),m(ds,Ot,null),e(Ot,_m),e(Ot,fa),e(fa,vm),f(r,xl,u),f(r,Ue,u),e(Ue,Vt),e(Vt,pa),m(ls,pa,null),e(Ue,bm),e(Ue,ha),e(ha,Sm),f(r,Dl,u),f(r,Nt,u),e(Nt,$m),e(Nt,cs),e(cs,Em),e(Nt,xm),f(r,yl,u),f(r,$,u),m(us,$,null),e($,Dm),e($,ma),e(ma,ym),e($,wm),e($,N),e(N,Zn),e(Zn,Pm),e(N,Tm),e(N,ga),e(ga,Mm),e(N,Cm),e(N,_a),e(_a,km),e(N,Am),e(N,va),e(va,Om),e(N,Vm),e(N,eo),e(eo,Nm),e(N,Fm),e(N,to),e(to,Im),e(N,Lm),e(N,ro),e(ro,qm),e(N,Km),e($,Rm),e($,so),e(so,Qm),e(so,fs),e(fs,jm),e($,Um),e($,Ft),m(ps,Ft,null),e(Ft,Bm),e(Ft,ba),e(ba,Wm),e($,Hm),e($,It),m(hs,It,null),e(It,Gm),e(It,Sa),e(Sa,zm),e($,Ym),e($,ye),m(ms,ye,null),e(ye,Jm),e(ye,$a),e($a,Xm),e(ye,Zm),e(ye,$e),e($e,eg),e($e,Ea),e(Ea,tg),e($e,rg),e($e,xa),e(xa,sg),e($e,ng),e($e,Da),e(Da,og),e($e,ig),e($,ag),e($,Lt),m(gs,Lt,null),e(Lt,dg),e(Lt,ya),e(ya,lg),e($,cg),e($,qt),m(_s,qt,null),e(qt,ug),e(qt,wa),e(wa,fg),f(r,wl,u),f(r,Be,u),e(Be,Kt),e(Kt,Pa),m(vs,Pa,null),e(Be,pg),e(Be,Ta),e(Ta,hg),f(r,Pl,u),f(r,Rt,u),e(Rt,mg),e(Rt,bs),e(bs,gg),e(Rt,_g),f(r,Tl,u),f(r,E,u),m(Ss,E,null),e(E,vg),e(E,Ma),e(Ma,bg),e(E,Sg),e(E,no),e(no,$g),e(no,$s),e($s,Eg),e(E,xg),e(E,F),e(F,oo),e(oo,Dg),e(F,yg),e(F,Ca),e(Ca,wg),e(F,Pg),e(F,ka),e(ka,Tg),e(F,Mg),e(F,Aa),e(Aa,Cg),e(F,kg),e(F,io),e(io,Ag),e(F,Og),e(F,ao),e(ao,Vg),e(F,Ng),e(F,lo),e(lo,Fg),e(F,Ig),e(E,Lg),e(E,Qt),m(Es,Qt,null),e(Qt,qg),e(Qt,Oa),e(Oa,Kg),e(E,Rg),e(E,we),m(xs,we,null),e(we,Qg),e(we,Va),e(Va,jg),e(we,Ug),e(we,We),e(We,Bg),e(We,Na),e(Na,Wg),e(We,Hg),e(We,Fa),e(Fa,Gg),e(We,zg),e(E,Yg),e(E,jt),m(Ds,jt,null),e(jt,Jg),e(jt,Ia),e(Ia,Xg),e(E,Zg),e(E,Ut),m(ys,Ut,null),e(Ut,e_),e(Ut,La),e(La,t_),e(E,r_),e(E,Bt),m(ws,Bt,null),e(Bt,s_),e(Bt,qa),e(qa,n_),f(r,Ml,u),f(r,He,u),e(He,Wt),e(Wt,Ka),m(Ps,Ka,null),e(He,o_),e(He,Ra),e(Ra,i_),f(r,Cl,u),f(r,Ht,u),e(Ht,a_),e(Ht,Ts),e(Ts,d_),e(Ht,l_),f(r,kl,u),f(r,M,u),m(Ms,M,null),e(M,c_),e(M,co),e(co,u_),e(co,Cs),e(Cs,f_),e(M,p_),e(M,I),e(I,uo),e(uo,h_),e(I,m_),e(I,Qa),e(Qa,g_),e(I,__),e(I,ja),e(ja,v_),e(I,b_),e(I,Ua),e(Ua,S_),e(I,$_),e(I,fo),e(fo,E_),e(I,x_),e(I,po),e(po,D_),e(I,y_),e(I,ho),e(ho,w_),e(I,P_),e(M,T_),e(M,mo),e(mo,M_),e(mo,ks),e(ks,C_),e(M,k_),e(M,Gt),m(As,Gt,null),e(Gt,A_),e(Gt,Ba),e(Ba,O_),e(M,V_),e(M,zt),m(Os,zt,null),e(zt,N_),e(zt,Wa),e(Wa,F_),e(M,I_),e(M,Yt),m(Vs,Yt,null),e(Yt,L_),e(Yt,Ha),e(Ha,q_),f(r,Al,u),f(r,Ge,u),e(Ge,Jt),e(Jt,Ga),m(Ns,Ga,null),e(Ge,K_),e(Ge,za),e(za,R_),f(r,Ol,u),f(r,Xt,u),e(Xt,Q_),e(Xt,Fs),e(Fs,j_),e(Xt,U_),f(r,Vl,u),m(Zt,r,u),f(r,Nl,u),f(r,X,u),m(Is,X,null),e(X,B_),e(X,Ya),e(Ya,W_),e(X,H_),e(X,L),e(L,go),e(go,G_),e(L,z_),e(L,Ja),e(Ja,Y_),e(L,J_),e(L,Xa),e(Xa,X_),e(L,Z_),e(L,Za),e(Za,ev),e(L,tv),e(L,_o),e(_o,rv),e(L,sv),e(L,vo),e(vo,nv),e(L,ov),e(L,bo),e(bo,iv),e(L,av),e(X,dv),e(X,So),e(So,lv),e(So,Ls),e(Ls,cv),e(X,uv),e(X,ed),e(ed,fv),f(r,Fl,u),f(r,ze,u),e(ze,er),e(er,td),m(qs,td,null),e(ze,pv),e(ze,rd),e(rd,hv),f(r,Il,u),f(r,Pe,u),e(Pe,mv),e(Pe,Ks),e(Ks,gv),e(Pe,_v),e(Pe,Rs),e(Rs,vv),e(Pe,bv),f(r,Ll,u),f(r,H,u),m(Qs,H,null),e(H,Sv),e(H,tr),e(tr,$v),e(tr,js),e(js,Ev),e(tr,xv),e(tr,Us),e(Us,Dv),e(H,yv),e(H,q),e(q,$o),e($o,wv),e(q,Pv),e(q,sd),e(sd,Tv),e(q,Mv),e(q,nd),e(nd,Cv),e(q,kv),e(q,od),e(od,Av),e(q,Ov),e(q,Eo),e(Eo,Vv),e(q,Nv),e(q,xo),e(xo,Fv),e(q,Iv),e(q,Do),e(Do,Lv),e(q,qv),e(H,Kv),e(H,rr),m(Bs,rr,null),e(rr,Rv),e(rr,Ws),e(Ws,Qv),e(Ws,id),e(id,jv),e(Ws,Uv),e(H,Bv),e(H,sr),m(Hs,sr,null),e(sr,Wv),e(sr,ad),e(ad,Hv),e(H,Gv),e(H,nr),m(Gs,nr,null),e(nr,zv),e(nr,dd),e(dd,Yv),f(r,ql,u),f(r,Ye,u),e(Ye,or),e(or,ld),m(zs,ld,null),e(Ye,Jv),e(Ye,cd),e(cd,Xv),f(r,Kl,u),f(r,yo,u),e(yo,Zv),f(r,Rl,u),f(r,G,u),m(Ys,G,null),e(G,eb),e(G,wo),e(wo,tb),e(wo,Js),e(Js,rb),e(G,sb),e(G,K),e(K,Po),e(Po,nb),e(K,ob),e(K,ud),e(ud,ib),e(K,ab),e(K,fd),e(fd,db),e(K,lb),e(K,pd),e(pd,cb),e(K,ub),e(K,To),e(To,fb),e(K,pb),e(K,Mo),e(Mo,hb),e(K,mb),e(K,Co),e(Co,gb),e(K,_b),e(G,vb),e(G,ir),m(Xs,ir,null),e(ir,bb),e(ir,Zs),e(Zs,Sb),e(Zs,hd),e(hd,$b),e(Zs,Eb),e(G,xb),e(G,ar),m(en,ar,null),e(ar,Db),e(ar,md),e(md,yb),e(G,wb),e(G,dr),m(tn,dr,null),e(dr,Pb),e(dr,gd),e(gd,Tb),f(r,Ql,u),f(r,Je,u),e(Je,lr),e(lr,_d),m(rn,_d,null),e(Je,Mb),e(Je,vd),e(vd,Cb),f(r,jl,u),f(r,sn,u),e(sn,kb),e(sn,nn),e(nn,Ab),f(r,Ul,u),f(r,x,u),m(on,x,null),e(x,Ob),e(x,bd),e(bd,Vb),e(x,Nb),e(x,Sd),e(Sd,Fb),e(x,Ib),e(x,R),e(R,ko),e(ko,Lb),e(R,qb),e(R,$d),e($d,Kb),e(R,Rb),e(R,Ed),e(Ed,Qb),e(R,jb),e(R,xd),e(xd,Ub),e(R,Bb),e(R,Ao),e(Ao,Wb),e(R,Hb),e(R,Oo),e(Oo,Gb),e(R,zb),e(R,Vo),e(Vo,Yb),e(R,Jb),e(x,Xb),e(x,No),e(No,Zb),e(No,an),e(an,e1),e(x,t1),e(x,Te),m(dn,Te,null),e(Te,r1),e(Te,ln),e(ln,s1),e(ln,Dd),e(Dd,n1),e(ln,o1),e(Te,i1),e(Te,yd),e(yd,a1),e(x,d1),e(x,Q),m(cn,Q,null),e(Q,l1),e(Q,un),e(un,c1),e(un,wd),e(wd,u1),e(un,f1),e(Q,p1),e(Q,Pd),e(Pd,h1),e(Q,m1),e(Q,Td),e(Td,g1),e(Q,_1),e(Q,Md),e(Md,v1),e(Q,b1),e(Q,Cd),e(Cd,fn),e(fn,S1),e(fn,kd),e(kd,$1),e(fn,E1),e(Q,x1),e(Q,Xe),e(Xe,D1),e(Xe,Ad),e(Ad,y1),e(Xe,w1),e(Xe,Od),e(Od,P1),e(Xe,T1),e(x,M1),e(x,cr),m(pn,cr,null),e(cr,C1),e(cr,Vd),e(Vd,k1),e(x,A1),e(x,ur),m(hn,ur,null),e(ur,O1),e(ur,mn),e(mn,V1),e(mn,Nd),e(Nd,N1),e(mn,F1),f(r,Bl,u),f(r,Ze,u),e(Ze,fr),e(fr,Fd),m(gn,Fd,null),e(Ze,I1),e(Ze,Id),e(Id,L1),f(r,Wl,u),f(r,Ee,u),e(Ee,q1),e(Ee,Fo),e(Fo,K1),e(Ee,R1),e(Ee,_n),e(_n,Q1),e(Ee,j1),e(Ee,vn),e(vn,U1),f(r,Hl,u),f(r,z,u),m(bn,z,null),e(z,B1),e(z,Ld),e(Ld,W1),e(z,H1),e(z,j),e(j,Io),e(Io,G1),e(j,z1),e(j,qd),e(qd,Y1),e(j,J1),e(j,Kd),e(Kd,X1),e(j,Z1),e(j,Rd),e(Rd,e0),e(j,t0),e(j,Lo),e(Lo,r0),e(j,s0),e(j,qo),e(qo,n0),e(j,o0),e(j,Ko),e(Ko,i0),e(j,a0),e(z,d0),e(z,Ro),e(Ro,l0),e(Ro,Sn),e(Sn,c0),e(z,u0),e(z,pr),m($n,pr,null),e(pr,f0),e(pr,Qd),e(Qd,p0),e(z,h0),e(z,hr),m(En,hr,null),e(hr,m0),e(hr,jd),e(jd,g0),Gl=!0},p(r,[u]){const xn={};u&2&&(xn.$$scope={dirty:u,ctx:r}),Zt.$set(xn)},i(r){Gl||(g(gr.$$.fragment,r),g(_r.$$.fragment,r),g(br.$$.fragment,r),g(Sr.$$.fragment,r),g($r.$$.fragment,r),g(Dr.$$.fragment,r),g(yr.$$.fragment,r),g(wr.$$.fragment,r),g(Pr.$$.fragment,r),g(Tr.$$.fragment,r),g(Mr.$$.fragment,r),g(Cr.$$.fragment,r),g(Ar.$$.fragment,r),g(Or.$$.fragment,r),g(Vr.$$.fragment,r),g(Nr.$$.fragment,r),g(Ir.$$.fragment,r),g(qr.$$.fragment,r),g(Kr.$$.fragment,r),g(Rr.$$.fragment,r),g(Qr.$$.fragment,r),g(Ur.$$.fragment,r),g(zr.$$.fragment,r),g(Yr.$$.fragment,r),g(Jr.$$.fragment,r),g(Xr.$$.fragment,r),g(Zr.$$.fragment,r),g(es.$$.fragment,r),g(rs.$$.fragment,r),g(ns.$$.fragment,r),g(os.$$.fragment,r),g(as.$$.fragment,r),g(ds.$$.fragment,r),g(ls.$$.fragment,r),g(us.$$.fragment,r),g(ps.$$.fragment,r),g(hs.$$.fragment,r),g(ms.$$.fragment,r),g(gs.$$.fragment,r),g(_s.$$.fragment,r),g(vs.$$.fragment,r),g(Ss.$$.fragment,r),g(Es.$$.fragment,r),g(xs.$$.fragment,r),g(Ds.$$.fragment,r),g(ys.$$.fragment,r),g(ws.$$.fragment,r),g(Ps.$$.fragment,r),g(Ms.$$.fragment,r),g(As.$$.fragment,r),g(Os.$$.fragment,r),g(Vs.$$.fragment,r),g(Ns.$$.fragment,r),g(Zt.$$.fragment,r),g(Is.$$.fragment,r),g(qs.$$.fragment,r),g(Qs.$$.fragment,r),g(Bs.$$.fragment,r),g(Hs.$$.fragment,r),g(Gs.$$.fragment,r),g(zs.$$.fragment,r),g(Ys.$$.fragment,r),g(Xs.$$.fragment,r),g(en.$$.fragment,r),g(tn.$$.fragment,r),g(rn.$$.fragment,r),g(on.$$.fragment,r),g(dn.$$.fragment,r),g(cn.$$.fragment,r),g(pn.$$.fragment,r),g(hn.$$.fragment,r),g(gn.$$.fragment,r),g(bn.$$.fragment,r),g($n.$$.fragment,r),g(En.$$.fragment,r),Gl=!0)},o(r){_(gr.$$.fragment,r),_(_r.$$.fragment,r),_(br.$$.fragment,r),_(Sr.$$.fragment,r),_($r.$$.fragment,r),_(Dr.$$.fragment,r),_(yr.$$.fragment,r),_(wr.$$.fragment,r),_(Pr.$$.fragment,r),_(Tr.$$.fragment,r),_(Mr.$$.fragment,r),_(Cr.$$.fragment,r),_(Ar.$$.fragment,r),_(Or.$$.fragment,r),_(Vr.$$.fragment,r),_(Nr.$$.fragment,r),_(Ir.$$.fragment,r),_(qr.$$.fragment,r),_(Kr.$$.fragment,r),_(Rr.$$.fragment,r),_(Qr.$$.fragment,r),_(Ur.$$.fragment,r),_(zr.$$.fragment,r),_(Yr.$$.fragment,r),_(Jr.$$.fragment,r),_(Xr.$$.fragment,r),_(Zr.$$.fragment,r),_(es.$$.fragment,r),_(rs.$$.fragment,r),_(ns.$$.fragment,r),_(os.$$.fragment,r),_(as.$$.fragment,r),_(ds.$$.fragment,r),_(ls.$$.fragment,r),_(us.$$.fragment,r),_(ps.$$.fragment,r),_(hs.$$.fragment,r),_(ms.$$.fragment,r),_(gs.$$.fragment,r),_(_s.$$.fragment,r),_(vs.$$.fragment,r),_(Ss.$$.fragment,r),_(Es.$$.fragment,r),_(xs.$$.fragment,r),_(Ds.$$.fragment,r),_(ys.$$.fragment,r),_(ws.$$.fragment,r),_(Ps.$$.fragment,r),_(Ms.$$.fragment,r),_(As.$$.fragment,r),_(Os.$$.fragment,r),_(Vs.$$.fragment,r),_(Ns.$$.fragment,r),_(Zt.$$.fragment,r),_(Is.$$.fragment,r),_(qs.$$.fragment,r),_(Qs.$$.fragment,r),_(Bs.$$.fragment,r),_(Hs.$$.fragment,r),_(Gs.$$.fragment,r),_(zs.$$.fragment,r),_(Ys.$$.fragment,r),_(Xs.$$.fragment,r),_(en.$$.fragment,r),_(tn.$$.fragment,r),_(rn.$$.fragment,r),_(on.$$.fragment,r),_(dn.$$.fragment,r),_(cn.$$.fragment,r),_(pn.$$.fragment,r),_(hn.$$.fragment,r),_(gn.$$.fragment,r),_(bn.$$.fragment,r),_($n.$$.fragment,r),_(En.$$.fragment,r),Gl=!1},d(r){t(Y),r&&t(et),r&&t(J),v(gr),r&&t(Hd),r&&t(yn),r&&t(Gd),r&&t(Ce),v(_r),r&&t(zd),r&&t(rt),r&&t(Yd),r&&t(st),r&&t(Jd),r&&t(Ae),v(br),r&&t(Xd),r&&t(C),r&&t(Zd),r&&t(Oe),v(Sr),r&&t(el),r&&t(Cn),r&&t(tl),r&&t(it),r&&t(rl),r&&t(Ve),v($r),r&&t(sl),r&&t(kn),r&&t(nl),r&&t(xe),r&&t(ol),r&&t(dt),r&&t(il),r&&t(Ne),v(Dr),r&&t(al),r&&t(Fe),v(yr),r&&t(dl),r&&t(Ie),v(wr),r&&t(ll),r&&t(Le),v(Pr),r&&t(cl),r&&t(qe),v(Tr),r&&t(ul),r&&t(Ke),v(Mr),r&&t(fl),r&&t(On),r&&t(pl),r&&t(w),v(Cr),v(Ar),v(Or),v(Vr),r&&t(hl),r&&t(Re),v(Nr),r&&t(ml),r&&t(_t),r&&t(gl),r&&t(P),v(Ir),v(qr),v(Kr),v(Rr),r&&t(_l),r&&t(Qe),v(Qr),r&&t(vl),r&&t(Et),r&&t(bl),r&&t(S),v(Ur),v(zr),v(Yr),v(Jr),v(Xr),v(Zr),r&&t(Sl),r&&t(je),v(es),r&&t($l),r&&t(Mt),r&&t(El),r&&t(T),v(rs),v(ns),v(os),v(as),v(ds),r&&t(xl),r&&t(Ue),v(ls),r&&t(Dl),r&&t(Nt),r&&t(yl),r&&t($),v(us),v(ps),v(hs),v(ms),v(gs),v(_s),r&&t(wl),r&&t(Be),v(vs),r&&t(Pl),r&&t(Rt),r&&t(Tl),r&&t(E),v(Ss),v(Es),v(xs),v(Ds),v(ys),v(ws),r&&t(Ml),r&&t(He),v(Ps),r&&t(Cl),r&&t(Ht),r&&t(kl),r&&t(M),v(Ms),v(As),v(Os),v(Vs),r&&t(Al),r&&t(Ge),v(Ns),r&&t(Ol),r&&t(Xt),r&&t(Vl),v(Zt,r),r&&t(Nl),r&&t(X),v(Is),r&&t(Fl),r&&t(ze),v(qs),r&&t(Il),r&&t(Pe),r&&t(Ll),r&&t(H),v(Qs),v(Bs),v(Hs),v(Gs),r&&t(ql),r&&t(Ye),v(zs),r&&t(Kl),r&&t(yo),r&&t(Rl),r&&t(G),v(Ys),v(Xs),v(en),v(tn),r&&t(Ql),r&&t(Je),v(rn),r&&t(jl),r&&t(sn),r&&t(Ul),r&&t(x),v(on),v(dn),v(cn),v(pn),v(hn),r&&t(Bl),r&&t(Ze),v(gn),r&&t(Wl),r&&t(Ee),r&&t(Hl),r&&t(z),v(bn),v($n),v(En)}}}const ME={local:"schedulers",sections:[{local:"what-is-a-scheduler",sections:[{local:"discrete-versus-continuous-schedulers",title:"Discrete versus continuous schedulers"}],title:"What is a scheduler?"},{local:"designing-reusable-schedulers",title:"Designing Re-usable schedulers"},{local:"api",sections:[{local:"diffusers.SchedulerMixin",title:"SchedulerMixin"},{local:"diffusers.schedulers.scheduling_utils.SchedulerOutput",title:"SchedulerOutput"},{local:"implemented-schedulers",sections:[{local:"diffusers.DDIMScheduler",title:"Denoising diffusion implicit models (DDIM)"},{local:"diffusers.DDPMScheduler",title:"Denoising diffusion probabilistic models (DDPM)"},{local:"diffusers.KarrasVeScheduler",title:"Variance exploding, stochastic sampling from Karras et. al"},{local:"diffusers.LMSDiscreteScheduler",title:"Linear multistep scheduler for discrete beta schedules"},{local:"diffusers.PNDMScheduler",title:"Pseudo numerical methods for diffusion models (PNDM)"},{local:"diffusers.ScoreSdeVeScheduler",title:"variance exploding stochastic differential equation (VE-SDE) scheduler"},{local:"diffusers.IPNDMScheduler",title:"improved pseudo numerical methods for diffusion models (iPNDM)"},{local:"diffusers.schedulers.ScoreSdeVpScheduler",title:"variance preserving stochastic differential equation (VP-SDE) scheduler"},{local:"diffusers.EulerDiscreteScheduler",title:"Euler scheduler"},{local:"diffusers.EulerAncestralDiscreteScheduler",title:"Euler Ancestral scheduler"},{local:"diffusers.VQDiffusionScheduler",title:"VQDiffusionScheduler"},{local:"diffusers.RePaintScheduler",title:"RePaint scheduler"}],title:"Implemented Schedulers"}],title:"API"}],title:"Schedulers"};function CE(Wd){return yE(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class NE extends $E{constructor(Y){super();EE(this,Y,CE,TE,xE,{})}}export{NE as default,ME as metadata};
