import{S as ds,i as vs,s as ws,e as o,k as u,w as R,t as a,M as ys,c as i,d as s,m as c,a as r,x as I,h as n,b as p,G as t,g as f,y as j,L as gs,q as C,o as T,B as L,v as _s}from"../../chunks/vendor-hf-doc-builder.js";import{I as be}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as Nt}from"../../chunks/CodeBlock-hf-doc-builder.js";function bs(Gt){let v,$e,w,E,te,N,Ke,se,Je,Ee,k,ze,oe,Qe,Ve,ke,y,P,ie,G,Xe,re,Ze,Pe,h,ae,et,tt,ne,st,ot,le,it,rt,B,at,pe,nt,lt,Ae,H,Me,g,A,fe,O,pt,ue,ft,Se,m,ut,ce,ct,ht,he,mt,dt,xe,Q,vt,De,W,qe,_,M,me,U,wt,de,yt,Re,V,gt,Ie,d,_t,ve,bt,$t,we,Et,kt,je,F,Ce,b,S,ye,Y,Pt,ge,At,Te,x,K,Mt,J,St,xt,Dt,$,qt,z,Rt,It,D,_e,jt,Ct,Tt,Le;return N=new be({}),G=new be({}),H=new Nt({props:{code:"pip3 install --pre torch --extra-index-url https://download.pytorch.org/whl/test/cpu",highlighted:'pip3 install --pre torch --extra-index-url https:<span class="hljs-regexp">//</span>download.pytorch.org<span class="hljs-regexp">/whl/</span>test/cpu'}}),O=new be({}),W=new Nt({props:{code:`# make sure you're logged in with \`huggingface-cli login\`
from diffusers import StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
pipe = pipe.to("mps")

# Recommended if your computer has < 64 GB of RAM
pipe.enable_attention_slicing()

prompt = "a photo of an astronaut riding a horse on mars"

# First-time "warmup" pass (see explanation above)
_ = pipe(prompt, num_inference_steps=1)

# Results match those from the CPU device after the warmup pass.
image = pipe(prompt).images[0]`,highlighted:`<span class="hljs-comment"># make sure you&#x27;re logged in with \`huggingface-cli login\`</span>
<span class="hljs-keyword">from</span> diffusers <span class="hljs-keyword">import</span> StableDiffusionPipeline

pipe = StableDiffusionPipeline.from_pretrained(<span class="hljs-string">&quot;runwayml/stable-diffusion-v1-5&quot;</span>)
pipe = pipe.to(<span class="hljs-string">&quot;mps&quot;</span>)

<span class="hljs-comment"># Recommended if your computer has &lt; 64 GB of RAM</span>
pipe.enable_attention_slicing()

prompt = <span class="hljs-string">&quot;a photo of an astronaut riding a horse on mars&quot;</span>

<span class="hljs-comment"># First-time &quot;warmup&quot; pass (see explanation above)</span>
_ = pipe(prompt, num_inference_steps=<span class="hljs-number">1</span>)

<span class="hljs-comment"># Results match those from the CPU device after the warmup pass.</span>
image = pipe(prompt).images[<span class="hljs-number">0</span>]`}}),U=new be({}),F=new Nt({props:{code:"pipeline.enable_attention_slicing()",highlighted:"pipeline.enable_attention_slicing()"}}),Y=new be({}),{c(){v=o("meta"),$e=u(),w=o("h1"),E=o("a"),te=o("span"),R(N.$$.fragment),Ke=u(),se=o("span"),Je=a("How to use Stable Diffusion in Apple Silicon (M1/M2)"),Ee=u(),k=o("p"),ze=a("\u{1F917} Diffusers is compatible with Apple silicon for Stable Diffusion inference, using the PyTorch "),oe=o("code"),Qe=a("mps"),Ve=a(" device. These are the steps you need to follow to use your M1 or M2 computer with Stable Diffusion."),ke=u(),y=o("h2"),P=o("a"),ie=o("span"),R(G.$$.fragment),Xe=u(),re=o("span"),Ze=a("Requirements"),Pe=u(),h=o("ul"),ae=o("li"),et=a("Mac computer with Apple silicon (M1/M2) hardware."),tt=u(),ne=o("li"),st=a("macOS 12.6 or later (13.0 or later recommended)."),ot=u(),le=o("li"),it=a("arm64 version of Python."),rt=u(),B=o("li"),at=a("PyTorch 1.13.0 RC (Release Candidate). You can install it with "),pe=o("code"),nt=a("pip"),lt=a(" using:"),Ae=u(),R(H.$$.fragment),Me=u(),g=o("h2"),A=o("a"),fe=o("span"),R(O.$$.fragment),pt=u(),ue=o("span"),ft=a("Inference Pipeline"),Se=u(),m=o("p"),ut=a("The snippet below demonstrates how to use the "),ce=o("code"),ct=a("mps"),ht=a(" backend using the familiar "),he=o("code"),mt=a("to()"),dt=a(" interface to move the Stable Diffusion pipeline to your M1 or M2 device."),xe=u(),Q=o("p"),vt=a("We recommend to \u201Cprime\u201D the pipeline using an additional one-time pass through it. This is a temporary workaround for a weird issue we have detected: the first inference pass produces slightly different results than subsequent ones. You only need to do this pass once, and it\u2019s ok to use just one inference step and discard the result."),De=u(),R(W.$$.fragment),qe=u(),_=o("h2"),M=o("a"),me=o("span"),R(U.$$.fragment),wt=u(),de=o("span"),yt=a("Performance Recommendations"),Re=u(),V=o("p"),gt=a("M1/M2 performance is very sensitive to memory pressure. The system will automatically swap if it needs to, but performance will degrade significantly when it does."),Ie=u(),d=o("p"),_t=a("We recommend you use "),ve=o("em"),bt=a("attention slicing"),$t=a(" to reduce memory pressure during inference and prevent swapping, particularly if your computer has lass than 64 GB of system RAM, or if you generate images at non-standard resolutions larger than 512 \xD7 512 pixels. Attention slicing performs the costly attention operation in multiple steps instead of all at once. It usually has a performance impact of ~20% in computers without universal memory, but we have observed "),we=o("em"),Et=a("better performance"),kt=a(" in most Apple Silicon computers, unless you have 64 GB or more."),je=u(),R(F.$$.fragment),Ce=u(),b=o("h2"),S=o("a"),ye=o("span"),R(Y.$$.fragment),Pt=u(),ge=o("span"),At=a("Known Issues"),Te=u(),x=o("ul"),K=o("li"),Mt=a("As mentioned above, we are investigating a strange "),J=o("a"),St=a("first-time inference issue"),xt=a("."),Dt=u(),$=o("li"),qt=a("Generating multiple prompts in a batch "),z=o("a"),Rt=a("crashes or doesn\u2019t work reliably"),It=a(". We believe this is related to the "),D=o("a"),_e=o("code"),jt=a("mps"),Ct=a(" backend in PyTorch"),Tt=a(". For now, we recommend to iterate instead of batching."),this.h()},l(e){const l=ys('[data-svelte="svelte-1phssyn"]',document.head);v=i(l,"META",{name:!0,content:!0}),l.forEach(s),$e=c(e),w=i(e,"H1",{class:!0});var Ne=r(w);E=i(Ne,"A",{id:!0,class:!0,href:!0});var Bt=r(E);te=i(Bt,"SPAN",{});var Ht=r(te);I(N.$$.fragment,Ht),Ht.forEach(s),Bt.forEach(s),Ke=c(Ne),se=i(Ne,"SPAN",{});var Ot=r(se);Je=n(Ot,"How to use Stable Diffusion in Apple Silicon (M1/M2)"),Ot.forEach(s),Ne.forEach(s),Ee=c(e),k=i(e,"P",{});var Ge=r(k);ze=n(Ge,"\u{1F917} Diffusers is compatible with Apple silicon for Stable Diffusion inference, using the PyTorch "),oe=i(Ge,"CODE",{});var Wt=r(oe);Qe=n(Wt,"mps"),Wt.forEach(s),Ve=n(Ge," device. These are the steps you need to follow to use your M1 or M2 computer with Stable Diffusion."),Ge.forEach(s),ke=c(e),y=i(e,"H2",{class:!0});var Be=r(y);P=i(Be,"A",{id:!0,class:!0,href:!0});var Ut=r(P);ie=i(Ut,"SPAN",{});var Ft=r(ie);I(G.$$.fragment,Ft),Ft.forEach(s),Ut.forEach(s),Xe=c(Be),re=i(Be,"SPAN",{});var Yt=r(re);Ze=n(Yt,"Requirements"),Yt.forEach(s),Be.forEach(s),Pe=c(e),h=i(e,"UL",{});var q=r(h);ae=i(q,"LI",{});var Kt=r(ae);et=n(Kt,"Mac computer with Apple silicon (M1/M2) hardware."),Kt.forEach(s),tt=c(q),ne=i(q,"LI",{});var Jt=r(ne);st=n(Jt,"macOS 12.6 or later (13.0 or later recommended)."),Jt.forEach(s),ot=c(q),le=i(q,"LI",{});var zt=r(le);it=n(zt,"arm64 version of Python."),zt.forEach(s),rt=c(q),B=i(q,"LI",{});var He=r(B);at=n(He,"PyTorch 1.13.0 RC (Release Candidate). You can install it with "),pe=i(He,"CODE",{});var Qt=r(pe);nt=n(Qt,"pip"),Qt.forEach(s),lt=n(He," using:"),He.forEach(s),q.forEach(s),Ae=c(e),I(H.$$.fragment,e),Me=c(e),g=i(e,"H2",{class:!0});var Oe=r(g);A=i(Oe,"A",{id:!0,class:!0,href:!0});var Vt=r(A);fe=i(Vt,"SPAN",{});var Xt=r(fe);I(O.$$.fragment,Xt),Xt.forEach(s),Vt.forEach(s),pt=c(Oe),ue=i(Oe,"SPAN",{});var Zt=r(ue);ft=n(Zt,"Inference Pipeline"),Zt.forEach(s),Oe.forEach(s),Se=c(e),m=i(e,"P",{});var X=r(m);ut=n(X,"The snippet below demonstrates how to use the "),ce=i(X,"CODE",{});var es=r(ce);ct=n(es,"mps"),es.forEach(s),ht=n(X," backend using the familiar "),he=i(X,"CODE",{});var ts=r(he);mt=n(ts,"to()"),ts.forEach(s),dt=n(X," interface to move the Stable Diffusion pipeline to your M1 or M2 device."),X.forEach(s),xe=c(e),Q=i(e,"P",{});var ss=r(Q);vt=n(ss,"We recommend to \u201Cprime\u201D the pipeline using an additional one-time pass through it. This is a temporary workaround for a weird issue we have detected: the first inference pass produces slightly different results than subsequent ones. You only need to do this pass once, and it\u2019s ok to use just one inference step and discard the result."),ss.forEach(s),De=c(e),I(W.$$.fragment,e),qe=c(e),_=i(e,"H2",{class:!0});var We=r(_);M=i(We,"A",{id:!0,class:!0,href:!0});var os=r(M);me=i(os,"SPAN",{});var is=r(me);I(U.$$.fragment,is),is.forEach(s),os.forEach(s),wt=c(We),de=i(We,"SPAN",{});var rs=r(de);yt=n(rs,"Performance Recommendations"),rs.forEach(s),We.forEach(s),Re=c(e),V=i(e,"P",{});var as=r(V);gt=n(as,"M1/M2 performance is very sensitive to memory pressure. The system will automatically swap if it needs to, but performance will degrade significantly when it does."),as.forEach(s),Ie=c(e),d=i(e,"P",{});var Z=r(d);_t=n(Z,"We recommend you use "),ve=i(Z,"EM",{});var ns=r(ve);bt=n(ns,"attention slicing"),ns.forEach(s),$t=n(Z," to reduce memory pressure during inference and prevent swapping, particularly if your computer has lass than 64 GB of system RAM, or if you generate images at non-standard resolutions larger than 512 \xD7 512 pixels. Attention slicing performs the costly attention operation in multiple steps instead of all at once. It usually has a performance impact of ~20% in computers without universal memory, but we have observed "),we=i(Z,"EM",{});var ls=r(we);Et=n(ls,"better performance"),ls.forEach(s),kt=n(Z," in most Apple Silicon computers, unless you have 64 GB or more."),Z.forEach(s),je=c(e),I(F.$$.fragment,e),Ce=c(e),b=i(e,"H2",{class:!0});var Ue=r(b);S=i(Ue,"A",{id:!0,class:!0,href:!0});var ps=r(S);ye=i(ps,"SPAN",{});var fs=r(ye);I(Y.$$.fragment,fs),fs.forEach(s),ps.forEach(s),Pt=c(Ue),ge=i(Ue,"SPAN",{});var us=r(ge);At=n(us,"Known Issues"),us.forEach(s),Ue.forEach(s),Te=c(e),x=i(e,"UL",{});var Fe=r(x);K=i(Fe,"LI",{});var Ye=r(K);Mt=n(Ye,"As mentioned above, we are investigating a strange "),J=i(Ye,"A",{href:!0,rel:!0});var cs=r(J);St=n(cs,"first-time inference issue"),cs.forEach(s),xt=n(Ye,"."),Ye.forEach(s),Dt=c(Fe),$=i(Fe,"LI",{});var ee=r($);qt=n(ee,"Generating multiple prompts in a batch "),z=i(ee,"A",{href:!0,rel:!0});var hs=r(z);Rt=n(hs,"crashes or doesn\u2019t work reliably"),hs.forEach(s),It=n(ee,". We believe this is related to the "),D=i(ee,"A",{href:!0,rel:!0});var Lt=r(D);_e=i(Lt,"CODE",{});var ms=r(_e);jt=n(ms,"mps"),ms.forEach(s),Ct=n(Lt," backend in PyTorch"),Lt.forEach(s),Tt=n(ee,". For now, we recommend to iterate instead of batching."),ee.forEach(s),Fe.forEach(s),this.h()},h(){p(v,"name","hf:doc:metadata"),p(v,"content",JSON.stringify($s)),p(E,"id","how-to-use-stable-diffusion-in-apple-silicon-m1m2"),p(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(E,"href","#how-to-use-stable-diffusion-in-apple-silicon-m1m2"),p(w,"class","relative group"),p(P,"id","requirements"),p(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(P,"href","#requirements"),p(y,"class","relative group"),p(A,"id","inference-pipeline"),p(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(A,"href","#inference-pipeline"),p(g,"class","relative group"),p(M,"id","performance-recommendations"),p(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(M,"href","#performance-recommendations"),p(_,"class","relative group"),p(S,"id","known-issues"),p(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(S,"href","#known-issues"),p(b,"class","relative group"),p(J,"href","https://github.com/huggingface/diffusers/issues/372"),p(J,"rel","nofollow"),p(z,"href","https://github.com/huggingface/diffusers/issues/363"),p(z,"rel","nofollow"),p(D,"href","https://github.com/pytorch/pytorch/issues/84039"),p(D,"rel","nofollow")},m(e,l){t(document.head,v),f(e,$e,l),f(e,w,l),t(w,E),t(E,te),j(N,te,null),t(w,Ke),t(w,se),t(se,Je),f(e,Ee,l),f(e,k,l),t(k,ze),t(k,oe),t(oe,Qe),t(k,Ve),f(e,ke,l),f(e,y,l),t(y,P),t(P,ie),j(G,ie,null),t(y,Xe),t(y,re),t(re,Ze),f(e,Pe,l),f(e,h,l),t(h,ae),t(ae,et),t(h,tt),t(h,ne),t(ne,st),t(h,ot),t(h,le),t(le,it),t(h,rt),t(h,B),t(B,at),t(B,pe),t(pe,nt),t(B,lt),f(e,Ae,l),j(H,e,l),f(e,Me,l),f(e,g,l),t(g,A),t(A,fe),j(O,fe,null),t(g,pt),t(g,ue),t(ue,ft),f(e,Se,l),f(e,m,l),t(m,ut),t(m,ce),t(ce,ct),t(m,ht),t(m,he),t(he,mt),t(m,dt),f(e,xe,l),f(e,Q,l),t(Q,vt),f(e,De,l),j(W,e,l),f(e,qe,l),f(e,_,l),t(_,M),t(M,me),j(U,me,null),t(_,wt),t(_,de),t(de,yt),f(e,Re,l),f(e,V,l),t(V,gt),f(e,Ie,l),f(e,d,l),t(d,_t),t(d,ve),t(ve,bt),t(d,$t),t(d,we),t(we,Et),t(d,kt),f(e,je,l),j(F,e,l),f(e,Ce,l),f(e,b,l),t(b,S),t(S,ye),j(Y,ye,null),t(b,Pt),t(b,ge),t(ge,At),f(e,Te,l),f(e,x,l),t(x,K),t(K,Mt),t(K,J),t(J,St),t(K,xt),t(x,Dt),t(x,$),t($,qt),t($,z),t(z,Rt),t($,It),t($,D),t(D,_e),t(_e,jt),t(D,Ct),t($,Tt),Le=!0},p:gs,i(e){Le||(C(N.$$.fragment,e),C(G.$$.fragment,e),C(H.$$.fragment,e),C(O.$$.fragment,e),C(W.$$.fragment,e),C(U.$$.fragment,e),C(F.$$.fragment,e),C(Y.$$.fragment,e),Le=!0)},o(e){T(N.$$.fragment,e),T(G.$$.fragment,e),T(H.$$.fragment,e),T(O.$$.fragment,e),T(W.$$.fragment,e),T(U.$$.fragment,e),T(F.$$.fragment,e),T(Y.$$.fragment,e),Le=!1},d(e){s(v),e&&s($e),e&&s(w),L(N),e&&s(Ee),e&&s(k),e&&s(ke),e&&s(y),L(G),e&&s(Pe),e&&s(h),e&&s(Ae),L(H,e),e&&s(Me),e&&s(g),L(O),e&&s(Se),e&&s(m),e&&s(xe),e&&s(Q),e&&s(De),L(W,e),e&&s(qe),e&&s(_),L(U),e&&s(Re),e&&s(V),e&&s(Ie),e&&s(d),e&&s(je),L(F,e),e&&s(Ce),e&&s(b),L(Y),e&&s(Te),e&&s(x)}}}const $s={local:"how-to-use-stable-diffusion-in-apple-silicon-m1m2",sections:[{local:"requirements",title:"Requirements"},{local:"inference-pipeline",title:"Inference Pipeline"},{local:"performance-recommendations",title:"Performance Recommendations"},{local:"known-issues",title:"Known Issues"}],title:"How to use Stable Diffusion in Apple Silicon (M1/M2)"};function Es(Gt){return _s(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ms extends ds{constructor(v){super();vs(this,v,Es,bs,ws,{})}}export{Ms as default,$s as metadata};
