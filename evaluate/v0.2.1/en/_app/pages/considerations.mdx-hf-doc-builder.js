import{S as ss,i as ns,s as ls,e as o,k as d,w as O,t as s,M as fs,c as r,d as t,m as c,a as i,x as M,h as n,b as h,N as is,G as a,g as f,y as G,q as L,o as H,B as q,v as hs}from"../chunks/vendor-hf-doc-builder.js";import{T as ds}from"../chunks/Tip-hf-doc-builder.js";import{I as ee}from"../chunks/IconCopyLink-hf-doc-builder.js";function cs(Gt){let u;return{c(){u=s("Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data.")},l(g){u=n(g,"Training and evaluating on the same split can misrepresent your results! If you overfit on your training data the evaluation results on that split will look great but the model will perform poorly on new data.")},m(g,w){f(g,u,w)},d(g){g&&t(u)}}}function ps(Gt){let u,g,w,D,et,te,La,tt,Ha,Lt,Ae,qa,Ht,Ie,Da,qt,k,U,at,ae,Ua,ot,Ca,Dt,Se,Ra,Ut,b,Te,rt,Wa,za,Ba,Ne,it,Fa,ja,Ja,xe,st,Ka,Qa,Ct,p,Va,nt,Xa,Ya,lt,Za,eo,ft,to,ao,ht,oo,ro,dt,io,so,Rt,Oe,no,Wt,Me,lo,zt,C,Bt,R,fo,oe,ho,co,Ft,A,W,ct,re,po,pt,uo,jt,_,mo,ie,vo,yo,ut,wo,go,Jt,Ge,bo,Kt,m,_o,mt,se,Eo,$o,vt,ne,Po,ko,yt,le,Ao,Io,Qt,z,So,fe,To,No,Vt,Le,He,qr,Xt,B,xo,he,Oo,Mo,Yt,qe,De,Dr,Zt,Ue,Go,ea,I,F,wt,de,Lo,gt,Ho,ta,Ce,qo,aa,ce,bt,Do,Uo,oa,pe,_t,Co,Ro,ra,Re,Wo,ia,S,j,Et,ue,zo,$t,Bo,sa,We,Fo,na,ze,jo,la,T,J,Pt,me,Jo,kt,Ko,fa,E,Qo,At,Vo,Xo,It,Yo,Zo,ha,K,er,ve,tr,ar,da,$,or,ye,rr,ir,we,sr,nr,ca,Be,lr,pa,N,Q,St,ge,fr,Tt,hr,ua,P,dr,Nt,cr,pr,xt,ur,mr,ma,Fe,vr,va,je,yr,ya,Je,wr,wa,x,V,Ot,be,gr,Mt,br,ga,X,_r,_e,Er,$r,ba,v,Pr,Ee,kr,Ar,$e,Ir,Sr,Pe,Tr,Nr,_a,Ke,xr,Ea;return te=new ee({}),ae=new ee({}),C=new ds({props:{warning:!0,$$slots:{default:[cs]},$$scope:{ctx:Gt}}}),re=new ee({}),de=new ee({}),ue=new ee({}),me=new ee({}),ge=new ee({}),be=new ee({}),{c(){u=o("meta"),g=d(),w=o("h1"),D=o("a"),et=o("span"),O(te.$$.fragment),La=d(),tt=o("span"),Ha=s("Considerations for model evaluation"),Lt=d(),Ae=o("p"),qa=s("Developing an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and \u{1F917} Evaluate is here to help!"),Ht=d(),Ie=o("p"),Da=s("Here are some things to keep in mind when evaluating your model using the \u{1F917} Evaluate library:"),qt=d(),k=o("h2"),U=o("a"),at=o("span"),O(ae.$$.fragment),Ua=d(),ot=o("span"),Ca=s("Properly splitting your data"),Dt=d(),Se=o("p"),Ra=s("Good evaluation generally requires three splits of your dataset:"),Ut=d(),b=o("ul"),Te=o("li"),rt=o("strong"),Wa=s("train"),za=s(": this is used for training your model."),Ba=d(),Ne=o("li"),it=o("strong"),Fa=s("validation"),ja=s(": this is used for validating the model hyperparameters."),Ja=d(),xe=o("li"),st=o("strong"),Ka=s("test"),Qa=s(": this is used for evaluating your model."),Ct=d(),p=o("p"),Va=s("Many of the datasets on the \u{1F917} Hub are separated into 2 splits: "),nt=o("code"),Xa=s("train"),Ya=s(" and "),lt=o("code"),Za=s("validation"),eo=s("; others are split into 3 splits ("),ft=o("code"),to=s("train"),ao=s(", "),ht=o("code"),oo=s("validation"),ro=s(" and "),dt=o("code"),io=s("test"),so=s(") \u2014 make sure to use the right split for the right purpose!"),Rt=d(),Oe=o("p"),no=s("Some datasets on the \u{1F917} Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split."),Wt=d(),Me=o("p"),lo=s("If the dataset you\u2019re using doesn\u2019t have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and  which you want to use for hyperparameter tuning or final evaluation."),zt=d(),O(C.$$.fragment),Bt=d(),R=o("p"),fo=s("Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out "),oe=o("a"),ho=s("this thread"),co=s(" for a more in-depth discussion of dataset splitting!"),Ft=d(),A=o("h2"),W=o("a"),ct=o("span"),O(re.$$.fragment),po=d(),pt=o("span"),uo=s("The impact of class imbalance"),jt=d(),_=o("p"),mo=s("While many academic datasets, such as the "),ie=o("a"),vo=s("IMDb dataset"),yo=s(" of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a "),ut=o("em"),wo=s("balanced dataset"),go=s(" corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset."),Jt=d(),Ge=o("p"),bo=s("Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \u201Cnon-fraud\u201D cases and 1 \u201Cfraud\u201D case. A simple model that always predicts \u201Cnon-fraud\u201D cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case."),Kt=d(),m=o("p"),_o=s("Often, using more than one metric can help get a better idea of your model\u2019s performance from different points of view. For instance, metrics like "),mt=o("strong"),se=o("a"),Eo=s("recall"),$o=s(" and "),vt=o("strong"),ne=o("a"),Po=s("precision"),ko=s(" can be used together, and the "),yt=o("strong"),le=o("a"),Ao=s("f1 score"),Io=s(" is actually the harmonic mean of the two."),Qt=d(),z=o("p"),So=s("In cases where a dataset is balanced, using "),fe=o("a"),To=s("accuracy"),No=s(" can reflect the overall model performance:"),Vt=d(),Le=o("p"),He=o("img"),Xt=d(),B=o("p"),xo=s("In cases where there is an imbalance, using "),he=o("a"),Oo=s("F1 score"),Mo=s(" can be a better representation of performance, given that it encompasses both precision and recall."),Yt=d(),qe=o("p"),De=o("img"),Zt=d(),Ue=o("p"),Go=s("Using accuracy in an imbalanced setting is less ideal, since it is not sensitive to minority classes and will not faithfully reflect model performance on them."),ea=d(),I=o("h2"),F=o("a"),wt=o("span"),O(de.$$.fragment),Lo=d(),gt=o("span"),Ho=s("Offline vs. online model evaluation"),ta=d(),Ce=o("p"),qo=s("There are multiple ways to evaluate models, and an important distinction is offline versus online evaluation:"),aa=d(),ce=o("p"),bt=o("strong"),Do=s("Offline evaluation"),Uo=s(" is done before deploying a model or using insights generated from a model, using static datasets and metrics."),oa=d(),pe=o("p"),_t=o("strong"),Co=s("Online evaluation"),Ro=s(" means evaluating how a model is performing after deployment and during its use in production."),ra=d(),Re=o("p"),Wo=s("These two types of evaluation can use different metrics and measure different aspects of model performance. For example, offline evaluation can compare a model to other models based on their performance on common benchmarks, whereas online evaluation will evaluate aspects such as latency and accuracy of the model based on production data (for example, the number of user queries that it was able to address)."),ia=d(),S=o("h2"),j=o("a"),Et=o("span"),O(ue.$$.fragment),zo=d(),$t=o("span"),Bo=s("Trade-offs in model evaluation"),sa=d(),We=o("p"),Fo=s("When evaluating models in practice, there are often trade-offs that have to be made between different aspects of model performance: for instance, choosing a model that is slightly less accurate but that has a faster inference time, compared to a high-accuracy that has a higher memory footprint and requires access to more GPUs."),na=d(),ze=o("p"),jo=s("Here are other aspects of model performance to consider during evaluation:"),la=d(),T=o("h3"),J=o("a"),Pt=o("span"),O(me.$$.fragment),Jo=d(),kt=o("span"),Ko=s("Interpretability"),fa=d(),E=o("p"),Qo=s("When evaluating models, "),At=o("strong"),Vo=s("interpretability"),Xo=s(" (i.e. the ability to "),It=o("em"),Yo=s("interpret"),Zo=s(" results)  can be very important, especially when deploying models in production."),ha=d(),K=o("p"),er=s("For instance, metrics such as "),ve=o("a"),tr=s("exact match"),ar=s(" have a set range (between 0 and 1, or 0% and 100%) and are easily understandable to users: for a pair of strings, the exact match score is 1 if the two strings are the exact same, and 0 otherwise."),da=d(),$=o("p"),or=s("Other metrics, such as "),ye=o("a"),rr=s("BLEU"),ir=s(" are harder to interpret: while they also range between 0 and 1, they can vary greatly depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used (see the "),we=o("a"),sr=s("metric card"),nr=s(" for more information about BLEU limitations). This means that it is difficult to interpret a BLEU score without having more information about the procedure used for obtaining it."),ca=d(),Be=o("p"),lr=s("Interpretability can be more or less important depending on the evaluation use case, but it is a useful aspect of model evaluation to keep in mind, since communicating and comparing model evaluations is an important part of responsible machine learning."),pa=d(),N=o("h3"),Q=o("a"),St=o("span"),O(ge.$$.fragment),fr=d(),Tt=o("span"),hr=s("Inference speed and memory footprint"),ua=d(),P=o("p"),dr=s("While recent years have seen increasingly large ML models achieve high performance on a large variety of tasks and benchmarks, deploying these multi-billion parameter models in practice can be a challenge in itself, and many organizations lack the resources for this. This is why considering the "),Nt=o("strong"),cr=s("inference speed"),pr=s(" and "),xt=o("strong"),ur=s("memory footprint"),mr=s(" of models is important, especially when doing online model evaluation."),ma=d(),Fe=o("p"),vr=s("Inference speed refers to the time that it takes for a model to make a prediction \u2014 this will vary depending on the hardware used and the way in which models are queried, e.g. in real time via an API or in batch jobs that run once a day."),va=d(),je=o("p"),yr=s("Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method."),ya=d(),Je=o("p"),wr=s("When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation."),wa=d(),x=o("h2"),V=o("a"),Ot=o("span"),O(be.$$.fragment),gr=d(),Mt=o("span"),br=s("Limitations and bias"),ga=d(),X=o("p"),_r=s("All models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via "),_e=o("a"),Er=s("model cards"),$r=s(" which document the training and evaluation process."),ba=d(),v=o("p"),Pr=s("Measuring biases can be done by evaluating models on datasets such as "),Ee=o("a"),kr=s("Wino Bias"),Ar=s(" or "),$e=o("a"),Ir=s("MD Gender Bias"),Sr=s(", and by doing "),Pe=o("a"),Tr=s("Interactive Error Analyis"),Nr=s(" to try to identify which subsets of the evaluation dataset a model performs poorly on."),_a=d(),Ke=o("p"),xr=s("We are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets \u2014 stay tuned for more documentation on this topic!"),this.h()},l(e){const l=fs('[data-svelte="svelte-1phssyn"]',document.head);u=r(l,"META",{name:!0,content:!0}),l.forEach(t),g=c(e),w=r(e,"H1",{class:!0});var ke=i(w);D=r(ke,"A",{id:!0,class:!0,href:!0});var Ur=i(D);et=r(Ur,"SPAN",{});var Cr=i(et);M(te.$$.fragment,Cr),Cr.forEach(t),Ur.forEach(t),La=c(ke),tt=r(ke,"SPAN",{});var Rr=i(tt);Ha=n(Rr,"Considerations for model evaluation"),Rr.forEach(t),ke.forEach(t),Lt=c(e),Ae=r(e,"P",{});var Wr=i(Ae);qa=n(Wr,"Developing an ML model is rarely a one-shot deal: it often involves multiple stages of defining the model architecture and tuning hyper-parameters before converging on a final set. Responsible model evaluation is a key part of this process, and \u{1F917} Evaluate is here to help!"),Wr.forEach(t),Ht=c(e),Ie=r(e,"P",{});var zr=i(Ie);Da=n(zr,"Here are some things to keep in mind when evaluating your model using the \u{1F917} Evaluate library:"),zr.forEach(t),qt=c(e),k=r(e,"H2",{class:!0});var $a=i(k);U=r($a,"A",{id:!0,class:!0,href:!0});var Br=i(U);at=r(Br,"SPAN",{});var Fr=i(at);M(ae.$$.fragment,Fr),Fr.forEach(t),Br.forEach(t),Ua=c($a),ot=r($a,"SPAN",{});var jr=i(ot);Ca=n(jr,"Properly splitting your data"),jr.forEach(t),$a.forEach(t),Dt=c(e),Se=r(e,"P",{});var Jr=i(Se);Ra=n(Jr,"Good evaluation generally requires three splits of your dataset:"),Jr.forEach(t),Ut=c(e),b=r(e,"UL",{});var Qe=i(b);Te=r(Qe,"LI",{});var Or=i(Te);rt=r(Or,"STRONG",{});var Kr=i(rt);Wa=n(Kr,"train"),Kr.forEach(t),za=n(Or,": this is used for training your model."),Or.forEach(t),Ba=c(Qe),Ne=r(Qe,"LI",{});var Mr=i(Ne);it=r(Mr,"STRONG",{});var Qr=i(it);Fa=n(Qr,"validation"),Qr.forEach(t),ja=n(Mr,": this is used for validating the model hyperparameters."),Mr.forEach(t),Ja=c(Qe),xe=r(Qe,"LI",{});var Gr=i(xe);st=r(Gr,"STRONG",{});var Vr=i(st);Ka=n(Vr,"test"),Vr.forEach(t),Qa=n(Gr,": this is used for evaluating your model."),Gr.forEach(t),Qe.forEach(t),Ct=c(e),p=r(e,"P",{});var y=i(p);Va=n(y,"Many of the datasets on the \u{1F917} Hub are separated into 2 splits: "),nt=r(y,"CODE",{});var Xr=i(nt);Xa=n(Xr,"train"),Xr.forEach(t),Ya=n(y," and "),lt=r(y,"CODE",{});var Yr=i(lt);Za=n(Yr,"validation"),Yr.forEach(t),eo=n(y,"; others are split into 3 splits ("),ft=r(y,"CODE",{});var Zr=i(ft);to=n(Zr,"train"),Zr.forEach(t),ao=n(y,", "),ht=r(y,"CODE",{});var ei=i(ht);oo=n(ei,"validation"),ei.forEach(t),ro=n(y," and "),dt=r(y,"CODE",{});var ti=i(dt);io=n(ti,"test"),ti.forEach(t),so=n(y,") \u2014 make sure to use the right split for the right purpose!"),y.forEach(t),Rt=c(e),Oe=r(e,"P",{});var ai=i(Oe);no=n(ai,"Some datasets on the \u{1F917} Hub are already separated into these three splits. However, there are also many that only have a train/validation or only train split."),ai.forEach(t),Wt=c(e),Me=r(e,"P",{});var oi=i(Me);lo=n(oi,"If the dataset you\u2019re using doesn\u2019t have a predefined train-test split, it is up to you to define which part of the dataset you want to use for training your model and  which you want to use for hyperparameter tuning or final evaluation."),oi.forEach(t),zt=c(e),M(C.$$.fragment,e),Bt=c(e),R=r(e,"P",{});var Pa=i(R);fo=n(Pa,"Depending on the size of the dataset, you can keep anywhere from 10-30% for evaluation and the rest for training, while aiming to set up the test set to reflect the production data as close as possible. Check out "),oe=r(Pa,"A",{href:!0,rel:!0});var ri=i(oe);ho=n(ri,"this thread"),ri.forEach(t),co=n(Pa," for a more in-depth discussion of dataset splitting!"),Pa.forEach(t),Ft=c(e),A=r(e,"H2",{class:!0});var ka=i(A);W=r(ka,"A",{id:!0,class:!0,href:!0});var ii=i(W);ct=r(ii,"SPAN",{});var si=i(ct);M(re.$$.fragment,si),si.forEach(t),ii.forEach(t),po=c(ka),pt=r(ka,"SPAN",{});var ni=i(pt);uo=n(ni,"The impact of class imbalance"),ni.forEach(t),ka.forEach(t),jt=c(e),_=r(e,"P",{});var Ve=i(_);mo=n(Ve,"While many academic datasets, such as the "),ie=r(Ve,"A",{href:!0,rel:!0});var li=i(ie);vo=n(li,"IMDb dataset"),li.forEach(t),yo=n(Ve," of movie reviews, are perfectly balanced, most real-world datasets are not. In machine learning a "),ut=r(Ve,"EM",{});var fi=i(ut);wo=n(fi,"balanced dataset"),fi.forEach(t),go=n(Ve," corresponds to a datasets where all labels are represented equally. In the case of the IMDb dataset this means that there are as many positive as negative reviews in the dataset. In an imbalanced dataset this is not the case: in fraud detection for example there are usually many more non-fraud cases than fraud cases in the dataset."),Ve.forEach(t),Jt=c(e),Ge=r(e,"P",{});var hi=i(Ge);bo=n(hi,"Having an imbalanced dataset can skew the results of your metrics. Imagine a dataset with 99 \u201Cnon-fraud\u201D cases and 1 \u201Cfraud\u201D case. A simple model that always predicts \u201Cnon-fraud\u201D cases would give yield a 99% accuracy which might sound good at first until you realize that you will never catch a fraud case."),hi.forEach(t),Kt=c(e),m=r(e,"P",{});var Y=i(m);_o=n(Y,"Often, using more than one metric can help get a better idea of your model\u2019s performance from different points of view. For instance, metrics like "),mt=r(Y,"STRONG",{});var di=i(mt);se=r(di,"A",{href:!0,rel:!0});var ci=i(se);Eo=n(ci,"recall"),ci.forEach(t),di.forEach(t),$o=n(Y," and "),vt=r(Y,"STRONG",{});var pi=i(vt);ne=r(pi,"A",{href:!0,rel:!0});var ui=i(ne);Po=n(ui,"precision"),ui.forEach(t),pi.forEach(t),ko=n(Y," can be used together, and the "),yt=r(Y,"STRONG",{});var mi=i(yt);le=r(mi,"A",{href:!0,rel:!0});var vi=i(le);Ao=n(vi,"f1 score"),vi.forEach(t),mi.forEach(t),Io=n(Y," is actually the harmonic mean of the two."),Y.forEach(t),Qt=c(e),z=r(e,"P",{});var Aa=i(z);So=n(Aa,"In cases where a dataset is balanced, using "),fe=r(Aa,"A",{href:!0,rel:!0});var yi=i(fe);To=n(yi,"accuracy"),yi.forEach(t),No=n(Aa," can reflect the overall model performance:"),Aa.forEach(t),Vt=c(e),Le=r(e,"P",{});var wi=i(Le);He=r(wi,"IMG",{src:!0,alt:!0}),wi.forEach(t),Xt=c(e),B=r(e,"P",{});var Ia=i(B);xo=n(Ia,"In cases where there is an imbalance, using "),he=r(Ia,"A",{href:!0,rel:!0});var gi=i(he);Oo=n(gi,"F1 score"),gi.forEach(t),Mo=n(Ia," can be a better representation of performance, given that it encompasses both precision and recall."),Ia.forEach(t),Yt=c(e),qe=r(e,"P",{});var bi=i(qe);De=r(bi,"IMG",{src:!0,alt:!0}),bi.forEach(t),Zt=c(e),Ue=r(e,"P",{});var _i=i(Ue);Go=n(_i,"Using accuracy in an imbalanced setting is less ideal, since it is not sensitive to minority classes and will not faithfully reflect model performance on them."),_i.forEach(t),ea=c(e),I=r(e,"H2",{class:!0});var Sa=i(I);F=r(Sa,"A",{id:!0,class:!0,href:!0});var Ei=i(F);wt=r(Ei,"SPAN",{});var $i=i(wt);M(de.$$.fragment,$i),$i.forEach(t),Ei.forEach(t),Lo=c(Sa),gt=r(Sa,"SPAN",{});var Pi=i(gt);Ho=n(Pi,"Offline vs. online model evaluation"),Pi.forEach(t),Sa.forEach(t),ta=c(e),Ce=r(e,"P",{});var ki=i(Ce);qo=n(ki,"There are multiple ways to evaluate models, and an important distinction is offline versus online evaluation:"),ki.forEach(t),aa=c(e),ce=r(e,"P",{});var Lr=i(ce);bt=r(Lr,"STRONG",{});var Ai=i(bt);Do=n(Ai,"Offline evaluation"),Ai.forEach(t),Uo=n(Lr," is done before deploying a model or using insights generated from a model, using static datasets and metrics."),Lr.forEach(t),oa=c(e),pe=r(e,"P",{});var Hr=i(pe);_t=r(Hr,"STRONG",{});var Ii=i(_t);Co=n(Ii,"Online evaluation"),Ii.forEach(t),Ro=n(Hr," means evaluating how a model is performing after deployment and during its use in production."),Hr.forEach(t),ra=c(e),Re=r(e,"P",{});var Si=i(Re);Wo=n(Si,"These two types of evaluation can use different metrics and measure different aspects of model performance. For example, offline evaluation can compare a model to other models based on their performance on common benchmarks, whereas online evaluation will evaluate aspects such as latency and accuracy of the model based on production data (for example, the number of user queries that it was able to address)."),Si.forEach(t),ia=c(e),S=r(e,"H2",{class:!0});var Ta=i(S);j=r(Ta,"A",{id:!0,class:!0,href:!0});var Ti=i(j);Et=r(Ti,"SPAN",{});var Ni=i(Et);M(ue.$$.fragment,Ni),Ni.forEach(t),Ti.forEach(t),zo=c(Ta),$t=r(Ta,"SPAN",{});var xi=i($t);Bo=n(xi,"Trade-offs in model evaluation"),xi.forEach(t),Ta.forEach(t),sa=c(e),We=r(e,"P",{});var Oi=i(We);Fo=n(Oi,"When evaluating models in practice, there are often trade-offs that have to be made between different aspects of model performance: for instance, choosing a model that is slightly less accurate but that has a faster inference time, compared to a high-accuracy that has a higher memory footprint and requires access to more GPUs."),Oi.forEach(t),na=c(e),ze=r(e,"P",{});var Mi=i(ze);jo=n(Mi,"Here are other aspects of model performance to consider during evaluation:"),Mi.forEach(t),la=c(e),T=r(e,"H3",{class:!0});var Na=i(T);J=r(Na,"A",{id:!0,class:!0,href:!0});var Gi=i(J);Pt=r(Gi,"SPAN",{});var Li=i(Pt);M(me.$$.fragment,Li),Li.forEach(t),Gi.forEach(t),Jo=c(Na),kt=r(Na,"SPAN",{});var Hi=i(kt);Ko=n(Hi,"Interpretability"),Hi.forEach(t),Na.forEach(t),fa=c(e),E=r(e,"P",{});var Xe=i(E);Qo=n(Xe,"When evaluating models, "),At=r(Xe,"STRONG",{});var qi=i(At);Vo=n(qi,"interpretability"),qi.forEach(t),Xo=n(Xe," (i.e. the ability to "),It=r(Xe,"EM",{});var Di=i(It);Yo=n(Di,"interpret"),Di.forEach(t),Zo=n(Xe," results)  can be very important, especially when deploying models in production."),Xe.forEach(t),ha=c(e),K=r(e,"P",{});var xa=i(K);er=n(xa,"For instance, metrics such as "),ve=r(xa,"A",{href:!0,rel:!0});var Ui=i(ve);tr=n(Ui,"exact match"),Ui.forEach(t),ar=n(xa," have a set range (between 0 and 1, or 0% and 100%) and are easily understandable to users: for a pair of strings, the exact match score is 1 if the two strings are the exact same, and 0 otherwise."),xa.forEach(t),da=c(e),$=r(e,"P",{});var Ye=i($);or=n(Ye,"Other metrics, such as "),ye=r(Ye,"A",{href:!0,rel:!0});var Ci=i(ye);rr=n(Ci,"BLEU"),Ci.forEach(t),ir=n(Ye," are harder to interpret: while they also range between 0 and 1, they can vary greatly depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used (see the "),we=r(Ye,"A",{href:!0,rel:!0});var Ri=i(we);sr=n(Ri,"metric card"),Ri.forEach(t),nr=n(Ye," for more information about BLEU limitations). This means that it is difficult to interpret a BLEU score without having more information about the procedure used for obtaining it."),Ye.forEach(t),ca=c(e),Be=r(e,"P",{});var Wi=i(Be);lr=n(Wi,"Interpretability can be more or less important depending on the evaluation use case, but it is a useful aspect of model evaluation to keep in mind, since communicating and comparing model evaluations is an important part of responsible machine learning."),Wi.forEach(t),pa=c(e),N=r(e,"H3",{class:!0});var Oa=i(N);Q=r(Oa,"A",{id:!0,class:!0,href:!0});var zi=i(Q);St=r(zi,"SPAN",{});var Bi=i(St);M(ge.$$.fragment,Bi),Bi.forEach(t),zi.forEach(t),fr=c(Oa),Tt=r(Oa,"SPAN",{});var Fi=i(Tt);hr=n(Fi,"Inference speed and memory footprint"),Fi.forEach(t),Oa.forEach(t),ua=c(e),P=r(e,"P",{});var Ze=i(P);dr=n(Ze,"While recent years have seen increasingly large ML models achieve high performance on a large variety of tasks and benchmarks, deploying these multi-billion parameter models in practice can be a challenge in itself, and many organizations lack the resources for this. This is why considering the "),Nt=r(Ze,"STRONG",{});var ji=i(Nt);cr=n(ji,"inference speed"),ji.forEach(t),pr=n(Ze," and "),xt=r(Ze,"STRONG",{});var Ji=i(xt);ur=n(Ji,"memory footprint"),Ji.forEach(t),mr=n(Ze," of models is important, especially when doing online model evaluation."),Ze.forEach(t),ma=c(e),Fe=r(e,"P",{});var Ki=i(Fe);vr=n(Ki,"Inference speed refers to the time that it takes for a model to make a prediction \u2014 this will vary depending on the hardware used and the way in which models are queried, e.g. in real time via an API or in batch jobs that run once a day."),Ki.forEach(t),va=c(e),je=r(e,"P",{});var Qi=i(je);yr=n(Qi,"Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method."),Qi.forEach(t),ya=c(e),Je=r(e,"P",{});var Vi=i(Je);wr=n(Vi,"When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation."),Vi.forEach(t),wa=c(e),x=r(e,"H2",{class:!0});var Ma=i(x);V=r(Ma,"A",{id:!0,class:!0,href:!0});var Xi=i(V);Ot=r(Xi,"SPAN",{});var Yi=i(Ot);M(be.$$.fragment,Yi),Yi.forEach(t),Xi.forEach(t),gr=c(Ma),Mt=r(Ma,"SPAN",{});var Zi=i(Mt);br=n(Zi,"Limitations and bias"),Zi.forEach(t),Ma.forEach(t),ga=c(e),X=r(e,"P",{});var Ga=i(X);_r=n(Ga,"All models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via "),_e=r(Ga,"A",{href:!0,rel:!0});var es=i(_e);Er=n(es,"model cards"),es.forEach(t),$r=n(Ga," which document the training and evaluation process."),Ga.forEach(t),ba=c(e),v=r(e,"P",{});var Z=i(v);Pr=n(Z,"Measuring biases can be done by evaluating models on datasets such as "),Ee=r(Z,"A",{href:!0,rel:!0});var ts=i(Ee);kr=n(ts,"Wino Bias"),ts.forEach(t),Ar=n(Z," or "),$e=r(Z,"A",{href:!0,rel:!0});var as=i($e);Ir=n(as,"MD Gender Bias"),as.forEach(t),Sr=n(Z,", and by doing "),Pe=r(Z,"A",{href:!0,rel:!0});var os=i(Pe);Tr=n(os,"Interactive Error Analyis"),os.forEach(t),Nr=n(Z," to try to identify which subsets of the evaluation dataset a model performs poorly on."),Z.forEach(t),_a=c(e),Ke=r(e,"P",{});var rs=i(Ke);xr=n(rs,"We are currently working on additional measurements that can be used to quantify different dimensions of bias in both models and datasets \u2014 stay tuned for more documentation on this topic!"),rs.forEach(t),this.h()},h(){h(u,"name","hf:doc:metadata"),h(u,"content",JSON.stringify(us)),h(D,"id","considerations-for-model-evaluation"),h(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(D,"href","#considerations-for-model-evaluation"),h(w,"class","relative group"),h(U,"id","properly-splitting-your-data"),h(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(U,"href","#properly-splitting-your-data"),h(k,"class","relative group"),h(oe,"href","https://discuss.huggingface.co/t/how-to-split-main-dataset-into-train-dev-test-as-datasetdict/1090"),h(oe,"rel","nofollow"),h(W,"id","the-impact-of-class-imbalance"),h(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(W,"href","#the-impact-of-class-imbalance"),h(A,"class","relative group"),h(ie,"href","https://huggingface.co/datasets/imdb"),h(ie,"rel","nofollow"),h(se,"href","https://huggingface.co/metrics/recall"),h(se,"rel","nofollow"),h(ne,"href","https://huggingface.co/metrics/precision"),h(ne,"rel","nofollow"),h(le,"href","https://huggingface.co/metrics/f1"),h(le,"rel","nofollow"),h(fe,"href","https://huggingface.co/metrics/accuracy"),h(fe,"rel","nofollow"),is(He.src,qr="https://huggingface.co/datasets/evaluate/media/resolve/main/balanced-classes.png")||h(He,"src",qr),h(He,"alt","Balanced Labels"),h(he,"href","https://huggingface.co/metrics/f1"),h(he,"rel","nofollow"),is(De.src,Dr="https://huggingface.co/datasets/evaluate/media/resolve/main/imbalanced-classes.png")||h(De,"src",Dr),h(De,"alt","Imbalanced Labels"),h(F,"id","offline-vs-online-model-evaluation"),h(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(F,"href","#offline-vs-online-model-evaluation"),h(I,"class","relative group"),h(j,"id","tradeoffs-in-model-evaluation"),h(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(j,"href","#tradeoffs-in-model-evaluation"),h(S,"class","relative group"),h(J,"id","interpretability"),h(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(J,"href","#interpretability"),h(T,"class","relative group"),h(ve,"href","https://huggingface.co/spaces/evaluate-metric/exact_match"),h(ve,"rel","nofollow"),h(ye,"href","https://huggingface.co/spaces/evaluate-metric/exact_match"),h(ye,"rel","nofollow"),h(we,"href","https://huggingface.co/spaces/evaluate-metric/bleu/blob/main/README.md"),h(we,"rel","nofollow"),h(Q,"id","inference-speed-and-memory-footprint"),h(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Q,"href","#inference-speed-and-memory-footprint"),h(N,"class","relative group"),h(V,"id","limitations-and-bias"),h(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(V,"href","#limitations-and-bias"),h(x,"class","relative group"),h(_e,"href","https://huggingface.co/course/chapter4/4?fw=pt"),h(_e,"rel","nofollow"),h(Ee,"href","https://huggingface.co/datasets/wino_bias"),h(Ee,"rel","nofollow"),h($e,"href","https://huggingface.co/datasets/md_gender_bias"),h($e,"rel","nofollow"),h(Pe,"href","https://huggingface.co/spaces/nazneen/error-analysis"),h(Pe,"rel","nofollow")},m(e,l){a(document.head,u),f(e,g,l),f(e,w,l),a(w,D),a(D,et),G(te,et,null),a(w,La),a(w,tt),a(tt,Ha),f(e,Lt,l),f(e,Ae,l),a(Ae,qa),f(e,Ht,l),f(e,Ie,l),a(Ie,Da),f(e,qt,l),f(e,k,l),a(k,U),a(U,at),G(ae,at,null),a(k,Ua),a(k,ot),a(ot,Ca),f(e,Dt,l),f(e,Se,l),a(Se,Ra),f(e,Ut,l),f(e,b,l),a(b,Te),a(Te,rt),a(rt,Wa),a(Te,za),a(b,Ba),a(b,Ne),a(Ne,it),a(it,Fa),a(Ne,ja),a(b,Ja),a(b,xe),a(xe,st),a(st,Ka),a(xe,Qa),f(e,Ct,l),f(e,p,l),a(p,Va),a(p,nt),a(nt,Xa),a(p,Ya),a(p,lt),a(lt,Za),a(p,eo),a(p,ft),a(ft,to),a(p,ao),a(p,ht),a(ht,oo),a(p,ro),a(p,dt),a(dt,io),a(p,so),f(e,Rt,l),f(e,Oe,l),a(Oe,no),f(e,Wt,l),f(e,Me,l),a(Me,lo),f(e,zt,l),G(C,e,l),f(e,Bt,l),f(e,R,l),a(R,fo),a(R,oe),a(oe,ho),a(R,co),f(e,Ft,l),f(e,A,l),a(A,W),a(W,ct),G(re,ct,null),a(A,po),a(A,pt),a(pt,uo),f(e,jt,l),f(e,_,l),a(_,mo),a(_,ie),a(ie,vo),a(_,yo),a(_,ut),a(ut,wo),a(_,go),f(e,Jt,l),f(e,Ge,l),a(Ge,bo),f(e,Kt,l),f(e,m,l),a(m,_o),a(m,mt),a(mt,se),a(se,Eo),a(m,$o),a(m,vt),a(vt,ne),a(ne,Po),a(m,ko),a(m,yt),a(yt,le),a(le,Ao),a(m,Io),f(e,Qt,l),f(e,z,l),a(z,So),a(z,fe),a(fe,To),a(z,No),f(e,Vt,l),f(e,Le,l),a(Le,He),f(e,Xt,l),f(e,B,l),a(B,xo),a(B,he),a(he,Oo),a(B,Mo),f(e,Yt,l),f(e,qe,l),a(qe,De),f(e,Zt,l),f(e,Ue,l),a(Ue,Go),f(e,ea,l),f(e,I,l),a(I,F),a(F,wt),G(de,wt,null),a(I,Lo),a(I,gt),a(gt,Ho),f(e,ta,l),f(e,Ce,l),a(Ce,qo),f(e,aa,l),f(e,ce,l),a(ce,bt),a(bt,Do),a(ce,Uo),f(e,oa,l),f(e,pe,l),a(pe,_t),a(_t,Co),a(pe,Ro),f(e,ra,l),f(e,Re,l),a(Re,Wo),f(e,ia,l),f(e,S,l),a(S,j),a(j,Et),G(ue,Et,null),a(S,zo),a(S,$t),a($t,Bo),f(e,sa,l),f(e,We,l),a(We,Fo),f(e,na,l),f(e,ze,l),a(ze,jo),f(e,la,l),f(e,T,l),a(T,J),a(J,Pt),G(me,Pt,null),a(T,Jo),a(T,kt),a(kt,Ko),f(e,fa,l),f(e,E,l),a(E,Qo),a(E,At),a(At,Vo),a(E,Xo),a(E,It),a(It,Yo),a(E,Zo),f(e,ha,l),f(e,K,l),a(K,er),a(K,ve),a(ve,tr),a(K,ar),f(e,da,l),f(e,$,l),a($,or),a($,ye),a(ye,rr),a($,ir),a($,we),a(we,sr),a($,nr),f(e,ca,l),f(e,Be,l),a(Be,lr),f(e,pa,l),f(e,N,l),a(N,Q),a(Q,St),G(ge,St,null),a(N,fr),a(N,Tt),a(Tt,hr),f(e,ua,l),f(e,P,l),a(P,dr),a(P,Nt),a(Nt,cr),a(P,pr),a(P,xt),a(xt,ur),a(P,mr),f(e,ma,l),f(e,Fe,l),a(Fe,vr),f(e,va,l),f(e,je,l),a(je,yr),f(e,ya,l),f(e,Je,l),a(Je,wr),f(e,wa,l),f(e,x,l),a(x,V),a(V,Ot),G(be,Ot,null),a(x,gr),a(x,Mt),a(Mt,br),f(e,ga,l),f(e,X,l),a(X,_r),a(X,_e),a(_e,Er),a(X,$r),f(e,ba,l),f(e,v,l),a(v,Pr),a(v,Ee),a(Ee,kr),a(v,Ar),a(v,$e),a($e,Ir),a(v,Sr),a(v,Pe),a(Pe,Tr),a(v,Nr),f(e,_a,l),f(e,Ke,l),a(Ke,xr),Ea=!0},p(e,[l]){const ke={};l&2&&(ke.$$scope={dirty:l,ctx:e}),C.$set(ke)},i(e){Ea||(L(te.$$.fragment,e),L(ae.$$.fragment,e),L(C.$$.fragment,e),L(re.$$.fragment,e),L(de.$$.fragment,e),L(ue.$$.fragment,e),L(me.$$.fragment,e),L(ge.$$.fragment,e),L(be.$$.fragment,e),Ea=!0)},o(e){H(te.$$.fragment,e),H(ae.$$.fragment,e),H(C.$$.fragment,e),H(re.$$.fragment,e),H(de.$$.fragment,e),H(ue.$$.fragment,e),H(me.$$.fragment,e),H(ge.$$.fragment,e),H(be.$$.fragment,e),Ea=!1},d(e){t(u),e&&t(g),e&&t(w),q(te),e&&t(Lt),e&&t(Ae),e&&t(Ht),e&&t(Ie),e&&t(qt),e&&t(k),q(ae),e&&t(Dt),e&&t(Se),e&&t(Ut),e&&t(b),e&&t(Ct),e&&t(p),e&&t(Rt),e&&t(Oe),e&&t(Wt),e&&t(Me),e&&t(zt),q(C,e),e&&t(Bt),e&&t(R),e&&t(Ft),e&&t(A),q(re),e&&t(jt),e&&t(_),e&&t(Jt),e&&t(Ge),e&&t(Kt),e&&t(m),e&&t(Qt),e&&t(z),e&&t(Vt),e&&t(Le),e&&t(Xt),e&&t(B),e&&t(Yt),e&&t(qe),e&&t(Zt),e&&t(Ue),e&&t(ea),e&&t(I),q(de),e&&t(ta),e&&t(Ce),e&&t(aa),e&&t(ce),e&&t(oa),e&&t(pe),e&&t(ra),e&&t(Re),e&&t(ia),e&&t(S),q(ue),e&&t(sa),e&&t(We),e&&t(na),e&&t(ze),e&&t(la),e&&t(T),q(me),e&&t(fa),e&&t(E),e&&t(ha),e&&t(K),e&&t(da),e&&t($),e&&t(ca),e&&t(Be),e&&t(pa),e&&t(N),q(ge),e&&t(ua),e&&t(P),e&&t(ma),e&&t(Fe),e&&t(va),e&&t(je),e&&t(ya),e&&t(Je),e&&t(wa),e&&t(x),q(be),e&&t(ga),e&&t(X),e&&t(ba),e&&t(v),e&&t(_a),e&&t(Ke)}}}const us={local:"considerations-for-model-evaluation",sections:[{local:"properly-splitting-your-data",title:"Properly splitting your data"},{local:"the-impact-of-class-imbalance",title:"The impact of class imbalance"},{local:"offline-vs-online-model-evaluation",title:"Offline vs. online model evaluation"},{local:"tradeoffs-in-model-evaluation",sections:[{local:"interpretability",title:"Interpretability"},{local:"inference-speed-and-memory-footprint",title:"Inference speed and memory footprint"}],title:"Trade-offs in model evaluation"},{local:"limitations-and-bias",title:"Limitations and bias"}],title:"Considerations for model evaluation"};function ms(Gt){return hs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gs extends ss{constructor(u){super();ns(this,u,ms,ps,ls,{})}}export{gs as default,us as metadata};
