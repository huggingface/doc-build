import{S as Ra,i as Ua,s as Ha,e as s,k as c,w as $,t as l,M as Wa,c as n,d as a,m as d,a as r,x as y,h as i,b as v,G as e,g as f,y as E,q as w,o as x,B as q,v as Ga,L as fa}from"../../chunks/vendor-hf-doc-builder.js";import{D as L}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ha}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Va}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as ma}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ja(O){let u,j,h,p,_;return p=new ha({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){u=s("p"),j=l("Examples:"),h=c(),$(p.$$.fragment)},l(o){u=n(o,"P",{});var g=r(u);j=i(g,"Examples:"),g.forEach(a),h=d(o),y(p.$$.fragment,o)},m(o,g){f(o,u,g),e(u,j),f(o,h,g),E(p,o,g),_=!0},p:fa,i(o){_||(w(p.$$.fragment,o),_=!0)},o(o){x(p.$$.fragment,o),_=!1},d(o){o&&a(u),o&&a(h),q(p,o)}}}function Ka(O){let u,j,h,p,_;return p=new ha({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("text-classification")
data =  Dataset.from_dict(load_dataset("imdb")["test"][:2])
results = e.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    input_column="text",
    label_column="label",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;text&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;label&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){u=s("p"),j=l("Examples:"),h=c(),$(p.$$.fragment)},l(o){u=n(o,"P",{});var g=r(u);j=i(g,"Examples:"),g.forEach(a),h=d(o),y(p.$$.fragment,o)},m(o,g){f(o,u,g),e(u,j),f(o,h,g),E(p,o,g),_=!0},p:fa,i(o){_||(w(p.$$.fragment,o),_=!0)},o(o){x(p.$$.fragment,o),_=!1},d(o){o&&a(u),o&&a(h),q(p,o)}}}function Qa(O){let u,j,h,p,_;return p=new ha({props:{code:`from evaluate import evaluator
from datasets import Dataset, load_dataset
e = evaluator("image-classification")
data =  Dataset.from_dict(load_dataset("beans")["test"][:2])
results = e.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    metric="accuracy",
    input_column="image",
    label_column="labels",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>e = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data =  Dataset.from_dict(load_dataset(<span class="hljs-string">&quot;beans&quot;</span>)[<span class="hljs-string">&quot;test&quot;</span>][:<span class="hljs-number">2</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>results = e.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    input_column=<span class="hljs-string">&quot;image&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){u=s("p"),j=l("Examples:"),h=c(),$(p.$$.fragment)},l(o){u=n(o,"P",{});var g=r(u);j=i(g,"Examples:"),g.forEach(a),h=d(o),y(p.$$.fragment,o)},m(o,g){f(o,u,g),e(u,j),f(o,h,g),E(p,o,g),_=!0},p:fa,i(o){_||(w(p.$$.fragment,o),_=!0)},o(o){x(p.$$.fragment,o),_=!1},d(o){o&&a(u),o&&a(h),q(p,o)}}}function Xa(O){let u,j,h,p,_,o,g,we,rt,Re,pe,lt,Ue,F,S,xe,X,it,qe,ct,He,ue,dt,We,C,Y,pt,P,ut,me,mt,ft,je,ht,gt,ke,vt,_t,bt,B,Ge,fe,$t,Je,b,Z,yt,Te,Et,wt,V,ee,xt,Ce,qt,jt,R,te,kt,ae,Tt,Pe,Ct,Pt,Dt,U,oe,It,De,Nt,Lt,H,se,Mt,Ie,At,zt,W,ne,Ft,Ne,Ot,Ke,he,St,Qe,D,re,Bt,T,Vt,ge,Rt,Ut,Le,Ht,Wt,Me,Gt,Jt,Ae,Kt,Qt,Xt,M,le,Yt,ze,Zt,ea,G,Xe,I,ie,ta,N,aa,ve,oa,sa,Fe,na,ra,Oe,la,ia,ca,A,ce,da,Se,pa,ua,J,Ye;return o=new Va({}),X=new Va({}),Y=new L({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/__init__.py#L73",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),B=new ma({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Ja]},$$scope:{ctx:O}}}),Z=new L({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L55"}}),ee=new L({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:""},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L271"}}),te=new L({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L115"}}),oe=new L({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L167",returnDescription:`
<p>metric inputs.
<code>list</code>:  pipeline inputs.</p>
`,returnType:`
<p><code>dict</code></p>
`}}),se=new L({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L243",returnDescription:`
<p>The loaded metric.</p>
`}}),ne=new L({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"tokenizer",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"feature_extractor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L199",returnDescription:`
<p>The initialized pipeline.</p>
`}}),re=new L({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/text_classification.py#L20"}}),le=new L({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;&#x201D;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/text_classification.py#L41",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),G=new ma({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Ka]},$$scope:{ctx:O}}}),ie=new L({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/image_classification.py#L20"}}),ce=new L({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>&#x201D; &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;labels&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/image_classification.py#L37",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),J=new ma({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Qa]},$$scope:{ctx:O}}}),{c(){u=s("meta"),j=c(),h=s("h1"),p=s("a"),_=s("span"),$(o.$$.fragment),g=c(),we=s("span"),rt=l("Evaluator"),Re=c(),pe=s("p"),lt=l("The evaluator classes for automatic evaluation."),Ue=c(),F=s("h2"),S=s("a"),xe=s("span"),$(X.$$.fragment),it=c(),qe=s("span"),ct=l("Evaluator classes"),He=c(),ue=s("p"),dt=l("The main entry point for using the evaluator:"),We=c(),C=s("div"),$(Y.$$.fragment),pt=c(),P=s("p"),ut=l("Utility factory method to build an "),me=s("a"),mt=l("Evaluator"),ft=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),je=s("code"),ht=l("pipeline"),gt=l(" functionalify from "),ke=s("code"),vt=l("transformers"),_t=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),bt=c(),$(B.$$.fragment),Ge=c(),fe=s("p"),$t=l("The base class for all evaluator classes:"),Je=c(),b=s("div"),$(Z.$$.fragment),yt=c(),Te=s("p"),Et=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),wt=c(),V=s("div"),$(ee.$$.fragment),xt=c(),Ce=s("p"),qt=l("Compute and return metrics."),jt=c(),R=s("div"),$(te.$$.fragment),kt=c(),ae=s("p"),Tt=l("A core method of the "),Pe=s("code"),Ct=l("Evaluator"),Pt=l(" class, which processes the pipeline outputs for compatibility with the metric."),Dt=c(),U=s("div"),$(oe.$$.fragment),It=c(),De=s("p"),Nt=l("Prepare data."),Lt=c(),H=s("div"),$(se.$$.fragment),Mt=c(),Ie=s("p"),At=l("Prepare metric."),zt=c(),W=s("div"),$(ne.$$.fragment),Ft=c(),Ne=s("p"),Ot=l("Prepare pipeline."),Ke=c(),he=s("p"),St=l("The class for text classification evaluation:"),Qe=c(),D=s("div"),$(re.$$.fragment),Bt=c(),T=s("p"),Vt=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),ge=s("a"),Rt=l("evaluator()"),Ut=l(` using the default task name
`),Le=s("code"),Ht=l("text-classification"),Wt=l(" or with a "),Me=s("code"),Gt=l('"sentiment-analysis"'),Jt=l(` alias.
Methods in this class assume a data format compatible with the `),Ae=s("code"),Kt=l("TextClassificationPipeline"),Qt=l(` - a single textual
feature as input and a categorical label as output.`),Xt=c(),M=s("div"),$(le.$$.fragment),Yt=c(),ze=s("p"),Zt=l("Compute the metric for a given pipeline and dataset combination."),ea=c(),$(G.$$.fragment),Xe=c(),I=s("div"),$(ie.$$.fragment),ta=c(),N=s("p"),aa=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),ve=s("a"),oa=l("evaluator()"),sa=l(` using the default task name
`),Fe=s("code"),na=l("image-classification"),ra=l(`.
Methods in this class assume a data format compatible with the `),Oe=s("code"),la=l("ImageClassificationPipeline"),ia=l("."),ca=c(),A=s("div"),$(ce.$$.fragment),da=c(),Se=s("p"),pa=l("Compute the metric for a given pipeline and dataset combination."),ua=c(),$(J.$$.fragment),this.h()},l(t){const m=Wa('[data-svelte="svelte-1phssyn"]',document.head);u=n(m,"META",{name:!0,content:!0}),m.forEach(a),j=d(t),h=n(t,"H1",{class:!0});var de=r(h);p=n(de,"A",{id:!0,class:!0,href:!0});var Be=r(p);_=n(Be,"SPAN",{});var Ve=r(_);y(o.$$.fragment,Ve),Ve.forEach(a),Be.forEach(a),g=d(de),we=n(de,"SPAN",{});var ga=r(we);rt=i(ga,"Evaluator"),ga.forEach(a),de.forEach(a),Re=d(t),pe=n(t,"P",{});var va=r(pe);lt=i(va,"The evaluator classes for automatic evaluation."),va.forEach(a),Ue=d(t),F=n(t,"H2",{class:!0});var Ze=r(F);S=n(Ze,"A",{id:!0,class:!0,href:!0});var _a=r(S);xe=n(_a,"SPAN",{});var ba=r(xe);y(X.$$.fragment,ba),ba.forEach(a),_a.forEach(a),it=d(Ze),qe=n(Ze,"SPAN",{});var $a=r(qe);ct=i($a,"Evaluator classes"),$a.forEach(a),Ze.forEach(a),He=d(t),ue=n(t,"P",{});var ya=r(ue);dt=i(ya,"The main entry point for using the evaluator:"),ya.forEach(a),We=d(t),C=n(t,"DIV",{class:!0});var _e=r(C);y(Y.$$.fragment,_e),pt=d(_e),P=n(_e,"P",{});var K=r(P);ut=i(K,"Utility factory method to build an "),me=n(K,"A",{href:!0});var Ea=r(me);mt=i(Ea,"Evaluator"),Ea.forEach(a),ft=i(K,`.
Evaluators encapsulate a task and a default metric name. They leverage `),je=n(K,"CODE",{});var wa=r(je);ht=i(wa,"pipeline"),wa.forEach(a),gt=i(K," functionalify from "),ke=n(K,"CODE",{});var xa=r(ke);vt=i(xa,"transformers"),xa.forEach(a),_t=i(K,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),K.forEach(a),bt=d(_e),y(B.$$.fragment,_e),_e.forEach(a),Ge=d(t),fe=n(t,"P",{});var qa=r(fe);$t=i(qa,"The base class for all evaluator classes:"),qa.forEach(a),Je=d(t),b=n(t,"DIV",{class:!0});var k=r(b);y(Z.$$.fragment,k),yt=d(k),Te=n(k,"P",{});var ja=r(Te);Et=i(ja,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),ja.forEach(a),wt=d(k),V=n(k,"DIV",{class:!0});var et=r(V);y(ee.$$.fragment,et),xt=d(et),Ce=n(et,"P",{});var ka=r(Ce);qt=i(ka,"Compute and return metrics."),ka.forEach(a),et.forEach(a),jt=d(k),R=n(k,"DIV",{class:!0});var tt=r(R);y(te.$$.fragment,tt),kt=d(tt),ae=n(tt,"P",{});var at=r(ae);Tt=i(at,"A core method of the "),Pe=n(at,"CODE",{});var Ta=r(Pe);Ct=i(Ta,"Evaluator"),Ta.forEach(a),Pt=i(at," class, which processes the pipeline outputs for compatibility with the metric."),at.forEach(a),tt.forEach(a),Dt=d(k),U=n(k,"DIV",{class:!0});var ot=r(U);y(oe.$$.fragment,ot),It=d(ot),De=n(ot,"P",{});var Ca=r(De);Nt=i(Ca,"Prepare data."),Ca.forEach(a),ot.forEach(a),Lt=d(k),H=n(k,"DIV",{class:!0});var st=r(H);y(se.$$.fragment,st),Mt=d(st),Ie=n(st,"P",{});var Pa=r(Ie);At=i(Pa,"Prepare metric."),Pa.forEach(a),st.forEach(a),zt=d(k),W=n(k,"DIV",{class:!0});var nt=r(W);y(ne.$$.fragment,nt),Ft=d(nt),Ne=n(nt,"P",{});var Da=r(Ne);Ot=i(Da,"Prepare pipeline."),Da.forEach(a),nt.forEach(a),k.forEach(a),Ke=d(t),he=n(t,"P",{});var Ia=r(he);St=i(Ia,"The class for text classification evaluation:"),Ia.forEach(a),Qe=d(t),D=n(t,"DIV",{class:!0});var be=r(D);y(re.$$.fragment,be),Bt=d(be),T=n(be,"P",{});var z=r(T);Vt=i(z,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),ge=n(z,"A",{href:!0});var Na=r(ge);Rt=i(Na,"evaluator()"),Na.forEach(a),Ut=i(z,` using the default task name
`),Le=n(z,"CODE",{});var La=r(Le);Ht=i(La,"text-classification"),La.forEach(a),Wt=i(z," or with a "),Me=n(z,"CODE",{});var Ma=r(Me);Gt=i(Ma,'"sentiment-analysis"'),Ma.forEach(a),Jt=i(z,` alias.
Methods in this class assume a data format compatible with the `),Ae=n(z,"CODE",{});var Aa=r(Ae);Kt=i(Aa,"TextClassificationPipeline"),Aa.forEach(a),Qt=i(z,` - a single textual
feature as input and a categorical label as output.`),z.forEach(a),Xt=d(be),M=n(be,"DIV",{class:!0});var $e=r(M);y(le.$$.fragment,$e),Yt=d($e),ze=n($e,"P",{});var za=r(ze);Zt=i(za,"Compute the metric for a given pipeline and dataset combination."),za.forEach(a),ea=d($e),y(G.$$.fragment,$e),$e.forEach(a),be.forEach(a),Xe=d(t),I=n(t,"DIV",{class:!0});var ye=r(I);y(ie.$$.fragment,ye),ta=d(ye),N=n(ye,"P",{});var Q=r(N);aa=i(Q,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),ve=n(Q,"A",{href:!0});var Fa=r(ve);oa=i(Fa,"evaluator()"),Fa.forEach(a),sa=i(Q,` using the default task name
`),Fe=n(Q,"CODE",{});var Oa=r(Fe);na=i(Oa,"image-classification"),Oa.forEach(a),ra=i(Q,`.
Methods in this class assume a data format compatible with the `),Oe=n(Q,"CODE",{});var Sa=r(Oe);la=i(Sa,"ImageClassificationPipeline"),Sa.forEach(a),ia=i(Q,"."),Q.forEach(a),ca=d(ye),A=n(ye,"DIV",{class:!0});var Ee=r(A);y(ce.$$.fragment,Ee),da=d(Ee),Se=n(Ee,"P",{});var Ba=r(Se);pa=i(Ba,"Compute the metric for a given pipeline and dataset combination."),Ba.forEach(a),ua=d(Ee),y(J.$$.fragment,Ee),Ee.forEach(a),ye.forEach(a),this.h()},h(){v(u,"name","hf:doc:metadata"),v(u,"content",JSON.stringify(Ya)),v(p,"id","evaluator"),v(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(p,"href","#evaluator"),v(h,"class","relative group"),v(S,"id","evaluate.evaluator"),v(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(S,"href","#evaluate.evaluator"),v(F,"class","relative group"),v(me,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.Evaluator"),v(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(ge,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),v(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(ve,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),v(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){e(document.head,u),f(t,j,m),f(t,h,m),e(h,p),e(p,_),E(o,_,null),e(h,g),e(h,we),e(we,rt),f(t,Re,m),f(t,pe,m),e(pe,lt),f(t,Ue,m),f(t,F,m),e(F,S),e(S,xe),E(X,xe,null),e(F,it),e(F,qe),e(qe,ct),f(t,He,m),f(t,ue,m),e(ue,dt),f(t,We,m),f(t,C,m),E(Y,C,null),e(C,pt),e(C,P),e(P,ut),e(P,me),e(me,mt),e(P,ft),e(P,je),e(je,ht),e(P,gt),e(P,ke),e(ke,vt),e(P,_t),e(C,bt),E(B,C,null),f(t,Ge,m),f(t,fe,m),e(fe,$t),f(t,Je,m),f(t,b,m),E(Z,b,null),e(b,yt),e(b,Te),e(Te,Et),e(b,wt),e(b,V),E(ee,V,null),e(V,xt),e(V,Ce),e(Ce,qt),e(b,jt),e(b,R),E(te,R,null),e(R,kt),e(R,ae),e(ae,Tt),e(ae,Pe),e(Pe,Ct),e(ae,Pt),e(b,Dt),e(b,U),E(oe,U,null),e(U,It),e(U,De),e(De,Nt),e(b,Lt),e(b,H),E(se,H,null),e(H,Mt),e(H,Ie),e(Ie,At),e(b,zt),e(b,W),E(ne,W,null),e(W,Ft),e(W,Ne),e(Ne,Ot),f(t,Ke,m),f(t,he,m),e(he,St),f(t,Qe,m),f(t,D,m),E(re,D,null),e(D,Bt),e(D,T),e(T,Vt),e(T,ge),e(ge,Rt),e(T,Ut),e(T,Le),e(Le,Ht),e(T,Wt),e(T,Me),e(Me,Gt),e(T,Jt),e(T,Ae),e(Ae,Kt),e(T,Qt),e(D,Xt),e(D,M),E(le,M,null),e(M,Yt),e(M,ze),e(ze,Zt),e(M,ea),E(G,M,null),f(t,Xe,m),f(t,I,m),E(ie,I,null),e(I,ta),e(I,N),e(N,aa),e(N,ve),e(ve,oa),e(N,sa),e(N,Fe),e(Fe,na),e(N,ra),e(N,Oe),e(Oe,la),e(N,ia),e(I,ca),e(I,A),E(ce,A,null),e(A,da),e(A,Se),e(Se,pa),e(A,ua),E(J,A,null),Ye=!0},p(t,[m]){const de={};m&2&&(de.$$scope={dirty:m,ctx:t}),B.$set(de);const Be={};m&2&&(Be.$$scope={dirty:m,ctx:t}),G.$set(Be);const Ve={};m&2&&(Ve.$$scope={dirty:m,ctx:t}),J.$set(Ve)},i(t){Ye||(w(o.$$.fragment,t),w(X.$$.fragment,t),w(Y.$$.fragment,t),w(B.$$.fragment,t),w(Z.$$.fragment,t),w(ee.$$.fragment,t),w(te.$$.fragment,t),w(oe.$$.fragment,t),w(se.$$.fragment,t),w(ne.$$.fragment,t),w(re.$$.fragment,t),w(le.$$.fragment,t),w(G.$$.fragment,t),w(ie.$$.fragment,t),w(ce.$$.fragment,t),w(J.$$.fragment,t),Ye=!0)},o(t){x(o.$$.fragment,t),x(X.$$.fragment,t),x(Y.$$.fragment,t),x(B.$$.fragment,t),x(Z.$$.fragment,t),x(ee.$$.fragment,t),x(te.$$.fragment,t),x(oe.$$.fragment,t),x(se.$$.fragment,t),x(ne.$$.fragment,t),x(re.$$.fragment,t),x(le.$$.fragment,t),x(G.$$.fragment,t),x(ie.$$.fragment,t),x(ce.$$.fragment,t),x(J.$$.fragment,t),Ye=!1},d(t){a(u),t&&a(j),t&&a(h),q(o),t&&a(Re),t&&a(pe),t&&a(Ue),t&&a(F),q(X),t&&a(He),t&&a(ue),t&&a(We),t&&a(C),q(Y),q(B),t&&a(Ge),t&&a(fe),t&&a(Je),t&&a(b),q(Z),q(ee),q(te),q(oe),q(se),q(ne),t&&a(Ke),t&&a(he),t&&a(Qe),t&&a(D),q(re),q(le),q(G),t&&a(Xe),t&&a(I),q(ie),q(ce),q(J)}}}const Ya={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"}],title:"Evaluator"};function Za(O){return Ga(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class no extends Ra{constructor(u){super();Ua(this,u,Za,Xa,Ha,{})}}export{no as default,Ya as metadata};
