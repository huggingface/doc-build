import{S as pr,i as ur,s as mr,e as o,k as u,w as b,t as c,M as fr,c as n,d as a,m,a as r,x as w,h as d,b as g,G as e,g as $,y,q as E,o as q,B as k,v as gr,L as De}from"../../chunks/vendor-hf-doc-builder.js";import{T as fn}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ie}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as mt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Ne}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function hr(j){let l,v,i,p,_;return p=new Ie({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(p.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=d(f,"Examples:"),f.forEach(a),i=m(s),w(p.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(p,s,f),_=!0},p:De,i(s){_||(E(p.$$.fragment,s),_=!0)},o(s){q(p.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(p,s)}}}function vr(j){let l,v,i,p,_;return p=new Ie({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("image-classification")
data = load_dataset("beans", split="test[:40]")
results = task_evaluator.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    label_column="labels",
    metric="accuracy",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap"
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:40]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(p.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=d(f,"Examples:"),f.forEach(a),i=m(s),w(p.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(p,s,f),_=!0},p:De,i(s){_||(E(p.$$.fragment,s),_=!0)},o(s){q(p.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(p,s)}}}function _r(j){let l,v,i,p,_;return p=new Ie({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;sshleifer/tiny-distilbert-base-cased-distilled-squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(p.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=d(f,"Examples:"),f.forEach(a),i=m(s),w(p.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(p,s,f),_=!0},p:De,i(s){_||(E(p.$$.fragment,s),_=!0)},o(s){q(p.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(p,s)}}}function $r(j){let l,v,i,p,_;return{c(){l=o("p"),v=c("Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass "),i=o("code"),p=c("squad_v2_format=True"),_=c(` to
the compute() call.`)},l(s){l=n(s,"P",{});var f=r(l);v=d(f,"Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass "),i=n(f,"CODE",{});var F=r(i);p=d(F,"squad_v2_format=True"),F.forEach(a),_=d(f,` to
the compute() call.`),f.forEach(a)},m(s,f){$(s,l,f),e(l,v),e(l,i),e(i,p),e(l,_)},d(s){s&&a(l)}}}function br(j){let l,v;return l=new Ie({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="mrm8488/bert-tiny-finetuned-squadv2",
    data=data,
    metric="squad_v2",
    squad_v2_format=True,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad_v2&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad_v2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    squad_v2_format=<span class="hljs-literal">True</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){b(l.$$.fragment)},l(i){w(l.$$.fragment,i)},m(i,p){y(l,i,p),v=!0},p:De,i(i){v||(E(l.$$.fragment,i),v=!0)},o(i){q(l.$$.fragment,i),v=!1},d(i){k(l,i)}}}function wr(j){let l,v,i,p,_;return p=new Ie({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = task_evaluator.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(p.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=d(f,"Examples:"),f.forEach(a),i=m(s),w(p.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(p,s,f),_=!0},p:De,i(s){_||(E(p.$$.fragment,s),_=!0)},o(s){q(p.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(p,s)}}}function yr(j){let l,v,i,p,_;return p=new Ie({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("token-classification")
data = load_dataset("conll2003", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="elastic/distilbert-base-uncased-finetuned-conll03-english",
    data=data,
    metric="seqeval",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;token-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;conll2003&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;elastic/distilbert-base-uncased-finetuned-conll03-english&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;seqeval&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){l=o("p"),v=c("Examples:"),i=u(),b(p.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=d(f,"Examples:"),f.forEach(a),i=m(s),w(p.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(p,s,f),_=!0},p:De,i(s){_||(E(p.$$.fragment,s),_=!0)},o(s){q(p.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(p,s)}}}function Er(j){let l,v,i,p,_;return p=new Ie({props:{code:`dataset = Dataset.from_dict(
    mapping={
        "tokens": [["New", "York", "is", "a", "city", "and", "Felix", "a", "person", "."]],
        "ner_tags": [[1, 2, 0, 0, 0, 0, 3, 0, 0, 0]],
    },
    features=Features({
        "tokens": Sequence(feature=Value(dtype="string")),
        "ner_tags": Sequence(feature=ClassLabel(names=["O", "B-LOC", "I-LOC", "B-PER", "I-PER"])),
        }),
)`,highlighted:`dataset = Dataset.from_dict(
    mapping={
        <span class="hljs-string">&quot;tokens&quot;</span>: [[<span class="hljs-string">&quot;New&quot;</span>, <span class="hljs-string">&quot;York&quot;</span>, <span class="hljs-string">&quot;is&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;city&quot;</span>, <span class="hljs-string">&quot;and&quot;</span>, <span class="hljs-string">&quot;Felix&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;person&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]],
        <span class="hljs-string">&quot;ner_tags&quot;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]],
    },
    features=Features({
        <span class="hljs-string">&quot;tokens&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;string&quot;</span>)),
        <span class="hljs-string">&quot;ner_tags&quot;</span>: <span class="hljs-type">Sequence</span>(feature=ClassLabel(names=[<span class="hljs-string">&quot;O&quot;</span>, <span class="hljs-string">&quot;B-LOC&quot;</span>, <span class="hljs-string">&quot;I-LOC&quot;</span>, <span class="hljs-string">&quot;B-PER&quot;</span>, <span class="hljs-string">&quot;I-PER&quot;</span>])),
        }),
)`}}),{c(){l=o("p"),v=c("For example, the following dataset format is accepted by the evaluator:"),i=u(),b(p.$$.fragment)},l(s){l=n(s,"P",{});var f=r(l);v=d(f,"For example, the following dataset format is accepted by the evaluator:"),f.forEach(a),i=m(s),w(p.$$.fragment,s)},m(s,f){$(s,l,f),e(l,v),$(s,i,f),y(p,s,f),_=!0},p:De,i(s){_||(E(p.$$.fragment,s),_=!0)},o(s){q(p.$$.fragment,s),_=!1},d(s){s&&a(l),s&&a(i),k(p,s)}}}function qr(j){let l,v;return l=new Ne({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example-2",$$slots:{default:[Er]},$$scope:{ctx:j}}}),{c(){b(l.$$.fragment)},l(i){w(l.$$.fragment,i)},m(i,p){y(l,i,p),v=!0},p(i,p){const _={};p&2&&(_.$$scope={dirty:p,ctx:i}),l.$set(_)},i(i){v||(E(l.$$.fragment,i),v=!0)},o(i){q(l.$$.fragment,i),v=!1},d(i){k(l,i)}}}function kr(j){let l,v,i,p,_,s,f,F;return f=new Ie({props:{code:`dataset = Dataset.from_dict(
    mapping={
        "tokens": [["New York is a city and Felix a person."]],
        "starts": [[0, 23]],
        "ends": [[7, 27]],
        "ner_tags": [["LOC", "PER"]],
    },
    features=Features({
        "tokens": Value(dtype="string"),
        "starts": Sequence(feature=Value(dtype="int32")),
        "ends": Sequence(feature=Value(dtype="int32")),
        "ner_tags": Sequence(feature=Value(dtype="string")),
    }),
)`,highlighted:`dataset = Dataset.from_dict(
    mapping={
        <span class="hljs-string">&quot;tokens&quot;</span>: [[<span class="hljs-string">&quot;New York is a city and Felix a person.&quot;</span>]],
        <span class="hljs-string">&quot;starts&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">23</span>]],
        <span class="hljs-string">&quot;ends&quot;</span>: [[<span class="hljs-number">7</span>, <span class="hljs-number">27</span>]],
        <span class="hljs-string">&quot;ner_tags&quot;</span>: [[<span class="hljs-string">&quot;LOC&quot;</span>, <span class="hljs-string">&quot;PER&quot;</span>]],
    },
    features=Features({
        <span class="hljs-string">&quot;tokens&quot;</span>: Value(dtype=<span class="hljs-string">&quot;string&quot;</span>),
        <span class="hljs-string">&quot;starts&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;int32&quot;</span>)),
        <span class="hljs-string">&quot;ends&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;int32&quot;</span>)),
        <span class="hljs-string">&quot;ner_tags&quot;</span>: <span class="hljs-type">Sequence</span>(feature=Value(dtype=<span class="hljs-string">&quot;string&quot;</span>)),
    }),
)`}}),{c(){l=o("p"),v=c("For example, the following dataset format is "),i=o("strong"),p=c("not"),_=c(" accepted by the evaluator:"),s=u(),b(f.$$.fragment)},l(C){l=n(C,"P",{});var U=r(l);v=d(U,"For example, the following dataset format is "),i=n(U,"STRONG",{});var K=r(i);p=d(K,"not"),K.forEach(a),_=d(U," accepted by the evaluator:"),U.forEach(a),s=m(C),w(f.$$.fragment,C)},m(C,U){$(C,l,U),e(l,v),e(l,i),e(i,p),e(l,_),$(C,s,U),y(f,C,U),F=!0},p:De,i(C){F||(E(f.$$.fragment,C),F=!0)},o(C){q(f.$$.fragment,C),F=!1},d(C){C&&a(l),C&&a(s),k(f,C)}}}function xr(j){let l,v;return l=new Ne({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example-3",$$slots:{default:[kr]},$$scope:{ctx:j}}}),{c(){b(l.$$.fragment)},l(i){w(l.$$.fragment,i)},m(i,p){y(l,i,p),v=!0},p(i,p){const _={};p&2&&(_.$$scope={dirty:p,ctx:i}),l.$set(_)},i(i){v||(E(l.$$.fragment,i),v=!0)},o(i){q(l.$$.fragment,i),v=!1},d(i){k(l,i)}}}function jr(j){let l,v,i,p,_,s,f,F,C,U,K,Xa,$a,W,ne,Tt,Ae,Za,Ct,es,ba,ft,ts,wa,L,Fe,as,S,ss,gt,os,ns,Pt,rs,ls,Nt,is,cs,ds,re,ya,ht,ps,Ea,x,Ue,us,Dt,ms,fs,le,Oe,gs,It,hs,vs,ie,ze,_s,At,$s,bs,ce,Le,ws,Ft,ys,Es,de,Se,qs,Ut,ks,xs,pe,Me,js,Re,Ts,Ot,Cs,Ps,Ns,ue,Qe,Ds,zt,Is,As,me,Ve,Fs,Lt,Us,Os,fe,Be,zs,St,Ls,qa,X,ge,Mt,Ge,Ss,Rt,Ms,ka,Z,he,Qt,He,Rs,Vt,Qs,xa,M,Ye,Vs,R,Bs,vt,Gs,Hs,Bt,Ys,Js,Gt,Ks,Ws,Xs,V,Je,Zs,Ht,eo,to,ve,ja,ee,_e,Yt,Ke,ao,Jt,so,Ta,D,We,oo,Xe,no,$e,Kt,ro,lo,io,co,te,po,_t,uo,mo,Wt,fo,go,ho,Ze,vo,et,Xt,_o,$o,bo,A,tt,wo,Zt,yo,Eo,be,qo,we,ko,ye,Ca,ae,Ee,ea,at,xo,ta,jo,Pa,Q,st,To,O,Co,$t,Po,No,aa,Do,Io,sa,Ao,Fo,oa,Uo,Oo,zo,B,ot,Lo,na,So,Mo,qe,Na,se,ke,ra,nt,Ro,la,Qo,Da,I,rt,Vo,ia,Bo,Go,oe,Ho,bt,Yo,Jo,ca,Ko,Wo,Xo,lt,Zo,da,en,tn,an,N,it,sn,pa,on,nn,ct,rn,dt,ln,cn,dn,xe,pn,je,un,Te,Ia;return s=new mt({}),Ae=new mt({}),Fe=new P({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
<li><code>&quot;token-classification&quot;</code>: will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.TokenClassificationEvaluator">TokenClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/__init__.py#L85",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),re=new Ne({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[hr]},$$scope:{ctx:j}}}),Ue=new P({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L100"}}),Oe=new P({props:{name:"check_required_columns",anchor:"evaluate.Evaluator.check_required_columns",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"columns_names",val:": typing.Dict[str, str]"}],parametersDescription:[{anchor:"evaluate.Evaluator.check_required_columns.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>) &#x2014;
Specifies the dataset we will run evaluation on.`,name:"data"},{anchor:"evaluate.Evaluator.check_required_columns.columns_names",description:"<strong>columns_names</strong> (<code>List[str]</code>) &#x2014;",name:"columns_names"},{anchor:"evaluate.Evaluator.check_required_columns.List",description:"<strong>List</strong> of column names to check in the dataset. The keys are the arguments to the compute() method, &#x2014;",name:"List"},{anchor:"evaluate.Evaluator.check_required_columns.while",description:"<strong>while</strong> the values are the column names to check. &#x2014;",name:"while"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L287"}}),ze=new P({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:": typing.Dict"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L449"}}),Le=new P({props:{name:"get_dataset_split",anchor:"evaluate.Evaluator.get_dataset_split",parameters:[{name:"data",val:""},{name:"subset",val:" = None"},{name:"split",val:" = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.get_dataset_split.data",description:"<strong>data</strong> (<code>str</code>) &#x2014; Name of dataset",name:"data"},{anchor:"evaluate.Evaluator.get_dataset_split.subset",description:"<strong>subset</strong> (<code>str</code>) &#x2014; Name of config for datasets with multiple configurations (e.g. &#x2018;glue/cola&#x2019;)",name:"subset"},{anchor:"evaluate.Evaluator.get_dataset_split.split",description:"<strong>split</strong> (<code>str</code>, defaults to None) &#x2014; Split to use",name:"split"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L304",returnDescription:`
<p><code>str</code> containing which split to use</p>
`,returnType:`
<p><code>split</code></p>
`}}),Se=new P({props:{name:"load_data",anchor:"evaluate.Evaluator.load_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"subset",val:": str = None"},{name:"split",val:": str = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.load_data.data",description:"<strong>data</strong> (<code>Dataset</code> or <code>str</code>, defaults to None) &#x2014; Specifies the dataset we will run evaluation on. If it is of",name:"data"},{anchor:"evaluate.Evaluator.load_data.type",description:"<strong>type</strong> <code>str</code>, we treat it as the dataset name, and load it. Otherwise we assume it represents a pre-loaded dataset. &#x2014;",name:"type"},{anchor:"evaluate.Evaluator.load_data.subset",description:`<strong>subset</strong> (<code>str</code>, defaults to None) &#x2014; Specifies dataset subset to be passed to <code>name</code> in <code>load_dataset</code>. To be
used with datasets with several configurations (e.g. glue/sst2).`,name:"subset"},{anchor:"evaluate.Evaluator.load_data.split",description:`<strong>split</strong> (<code>str</code>, defaults to None) &#x2014;
User-defined dataset split by name (e.g. train, validation, test). Supports slice-split (test[:n]).
If not defined and data is a <code>str</code> type, will automatically select the best one via <code>choose_split()</code>.`,name:"split"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L321",returnDescription:`
<p>Loaded dataset which will be used for evaluation.</p>
`,returnType:`
<p>data (<code>Dataset</code>)</p>
`}}),Me=new P({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L209"}}),Qe=new P({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": Dataset"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>Dataset</code>) &#x2014; Specifies the dataset we will run evaluation on.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L348",returnDescription:`
<p>metric inputs.
<code>list</code>:  pipeline inputs.</p>
`,returnType:`
<p><code>dict</code></p>
`}}),Ve=new P({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L419",returnDescription:`
<p>The loaded metric.</p>
`}}),Be=new P({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"tokenizer",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"feature_extractor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"device",val:": int = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L367",returnDescription:`
<p>The initialized pipeline.</p>
`}}),Ge=new mt({}),He=new mt({}),Ye=new P({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/image_classification.py#L45"}}),Je=new P({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"subset",val:": typing.Optional[str] = None"},{name:"split",val:": typing.Optional[str] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"feature_extractor",val:": typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"device",val:": int = None"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'image'"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.split",description:`<strong>split</strong> (<code>str</code>, defaults to None) &#x2014;
Defines which dataset split to load. If None is passed, infers based on the <code>choose_split</code> function.`,name:"split"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.ImageClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.device",description:`<strong>device</strong> (<code>int</code>, defaults to <code>None</code>) &#x2014;
Device ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive
integer will run the model on the associated CUDA device ID. If<code>None</code> is provided it will be inferred and
CUDA:0 used if available, CPU otherwise.`,name:"device"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/image_classification.py#L64",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ve=new Ne({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[vr]},$$scope:{ctx:j}}}),Ke=new mt({}),We=new P({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/question_answering.py#L74"}}),tt=new P({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"subset",val:": typing.Optional[str] = None"},{name:"split",val:": typing.Optional[str] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"device",val:": int = None"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"},{name:"squad_v2_format",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.split",description:`<strong>split</strong> (<code>str</code>, defaults to None) &#x2014;
Defines which dataset split to load. If None is passed, infers based on the <code>choose_split</code> function.`,name:"split"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.device",description:`<strong>device</strong> (<code>int</code>, defaults to <code>None</code>) &#x2014;
Device ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive
integer will run the model on the associated CUDA device ID. If<code>None</code> is provided it will be inferred and
CUDA:0 used if available, CPU otherwise.`,name:"device"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/question_answering.py#L143",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),be=new Ne({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[_r]},$$scope:{ctx:j}}}),we=new fn({props:{$$slots:{default:[$r]},$$scope:{ctx:j}}}),ye=new Ne({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example-2",$$slots:{default:[br]},$$scope:{ctx:j}}}),at=new mt({}),st=new P({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/text_classification.py#L47"}}),ot=new P({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"subset",val:": typing.Optional[str] = None"},{name:"split",val:": typing.Optional[str] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"feature_extractor",val:": typing.Union[str, ForwardRef('FeatureExtractionMixin'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"device",val:": int = None"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'text'"},{name:"second_input_column",val:": typing.Optional[str] = None"},{name:"label_column",val:": str = 'label'"},{name:"label_mapping",val:": typing.Union[typing.Dict[str, numbers.Number], NoneType] = None"}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.split",description:`<strong>split</strong> (<code>str</code>, defaults to None) &#x2014;
Defines which dataset split to load. If None is passed, infers based on the <code>choose_split</code> function.`,name:"split"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.TextClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.device",description:`<strong>device</strong> (<code>int</code>, defaults to <code>None</code>) &#x2014;
Device ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive
integer will run the model on the associated CUDA device ID. If<code>None</code> is provided it will be inferred and
CUDA:0 used if available, CPU otherwise.`,name:"device"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/text_classification.py#L85",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),qe=new Ne({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[wr]},$$scope:{ctx:j}}}),nt=new mt({}),rt=new P({props:{name:"class evaluate.TokenClassificationEvaluator",anchor:"evaluate.TokenClassificationEvaluator",parameters:[{name:"task",val:" = 'token-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/token_classification.py#L87"}}),it=new P({props:{name:"compute",anchor:"evaluate.TokenClassificationEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"subset",val:": typing.Optional[str] = None"},{name:"split",val:": str = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"device",val:": typing.Optional[int] = None"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"input_column",val:": str = 'tokens'"},{name:"label_column",val:": str = 'ner_tags'"},{name:"join_by",val:": typing.Optional[str] = ' '"}],parametersDescription:[{anchor:"evaluate.TokenClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TokenClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.TokenClassificationEvaluator.compute.split",description:`<strong>split</strong> (<code>str</code>, defaults to None) &#x2014;
Defines which dataset split to load. If None is passed, infers based on the <code>choose_split</code> function.`,name:"split"},{anchor:"evaluate.TokenClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.TokenClassificationEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.TokenClassificationEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.TokenClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TokenClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TokenClassificationEvaluator.compute.device",description:`<strong>device</strong> (<code>int</code>, defaults to <code>None</code>) &#x2014;
Device ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive
integer will run the model on the associated CUDA device ID. If<code>None</code> is provided it will be inferred and
CUDA:0 used if available, CPU otherwise.`,name:"device"},{anchor:"evaluate.TokenClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/token_classification.py#L210",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),xe=new Ne({props:{anchor:"evaluate.TokenClassificationEvaluator.compute.example",$$slots:{default:[yr]},$$scope:{ctx:j}}}),je=new fn({props:{$$slots:{default:[qr]},$$scope:{ctx:j}}}),Te=new fn({props:{warning:!0,$$slots:{default:[xr]},$$scope:{ctx:j}}}),{c(){l=o("meta"),v=u(),i=o("h1"),p=o("a"),_=o("span"),b(s.$$.fragment),f=u(),F=o("span"),C=c("Evaluator"),U=u(),K=o("p"),Xa=c("The evaluator classes for automatic evaluation."),$a=u(),W=o("h2"),ne=o("a"),Tt=o("span"),b(Ae.$$.fragment),Za=u(),Ct=o("span"),es=c("Evaluator classes"),ba=u(),ft=o("p"),ts=c("The main entry point for using the evaluator:"),wa=u(),L=o("div"),b(Fe.$$.fragment),as=u(),S=o("p"),ss=c("Utility factory method to build an "),gt=o("a"),os=c("Evaluator"),ns=c(`.
Evaluators encapsulate a task and a default metric name. They leverage `),Pt=o("code"),rs=c("pipeline"),ls=c(" functionalify from "),Nt=o("code"),is=c("transformers"),cs=c(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),ds=u(),b(re.$$.fragment),ya=u(),ht=o("p"),ps=c("The base class for all evaluator classes:"),Ea=u(),x=o("div"),b(Ue.$$.fragment),us=u(),Dt=o("p"),ms=c(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),fs=u(),le=o("div"),b(Oe.$$.fragment),gs=u(),It=o("p"),hs=c("Ensure the columns required for the evaluation are present in the dataset."),vs=u(),ie=o("div"),b(ze.$$.fragment),_s=u(),At=o("p"),$s=c("Compute and return metrics."),bs=u(),ce=o("div"),b(Le.$$.fragment),ws=u(),Ft=o("p"),ys=c("Infers which split to use if None is given."),Es=u(),de=o("div"),b(Se.$$.fragment),qs=u(),Ut=o("p"),ks=c("Load dataset with given subset and split."),xs=u(),pe=o("div"),b(Me.$$.fragment),js=u(),Re=o("p"),Ts=c("A core method of the "),Ot=o("code"),Cs=c("Evaluator"),Ps=c(" class, which processes the pipeline outputs for compatibility with the metric."),Ns=u(),ue=o("div"),b(Qe.$$.fragment),Ds=u(),zt=o("p"),Is=c("Prepare data."),As=u(),me=o("div"),b(Ve.$$.fragment),Fs=u(),Lt=o("p"),Us=c("Prepare metric."),Os=u(),fe=o("div"),b(Be.$$.fragment),zs=u(),St=o("p"),Ls=c("Prepare pipeline."),qa=u(),X=o("h2"),ge=o("a"),Mt=o("span"),b(Ge.$$.fragment),Ss=u(),Rt=o("span"),Ms=c("The task specific evaluators"),ka=u(),Z=o("h3"),he=o("a"),Qt=o("span"),b(He.$$.fragment),Rs=u(),Vt=o("span"),Qs=c("ImageClassificationEvaluator"),xa=u(),M=o("div"),b(Ye.$$.fragment),Vs=u(),R=o("p"),Bs=c(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),vt=o("a"),Gs=c("evaluator()"),Hs=c(` using the default task name
`),Bt=o("code"),Ys=c("image-classification"),Js=c(`.
Methods in this class assume a data format compatible with the `),Gt=o("code"),Ks=c("ImageClassificationPipeline"),Ws=c("."),Xs=u(),V=o("div"),b(Je.$$.fragment),Zs=u(),Ht=o("p"),eo=c("Compute the metric for a given pipeline and dataset combination."),to=u(),b(ve.$$.fragment),ja=u(),ee=o("h3"),_e=o("a"),Yt=o("span"),b(Ke.$$.fragment),ao=u(),Jt=o("span"),so=c("QuestionAnsweringEvaluator"),Ta=u(),D=o("div"),b(We.$$.fragment),oo=u(),Xe=o("p"),no=c(`Question answering evaluator. This evaluator handles
`),$e=o("a"),Kt=o("strong"),ro=c("extractive"),lo=c(" question answering"),io=c(`,
where the answer to the question is extracted from a context.`),co=u(),te=o("p"),po=c("This question answering evaluator can currently be loaded from "),_t=o("a"),uo=c("evaluator()"),mo=c(` using the default task name
`),Wt=o("code"),fo=c("question-answering"),go=c("."),ho=u(),Ze=o("p"),vo=c(`Methods in this class assume a data format compatible with the
`),et=o("a"),Xt=o("code"),_o=c("QuestionAnsweringPipeline"),$o=c("."),bo=u(),A=o("div"),b(tt.$$.fragment),wo=u(),Zt=o("p"),yo=c("Compute the metric for a given pipeline and dataset combination."),Eo=u(),b(be.$$.fragment),qo=u(),b(we.$$.fragment),ko=u(),b(ye.$$.fragment),Ca=u(),ae=o("h3"),Ee=o("a"),ea=o("span"),b(at.$$.fragment),xo=u(),ta=o("span"),jo=c("TextClassificationEvaluator"),Pa=u(),Q=o("div"),b(st.$$.fragment),To=u(),O=o("p"),Co=c(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),$t=o("a"),Po=c("evaluator()"),No=c(` using the default task name
`),aa=o("code"),Do=c("text-classification"),Io=c(" or with a "),sa=o("code"),Ao=c('"sentiment-analysis"'),Fo=c(` alias.
Methods in this class assume a data format compatible with the `),oa=o("code"),Uo=c("TextClassificationPipeline"),Oo=c(` - a single textual
feature as input and a categorical label as output.`),zo=u(),B=o("div"),b(ot.$$.fragment),Lo=u(),na=o("p"),So=c("Compute the metric for a given pipeline and dataset combination."),Mo=u(),b(qe.$$.fragment),Na=u(),se=o("h3"),ke=o("a"),ra=o("span"),b(nt.$$.fragment),Ro=u(),la=o("span"),Qo=c("TokenClassificationEvaluator"),Da=u(),I=o("div"),b(rt.$$.fragment),Vo=u(),ia=o("p"),Bo=c("Token classification evaluator."),Go=u(),oe=o("p"),Ho=c("This token classification evaluator can currently be loaded from "),bt=o("a"),Yo=c("evaluator()"),Jo=c(` using the default task name
`),ca=o("code"),Ko=c("token-classification"),Wo=c("."),Xo=u(),lt=o("p"),Zo=c("Methods in this class assume a data format compatible with the "),da=o("code"),en=c("TokenClassificationPipeline"),tn=c("."),an=u(),N=o("div"),b(it.$$.fragment),sn=u(),pa=o("p"),on=c("Compute the metric for a given pipeline and dataset combination."),nn=u(),ct=o("p"),rn=c("The dataset input and label columns are expected to be formatted as a list of words and a list of labels respectively, following "),dt=o("a"),ln=c("conll2003 dataset"),cn=c(". Datasets whose inputs are single strings, and labels are a list of offset are not supported."),dn=u(),b(xe.$$.fragment),pn=u(),b(je.$$.fragment),un=u(),b(Te.$$.fragment),this.h()},l(t){const h=fr('[data-svelte="svelte-1phssyn"]',document.head);l=n(h,"META",{name:!0,content:!0}),h.forEach(a),v=m(t),i=n(t,"H1",{class:!0});var pt=r(i);p=n(pt,"A",{id:!0,class:!0,href:!0});var ua=r(p);_=n(ua,"SPAN",{});var ma=r(_);w(s.$$.fragment,ma),ma.forEach(a),ua.forEach(a),f=m(pt),F=n(pt,"SPAN",{});var fa=r(F);C=d(fa,"Evaluator"),fa.forEach(a),pt.forEach(a),U=m(t),K=n(t,"P",{});var ga=r(K);Xa=d(ga,"The evaluator classes for automatic evaluation."),ga.forEach(a),$a=m(t),W=n(t,"H2",{class:!0});var ut=r(W);ne=n(ut,"A",{id:!0,class:!0,href:!0});var ha=r(ne);Tt=n(ha,"SPAN",{});var va=r(Tt);w(Ae.$$.fragment,va),va.forEach(a),ha.forEach(a),Za=m(ut),Ct=n(ut,"SPAN",{});var _a=r(Ct);es=d(_a,"Evaluator classes"),_a.forEach(a),ut.forEach(a),ba=m(t),ft=n(t,"P",{});var gn=r(ft);ts=d(gn,"The main entry point for using the evaluator:"),gn.forEach(a),wa=m(t),L=n(t,"DIV",{class:!0});var wt=r(L);w(Fe.$$.fragment,wt),as=m(wt),S=n(wt,"P",{});var Ce=r(S);ss=d(Ce,"Utility factory method to build an "),gt=n(Ce,"A",{href:!0});var hn=r(gt);os=d(hn,"Evaluator"),hn.forEach(a),ns=d(Ce,`.
Evaluators encapsulate a task and a default metric name. They leverage `),Pt=n(Ce,"CODE",{});var vn=r(Pt);rs=d(vn,"pipeline"),vn.forEach(a),ls=d(Ce," functionalify from "),Nt=n(Ce,"CODE",{});var _n=r(Nt);is=d(_n,"transformers"),_n.forEach(a),cs=d(Ce,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),Ce.forEach(a),ds=m(wt),w(re.$$.fragment,wt),wt.forEach(a),ya=m(t),ht=n(t,"P",{});var $n=r(ht);ps=d($n,"The base class for all evaluator classes:"),$n.forEach(a),Ea=m(t),x=n(t,"DIV",{class:!0});var T=r(x);w(Ue.$$.fragment,T),us=m(T),Dt=n(T,"P",{});var bn=r(Dt);ms=d(bn,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),bn.forEach(a),fs=m(T),le=n(T,"DIV",{class:!0});var Aa=r(le);w(Oe.$$.fragment,Aa),gs=m(Aa),It=n(Aa,"P",{});var wn=r(It);hs=d(wn,"Ensure the columns required for the evaluation are present in the dataset."),wn.forEach(a),Aa.forEach(a),vs=m(T),ie=n(T,"DIV",{class:!0});var Fa=r(ie);w(ze.$$.fragment,Fa),_s=m(Fa),At=n(Fa,"P",{});var yn=r(At);$s=d(yn,"Compute and return metrics."),yn.forEach(a),Fa.forEach(a),bs=m(T),ce=n(T,"DIV",{class:!0});var Ua=r(ce);w(Le.$$.fragment,Ua),ws=m(Ua),Ft=n(Ua,"P",{});var En=r(Ft);ys=d(En,"Infers which split to use if None is given."),En.forEach(a),Ua.forEach(a),Es=m(T),de=n(T,"DIV",{class:!0});var Oa=r(de);w(Se.$$.fragment,Oa),qs=m(Oa),Ut=n(Oa,"P",{});var qn=r(Ut);ks=d(qn,"Load dataset with given subset and split."),qn.forEach(a),Oa.forEach(a),xs=m(T),pe=n(T,"DIV",{class:!0});var za=r(pe);w(Me.$$.fragment,za),js=m(za),Re=n(za,"P",{});var La=r(Re);Ts=d(La,"A core method of the "),Ot=n(La,"CODE",{});var kn=r(Ot);Cs=d(kn,"Evaluator"),kn.forEach(a),Ps=d(La," class, which processes the pipeline outputs for compatibility with the metric."),La.forEach(a),za.forEach(a),Ns=m(T),ue=n(T,"DIV",{class:!0});var Sa=r(ue);w(Qe.$$.fragment,Sa),Ds=m(Sa),zt=n(Sa,"P",{});var xn=r(zt);Is=d(xn,"Prepare data."),xn.forEach(a),Sa.forEach(a),As=m(T),me=n(T,"DIV",{class:!0});var Ma=r(me);w(Ve.$$.fragment,Ma),Fs=m(Ma),Lt=n(Ma,"P",{});var jn=r(Lt);Us=d(jn,"Prepare metric."),jn.forEach(a),Ma.forEach(a),Os=m(T),fe=n(T,"DIV",{class:!0});var Ra=r(fe);w(Be.$$.fragment,Ra),zs=m(Ra),St=n(Ra,"P",{});var Tn=r(St);Ls=d(Tn,"Prepare pipeline."),Tn.forEach(a),Ra.forEach(a),T.forEach(a),qa=m(t),X=n(t,"H2",{class:!0});var Qa=r(X);ge=n(Qa,"A",{id:!0,class:!0,href:!0});var Cn=r(ge);Mt=n(Cn,"SPAN",{});var Pn=r(Mt);w(Ge.$$.fragment,Pn),Pn.forEach(a),Cn.forEach(a),Ss=m(Qa),Rt=n(Qa,"SPAN",{});var Nn=r(Rt);Ms=d(Nn,"The task specific evaluators"),Nn.forEach(a),Qa.forEach(a),ka=m(t),Z=n(t,"H3",{class:!0});var Va=r(Z);he=n(Va,"A",{id:!0,class:!0,href:!0});var Dn=r(he);Qt=n(Dn,"SPAN",{});var In=r(Qt);w(He.$$.fragment,In),In.forEach(a),Dn.forEach(a),Rs=m(Va),Vt=n(Va,"SPAN",{});var An=r(Vt);Qs=d(An,"ImageClassificationEvaluator"),An.forEach(a),Va.forEach(a),xa=m(t),M=n(t,"DIV",{class:!0});var yt=r(M);w(Ye.$$.fragment,yt),Vs=m(yt),R=n(yt,"P",{});var Pe=r(R);Bs=d(Pe,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),vt=n(Pe,"A",{href:!0});var Fn=r(vt);Gs=d(Fn,"evaluator()"),Fn.forEach(a),Hs=d(Pe,` using the default task name
`),Bt=n(Pe,"CODE",{});var Un=r(Bt);Ys=d(Un,"image-classification"),Un.forEach(a),Js=d(Pe,`.
Methods in this class assume a data format compatible with the `),Gt=n(Pe,"CODE",{});var On=r(Gt);Ks=d(On,"ImageClassificationPipeline"),On.forEach(a),Ws=d(Pe,"."),Pe.forEach(a),Xs=m(yt),V=n(yt,"DIV",{class:!0});var Et=r(V);w(Je.$$.fragment,Et),Zs=m(Et),Ht=n(Et,"P",{});var zn=r(Ht);eo=d(zn,"Compute the metric for a given pipeline and dataset combination."),zn.forEach(a),to=m(Et),w(ve.$$.fragment,Et),Et.forEach(a),yt.forEach(a),ja=m(t),ee=n(t,"H3",{class:!0});var Ba=r(ee);_e=n(Ba,"A",{id:!0,class:!0,href:!0});var Ln=r(_e);Yt=n(Ln,"SPAN",{});var Sn=r(Yt);w(Ke.$$.fragment,Sn),Sn.forEach(a),Ln.forEach(a),ao=m(Ba),Jt=n(Ba,"SPAN",{});var Mn=r(Jt);so=d(Mn,"QuestionAnsweringEvaluator"),Mn.forEach(a),Ba.forEach(a),Ta=m(t),D=n(t,"DIV",{class:!0});var G=r(D);w(We.$$.fragment,G),oo=m(G),Xe=n(G,"P",{});var Ga=r(Xe);no=d(Ga,`Question answering evaluator. This evaluator handles
`),$e=n(Ga,"A",{href:!0,rel:!0});var mn=r($e);Kt=n(mn,"STRONG",{});var Rn=r(Kt);ro=d(Rn,"extractive"),Rn.forEach(a),lo=d(mn," question answering"),mn.forEach(a),io=d(Ga,`,
where the answer to the question is extracted from a context.`),Ga.forEach(a),co=m(G),te=n(G,"P",{});var qt=r(te);po=d(qt,"This question answering evaluator can currently be loaded from "),_t=n(qt,"A",{href:!0});var Qn=r(_t);uo=d(Qn,"evaluator()"),Qn.forEach(a),mo=d(qt,` using the default task name
`),Wt=n(qt,"CODE",{});var Vn=r(Wt);fo=d(Vn,"question-answering"),Vn.forEach(a),go=d(qt,"."),qt.forEach(a),ho=m(G),Ze=n(G,"P",{});var Ha=r(Ze);vo=d(Ha,`Methods in this class assume a data format compatible with the
`),et=n(Ha,"A",{href:!0,rel:!0});var Bn=r(et);Xt=n(Bn,"CODE",{});var Gn=r(Xt);_o=d(Gn,"QuestionAnsweringPipeline"),Gn.forEach(a),Bn.forEach(a),$o=d(Ha,"."),Ha.forEach(a),bo=m(G),A=n(G,"DIV",{class:!0});var H=r(A);w(tt.$$.fragment,H),wo=m(H),Zt=n(H,"P",{});var Hn=r(Zt);yo=d(Hn,"Compute the metric for a given pipeline and dataset combination."),Hn.forEach(a),Eo=m(H),w(be.$$.fragment,H),qo=m(H),w(we.$$.fragment,H),ko=m(H),w(ye.$$.fragment,H),H.forEach(a),G.forEach(a),Ca=m(t),ae=n(t,"H3",{class:!0});var Ya=r(ae);Ee=n(Ya,"A",{id:!0,class:!0,href:!0});var Yn=r(Ee);ea=n(Yn,"SPAN",{});var Jn=r(ea);w(at.$$.fragment,Jn),Jn.forEach(a),Yn.forEach(a),xo=m(Ya),ta=n(Ya,"SPAN",{});var Kn=r(ta);jo=d(Kn,"TextClassificationEvaluator"),Kn.forEach(a),Ya.forEach(a),Pa=m(t),Q=n(t,"DIV",{class:!0});var kt=r(Q);w(st.$$.fragment,kt),To=m(kt),O=n(kt,"P",{});var Y=r(O);Co=d(Y,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),$t=n(Y,"A",{href:!0});var Wn=r($t);Po=d(Wn,"evaluator()"),Wn.forEach(a),No=d(Y,` using the default task name
`),aa=n(Y,"CODE",{});var Xn=r(aa);Do=d(Xn,"text-classification"),Xn.forEach(a),Io=d(Y," or with a "),sa=n(Y,"CODE",{});var Zn=r(sa);Ao=d(Zn,'"sentiment-analysis"'),Zn.forEach(a),Fo=d(Y,` alias.
Methods in this class assume a data format compatible with the `),oa=n(Y,"CODE",{});var er=r(oa);Uo=d(er,"TextClassificationPipeline"),er.forEach(a),Oo=d(Y,` - a single textual
feature as input and a categorical label as output.`),Y.forEach(a),zo=m(kt),B=n(kt,"DIV",{class:!0});var xt=r(B);w(ot.$$.fragment,xt),Lo=m(xt),na=n(xt,"P",{});var tr=r(na);So=d(tr,"Compute the metric for a given pipeline and dataset combination."),tr.forEach(a),Mo=m(xt),w(qe.$$.fragment,xt),xt.forEach(a),kt.forEach(a),Na=m(t),se=n(t,"H3",{class:!0});var Ja=r(se);ke=n(Ja,"A",{id:!0,class:!0,href:!0});var ar=r(ke);ra=n(ar,"SPAN",{});var sr=r(ra);w(nt.$$.fragment,sr),sr.forEach(a),ar.forEach(a),Ro=m(Ja),la=n(Ja,"SPAN",{});var or=r(la);Qo=d(or,"TokenClassificationEvaluator"),or.forEach(a),Ja.forEach(a),Da=m(t),I=n(t,"DIV",{class:!0});var J=r(I);w(rt.$$.fragment,J),Vo=m(J),ia=n(J,"P",{});var nr=r(ia);Bo=d(nr,"Token classification evaluator."),nr.forEach(a),Go=m(J),oe=n(J,"P",{});var jt=r(oe);Ho=d(jt,"This token classification evaluator can currently be loaded from "),bt=n(jt,"A",{href:!0});var rr=r(bt);Yo=d(rr,"evaluator()"),rr.forEach(a),Jo=d(jt,` using the default task name
`),ca=n(jt,"CODE",{});var lr=r(ca);Ko=d(lr,"token-classification"),lr.forEach(a),Wo=d(jt,"."),jt.forEach(a),Xo=m(J),lt=n(J,"P",{});var Ka=r(lt);Zo=d(Ka,"Methods in this class assume a data format compatible with the "),da=n(Ka,"CODE",{});var ir=r(da);en=d(ir,"TokenClassificationPipeline"),ir.forEach(a),tn=d(Ka,"."),Ka.forEach(a),an=m(J),N=n(J,"DIV",{class:!0});var z=r(N);w(it.$$.fragment,z),sn=m(z),pa=n(z,"P",{});var cr=r(pa);on=d(cr,"Compute the metric for a given pipeline and dataset combination."),cr.forEach(a),nn=m(z),ct=n(z,"P",{});var Wa=r(ct);rn=d(Wa,"The dataset input and label columns are expected to be formatted as a list of words and a list of labels respectively, following "),dt=n(Wa,"A",{href:!0,rel:!0});var dr=r(dt);ln=d(dr,"conll2003 dataset"),dr.forEach(a),cn=d(Wa,". Datasets whose inputs are single strings, and labels are a list of offset are not supported."),Wa.forEach(a),dn=m(z),w(xe.$$.fragment,z),pn=m(z),w(je.$$.fragment,z),un=m(z),w(Te.$$.fragment,z),z.forEach(a),J.forEach(a),this.h()},h(){g(l,"name","hf:doc:metadata"),g(l,"content",JSON.stringify(Tr)),g(p,"id","evaluator"),g(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(p,"href","#evaluator"),g(i,"class","relative group"),g(ne,"id","evaluate.evaluator"),g(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(ne,"href","#evaluate.evaluator"),g(W,"class","relative group"),g(gt,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.Evaluator"),g(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ge,"id","the-task-specific-evaluators"),g(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(ge,"href","#the-task-specific-evaluators"),g(X,"class","relative group"),g(he,"id","evaluate.ImageClassificationEvaluator"),g(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(he,"href","#evaluate.ImageClassificationEvaluator"),g(Z,"class","relative group"),g(vt,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),g(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(_e,"id","evaluate.QuestionAnsweringEvaluator"),g(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(_e,"href","#evaluate.QuestionAnsweringEvaluator"),g(ee,"class","relative group"),g($e,"href","https://huggingface.co/docs/transformers/task_summary#extractive-question-answering"),g($e,"rel","nofollow"),g(_t,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),g(et,"href","https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline"),g(et,"rel","nofollow"),g(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(Ee,"id","evaluate.TextClassificationEvaluator"),g(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(Ee,"href","#evaluate.TextClassificationEvaluator"),g(ae,"class","relative group"),g($t,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),g(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(ke,"id","evaluate.TokenClassificationEvaluator"),g(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(ke,"href","#evaluate.TokenClassificationEvaluator"),g(se,"class","relative group"),g(bt,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),g(dt,"href","https://huggingface.co/datasets/conll2003"),g(dt,"rel","nofollow"),g(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),g(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,l),$(t,v,h),$(t,i,h),e(i,p),e(p,_),y(s,_,null),e(i,f),e(i,F),e(F,C),$(t,U,h),$(t,K,h),e(K,Xa),$(t,$a,h),$(t,W,h),e(W,ne),e(ne,Tt),y(Ae,Tt,null),e(W,Za),e(W,Ct),e(Ct,es),$(t,ba,h),$(t,ft,h),e(ft,ts),$(t,wa,h),$(t,L,h),y(Fe,L,null),e(L,as),e(L,S),e(S,ss),e(S,gt),e(gt,os),e(S,ns),e(S,Pt),e(Pt,rs),e(S,ls),e(S,Nt),e(Nt,is),e(S,cs),e(L,ds),y(re,L,null),$(t,ya,h),$(t,ht,h),e(ht,ps),$(t,Ea,h),$(t,x,h),y(Ue,x,null),e(x,us),e(x,Dt),e(Dt,ms),e(x,fs),e(x,le),y(Oe,le,null),e(le,gs),e(le,It),e(It,hs),e(x,vs),e(x,ie),y(ze,ie,null),e(ie,_s),e(ie,At),e(At,$s),e(x,bs),e(x,ce),y(Le,ce,null),e(ce,ws),e(ce,Ft),e(Ft,ys),e(x,Es),e(x,de),y(Se,de,null),e(de,qs),e(de,Ut),e(Ut,ks),e(x,xs),e(x,pe),y(Me,pe,null),e(pe,js),e(pe,Re),e(Re,Ts),e(Re,Ot),e(Ot,Cs),e(Re,Ps),e(x,Ns),e(x,ue),y(Qe,ue,null),e(ue,Ds),e(ue,zt),e(zt,Is),e(x,As),e(x,me),y(Ve,me,null),e(me,Fs),e(me,Lt),e(Lt,Us),e(x,Os),e(x,fe),y(Be,fe,null),e(fe,zs),e(fe,St),e(St,Ls),$(t,qa,h),$(t,X,h),e(X,ge),e(ge,Mt),y(Ge,Mt,null),e(X,Ss),e(X,Rt),e(Rt,Ms),$(t,ka,h),$(t,Z,h),e(Z,he),e(he,Qt),y(He,Qt,null),e(Z,Rs),e(Z,Vt),e(Vt,Qs),$(t,xa,h),$(t,M,h),y(Ye,M,null),e(M,Vs),e(M,R),e(R,Bs),e(R,vt),e(vt,Gs),e(R,Hs),e(R,Bt),e(Bt,Ys),e(R,Js),e(R,Gt),e(Gt,Ks),e(R,Ws),e(M,Xs),e(M,V),y(Je,V,null),e(V,Zs),e(V,Ht),e(Ht,eo),e(V,to),y(ve,V,null),$(t,ja,h),$(t,ee,h),e(ee,_e),e(_e,Yt),y(Ke,Yt,null),e(ee,ao),e(ee,Jt),e(Jt,so),$(t,Ta,h),$(t,D,h),y(We,D,null),e(D,oo),e(D,Xe),e(Xe,no),e(Xe,$e),e($e,Kt),e(Kt,ro),e($e,lo),e(Xe,io),e(D,co),e(D,te),e(te,po),e(te,_t),e(_t,uo),e(te,mo),e(te,Wt),e(Wt,fo),e(te,go),e(D,ho),e(D,Ze),e(Ze,vo),e(Ze,et),e(et,Xt),e(Xt,_o),e(Ze,$o),e(D,bo),e(D,A),y(tt,A,null),e(A,wo),e(A,Zt),e(Zt,yo),e(A,Eo),y(be,A,null),e(A,qo),y(we,A,null),e(A,ko),y(ye,A,null),$(t,Ca,h),$(t,ae,h),e(ae,Ee),e(Ee,ea),y(at,ea,null),e(ae,xo),e(ae,ta),e(ta,jo),$(t,Pa,h),$(t,Q,h),y(st,Q,null),e(Q,To),e(Q,O),e(O,Co),e(O,$t),e($t,Po),e(O,No),e(O,aa),e(aa,Do),e(O,Io),e(O,sa),e(sa,Ao),e(O,Fo),e(O,oa),e(oa,Uo),e(O,Oo),e(Q,zo),e(Q,B),y(ot,B,null),e(B,Lo),e(B,na),e(na,So),e(B,Mo),y(qe,B,null),$(t,Na,h),$(t,se,h),e(se,ke),e(ke,ra),y(nt,ra,null),e(se,Ro),e(se,la),e(la,Qo),$(t,Da,h),$(t,I,h),y(rt,I,null),e(I,Vo),e(I,ia),e(ia,Bo),e(I,Go),e(I,oe),e(oe,Ho),e(oe,bt),e(bt,Yo),e(oe,Jo),e(oe,ca),e(ca,Ko),e(oe,Wo),e(I,Xo),e(I,lt),e(lt,Zo),e(lt,da),e(da,en),e(lt,tn),e(I,an),e(I,N),y(it,N,null),e(N,sn),e(N,pa),e(pa,on),e(N,nn),e(N,ct),e(ct,rn),e(ct,dt),e(dt,ln),e(ct,cn),e(N,dn),y(xe,N,null),e(N,pn),y(je,N,null),e(N,un),y(Te,N,null),Ia=!0},p(t,[h]){const pt={};h&2&&(pt.$$scope={dirty:h,ctx:t}),re.$set(pt);const ua={};h&2&&(ua.$$scope={dirty:h,ctx:t}),ve.$set(ua);const ma={};h&2&&(ma.$$scope={dirty:h,ctx:t}),be.$set(ma);const fa={};h&2&&(fa.$$scope={dirty:h,ctx:t}),we.$set(fa);const ga={};h&2&&(ga.$$scope={dirty:h,ctx:t}),ye.$set(ga);const ut={};h&2&&(ut.$$scope={dirty:h,ctx:t}),qe.$set(ut);const ha={};h&2&&(ha.$$scope={dirty:h,ctx:t}),xe.$set(ha);const va={};h&2&&(va.$$scope={dirty:h,ctx:t}),je.$set(va);const _a={};h&2&&(_a.$$scope={dirty:h,ctx:t}),Te.$set(_a)},i(t){Ia||(E(s.$$.fragment,t),E(Ae.$$.fragment,t),E(Fe.$$.fragment,t),E(re.$$.fragment,t),E(Ue.$$.fragment,t),E(Oe.$$.fragment,t),E(ze.$$.fragment,t),E(Le.$$.fragment,t),E(Se.$$.fragment,t),E(Me.$$.fragment,t),E(Qe.$$.fragment,t),E(Ve.$$.fragment,t),E(Be.$$.fragment,t),E(Ge.$$.fragment,t),E(He.$$.fragment,t),E(Ye.$$.fragment,t),E(Je.$$.fragment,t),E(ve.$$.fragment,t),E(Ke.$$.fragment,t),E(We.$$.fragment,t),E(tt.$$.fragment,t),E(be.$$.fragment,t),E(we.$$.fragment,t),E(ye.$$.fragment,t),E(at.$$.fragment,t),E(st.$$.fragment,t),E(ot.$$.fragment,t),E(qe.$$.fragment,t),E(nt.$$.fragment,t),E(rt.$$.fragment,t),E(it.$$.fragment,t),E(xe.$$.fragment,t),E(je.$$.fragment,t),E(Te.$$.fragment,t),Ia=!0)},o(t){q(s.$$.fragment,t),q(Ae.$$.fragment,t),q(Fe.$$.fragment,t),q(re.$$.fragment,t),q(Ue.$$.fragment,t),q(Oe.$$.fragment,t),q(ze.$$.fragment,t),q(Le.$$.fragment,t),q(Se.$$.fragment,t),q(Me.$$.fragment,t),q(Qe.$$.fragment,t),q(Ve.$$.fragment,t),q(Be.$$.fragment,t),q(Ge.$$.fragment,t),q(He.$$.fragment,t),q(Ye.$$.fragment,t),q(Je.$$.fragment,t),q(ve.$$.fragment,t),q(Ke.$$.fragment,t),q(We.$$.fragment,t),q(tt.$$.fragment,t),q(be.$$.fragment,t),q(we.$$.fragment,t),q(ye.$$.fragment,t),q(at.$$.fragment,t),q(st.$$.fragment,t),q(ot.$$.fragment,t),q(qe.$$.fragment,t),q(nt.$$.fragment,t),q(rt.$$.fragment,t),q(it.$$.fragment,t),q(xe.$$.fragment,t),q(je.$$.fragment,t),q(Te.$$.fragment,t),Ia=!1},d(t){a(l),t&&a(v),t&&a(i),k(s),t&&a(U),t&&a(K),t&&a($a),t&&a(W),k(Ae),t&&a(ba),t&&a(ft),t&&a(wa),t&&a(L),k(Fe),k(re),t&&a(ya),t&&a(ht),t&&a(Ea),t&&a(x),k(Ue),k(Oe),k(ze),k(Le),k(Se),k(Me),k(Qe),k(Ve),k(Be),t&&a(qa),t&&a(X),k(Ge),t&&a(ka),t&&a(Z),k(He),t&&a(xa),t&&a(M),k(Ye),k(Je),k(ve),t&&a(ja),t&&a(ee),k(Ke),t&&a(Ta),t&&a(D),k(We),k(tt),k(be),k(we),k(ye),t&&a(Ca),t&&a(ae),k(at),t&&a(Pa),t&&a(Q),k(st),k(ot),k(qe),t&&a(Na),t&&a(se),k(nt),t&&a(Da),t&&a(I),k(rt),k(it),k(xe),k(je),k(Te)}}}const Tr={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"},{local:"evaluate.TokenClassificationEvaluator",title:"TokenClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Cr(j){return gr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ur extends pr{constructor(l){super();ur(this,l,Cr,jr,mr,{})}}export{Ur as default,Tr as metadata};
