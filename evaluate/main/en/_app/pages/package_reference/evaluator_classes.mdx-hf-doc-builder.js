import{S as As,i as Ds,s as Is,e as o,k as d,w as _,t as l,M as Ns,c as s,d as a,m as p,a as n,x as b,h as i,b as u,G as e,g as v,y as w,q as $,o as E,B as y,v as Qs,L as It}from"../../chunks/vendor-hf-doc-builder.js";import{T as Us}from"../../chunks/Tip-hf-doc-builder.js";import{D}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Nt}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ye}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Dt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ms(T){let c,x,f,m,q;return m=new Nt({props:{code:`from evaluate import evaluator
# Sentiment analysis evaluator
evaluator("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Sentiment analysis evaluator</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>evaluator(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,x),v(r,f,g),w(m,r,g),q=!0},p:It,i(r){q||($(m.$$.fragment,r),q=!0)},o(r){E(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Ss(T){let c,x,f,m,q;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("image-classification")
data = load_dataset("beans", split="test[:40]")
results = task_evaluator.compute(
    model_or_pipeline="nateraw/vit-base-beans",
    data=data,
    label_column="labels",
    metric="accuracy",
    label_mapping={'angular_leaf_spot': 0, 'bean_rust': 1, 'healthy': 2},
    strategy="bootstrap"
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;image-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;beans&quot;</span>, split=<span class="hljs-string">&quot;test[:40]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;nateraw/vit-base-beans&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_column=<span class="hljs-string">&quot;labels&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&#x27;angular_leaf_spot&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;bean_rust&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;healthy&#x27;</span>: <span class="hljs-number">2</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,x),v(r,f,g),w(m,r,g),q=!0},p:It,i(r){q||($(m.$$.fragment,r),q=!0)},o(r){E(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function zs(T){let c,x,f,m,q;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="sshleifer/tiny-distilbert-base-cased-distilled-squad",
    data=data,
    metric="squad",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;sshleifer/tiny-distilbert-base-cased-distilled-squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,x),v(r,f,g),w(m,r,g),q=!0},p:It,i(r){q||($(m.$$.fragment,r),q=!0)},o(r){E(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Ls(T){let c,x,f,m,q;return{c(){c=o("p"),x=l("Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass "),f=o("code"),m=l("squad_v2_format=True"),q=l(` to
the compute() call.`)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass "),f=s(g,"CODE",{});var J=n(f);m=i(J,"squad_v2_format=True"),J.forEach(a),q=i(g,` to
the compute() call.`),g.forEach(a)},m(r,g){v(r,c,g),e(c,x),e(c,f),e(f,m),e(c,q)},d(r){r&&a(c)}}}function Fs(T){let c,x;return c=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("question-answering")
data = load_dataset("squad_v2", split="validation[:2]")
results = task_evaluator.compute(
    model_or_pipeline="mrm8488/bert-tiny-finetuned-squadv2",
    data=data,
    metric="squad_v2",
    squad_v2_format=True,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;question-answering&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;squad_v2&quot;</span>, split=<span class="hljs-string">&quot;validation[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;mrm8488/bert-tiny-finetuned-squadv2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;squad_v2&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    squad_v2_format=<span class="hljs-literal">True</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){_(c.$$.fragment)},l(f){b(c.$$.fragment,f)},m(f,m){w(c,f,m),x=!0},p:It,i(f){x||($(c.$$.fragment,f),x=!0)},o(f){E(c.$$.fragment,f),x=!1},d(f){y(c,f)}}}function Os(T){let c,x,f,m,q;return m=new Nt({props:{code:`from evaluate import evaluator
from datasets import load_dataset
task_evaluator = evaluator("text-classification")
data = load_dataset("imdb", split="test[:2]")
results = task_evaluator.compute(
    model_or_pipeline="huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli",
    data=data,
    metric="accuracy",
    label_mapping={"LABEL_0": 0.0, "LABEL_1": 1.0},
    strategy="bootstrap",
    n_resamples=10,
    random_state=0
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> evaluator
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>task_evaluator = evaluator(<span class="hljs-string">&quot;text-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>data = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=<span class="hljs-string">&quot;test[:2]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>results = task_evaluator.compute(
<span class="hljs-meta">&gt;&gt;&gt; </span>    model_or_pipeline=<span class="hljs-string">&quot;huggingface/prunebert-base-uncased-6-finepruned-w-distil-mnli&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    data=data,
<span class="hljs-meta">&gt;&gt;&gt; </span>    metric=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    label_mapping={<span class="hljs-string">&quot;LABEL_0&quot;</span>: <span class="hljs-number">0.0</span>, <span class="hljs-string">&quot;LABEL_1&quot;</span>: <span class="hljs-number">1.0</span>},
<span class="hljs-meta">&gt;&gt;&gt; </span>    strategy=<span class="hljs-string">&quot;bootstrap&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    n_resamples=<span class="hljs-number">10</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>    random_state=<span class="hljs-number">0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),{c(){c=o("p"),x=l("Examples:"),f=d(),_(m.$$.fragment)},l(r){c=s(r,"P",{});var g=n(c);x=i(g,"Examples:"),g.forEach(a),f=p(r),b(m.$$.fragment,r)},m(r,g){v(r,c,g),e(c,x),v(r,f,g),w(m,r,g),q=!0},p:It,i(r){q||($(m.$$.fragment,r),q=!0)},o(r){E(m.$$.fragment,r),q=!1},d(r){r&&a(c),r&&a(f),y(m,r)}}}function Rs(T){let c,x,f,m,q,r,g,J,la,Qt,ze,ia,Ut,R,K,Ze,he,ca,et,da,Mt,Le,pa,St,I,ge,ua,N,ma,Fe,fa,ha,tt,ga,va,at,_a,ba,wa,X,zt,Oe,$a,Lt,k,ve,Ea,ot,ya,xa,Y,_e,qa,st,ka,ja,Z,be,Ta,we,Ca,nt,Pa,Aa,Da,ee,$e,Ia,rt,Na,Qa,te,Ee,Ua,lt,Ma,Sa,ae,ye,za,it,La,Ft,V,oe,ct,xe,Fa,dt,Oa,Ot,B,se,pt,qe,Ra,ut,Va,Rt,Q,ke,Ba,U,Ha,Re,Ga,Wa,mt,Ja,Ka,ft,Xa,Ya,Za,S,je,eo,ht,to,ao,ne,Vt,H,re,gt,Te,oo,vt,so,Bt,j,Ce,no,Pe,ro,le,_t,lo,io,co,po,G,uo,Ve,mo,fo,bt,ho,go,vo,Ae,_o,De,wt,bo,wo,$o,C,Ie,Eo,$t,yo,xo,ie,qo,ce,ko,de,Ht,W,pe,Et,Ne,jo,yt,To,Gt,M,Qe,Co,A,Po,Be,Ao,Do,xt,Io,No,qt,Qo,Uo,kt,Mo,So,zo,z,Ue,Lo,jt,Fo,Oo,ue,Wt;return r=new Ye({}),he=new Ye({}),ge=new D({props:{name:"evaluate.evaluator",anchor:"evaluate.evaluator",parameters:[{name:"task",val:": str = None"}],parametersDescription:[{anchor:"evaluate.evaluator.task",description:`<strong>task</strong> (<code>str</code>) &#x2014;
The task defining which evaluator will be returned. Currently accepted tasks are:<ul>
<li><code>&quot;image-classification&quot;</code>: will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.ImageClassificationEvaluator">ImageClassificationEvaluator</a>.</li>
<li><code>&quot;question-answering&quot;</code>: will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.QuestionAnsweringEvaluator">QuestionAnsweringEvaluator</a>.</li>
<li><code>&quot;text-classification&quot;</code> (alias <code>&quot;sentiment-analysis&quot;</code> available): will return a <a href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.TextClassificationEvaluator">TextClassificationEvaluator</a>.</li>
</ul>`,name:"task"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/__init__.py#L79",returnDescription:`
<p>An evaluator suitable for the task.</p>
`,returnType:`
<p><a
  href="/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.Evaluator"
>Evaluator</a></p>
`}}),X=new Dt({props:{anchor:"evaluate.evaluator.example",$$slots:{default:[Ms]},$$scope:{ctx:T}}}),ve=new D({props:{name:"class evaluate.Evaluator",anchor:"evaluate.Evaluator",parameters:[{name:"task",val:": str"},{name:"default_metric_name",val:": str = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L50"}}),_e=new D({props:{name:"compute_metric",anchor:"evaluate.Evaluator.compute_metric",parameters:[{name:"metric",val:": EvaluationModule"},{name:"metric_inputs",val:": typing.Dict"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"random_state",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L332"}}),be=new D({props:{name:"predictions_processor",anchor:"evaluate.Evaluator.predictions_processor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L159"}}),$e=new D({props:{name:"prepare_data",anchor:"evaluate.Evaluator.prepare_data",parameters:[{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset]"},{name:"input_column",val:": str"},{name:"label_column",val:": str"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_data.data",description:"<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None) -- Specifies the dataset we will run evaluation on. If it is of type </code>str`, we treat it as the dataset\nname, and load it. Otherwise we assume it represents a pre-loaded dataset.",name:"data"},{anchor:"evaluate.Evaluator.prepare_data.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.Evaluator.prepare_data.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L218",returnDescription:`
<p>metric inputs.
<code>list</code>:  pipeline inputs.</p>
`,returnType:`
<p><code>dict</code></p>
`}}),Ee=new D({props:{name:"prepare_metric",anchor:"evaluate.Evaluator.prepare_metric",parameters:[{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule]"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_metric.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L302",returnDescription:`
<p>The loaded metric.</p>
`}}),ye=new D({props:{name:"prepare_pipeline",anchor:"evaluate.Evaluator.prepare_pipeline",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')]"},{name:"tokenizer",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"feature_extractor",val:": typing.Union[ForwardRef('PreTrainedTokenizerBase'), ForwardRef('FeatureExtractionMixin')] = None"},{name:"device",val:": int = None"}],parametersDescription:[{anchor:"evaluate.Evaluator.prepare_pipeline.model_or_pipeline",description:"<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, &#x2014;",name:"model_or_pipeline"},{anchor:"evaluate.Evaluator.prepare_pipeline.defaults",description:`<strong>defaults</strong> to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"defaults"},{anchor:"evaluate.Evaluator.prepare_pipeline.preprocessor",description:`<strong>preprocessor</strong> (<code>PreTrainedTokenizerBase</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default preprocessor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"preprocessor"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/base.py#L250",returnDescription:`
<p>The initialized pipeline.</p>
`}}),xe=new Ye({}),qe=new Ye({}),ke=new D({props:{name:"class evaluate.ImageClassificationEvaluator",anchor:"evaluate.ImageClassificationEvaluator",parameters:[{name:"task",val:" = 'image-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/image_classification.py#L20"}}),je=new D({props:{name:"compute",anchor:"evaluate.ImageClassificationEvaluator.compute",parameters:[{name:"input_column",val:": str = 'image'"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.ImageClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>image-classification</code>. If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.ImageClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.ImageClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
feature_extractor &#x2014; (<code>str</code> or <code>FeatureExtractionMixin</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default feature extractor if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.ImageClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.ImageClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.ImageClassificationEvaluator.compute.device",description:`<strong>device</strong> (<code>int</code>, defaults to <code>None</code>) &#x2014;
Device ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive
integer will run the model on the associated CUDA device ID. If<code>None</code> is provided it will be inferred and
CUDA:0 used if available, CPU otherwise.`,name:"device"},{anchor:"evaluate.ImageClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.ImageClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;image&quot;</code>) &#x2014;
the name of the column containing the images as PIL ImageFile in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.ImageClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/image_classification.py#L37",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ne=new Dt({props:{anchor:"evaluate.ImageClassificationEvaluator.compute.example",$$slots:{default:[Ss]},$$scope:{ctx:T}}}),Te=new Ye({}),Ce=new D({props:{name:"class evaluate.QuestionAnsweringEvaluator",anchor:"evaluate.QuestionAnsweringEvaluator",parameters:[{name:"task",val:" = 'question-answering'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/question_answering.py#L38"}}),Ie=new D({props:{name:"compute",anchor:"evaluate.QuestionAnsweringEvaluator.compute",parameters:[{name:"model_or_pipeline",val:": typing.Union[str, ForwardRef('Pipeline'), typing.Callable, ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')] = None"},{name:"data",val:": typing.Union[str, datasets.arrow_dataset.Dataset] = None"},{name:"metric",val:": typing.Union[str, evaluate.module.EvaluationModule] = None"},{name:"tokenizer",val:": typing.Union[str, ForwardRef('PreTrainedTokenizer'), NoneType] = None"},{name:"strategy",val:": typing.Literal['simple', 'bootstrap'] = 'simple'"},{name:"confidence_level",val:": float = 0.95"},{name:"n_resamples",val:": int = 9999"},{name:"device",val:": int = None"},{name:"random_state",val:": typing.Optional[int] = None"},{name:"question_column",val:": str = 'question'"},{name:"context_column",val:": str = 'context'"},{name:"id_column",val:": str = 'id'"},{name:"label_column",val:": str = 'answers'"},{name:"squad_v2_format",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"evaluate.QuestionAnsweringEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>question-answering</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.`,name:"metric"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.`,name:"tokenizer"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.strategy",description:`<strong>strategy</strong> (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;) &#x2014;
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"strategy"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.device",description:`<strong>device</strong> (<code>int</code>, defaults to <code>None</code>) &#x2014;
Device ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive
will run the model on the associated CUDA device ID. If<code>None</code> is provided it will be inferred and
CUDA:0 used if available, CPU otherwise.`,name:"device"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.question_column",description:`<strong>question_column</strong> (<code>str</code>, defaults to <code>&quot;question&quot;</code>) &#x2014;
the name of the column containing the question in the dataset specified by <code>data</code>.`,name:"question_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.context_column",description:`<strong>context_column</strong> (<code>str</code>, defaults to <code>&quot;context&quot;</code>) &#x2014;
the name of the column containing the context in the dataset specified by <code>data</code>.`,name:"context_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.id_column",description:`<strong>id_column</strong> (<code>str</code>, defaults to <code>&quot;id&quot;</code>) &#x2014;
the name of the column cointaing the identification field of the question and answer pair in the
dataset specified by <code>data</code>.`,name:"id_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;answers&quot;</code>) &#x2014;
the name of the column containing the answers in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.QuestionAnsweringEvaluator.compute.squad_v2_format",description:`<strong>squad_v2_format</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
whether the dataset follows the format of squad_v2 dataset where a question may have no answer in the context. If this parameter is not provided,
the format will be automatically inferred.`,name:"squad_v2_format"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/question_answering.py#L117",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ie=new Dt({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example",$$slots:{default:[zs]},$$scope:{ctx:T}}}),ce=new Us({props:{$$slots:{default:[Ls]},$$scope:{ctx:T}}}),de=new Dt({props:{anchor:"evaluate.QuestionAnsweringEvaluator.compute.example-2",$$slots:{default:[Fs]},$$scope:{ctx:T}}}),Ne=new Ye({}),Qe=new D({props:{name:"class evaluate.TextClassificationEvaluator",anchor:"evaluate.TextClassificationEvaluator",parameters:[{name:"task",val:" = 'text-classification'"},{name:"default_metric_name",val:" = None"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/text_classification.py#L20"}}),Ue=new D({props:{name:"compute",anchor:"evaluate.TextClassificationEvaluator.compute",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"evaluate.TextClassificationEvaluator.compute.model_or_pipeline",description:`<strong>model_or_pipeline</strong> (<code>str</code> or <code>Pipeline</code> or <code>Callable</code> or <code>PreTrainedModel</code> or <code>TFPreTrainedModel</code>, defaults to <code>None</code>) &#x2014;
If the argument in not specified, we initialize the default pipeline for the task (in this case
<code>text-classification</code> or its alias - <code>sentiment-analysis</code>). If the argument is of the type <code>str</code> or
is a model instance, we use it to initialize a new <code>Pipeline</code> with the given model. Otherwise we assume the
argument specifies a pre-initialized pipeline.`,name:"model_or_pipeline"},{anchor:"evaluate.TextClassificationEvaluator.compute.data",description:`<strong>data</strong> (<code>str</code> or <code>Dataset</code>, defaults to <code>None</code>) &#x2014;
Specifies the dataset we will run evaluation on. If it is of type <code>str</code>, we treat it as the dataset
name, and load it. Otherwise we assume it represents a pre-loaded dataset.`,name:"data"},{anchor:"evaluate.TextClassificationEvaluator.compute.metric",description:`<strong>metric</strong> (<code>str</code> or <code>EvaluationModule</code>, defaults to <code>None</code>) &#x2014;
Specifies the metric we use in evaluator. If it is of type <code>str</code>, we treat it as the metric name, and
load it. Otherwise we assume it represents a pre-loaded metric.
tokenizer &#x2014; (<code>str</code> or <code>PreTrainedTokenizer</code>, <em>optional</em>, defaults to <code>None</code>):
Argument can be used to overwrite a default tokenizer if <code>model_or_pipeline</code> represents a model for
which we build a pipeline. If <code>model_or_pipeline</code> is <code>None</code> or a pre-initialized pipeline, we ignore
this argument.
strategy &#x2014; (<code>Literal[&quot;simple&quot;, &quot;bootstrap&quot;]</code>, defaults to &#x201C;simple&#x201D;):
specifies the evaluation strategy. Possible values are:<ul>
<li><code>&quot;simple&quot;</code> - we evaluate the metric and return the scores.</li>
<li><code>&quot;bootstrap&quot;</code> - on top of computing the metric scores, we calculate the confidence interval for each
of the returned metric keys, using <code>scipy</code>&#x2019;s <code>bootstrap</code> method
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html" rel="nofollow">https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html</a>.</li>
</ul>`,name:"metric"},{anchor:"evaluate.TextClassificationEvaluator.compute.confidence_level",description:`<strong>confidence_level</strong> (<code>float</code>, defaults to <code>0.95</code>) &#x2014;
The <code>confidence_level</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"confidence_level"},{anchor:"evaluate.TextClassificationEvaluator.compute.n_resamples",description:`<strong>n_resamples</strong> (<code>int</code>, defaults to <code>9999</code>) &#x2014;
The <code>n_resamples</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen.`,name:"n_resamples"},{anchor:"evaluate.TextClassificationEvaluator.compute.device",description:`<strong>device</strong> (<code>int</code>, defaults to <code>None</code>) &#x2014;
Device ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive
integer will run the model on the associated CUDA device ID. If<code>None</code> is provided it will be inferred and
CUDA:0 used if available, CPU otherwise.`,name:"device"},{anchor:"evaluate.TextClassificationEvaluator.compute.random_state",description:`<strong>random_state</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The <code>random_state</code> value passed to <code>bootstrap</code> if <code>&quot;bootstrap&quot;</code> strategy is chosen. Useful for
debugging.`,name:"random_state"},{anchor:"evaluate.TextClassificationEvaluator.compute.input_column",description:`<strong>input_column</strong> (<code>str</code>, defaults to <code>&quot;text&quot;</code>) &#x2014;
the name of the column containing the text feature in the dataset specified by <code>data</code>.`,name:"input_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_column",description:`<strong>label_column</strong> (<code>str</code>, defaults to <code>&quot;label&quot;</code>) &#x2014;
the name of the column containing the labels in the dataset specified by <code>data</code>.`,name:"label_column"},{anchor:"evaluate.TextClassificationEvaluator.compute.label_mapping",description:`<strong>label_mapping</strong> (<code>Dict[str, Number]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
We want to map class labels defined by the model in the pipeline to values consistent with those
defined in the <code>label_column</code> of the <code>data</code> dataset.`,name:"label_mapping"}],source:"https://github.com/huggingface/evaluate/blob/main/src/evaluate/evaluator/text_classification.py#L41",returnDescription:`
<p>A <code>Dict</code>. The keys represent metric keys calculated for the <code>metric</code> spefied in function arguments. For the
<code>"simple"</code> strategy, the value is the metric score. For the <code>"bootstrap"</code> strategy, the value is a <code>Dict</code>
containing the score, the confidence interval and the standard error calculated for each metric key.</p>
`}}),ue=new Dt({props:{anchor:"evaluate.TextClassificationEvaluator.compute.example",$$slots:{default:[Os]},$$scope:{ctx:T}}}),{c(){c=o("meta"),x=d(),f=o("h1"),m=o("a"),q=o("span"),_(r.$$.fragment),g=d(),J=o("span"),la=l("Evaluator"),Qt=d(),ze=o("p"),ia=l("The evaluator classes for automatic evaluation."),Ut=d(),R=o("h2"),K=o("a"),Ze=o("span"),_(he.$$.fragment),ca=d(),et=o("span"),da=l("Evaluator classes"),Mt=d(),Le=o("p"),pa=l("The main entry point for using the evaluator:"),St=d(),I=o("div"),_(ge.$$.fragment),ua=d(),N=o("p"),ma=l("Utility factory method to build an "),Fe=o("a"),fa=l("Evaluator"),ha=l(`.
Evaluators encapsulate a task and a default metric name. They leverage `),tt=o("code"),ga=l("pipeline"),va=l(" functionalify from "),at=o("code"),_a=l("transformers"),ba=l(`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),wa=d(),_(X.$$.fragment),zt=d(),Oe=o("p"),$a=l("The base class for all evaluator classes:"),Lt=d(),k=o("div"),_(ve.$$.fragment),Ea=d(),ot=o("p"),ya=l(`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),xa=d(),Y=o("div"),_(_e.$$.fragment),qa=d(),st=o("p"),ka=l("Compute and return metrics."),ja=d(),Z=o("div"),_(be.$$.fragment),Ta=d(),we=o("p"),Ca=l("A core method of the "),nt=o("code"),Pa=l("Evaluator"),Aa=l(" class, which processes the pipeline outputs for compatibility with the metric."),Da=d(),ee=o("div"),_($e.$$.fragment),Ia=d(),rt=o("p"),Na=l("Prepare data."),Qa=d(),te=o("div"),_(Ee.$$.fragment),Ua=d(),lt=o("p"),Ma=l("Prepare metric."),Sa=d(),ae=o("div"),_(ye.$$.fragment),za=d(),it=o("p"),La=l("Prepare pipeline."),Ft=d(),V=o("h2"),oe=o("a"),ct=o("span"),_(xe.$$.fragment),Fa=d(),dt=o("span"),Oa=l("The task specific evaluators"),Ot=d(),B=o("h3"),se=o("a"),pt=o("span"),_(qe.$$.fragment),Ra=d(),ut=o("span"),Va=l("ImageClassificationEvaluator"),Rt=d(),Q=o("div"),_(ke.$$.fragment),Ba=d(),U=o("p"),Ha=l(`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Re=o("a"),Ga=l("evaluator()"),Wa=l(` using the default task name
`),mt=o("code"),Ja=l("image-classification"),Ka=l(`.
Methods in this class assume a data format compatible with the `),ft=o("code"),Xa=l("ImageClassificationPipeline"),Ya=l("."),Za=d(),S=o("div"),_(je.$$.fragment),eo=d(),ht=o("p"),to=l("Compute the metric for a given pipeline and dataset combination."),ao=d(),_(ne.$$.fragment),Vt=d(),H=o("h3"),re=o("a"),gt=o("span"),_(Te.$$.fragment),oo=d(),vt=o("span"),so=l("QuestionAnsweringEvaluator"),Bt=d(),j=o("div"),_(Ce.$$.fragment),no=d(),Pe=o("p"),ro=l(`Question answering evaluator. This evaluator handles
`),le=o("a"),_t=o("strong"),lo=l("extractive"),io=l(" question answering"),co=l(`,
where the answer to the question is extracted from a context.`),po=d(),G=o("p"),uo=l("This question answering evaluator can currently be loaded from "),Ve=o("a"),mo=l("evaluator()"),fo=l(` using the default task name
`),bt=o("code"),ho=l("question-answering"),go=l("."),vo=d(),Ae=o("p"),_o=l(`Methods in this class assume a data format compatible with the
`),De=o("a"),wt=o("code"),bo=l("QuestionAnsweringPipeline"),wo=l("."),$o=d(),C=o("div"),_(Ie.$$.fragment),Eo=d(),$t=o("p"),yo=l("Compute the metric for a given pipeline and dataset combination."),xo=d(),_(ie.$$.fragment),qo=d(),_(ce.$$.fragment),ko=d(),_(de.$$.fragment),Ht=d(),W=o("h3"),pe=o("a"),Et=o("span"),_(Ne.$$.fragment),jo=d(),yt=o("span"),To=l("TextClassificationEvaluator"),Gt=d(),M=o("div"),_(Qe.$$.fragment),Co=d(),A=o("p"),Po=l(`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Be=o("a"),Ao=l("evaluator()"),Do=l(` using the default task name
`),xt=o("code"),Io=l("text-classification"),No=l(" or with a "),qt=o("code"),Qo=l('"sentiment-analysis"'),Uo=l(` alias.
Methods in this class assume a data format compatible with the `),kt=o("code"),Mo=l("TextClassificationPipeline"),So=l(` - a single textual
feature as input and a categorical label as output.`),zo=d(),z=o("div"),_(Ue.$$.fragment),Lo=d(),jt=o("p"),Fo=l("Compute the metric for a given pipeline and dataset combination."),Oo=d(),_(ue.$$.fragment),this.h()},l(t){const h=Ns('[data-svelte="svelte-1phssyn"]',document.head);c=s(h,"META",{name:!0,content:!0}),h.forEach(a),x=p(t),f=s(t,"H1",{class:!0});var Me=n(f);m=s(Me,"A",{id:!0,class:!0,href:!0});var Tt=n(m);q=s(Tt,"SPAN",{});var Ct=n(q);b(r.$$.fragment,Ct),Ct.forEach(a),Tt.forEach(a),g=p(Me),J=s(Me,"SPAN",{});var Pt=n(J);la=i(Pt,"Evaluator"),Pt.forEach(a),Me.forEach(a),Qt=p(t),ze=s(t,"P",{});var At=n(ze);ia=i(At,"The evaluator classes for automatic evaluation."),At.forEach(a),Ut=p(t),R=s(t,"H2",{class:!0});var Se=n(R);K=s(Se,"A",{id:!0,class:!0,href:!0});var Vo=n(K);Ze=s(Vo,"SPAN",{});var Bo=n(Ze);b(he.$$.fragment,Bo),Bo.forEach(a),Vo.forEach(a),ca=p(Se),et=s(Se,"SPAN",{});var Ho=n(et);da=i(Ho,"Evaluator classes"),Ho.forEach(a),Se.forEach(a),Mt=p(t),Le=s(t,"P",{});var Go=n(Le);pa=i(Go,"The main entry point for using the evaluator:"),Go.forEach(a),St=p(t),I=s(t,"DIV",{class:!0});var He=n(I);b(ge.$$.fragment,He),ua=p(He),N=s(He,"P",{});var me=n(N);ma=i(me,"Utility factory method to build an "),Fe=s(me,"A",{href:!0});var Wo=n(Fe);fa=i(Wo,"Evaluator"),Wo.forEach(a),ha=i(me,`.
Evaluators encapsulate a task and a default metric name. They leverage `),tt=s(me,"CODE",{});var Jo=n(tt);ga=i(Jo,"pipeline"),Jo.forEach(a),va=i(me," functionalify from "),at=s(me,"CODE",{});var Ko=n(at);_a=i(Ko,"transformers"),Ko.forEach(a),ba=i(me,`
to simplify the evaluation of multiple combinations of models, datasets and metrics for a given task.`),me.forEach(a),wa=p(He),b(X.$$.fragment,He),He.forEach(a),zt=p(t),Oe=s(t,"P",{});var Xo=n(Oe);$a=i(Xo,"The base class for all evaluator classes:"),Xo.forEach(a),Lt=p(t),k=s(t,"DIV",{class:!0});var P=n(k);b(ve.$$.fragment,P),Ea=p(P),ot=s(P,"P",{});var Yo=n(ot);ya=i(Yo,`The Evaluator class is the class from which all evaluators inherit. Refer to this class for methods shared across
different evaluators.
Base class implementing evaluator operations.`),Yo.forEach(a),xa=p(P),Y=s(P,"DIV",{class:!0});var Jt=n(Y);b(_e.$$.fragment,Jt),qa=p(Jt),st=s(Jt,"P",{});var Zo=n(st);ka=i(Zo,"Compute and return metrics."),Zo.forEach(a),Jt.forEach(a),ja=p(P),Z=s(P,"DIV",{class:!0});var Kt=n(Z);b(be.$$.fragment,Kt),Ta=p(Kt),we=s(Kt,"P",{});var Xt=n(we);Ca=i(Xt,"A core method of the "),nt=s(Xt,"CODE",{});var es=n(nt);Pa=i(es,"Evaluator"),es.forEach(a),Aa=i(Xt," class, which processes the pipeline outputs for compatibility with the metric."),Xt.forEach(a),Kt.forEach(a),Da=p(P),ee=s(P,"DIV",{class:!0});var Yt=n(ee);b($e.$$.fragment,Yt),Ia=p(Yt),rt=s(Yt,"P",{});var ts=n(rt);Na=i(ts,"Prepare data."),ts.forEach(a),Yt.forEach(a),Qa=p(P),te=s(P,"DIV",{class:!0});var Zt=n(te);b(Ee.$$.fragment,Zt),Ua=p(Zt),lt=s(Zt,"P",{});var as=n(lt);Ma=i(as,"Prepare metric."),as.forEach(a),Zt.forEach(a),Sa=p(P),ae=s(P,"DIV",{class:!0});var ea=n(ae);b(ye.$$.fragment,ea),za=p(ea),it=s(ea,"P",{});var os=n(it);La=i(os,"Prepare pipeline."),os.forEach(a),ea.forEach(a),P.forEach(a),Ft=p(t),V=s(t,"H2",{class:!0});var ta=n(V);oe=s(ta,"A",{id:!0,class:!0,href:!0});var ss=n(oe);ct=s(ss,"SPAN",{});var ns=n(ct);b(xe.$$.fragment,ns),ns.forEach(a),ss.forEach(a),Fa=p(ta),dt=s(ta,"SPAN",{});var rs=n(dt);Oa=i(rs,"The task specific evaluators"),rs.forEach(a),ta.forEach(a),Ot=p(t),B=s(t,"H3",{class:!0});var aa=n(B);se=s(aa,"A",{id:!0,class:!0,href:!0});var ls=n(se);pt=s(ls,"SPAN",{});var is=n(pt);b(qe.$$.fragment,is),is.forEach(a),ls.forEach(a),Ra=p(aa),ut=s(aa,"SPAN",{});var cs=n(ut);Va=i(cs,"ImageClassificationEvaluator"),cs.forEach(a),aa.forEach(a),Rt=p(t),Q=s(t,"DIV",{class:!0});var Ge=n(Q);b(ke.$$.fragment,Ge),Ba=p(Ge),U=s(Ge,"P",{});var fe=n(U);Ha=i(fe,`Image classification evaluator.
This image classification evaluator can currently be loaded from `),Re=s(fe,"A",{href:!0});var ds=n(Re);Ga=i(ds,"evaluator()"),ds.forEach(a),Wa=i(fe,` using the default task name
`),mt=s(fe,"CODE",{});var ps=n(mt);Ja=i(ps,"image-classification"),ps.forEach(a),Ka=i(fe,`.
Methods in this class assume a data format compatible with the `),ft=s(fe,"CODE",{});var us=n(ft);Xa=i(us,"ImageClassificationPipeline"),us.forEach(a),Ya=i(fe,"."),fe.forEach(a),Za=p(Ge),S=s(Ge,"DIV",{class:!0});var We=n(S);b(je.$$.fragment,We),eo=p(We),ht=s(We,"P",{});var ms=n(ht);to=i(ms,"Compute the metric for a given pipeline and dataset combination."),ms.forEach(a),ao=p(We),b(ne.$$.fragment,We),We.forEach(a),Ge.forEach(a),Vt=p(t),H=s(t,"H3",{class:!0});var oa=n(H);re=s(oa,"A",{id:!0,class:!0,href:!0});var fs=n(re);gt=s(fs,"SPAN",{});var hs=n(gt);b(Te.$$.fragment,hs),hs.forEach(a),fs.forEach(a),oo=p(oa),vt=s(oa,"SPAN",{});var gs=n(vt);so=i(gs,"QuestionAnsweringEvaluator"),gs.forEach(a),oa.forEach(a),Bt=p(t),j=s(t,"DIV",{class:!0});var L=n(j);b(Ce.$$.fragment,L),no=p(L),Pe=s(L,"P",{});var sa=n(Pe);ro=i(sa,`Question answering evaluator. This evaluator handles
`),le=s(sa,"A",{href:!0,rel:!0});var Ro=n(le);_t=s(Ro,"STRONG",{});var vs=n(_t);lo=i(vs,"extractive"),vs.forEach(a),io=i(Ro," question answering"),Ro.forEach(a),co=i(sa,`,
where the answer to the question is extracted from a context.`),sa.forEach(a),po=p(L),G=s(L,"P",{});var Je=n(G);uo=i(Je,"This question answering evaluator can currently be loaded from "),Ve=s(Je,"A",{href:!0});var _s=n(Ve);mo=i(_s,"evaluator()"),_s.forEach(a),fo=i(Je,` using the default task name
`),bt=s(Je,"CODE",{});var bs=n(bt);ho=i(bs,"question-answering"),bs.forEach(a),go=i(Je,"."),Je.forEach(a),vo=p(L),Ae=s(L,"P",{});var na=n(Ae);_o=i(na,`Methods in this class assume a data format compatible with the
`),De=s(na,"A",{href:!0,rel:!0});var ws=n(De);wt=s(ws,"CODE",{});var $s=n(wt);bo=i($s,"QuestionAnsweringPipeline"),$s.forEach(a),ws.forEach(a),wo=i(na,"."),na.forEach(a),$o=p(L),C=s(L,"DIV",{class:!0});var F=n(C);b(Ie.$$.fragment,F),Eo=p(F),$t=s(F,"P",{});var Es=n($t);yo=i(Es,"Compute the metric for a given pipeline and dataset combination."),Es.forEach(a),xo=p(F),b(ie.$$.fragment,F),qo=p(F),b(ce.$$.fragment,F),ko=p(F),b(de.$$.fragment,F),F.forEach(a),L.forEach(a),Ht=p(t),W=s(t,"H3",{class:!0});var ra=n(W);pe=s(ra,"A",{id:!0,class:!0,href:!0});var ys=n(pe);Et=s(ys,"SPAN",{});var xs=n(Et);b(Ne.$$.fragment,xs),xs.forEach(a),ys.forEach(a),jo=p(ra),yt=s(ra,"SPAN",{});var qs=n(yt);To=i(qs,"TextClassificationEvaluator"),qs.forEach(a),ra.forEach(a),Gt=p(t),M=s(t,"DIV",{class:!0});var Ke=n(M);b(Qe.$$.fragment,Ke),Co=p(Ke),A=s(Ke,"P",{});var O=n(A);Po=i(O,`Text classification evaluator.
This text classification evaluator can currently be loaded from `),Be=s(O,"A",{href:!0});var ks=n(Be);Ao=i(ks,"evaluator()"),ks.forEach(a),Do=i(O,` using the default task name
`),xt=s(O,"CODE",{});var js=n(xt);Io=i(js,"text-classification"),js.forEach(a),No=i(O," or with a "),qt=s(O,"CODE",{});var Ts=n(qt);Qo=i(Ts,'"sentiment-analysis"'),Ts.forEach(a),Uo=i(O,` alias.
Methods in this class assume a data format compatible with the `),kt=s(O,"CODE",{});var Cs=n(kt);Mo=i(Cs,"TextClassificationPipeline"),Cs.forEach(a),So=i(O,` - a single textual
feature as input and a categorical label as output.`),O.forEach(a),zo=p(Ke),z=s(Ke,"DIV",{class:!0});var Xe=n(z);b(Ue.$$.fragment,Xe),Lo=p(Xe),jt=s(Xe,"P",{});var Ps=n(jt);Fo=i(Ps,"Compute the metric for a given pipeline and dataset combination."),Ps.forEach(a),Oo=p(Xe),b(ue.$$.fragment,Xe),Xe.forEach(a),Ke.forEach(a),this.h()},h(){u(c,"name","hf:doc:metadata"),u(c,"content",JSON.stringify(Vs)),u(m,"id","evaluator"),u(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(m,"href","#evaluator"),u(f,"class","relative group"),u(K,"id","evaluate.evaluator"),u(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(K,"href","#evaluate.evaluator"),u(R,"class","relative group"),u(Fe,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.Evaluator"),u(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(oe,"id","the-task-specific-evaluators"),u(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(oe,"href","#the-task-specific-evaluators"),u(V,"class","relative group"),u(se,"id","evaluate.ImageClassificationEvaluator"),u(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(se,"href","#evaluate.ImageClassificationEvaluator"),u(B,"class","relative group"),u(Re,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),u(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(re,"id","evaluate.QuestionAnsweringEvaluator"),u(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(re,"href","#evaluate.QuestionAnsweringEvaluator"),u(H,"class","relative group"),u(le,"href","https://huggingface.co/docs/transformers/task_summary#extractive-question-answering"),u(le,"rel","nofollow"),u(Ve,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),u(De,"href","https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.QuestionAnsweringPipeline"),u(De,"rel","nofollow"),u(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(pe,"id","evaluate.TextClassificationEvaluator"),u(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(pe,"href","#evaluate.TextClassificationEvaluator"),u(W,"class","relative group"),u(Be,"href","/docs/evaluate/main/en/package_reference/evaluator_classes#evaluate.evaluator"),u(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,c),v(t,x,h),v(t,f,h),e(f,m),e(m,q),w(r,q,null),e(f,g),e(f,J),e(J,la),v(t,Qt,h),v(t,ze,h),e(ze,ia),v(t,Ut,h),v(t,R,h),e(R,K),e(K,Ze),w(he,Ze,null),e(R,ca),e(R,et),e(et,da),v(t,Mt,h),v(t,Le,h),e(Le,pa),v(t,St,h),v(t,I,h),w(ge,I,null),e(I,ua),e(I,N),e(N,ma),e(N,Fe),e(Fe,fa),e(N,ha),e(N,tt),e(tt,ga),e(N,va),e(N,at),e(at,_a),e(N,ba),e(I,wa),w(X,I,null),v(t,zt,h),v(t,Oe,h),e(Oe,$a),v(t,Lt,h),v(t,k,h),w(ve,k,null),e(k,Ea),e(k,ot),e(ot,ya),e(k,xa),e(k,Y),w(_e,Y,null),e(Y,qa),e(Y,st),e(st,ka),e(k,ja),e(k,Z),w(be,Z,null),e(Z,Ta),e(Z,we),e(we,Ca),e(we,nt),e(nt,Pa),e(we,Aa),e(k,Da),e(k,ee),w($e,ee,null),e(ee,Ia),e(ee,rt),e(rt,Na),e(k,Qa),e(k,te),w(Ee,te,null),e(te,Ua),e(te,lt),e(lt,Ma),e(k,Sa),e(k,ae),w(ye,ae,null),e(ae,za),e(ae,it),e(it,La),v(t,Ft,h),v(t,V,h),e(V,oe),e(oe,ct),w(xe,ct,null),e(V,Fa),e(V,dt),e(dt,Oa),v(t,Ot,h),v(t,B,h),e(B,se),e(se,pt),w(qe,pt,null),e(B,Ra),e(B,ut),e(ut,Va),v(t,Rt,h),v(t,Q,h),w(ke,Q,null),e(Q,Ba),e(Q,U),e(U,Ha),e(U,Re),e(Re,Ga),e(U,Wa),e(U,mt),e(mt,Ja),e(U,Ka),e(U,ft),e(ft,Xa),e(U,Ya),e(Q,Za),e(Q,S),w(je,S,null),e(S,eo),e(S,ht),e(ht,to),e(S,ao),w(ne,S,null),v(t,Vt,h),v(t,H,h),e(H,re),e(re,gt),w(Te,gt,null),e(H,oo),e(H,vt),e(vt,so),v(t,Bt,h),v(t,j,h),w(Ce,j,null),e(j,no),e(j,Pe),e(Pe,ro),e(Pe,le),e(le,_t),e(_t,lo),e(le,io),e(Pe,co),e(j,po),e(j,G),e(G,uo),e(G,Ve),e(Ve,mo),e(G,fo),e(G,bt),e(bt,ho),e(G,go),e(j,vo),e(j,Ae),e(Ae,_o),e(Ae,De),e(De,wt),e(wt,bo),e(Ae,wo),e(j,$o),e(j,C),w(Ie,C,null),e(C,Eo),e(C,$t),e($t,yo),e(C,xo),w(ie,C,null),e(C,qo),w(ce,C,null),e(C,ko),w(de,C,null),v(t,Ht,h),v(t,W,h),e(W,pe),e(pe,Et),w(Ne,Et,null),e(W,jo),e(W,yt),e(yt,To),v(t,Gt,h),v(t,M,h),w(Qe,M,null),e(M,Co),e(M,A),e(A,Po),e(A,Be),e(Be,Ao),e(A,Do),e(A,xt),e(xt,Io),e(A,No),e(A,qt),e(qt,Qo),e(A,Uo),e(A,kt),e(kt,Mo),e(A,So),e(M,zo),e(M,z),w(Ue,z,null),e(z,Lo),e(z,jt),e(jt,Fo),e(z,Oo),w(ue,z,null),Wt=!0},p(t,[h]){const Me={};h&2&&(Me.$$scope={dirty:h,ctx:t}),X.$set(Me);const Tt={};h&2&&(Tt.$$scope={dirty:h,ctx:t}),ne.$set(Tt);const Ct={};h&2&&(Ct.$$scope={dirty:h,ctx:t}),ie.$set(Ct);const Pt={};h&2&&(Pt.$$scope={dirty:h,ctx:t}),ce.$set(Pt);const At={};h&2&&(At.$$scope={dirty:h,ctx:t}),de.$set(At);const Se={};h&2&&(Se.$$scope={dirty:h,ctx:t}),ue.$set(Se)},i(t){Wt||($(r.$$.fragment,t),$(he.$$.fragment,t),$(ge.$$.fragment,t),$(X.$$.fragment,t),$(ve.$$.fragment,t),$(_e.$$.fragment,t),$(be.$$.fragment,t),$($e.$$.fragment,t),$(Ee.$$.fragment,t),$(ye.$$.fragment,t),$(xe.$$.fragment,t),$(qe.$$.fragment,t),$(ke.$$.fragment,t),$(je.$$.fragment,t),$(ne.$$.fragment,t),$(Te.$$.fragment,t),$(Ce.$$.fragment,t),$(Ie.$$.fragment,t),$(ie.$$.fragment,t),$(ce.$$.fragment,t),$(de.$$.fragment,t),$(Ne.$$.fragment,t),$(Qe.$$.fragment,t),$(Ue.$$.fragment,t),$(ue.$$.fragment,t),Wt=!0)},o(t){E(r.$$.fragment,t),E(he.$$.fragment,t),E(ge.$$.fragment,t),E(X.$$.fragment,t),E(ve.$$.fragment,t),E(_e.$$.fragment,t),E(be.$$.fragment,t),E($e.$$.fragment,t),E(Ee.$$.fragment,t),E(ye.$$.fragment,t),E(xe.$$.fragment,t),E(qe.$$.fragment,t),E(ke.$$.fragment,t),E(je.$$.fragment,t),E(ne.$$.fragment,t),E(Te.$$.fragment,t),E(Ce.$$.fragment,t),E(Ie.$$.fragment,t),E(ie.$$.fragment,t),E(ce.$$.fragment,t),E(de.$$.fragment,t),E(Ne.$$.fragment,t),E(Qe.$$.fragment,t),E(Ue.$$.fragment,t),E(ue.$$.fragment,t),Wt=!1},d(t){a(c),t&&a(x),t&&a(f),y(r),t&&a(Qt),t&&a(ze),t&&a(Ut),t&&a(R),y(he),t&&a(Mt),t&&a(Le),t&&a(St),t&&a(I),y(ge),y(X),t&&a(zt),t&&a(Oe),t&&a(Lt),t&&a(k),y(ve),y(_e),y(be),y($e),y(Ee),y(ye),t&&a(Ft),t&&a(V),y(xe),t&&a(Ot),t&&a(B),y(qe),t&&a(Rt),t&&a(Q),y(ke),y(je),y(ne),t&&a(Vt),t&&a(H),y(Te),t&&a(Bt),t&&a(j),y(Ce),y(Ie),y(ie),y(ce),y(de),t&&a(Ht),t&&a(W),y(Ne),t&&a(Gt),t&&a(M),y(Qe),y(Ue),y(ue)}}}const Vs={local:"evaluator",sections:[{local:"evaluate.evaluator",title:"Evaluator classes"},{local:"the-task-specific-evaluators",sections:[{local:"evaluate.ImageClassificationEvaluator",title:"ImageClassificationEvaluator"},{local:"evaluate.QuestionAnsweringEvaluator",title:"QuestionAnsweringEvaluator"},{local:"evaluate.TextClassificationEvaluator",title:"TextClassificationEvaluator"}],title:"The task specific evaluators"}],title:"Evaluator"};function Bs(T){return Qs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ys extends As{constructor(c){super();Ds(this,c,Bs,Rs,Is,{})}}export{Ys as default,Vs as metadata};
