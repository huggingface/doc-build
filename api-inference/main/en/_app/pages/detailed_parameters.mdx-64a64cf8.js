import{S as CU,i as GU,s as UU,e as r,k as c,w as q,t as i,M as LU,c as o,d as t,m as p,a as l,x as v,h as u,b as d,N as BU,F as e,g as m,y,q as E,o as w,B as b,v as zU,L as P}from"../chunks/vendor-7c454903.js";import{T as Y}from"../chunks/Tip-735285fc.js";import{I as F}from"../chunks/IconCopyLink-5457534b.js";import{I as L,M as R,C as N}from"../chunks/InferenceApi-041dc1b2.js";function MU($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("facebook/bart-large-mnli"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"facebook/bart-large-mnli"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/facebook/bart-large-mnli"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function FU($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function JU($){let n,f;return n=new R({props:{$$slots:{default:[FU]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function KU($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function WU($){let n,f;return n=new R({props:{$$slots:{default:[KU]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function YU($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;, &quot;parameters&quot;: {&quot;candidate_labels&quot;: [&quot;refund&quot;, &quot;legal&quot;, &quot;faq&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function VU($){let n,f;return n=new R({props:{$$slots:{default:[YU]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function XU($){let n,f;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function QU($){let n,f;return n=new R({props:{$$slots:{default:[XU]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function ZU($){let n,f,s,h,_,k,A,j,T,O,D,ne,Re;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("Helsinki-NLP/opus-mt-ru-en"),A=i(`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=r("strong"),T=i("Recommended model"),O=i(": "),D=r("a"),ne=i("t5-base"),Re=i("."),this.h()},l(Q){n=o(Q,"P",{});var W=l(n);f=o(W,"STRONG",{});var at=l(f);s=u(at,"Recommended model"),at.forEach(t),h=u(W,`:
`),_=o(W,"A",{href:!0,rel:!0});var Fl=l(_);k=u(Fl,"Helsinki-NLP/opus-mt-ru-en"),Fl.forEach(t),A=u(W,`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=o(W,"STRONG",{});var Na=l(j);T=u(Na,"Recommended model"),Na.forEach(t),O=u(W,": "),D=o(W,"A",{href:!0,rel:!0});var Ne=l(D);ne=u(Ne,"t5-base"),Ne.forEach(t),Re=u(W,"."),W.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),d(_,"rel","nofollow"),d(D,"href","https://huggingface.co/t5-base"),d(D,"rel","nofollow")},m(Q,W){m(Q,n,W),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A),e(n,j),e(j,T),e(n,O),e(n,D),e(D,ne),e(n,Re)},d(Q){Q&&t(n)}}}function eL($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function tL($){let n,f;return n=new R({props:{$$slots:{default:[eL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function sL($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function aL($){let n,f;return n=new R({props:{$$slots:{default:[sL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function nL($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function rL($){let n,f;return n=new R({props:{$$slots:{default:[nL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function oL($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("facebook/bart-large-cnn"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"facebook/bart-large-cnn"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/facebook/bart-large-cnn"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function lL($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function iL($){let n,f;return n=new R({props:{$$slots:{default:[lL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function uL($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">\${</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function fL($){let n,f;return n=new R({props:{$$slots:{default:[uL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function cL($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;summary_text&quot;:&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function pL($){let n,f;return n=new R({props:{$$slots:{default:[cL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function dL($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("microsoft/DialoGPT-large"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"microsoft/DialoGPT-large"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/microsoft/DialoGPT-large"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function hL($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function gL($){let n,f;return n=new R({props:{$$slots:{default:[hL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function mL($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function _L($){let n,f;return n=new R({props:{$$slots:{default:[mL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function $L($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: {&quot;past_user_inputs&quot;: [&quot;Which movie is the best ?&quot;], &quot;generated_responses&quot;: [&quot;It is Die Hard for sure.&quot;], &quot;text&quot;:&quot;Can you explain why ?&quot;}}&#x27;</span> \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">${HF_API_TOKEN}</span>&quot;</span>\n<span class="hljs-comment"># {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;]}</span>'}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function qL($){let n,f;return n=new R({props:{$$slots:{default:[$L]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function vL($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("google/tapas-base-finetuned-wtq"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"google/tapas-base-finetuned-wtq"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function yL($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function EL($){let n,f;return n=new R({props:{$$slots:{default:[yL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function wL($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function bL($){let n,f;return n=new R({props:{$$slots:{default:[wL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function TL($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;query&quot;:&quot;How many stars does the transformers repository have?&quot;,&quot;table&quot;:{&quot;Repository&quot;:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],&quot;Stars&quot;:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],&quot;Contributors&quot;:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[0,1]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function jL($){let n,f;return n=new R({props:{$$slots:{default:[TL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function kL($){let n,f;return n=new N({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function AL($){let n,f;return n=new R({props:{$$slots:{default:[kL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function DL($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("deepset/roberta-base-squad2"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"deepset/roberta-base-squad2"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/deepset/roberta-base-squad2"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function OL($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function PL($){let n,f;return n=new R({props:{$$slots:{default:[OL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function RL($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function NL($){let n,f;return n=new R({props:{$$slots:{default:[RL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function xL($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;question&quot;:&quot;What is my name?&quot;,&quot;context&quot;:&quot;My name is Clara and I live in Berkeley.&quot;}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function SL($){let n,f;return n=new R({props:{$$slots:{default:[xL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function IL($){let n,f;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function HL($){let n,f;return n=new R({props:{$$slots:{default:[IL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function BL($){let n,f,s,h,_,k;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);f=o(j,"STRONG",{});var T=l(f);s=u(T,"Recommended model"),T.forEach(t),h=u(j,`:
`),_=o(j,"A",{href:!0,rel:!0});var O=l(_);k=u(O,"distilbert-base-uncased-finetuned-sst-2-english"),O.forEach(t),j.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),d(_,"rel","nofollow")},m(A,j){m(A,n,j),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k)},d(A){A&&t(n)}}}function CL($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function GL($){let n,f;return n=new R({props:{$$slots:{default:[CL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function UL($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function LL($){let n,f;return n=new R({props:{$$slots:{default:[UL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function zL($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [[{"label":"NEGATIVE","score":0.0001261125144083053},{"label":"POSITIVE","score":0.9998738765716553}]]`,highlighted:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [[{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053},{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553}]]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function ML($){let n,f;return n=new R({props:{$$slots:{default:[zL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function FL($){let n,f;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "NEGATIVE", "score": 0.0001},
            {"label": "POSITIVE", "score": 0.9999},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
        ]
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function JL($){let n,f;return n=new R({props:{$$slots:{default:[FL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function KL($){let n,f,s,h,_,k;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);f=o(j,"STRONG",{});var T=l(f);s=u(T,"Recommended model"),T.forEach(t),h=u(j,`:
`),_=o(j,"A",{href:!0,rel:!0});var O=l(_);k=u(O,"dbmdz/bert-large-cased-finetuned-conll03-english"),O.forEach(t),j.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),d(_,"rel","nofollow")},m(A,j){m(A,n,j),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k)},d(A){A&&t(n)}}}function WL($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function YL($){let n,f;return n=new R({props:{$$slots:{default:[WL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function VL($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function XL($){let n,f;return n=new R({props:{$$slots:{default:[VL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function QL($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9991337060928345,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:11,&quot;end&quot;:31},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9979912042617798,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:52,&quot;end&quot;:59}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function ZL($){let n,f;return n=new R({props:{$$slots:{default:[QL]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function ez($){let n,f;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function tz($){let n,f;return n=new R({props:{$$slots:{default:[ez]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function sz($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(": "),_=r("a"),k=i("gpt2"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,": "),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"gpt2"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/gpt2"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function az($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function nz($){let n,f;return n=new R({props:{$$slots:{default:[az]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function rz($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function oz($){let n,f;return n=new R({props:{$$slots:{default:[rz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function lz($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function iz($){let n,f;return n=new R({props:{$$slots:{default:[lz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function uz($){let n,f;return n=new N({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function fz($){let n,f;return n=new R({props:{$$slots:{default:[uz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function cz($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("bert-base-uncased"),A=i(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"bert-base-uncased"),D.forEach(t),A=u(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/bert-base-uncased"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function pz($){let n,f;return n=new N({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function dz($){let n,f;return n=new R({props:{$$slots:{default:[pz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function hz($){let n,f;return n=new N({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function gz($){let n,f;return n=new R({props:{$$slots:{default:[hz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function mz($){let n,f;return n=new N({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:0.16963955760002136,&quot;token&quot;:2053,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:0.07344776391983032,&quot;token&quot;:2498,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:0.05803241208195686,&quot;token&quot;:2748,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:0.043957844376564026,&quot;token&quot;:4242,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:0.04015745222568512,&quot;token&quot;:3722,&quot;token_str&quot;:&quot;simple&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function _z($){let n,f;return n=new R({props:{$$slots:{default:[mz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function $z($){let n,f;return n=new N({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function qz($){let n,f;return n=new R({props:{$$slots:{default:[$z]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function vz($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(": "),_=r("a"),k=i(`Check your
langage`),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,": "),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,`Check your
langage`),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function yz($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("English"),h=i(`:
`),_=r("a"),k=i("facebook/wav2vec2-large-960h-lv60-self"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"English"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function Ez($){let n,f;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("sample1.flac")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function wz($){let n,f;return n=new R({props:{$$slots:{default:[Ez]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function bz($){let n,f;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
            {
                headers: { Authorization: \`Bearer \${API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    <span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
    <span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
        <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
        <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
            <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
            {
                <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
                <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
                <span class="hljs-attr">body</span>: data,
            }
        );
        <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
        <span class="hljs-keyword">return</span> result;
    }
    <span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
        <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
    });
    <span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Tz($){let n,f;return n=new R({props:{$$slots:{default:[bz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function jz($){let n,f;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer \${HF_API_TOKEN}"
    # {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`    curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function kz($){let n,f;return n=new R({props:{$$slots:{default:[jz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Az($){let n,f;return n=new N({props:{code:`    self.assertEqual(
        data,
        {
            "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
        },
    )`,highlighted:`    self.assertEqual(
        data,
        {
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
        },
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Dz($){let n,f;return n=new R({props:{$$slots:{default:[Az]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Oz($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("Sentence-transformers"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"Sentence-transformers"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function Pz($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("superb/hubert-large-superb-er"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"superb/hubert-large-superb-er"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/superb/hubert-large-superb-er"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function Rz($){let n,f;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("sample1.flac")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Nz($){let n,f;return n=new R({props:{$$slots:{default:[Rz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function xz($){let n,f;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
            {
                headers: { Authorization: \`Bearer \${API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("sample1.flac").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Sz($){let n,f;return n=new R({props:{$$slots:{default:[xz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Iz($){let n,f;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
            -X POST \\
            --data-binary '@sample1.flac' \\
            -H "Authorization: Bearer \${HF_API_TOKEN}"
    # [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`    curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># [{&quot;score&quot;:0.5927661657333374,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:0.2002529799938202,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:0.12795612215995789,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:0.07902472466230392,&quot;label&quot;:&quot;sad&quot;}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Hz($){let n,f;return n=new R({props:{$$slots:{default:[Iz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Bz($){let n,f;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {"score": 0.5928, "label": "neu"},
            {"score": 0.2003, "label": "hap"},
            {"score": 0.128, "label": "ang"},
            {"score": 0.079, "label": "sad"},
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
            {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
        ],
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Cz($){let n,f;return n=new R({props:{$$slots:{default:[Bz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Gz($){let n,f,s,h,_,k,A;return{c(){n=r("p"),f=r("strong"),s=i("Recommended model"),h=i(`:
`),_=r("a"),k=i("facebook/detr-resnet-50"),A=i("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);f=o(T,"STRONG",{});var O=l(f);s=u(O,"Recommended model"),O.forEach(t),h=u(T,`:
`),_=o(T,"A",{href:!0,rel:!0});var D=l(_);k=u(D,"facebook/detr-resnet-50"),D.forEach(t),A=u(T,"."),T.forEach(t),this.h()},h(){d(_,"href","https://huggingface.co/facebook/detr-resnet-50"),d(_,"rel","nofollow")},m(j,T){m(j,n,T),e(n,f),e(f,s),e(n,h),e(n,_),e(_,k),e(n,A)},d(j){j&&t(n)}}}function Uz($){let n,f;return n=new N({props:{code:`    import json
    import requests
    headers = {"Authorization": f"Bearer {API_TOKEN}"}
    API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
    def query(filename):
        with open(filename, "rb") as f:
            data = f.read()
        response = requests.request("POST", API_URL, headers=headers, data=data)
        return json.loads(response.content.decode("utf-8"))
    data = query("cats.jpg")`,highlighted:`    <span class="hljs-keyword">import</span> json
    <span class="hljs-keyword">import</span> requests
    headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
    API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
            data = f.read()
        response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
        <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
    data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Lz($){let n,f;return n=new R({props:{$$slots:{default:[Uz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function zz($){let n,f;return n=new N({props:{code:`    import fetch from "node-fetch";
    import fs from "fs";
    async function query(filename) {
        const data = fs.readFileSync(filename);
        const response = await fetch(
            "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
            {
                headers: { Authorization: \`Bearer \${API_TOKEN}\` },
                method: "POST",
                body: data,
            }
        );
        const result = await response.json();
        return result;
    }
    query("cats.jpg").then((response) => {
        console.log(JSON.stringify(response));
    });
    // [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    <span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
    <span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
    async <span class="hljs-keyword">function</span> query(filename) {
        const data = fs.readFileSync(filename);
        const response = await <span class="hljs-keyword">fetch</span>(
            &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
            {
                headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
                <span class="hljs-keyword">method</span>: &quot;POST&quot;,
                body: data,
            }
        );
        const result = await response.json();
        <span class="hljs-keyword">return</span> result;
    }
    query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
        console.log(<span class="hljs-type">JSON</span>.stringify(response));
    });
    // [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Mz($){let n,f;return n=new R({props:{$$slots:{default:[zz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Fz($){let n,f;return n=new N({props:{code:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
            -X POST \\
            --data-binary '@cats.jpg' \\
            -H "Authorization: Bearer \${HF_API_TOKEN}"
    # [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`    curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
            -X POST \\
            --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
            -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
    <span class="hljs-comment"># [{&quot;score&quot;:0.9982201457023621,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:40,&quot;ymin&quot;:70,&quot;xmax&quot;:175,&quot;ymax&quot;:117}},{&quot;score&quot;:0.9960021376609802,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:333,&quot;ymin&quot;:72,&quot;xmax&quot;:368,&quot;ymax&quot;:187}},{&quot;score&quot;:0.9954745173454285,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:0,&quot;ymin&quot;:1,&quot;xmax&quot;:639,&quot;ymax&quot;:473}},{&quot;score&quot;:0.9988006353378296,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:13,&quot;ymin&quot;:52,&quot;xmax&quot;:314,&quot;ymax&quot;:470}},{&quot;score&quot;:0.9986783862113953,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:345,&quot;ymin&quot;:23,&quot;xmax&quot;:640,&quot;ymax&quot;:368}}]</span>`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Jz($){let n,f;return n=new R({props:{$$slots:{default:[Fz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Kz($){let n,f;return n=new N({props:{code:`    self.assertEqual(
        deep_round(data, 4),
        [
            {
                "score": 0.9982,
                "label": "remote",
                "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
            },
            {
                "score": 0.9960,
                "label": "remote",
                "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
            },
            {
                "score": 0.9955,
                "label": "couch",
                "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
            },
            {
                "score": 0.9988,
                "label": "cat",
                "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
            },
            {
                "score": 0.9987,
                "label": "cat",
                "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
            },
        ],
    )`,highlighted:`    self.assertEqual(
        deep_round(data, <span class="hljs-number">4</span>),
        [
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
            },
            {
                <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
                <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
            },
        ],
    )`}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p:P,i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Wz($){let n,f;return n=new R({props:{$$slots:{default:[Kz]},$$scope:{ctx:$}}}),{c(){q(n.$$.fragment)},l(s){v(n.$$.fragment,s)},m(s,h){y(n,s,h),f=!0},p(s,h){const _={};h&2&&(_.$$scope={dirty:h,ctx:s}),n.$set(_)},i(s){f||(E(n.$$.fragment,s),f=!0)},o(s){w(n.$$.fragment,s),f=!1},d(s){b(n,s)}}}function Yz($){let n,f,s,h,_,k,A,j,T,O,D,ne,Re,Q,W,at,Fl,Na,Ne,bw,o$,Jl,Tw,l$,nt,bR,i$,rt,TR,u$,xe,ot,ed,xa,jw,td,kw,f$,Kl,Aw,c$,lt,p$,Sa,Dw,Ia,Ow,d$,Wl,Pw,h$,it,g$,Yl,Rw,m$,ut,sd,Ha,Vl,Nw,xw,ad,Sw,z,Ba,Ca,nd,Iw,Hw,Bw,Xl,Cw,Gw,Ga,Ua,rd,Uw,Lw,zw,Ql,Mw,Fw,La,Zl,Jw,Kw,de,Ww,od,Yw,Vw,ld,Xw,Qw,Zw,za,ei,eb,tb,ft,sb,id,ab,nb,rb,Ma,ti,ud,ob,lb,si,ib,ub,Fa,ai,fb,cb,ct,pb,fd,db,hb,gb,Ja,ni,mb,_b,pt,$b,cd,qb,vb,yb,Ka,ri,Eb,wb,dt,bb,pd,Tb,jb,_$,oi,kb,$$,li,Ab,q$,ht,v$,gt,dd,Wa,ii,Db,Ob,hd,Pb,Se,Ya,ui,gd,Rb,Nb,fi,xb,Sb,Va,ci,md,Ib,Hb,pi,Bb,Cb,Xa,di,_d,Gb,Ub,mt,Lb,$d,zb,Mb,y$,Ie,_t,qd,Qa,Fb,vd,Jb,E$,hi,Kb,w$,$t,b$,Za,Wb,en,Yb,T$,gi,Vb,j$,qt,k$,mi,Xb,A$,vt,yd,tn,_i,Qb,Zb,Ed,e0,Z,sn,an,wd,t0,s0,a0,$i,n0,r0,nn,qi,bd,o0,l0,vi,i0,u0,rn,yi,f0,c0,yt,p0,Td,d0,h0,g0,on,Ei,m0,_0,Et,$0,jd,q0,v0,y0,ln,wi,E0,w0,wt,b0,kd,T0,j0,D$,bi,k0,O$,bt,Ad,un,Ti,A0,D0,Dd,O0,Od,fn,ji,Pd,P0,R0,ki,N0,P$,He,Tt,Rd,cn,x0,Nd,S0,R$,jt,I0,Ai,H0,B0,N$,kt,x$,pn,C0,dn,G0,S$,Di,U0,I$,At,H$,Oi,L0,B$,Dt,xd,hn,Pi,z0,M0,Sd,F0,G,gn,mn,Id,J0,K0,W0,Ri,Y0,V0,_n,Ni,Hd,X0,Q0,xi,Z0,eT,$n,Si,tT,sT,he,aT,Bd,nT,rT,Cd,oT,lT,iT,qn,Ii,uT,fT,ge,cT,Gd,pT,dT,Ud,hT,gT,mT,vn,Hi,_T,$T,me,qT,Ld,vT,yT,zd,ET,wT,bT,yn,Bi,TT,jT,re,kT,Md,AT,DT,Fd,OT,PT,Jd,RT,NT,xT,En,Ci,ST,IT,_e,HT,Kd,BT,CT,Wd,GT,UT,LT,wn,Gi,zT,MT,Ot,FT,Yd,JT,KT,WT,bn,Ui,YT,VT,Pt,XT,Vd,QT,ZT,e3,Tn,Li,Xd,t3,s3,zi,a3,n3,jn,Mi,r3,o3,Rt,l3,Qd,i3,u3,f3,kn,Fi,c3,p3,Nt,d3,Zd,h3,g3,m3,An,Ji,_3,$3,xt,q3,eh,v3,y3,C$,Ki,E3,G$,St,th,Dn,Wi,w3,b3,sh,T3,ah,On,Yi,nh,j3,k3,Vi,A3,U$,Be,It,rh,Pn,D3,oh,O3,L$,Xi,P3,z$,Ht,M$,Rn,R3,Nn,N3,F$,Qi,x3,J$,Bt,K$,Zi,S3,W$,Ct,lh,xn,eu,I3,H3,ih,B3,x,Sn,In,uh,C3,G3,U3,fh,L3,Hn,tu,z3,M3,su,F3,J3,Bn,au,K3,W3,nu,Y3,V3,Cn,ru,X3,Q3,Gt,Z3,ch,ej,tj,sj,Gn,ou,ph,aj,nj,lu,rj,oj,Un,iu,lj,ij,$e,uj,dh,fj,cj,hh,pj,dj,hj,Ln,uu,gj,mj,qe,_j,gh,$j,qj,mh,vj,yj,Ej,zn,fu,wj,bj,ve,Tj,_h,jj,kj,$h,Aj,Dj,Oj,Mn,cu,Pj,Rj,oe,Nj,qh,xj,Sj,vh,Ij,Hj,yh,Bj,Cj,Gj,Fn,pu,Uj,Lj,ye,zj,Eh,Mj,Fj,wh,Jj,Kj,Wj,Jn,du,Yj,Vj,Ut,Xj,bh,Qj,Zj,e5,Kn,hu,t5,s5,Lt,a5,Th,n5,r5,o5,Wn,gu,jh,l5,i5,mu,u5,f5,Yn,_u,c5,p5,zt,d5,kh,h5,g5,m5,Vn,$u,_5,$5,Mt,q5,Ah,v5,y5,E5,Xn,qu,w5,b5,Ft,T5,Dh,j5,k5,Y$,vu,A5,V$,Jt,Oh,Qn,yu,D5,O5,Ph,P5,ie,Zn,Eu,Rh,R5,N5,wu,x5,S5,er,bu,Nh,I5,H5,Tu,B5,C5,tr,ju,G5,U5,ku,L5,z5,sr,Au,M5,F5,Du,J5,X$,Ce,Kt,xh,ar,K5,Sh,W5,Q$,Ou,Y5,Z$,Wt,eq,nr,V5,rr,X5,tq,Pu,Q5,sq,Yt,aq,Ru,Z5,nq,Vt,Ih,or,Nu,e4,t4,Hh,s4,J,lr,ir,Bh,a4,n4,r4,Ch,o4,ur,xu,l4,i4,Su,u4,f4,fr,Iu,c4,p4,Hu,d4,h4,cr,Bu,Gh,g4,m4,Cu,_4,$4,pr,Gu,q4,v4,Xt,y4,Uh,E4,w4,b4,dr,Uu,T4,j4,Qt,k4,Lh,A4,D4,O4,hr,Lu,P4,R4,Zt,N4,zh,x4,S4,rq,zu,I4,oq,es,lq,ts,Mh,gr,Mu,H4,B4,Fh,C4,ue,mr,Fu,Jh,G4,U4,Ju,L4,z4,_r,Ku,Kh,M4,F4,Wu,J4,K4,$r,Yu,Wh,W4,Y4,Vu,V4,X4,qr,Xu,Yh,Q4,Z4,Qu,ek,iq,Ge,ss,Vh,vr,tk,Xh,sk,uq,Zu,ak,fq,as,cq,Ue,nk,yr,rk,ok,Er,lk,pq,ef,ik,dq,ns,hq,tf,uk,gq,sf,fk,mq,rs,_q,os,Qh,wr,af,ck,pk,Zh,dk,fe,br,nf,eg,hk,gk,rf,mk,_k,Tr,of,tg,$k,qk,lf,vk,yk,jr,uf,sg,Ek,wk,ls,bk,ag,Tk,jk,kk,kr,ff,ng,Ak,Dk,is,Ok,rg,Pk,Rk,$q,Le,us,og,Ar,Nk,lg,xk,qq,cf,Sk,vq,fs,yq,Dr,Ik,Or,Hk,Eq,pf,Bk,wq,cs,bq,df,Ck,Tq,ps,ig,Pr,hf,Gk,Uk,ug,Lk,ee,Rr,Nr,fg,zk,Mk,Fk,gf,Jk,Kk,xr,mf,cg,Wk,Yk,_f,Vk,Xk,Sr,$f,Qk,Zk,ds,e7,pg,t7,s7,a7,Ir,qf,n7,r7,hs,o7,dg,l7,i7,u7,Hr,vf,f7,c7,gs,p7,hg,d7,h7,jq,yf,g7,kq,ms,Aq,_s,gg,Br,Ef,m7,_7,mg,$7,Cr,Gr,wf,_g,q7,v7,bf,y7,E7,Ur,Tf,$g,w7,b7,jf,T7,Dq,ze,$s,qg,Lr,j7,vg,k7,Oq,zr,A7,kf,D7,Pq,Me,qs,yg,Mr,O7,Eg,P7,Rq,Af,R7,Nq,vs,xq,Fe,N7,Fr,x7,S7,Jr,I7,Sq,Df,H7,Iq,ys,Hq,Of,B7,Bq,Es,wg,Kr,Pf,C7,G7,bg,U7,K,Wr,Yr,Tg,L7,z7,M7,Rf,F7,J7,Vr,Nf,jg,K7,W7,xf,Y7,V7,Xr,Sf,X7,Q7,S,Z7,kg,e6,t6,s6,a6,Ag,n6,r6,o6,l6,Dg,i6,u6,f6,c6,Og,p6,d6,Pg,h6,g6,m6,_6,Rg,$6,q6,Ng,v6,y6,E6,w6,xg,b6,T6,Sg,j6,k6,A6,Qr,If,Ig,D6,O6,Hf,P6,R6,Zr,Bf,N6,x6,ws,S6,Hg,I6,H6,B6,eo,Cf,C6,G6,bs,U6,Bg,L6,z6,M6,to,Gf,F6,J6,Ts,K6,Cg,W6,Y6,Cq,Uf,V6,Gq,js,Uq,ks,Gg,so,Lf,X6,Q6,Ug,Z6,te,ao,zf,Lg,e9,t9,Mf,s9,a9,no,Ff,zg,n9,r9,Jf,o9,l9,ro,Kf,Mg,i9,u9,Wf,f9,c9,oo,Yf,Fg,p9,d9,As,h9,Jg,g9,m9,_9,lo,Vf,Kg,$9,q9,Ds,v9,Wg,y9,E9,Lq,Je,Os,Yg,io,w9,Vg,b9,zq,Xf,T9,Mq,Ps,Fq,uo,j9,fo,k9,Jq,Qf,A9,Kq,Rs,Wq,Zf,D9,Yq,Ns,Xg,co,ec,O9,P9,Qg,R9,I,po,ho,Zg,N9,x9,S9,tc,I9,H9,go,sc,em,B9,C9,ac,G9,U9,mo,nc,L9,z9,Ee,M9,tm,F9,J9,sm,K9,W9,Y9,_o,rc,V9,X9,le,Q9,am,Z9,e8,nm,t8,s8,rm,a8,n8,r8,$o,oc,o8,l8,we,i8,om,u8,f8,lm,c8,p8,d8,qo,lc,h8,g8,xs,m8,im,_8,$8,q8,vo,ic,v8,y8,be,E8,um,w8,b8,fm,T8,j8,k8,yo,uc,A8,D8,Te,O8,cm,P8,R8,pm,N8,x8,S8,Eo,fc,I8,H8,je,B8,dm,C8,G8,hm,U8,L8,z8,wo,cc,M8,F8,Ss,J8,gm,K8,W8,Y8,bo,pc,V8,X8,Is,Q8,mm,Z8,eA,tA,To,dc,_m,sA,aA,hc,nA,rA,jo,gc,oA,lA,Hs,iA,$m,uA,fA,cA,ko,mc,pA,dA,Bs,hA,qm,gA,mA,_A,Ao,_c,$A,qA,Cs,vA,vm,yA,EA,Vq,$c,wA,Xq,Gs,Qq,Us,ym,Do,qc,bA,TA,Em,jA,wm,Oo,vc,bm,kA,AA,yc,DA,Zq,Ke,Ls,Tm,Po,OA,jm,PA,e1,zs,RA,Ec,NA,xA,t1,We,Ms,km,Ro,SA,Am,IA,s1,wc,HA,a1,Fs,n1,No,BA,xo,CA,r1,bc,GA,o1,Js,l1,Tc,UA,i1,Ks,Dm,So,jc,LA,zA,Om,MA,se,Io,Ho,Pm,FA,JA,KA,kc,WA,YA,Bo,Ac,Rm,VA,XA,Dc,QA,ZA,Co,Oc,eD,tD,Ws,sD,Nm,aD,nD,rD,Go,Pc,oD,lD,Ys,iD,xm,uD,fD,cD,Uo,Rc,pD,dD,Vs,hD,Sm,gD,mD,u1,Nc,_D,f1,Xs,c1,Qs,Im,Lo,xc,$D,qD,Hm,vD,ce,zo,Sc,Bm,yD,ED,Ic,wD,bD,Mo,Hc,Cm,TD,jD,Bc,kD,AD,Fo,Cc,Gm,DD,OD,Gc,PD,RD,Jo,Uc,Um,ND,xD,Lc,SD,p1,Ye,Zs,Lm,Ko,ID,zm,HD,d1,zc,BD,h1,ea,g1,ta,m1,pe,CD,Wo,GD,UD,Yo,LD,zD,Vo,MD,_1,Mc,FD,$1,sa,q1,Fc,JD,v1,aa,Mm,Xo,Jc,KD,WD,Fm,YD,Jm,Qo,Zo,Km,VD,XD,QD,Kc,ZD,y1,Wc,eO,E1,Yc,tO,w1,na,b1,ra,Wm,el,Vc,sO,aO,Ym,nO,Vm,tl,Xc,Xm,rO,oO,Qc,lO,T1,Ve,oa,Qm,sl,iO,Zm,uO,j1,Zc,fO,k1,la,A1,Xe,cO,al,pO,dO,nl,hO,D1,ep,gO,O1,ia,e_,rl,tp,mO,_O,t_,$O,ae,ol,ll,s_,qO,vO,yO,sp,EO,wO,il,ap,a_,bO,TO,np,jO,kO,ul,rp,AO,DO,ua,OO,n_,PO,RO,NO,fl,op,xO,SO,fa,IO,r_,HO,BO,CO,cl,lp,GO,UO,ca,LO,o_,zO,MO,P1,ip,FO,R1,pa,l_,pl,up,JO,KO,i_,WO,u_,dl,fp,f_,YO,VO,cp,XO,N1,pp,QO,x1,Qe,da,c_,hl,ZO,p_,eP,S1,dp,tP,I1,ha,H1,Ze,sP,gl,aP,nP,ml,rP,B1,hp,oP,C1,ga,G1,gp,lP,U1,ma,d_,_l,mp,iP,uP,h_,fP,g_,$l,ql,m_,cP,pP,dP,_p,hP,L1,$p,gP,z1,_a,M1,$a,__,vl,qp,mP,_P,$_,$P,yl,El,vp,q_,qP,vP,yp,yP,EP,wl,Ep,v_,wP,bP,wp,TP,F1,et,qa,y_,bl,jP,E_,kP,J1,bp,AP,K1,va,W1,Tl,DP,jl,OP,Y1,Tp,PP,V1,ya,X1,Ea,RP,kl,NP,xP,Q1,wa,w_,Al,jp,SP,IP,b_,HP,T_,Dl,Ol,j_,BP,CP,GP,kp,UP,Z1,Ap,LP,ev,ba,tv,Ta,k_,Pl,Dp,zP,MP,A_,FP,tt,Rl,Op,D_,JP,KP,Pp,WP,YP,Nl,Rp,O_,VP,XP,Np,QP,ZP,xl,xp,P_,eR,tR,Sp,sR,sv;return k=new F({}),Q=new F({}),xa=new F({}),lt=new Y({props:{$$slots:{default:[MU]},$$scope:{ctx:$}}}),it=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[VU],js:[WU],python:[JU]},$$scope:{ctx:$}}}),ht=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[QU]},$$scope:{ctx:$}}}),Qa=new F({}),$t=new Y({props:{$$slots:{default:[ZU]},$$scope:{ctx:$}}}),qt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[rL],js:[aL],python:[tL]},$$scope:{ctx:$}}}),cn=new F({}),kt=new Y({props:{$$slots:{default:[oL]},$$scope:{ctx:$}}}),At=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[pL],js:[fL],python:[iL]},$$scope:{ctx:$}}}),Pn=new F({}),Ht=new Y({props:{$$slots:{default:[dL]},$$scope:{ctx:$}}}),Bt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[qL],js:[_L],python:[gL]},$$scope:{ctx:$}}}),ar=new F({}),Wt=new Y({props:{$$slots:{default:[vL]},$$scope:{ctx:$}}}),Yt=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[jL],js:[bL],python:[EL]},$$scope:{ctx:$}}}),es=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[AL]},$$scope:{ctx:$}}}),vr=new F({}),as=new Y({props:{$$slots:{default:[DL]},$$scope:{ctx:$}}}),ns=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[SL],js:[NL],python:[PL]},$$scope:{ctx:$}}}),rs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[HL]},$$scope:{ctx:$}}}),Ar=new F({}),fs=new Y({props:{$$slots:{default:[BL]},$$scope:{ctx:$}}}),cs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[ML],js:[LL],python:[GL]},$$scope:{ctx:$}}}),ms=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[JL]},$$scope:{ctx:$}}}),Lr=new F({}),Mr=new F({}),vs=new Y({props:{$$slots:{default:[KL]},$$scope:{ctx:$}}}),ys=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[ZL],js:[XL],python:[YL]},$$scope:{ctx:$}}}),js=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[tz]},$$scope:{ctx:$}}}),io=new F({}),Ps=new Y({props:{$$slots:{default:[sz]},$$scope:{ctx:$}}}),Rs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[iz],js:[oz],python:[nz]},$$scope:{ctx:$}}}),Gs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[fz]},$$scope:{ctx:$}}}),Po=new F({}),Ro=new F({}),Fs=new Y({props:{$$slots:{default:[cz]},$$scope:{ctx:$}}}),Js=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[_z],js:[gz],python:[dz]},$$scope:{ctx:$}}}),Xs=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[qz]},$$scope:{ctx:$}}}),Ko=new F({}),ea=new Y({props:{$$slots:{default:[vz]},$$scope:{ctx:$}}}),ta=new Y({props:{$$slots:{default:[yz]},$$scope:{ctx:$}}}),sa=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[kz],js:[Tz],python:[wz]},$$scope:{ctx:$}}}),na=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[Dz]},$$scope:{ctx:$}}}),sl=new F({}),la=new Y({props:{$$slots:{default:[Oz]},$$scope:{ctx:$}}}),hl=new F({}),ha=new Y({props:{$$slots:{default:[Pz]},$$scope:{ctx:$}}}),ga=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[Hz],js:[Sz],python:[Nz]},$$scope:{ctx:$}}}),_a=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[Cz]},$$scope:{ctx:$}}}),bl=new F({}),va=new Y({props:{$$slots:{default:[Gz]},$$scope:{ctx:$}}}),ya=new L({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[Jz],js:[Mz],python:[Lz]},$$scope:{ctx:$}}}),ba=new L({props:{python:!0,js:!0,curl:!0,$$slots:{python:[Wz]},$$scope:{ctx:$}}}),{c(){n=r("meta"),f=c(),s=r("h1"),h=r("a"),_=r("span"),q(k.$$.fragment),A=c(),j=r("span"),T=i("Detailed parameters"),O=c(),D=r("h2"),ne=r("a"),Re=r("span"),q(Q.$$.fragment),W=c(),at=r("span"),Fl=i("Which task is used by this model ?"),Na=c(),Ne=r("p"),bw=i(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),o$=c(),Jl=r("p"),Tw=i("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),l$=c(),nt=r("img"),i$=c(),rt=r("img"),u$=c(),xe=r("h2"),ot=r("a"),ed=r("span"),q(xa.$$.fragment),jw=c(),td=r("span"),kw=i("Zero-shot classification task"),f$=c(),Kl=r("p"),Aw=i(`This task is a super useful to try it out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence and you get a result.`),c$=c(),q(lt.$$.fragment),p$=c(),Sa=r("p"),Dw=i("Available with: "),Ia=r("a"),Ow=i("\u{1F917} Transformers"),d$=c(),Wl=r("p"),Pw=i("Request:"),h$=c(),q(it.$$.fragment),g$=c(),Yl=r("p"),Rw=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),m$=c(),ut=r("table"),sd=r("thead"),Ha=r("tr"),Vl=r("th"),Nw=i("All parameters"),xw=c(),ad=r("th"),Sw=c(),z=r("tbody"),Ba=r("tr"),Ca=r("td"),nd=r("strong"),Iw=i("inputs"),Hw=i(" (required)"),Bw=c(),Xl=r("td"),Cw=i("a string or list of strings"),Gw=c(),Ga=r("tr"),Ua=r("td"),rd=r("strong"),Uw=i("parameters"),Lw=i(" (required)"),zw=c(),Ql=r("td"),Mw=i("a dict containing the following keys:"),Fw=c(),La=r("tr"),Zl=r("td"),Jw=i("candidate_labels (required)"),Kw=c(),de=r("td"),Ww=i("a list of strings that are potential classes for "),od=r("code"),Yw=i("inputs"),Vw=i(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),ld=r("code"),Xw=i("multi_label=True"),Qw=i(" and do the scaling on your end. )"),Zw=c(),za=r("tr"),ei=r("td"),eb=i("multi_label"),tb=c(),ft=r("td"),sb=i("(Default: "),id=r("code"),ab=i("false"),nb=i(") Boolean that is set to True if classes can overlap"),rb=c(),Ma=r("tr"),ti=r("td"),ud=r("strong"),ob=i("options"),lb=c(),si=r("td"),ib=i("a dict containing the following keys:"),ub=c(),Fa=r("tr"),ai=r("td"),fb=i("use_gpu"),cb=c(),ct=r("td"),pb=i("(Default: "),fd=r("code"),db=i("false"),hb=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),gb=c(),Ja=r("tr"),ni=r("td"),mb=i("use_cache"),_b=c(),pt=r("td"),$b=i("(Default: "),cd=r("code"),qb=i("true"),vb=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),yb=c(),Ka=r("tr"),ri=r("td"),Eb=i("wait_for_model"),wb=c(),dt=r("td"),bb=i("(Default: "),pd=r("code"),Tb=i("false"),jb=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),_$=c(),oi=r("p"),kb=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),$$=c(),li=r("p"),Ab=i("Response:"),q$=c(),q(ht.$$.fragment),v$=c(),gt=r("table"),dd=r("thead"),Wa=r("tr"),ii=r("th"),Db=i("Returned values"),Ob=c(),hd=r("th"),Pb=c(),Se=r("tbody"),Ya=r("tr"),ui=r("td"),gd=r("strong"),Rb=i("sequence"),Nb=c(),fi=r("td"),xb=i("The string sent as an input"),Sb=c(),Va=r("tr"),ci=r("td"),md=r("strong"),Ib=i("labels"),Hb=c(),pi=r("td"),Bb=i("The list of strings for labels that you sent (in order)"),Cb=c(),Xa=r("tr"),di=r("td"),_d=r("strong"),Gb=i("scores"),Ub=c(),mt=r("td"),Lb=i("a list of floats that correspond the the probability of label, in the same order as "),$d=r("code"),zb=i("labels"),Mb=i("."),y$=c(),Ie=r("h2"),_t=r("a"),qd=r("span"),q(Qa.$$.fragment),Fb=c(),vd=r("span"),Jb=i("Translation task"),E$=c(),hi=r("p"),Kb=i("This task is well known to translate text from one language to another"),w$=c(),q($t.$$.fragment),b$=c(),Za=r("p"),Wb=i("Available with: "),en=r("a"),Yb=i("\u{1F917} Transformers"),T$=c(),gi=r("p"),Vb=i("Example:"),j$=c(),q(qt.$$.fragment),k$=c(),mi=r("p"),Xb=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),A$=c(),vt=r("table"),yd=r("thead"),tn=r("tr"),_i=r("th"),Qb=i("All parameters"),Zb=c(),Ed=r("th"),e0=c(),Z=r("tbody"),sn=r("tr"),an=r("td"),wd=r("strong"),t0=i("inputs"),s0=i(" (required)"),a0=c(),$i=r("td"),n0=i("a string to be translated in the original languages"),r0=c(),nn=r("tr"),qi=r("td"),bd=r("strong"),o0=i("options"),l0=c(),vi=r("td"),i0=i("a dict containing the following keys:"),u0=c(),rn=r("tr"),yi=r("td"),f0=i("use_gpu"),c0=c(),yt=r("td"),p0=i("(Default: "),Td=r("code"),d0=i("false"),h0=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),g0=c(),on=r("tr"),Ei=r("td"),m0=i("use_cache"),_0=c(),Et=r("td"),$0=i("(Default: "),jd=r("code"),q0=i("true"),v0=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),y0=c(),ln=r("tr"),wi=r("td"),E0=i("wait_for_model"),w0=c(),wt=r("td"),b0=i("(Default: "),kd=r("code"),T0=i("false"),j0=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),D$=c(),bi=r("p"),k0=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),O$=c(),bt=r("table"),Ad=r("thead"),un=r("tr"),Ti=r("th"),A0=i("Returned values"),D0=c(),Dd=r("th"),O0=c(),Od=r("tbody"),fn=r("tr"),ji=r("td"),Pd=r("strong"),P0=i("translation_text"),R0=c(),ki=r("td"),N0=i("The string after translation"),P$=c(),He=r("h2"),Tt=r("a"),Rd=r("span"),q(cn.$$.fragment),x0=c(),Nd=r("span"),S0=i("Summarization task"),R$=c(),jt=r("p"),I0=i(`This task is well known to summarize text a big text into a small text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss you summarization needs,
please get in touch <`),Ai=r("a"),H0=i("api-enterprise@huggingface.co"),B0=i(">"),N$=c(),q(kt.$$.fragment),x$=c(),pn=r("p"),C0=i("Available with: "),dn=r("a"),G0=i("\u{1F917} Transformers"),S$=c(),Di=r("p"),U0=i("Example:"),I$=c(),q(At.$$.fragment),H$=c(),Oi=r("p"),L0=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),B$=c(),Dt=r("table"),xd=r("thead"),hn=r("tr"),Pi=r("th"),z0=i("All parameters"),M0=c(),Sd=r("th"),F0=c(),G=r("tbody"),gn=r("tr"),mn=r("td"),Id=r("strong"),J0=i("inputs"),K0=i(" (required)"),W0=c(),Ri=r("td"),Y0=i("a string to be summarized"),V0=c(),_n=r("tr"),Ni=r("td"),Hd=r("strong"),X0=i("parameters"),Q0=c(),xi=r("td"),Z0=i("a dict containing the following keys:"),eT=c(),$n=r("tr"),Si=r("td"),tT=i("min_length"),sT=c(),he=r("td"),aT=i("(Default: "),Bd=r("code"),nT=i("None"),rT=i("). Integer to define the minimum length "),Cd=r("strong"),oT=i("in tokens"),lT=i(" of the output summary."),iT=c(),qn=r("tr"),Ii=r("td"),uT=i("max_length"),fT=c(),ge=r("td"),cT=i("(Default: "),Gd=r("code"),pT=i("None"),dT=i("). Integer to define the maximum length "),Ud=r("strong"),hT=i("in tokens"),gT=i(" of the output summary."),mT=c(),vn=r("tr"),Hi=r("td"),_T=i("top_k"),$T=c(),me=r("td"),qT=i("(Default: "),Ld=r("code"),vT=i("None"),yT=i("). Integer to define the top tokens considered within the "),zd=r("code"),ET=i("sample"),wT=i(" operation to create new text."),bT=c(),yn=r("tr"),Bi=r("td"),TT=i("top_p"),jT=c(),re=r("td"),kT=i("(Default: "),Md=r("code"),AT=i("None"),DT=i("). Float to define the tokens that are within the "),Fd=r("code"),OT=i("sample"),PT=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Jd=r("code"),RT=i("top_p"),NT=i("."),xT=c(),En=r("tr"),Ci=r("td"),ST=i("temperature"),IT=c(),_e=r("td"),HT=i("(Default: "),Kd=r("code"),BT=i("1.0"),CT=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Wd=r("code"),GT=i("100.0"),UT=i(" is getting closer to uniform probability."),LT=c(),wn=r("tr"),Gi=r("td"),zT=i("repetition_penalty"),MT=c(),Ot=r("td"),FT=i("(Default: "),Yd=r("code"),JT=i("None"),KT=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),WT=c(),bn=r("tr"),Ui=r("td"),YT=i("max_time"),VT=c(),Pt=r("td"),XT=i("(Default: "),Vd=r("code"),QT=i("None"),ZT=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),e3=c(),Tn=r("tr"),Li=r("td"),Xd=r("strong"),t3=i("options"),s3=c(),zi=r("td"),a3=i("a dict containing the following keys:"),n3=c(),jn=r("tr"),Mi=r("td"),r3=i("use_gpu"),o3=c(),Rt=r("td"),l3=i("(Default: "),Qd=r("code"),i3=i("false"),u3=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),f3=c(),kn=r("tr"),Fi=r("td"),c3=i("use_cache"),p3=c(),Nt=r("td"),d3=i("(Default: "),Zd=r("code"),h3=i("true"),g3=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),m3=c(),An=r("tr"),Ji=r("td"),_3=i("wait_for_model"),$3=c(),xt=r("td"),q3=i("(Default: "),eh=r("code"),v3=i("false"),y3=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),C$=c(),Ki=r("p"),E3=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),G$=c(),St=r("table"),th=r("thead"),Dn=r("tr"),Wi=r("th"),w3=i("Returned values"),b3=c(),sh=r("th"),T3=c(),ah=r("tbody"),On=r("tr"),Yi=r("td"),nh=r("strong"),j3=i("summarization_text"),k3=c(),Vi=r("td"),A3=i("The string after translation"),U$=c(),Be=r("h2"),It=r("a"),rh=r("span"),q(Pn.$$.fragment),D3=c(),oh=r("span"),O3=i("Conversational task"),L$=c(),Xi=r("p"),P3=i(`This task corresponds to any chatbot like structure. Models tend to have
shorted max_length, so please check with caution when using a given
model if you need long range dependency or not.`),z$=c(),q(Ht.$$.fragment),M$=c(),Rn=r("p"),R3=i("Available with: "),Nn=r("a"),N3=i("\u{1F917} Transformers"),F$=c(),Qi=r("p"),x3=i("Example:"),J$=c(),q(Bt.$$.fragment),K$=c(),Zi=r("p"),S3=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),W$=c(),Ct=r("table"),lh=r("thead"),xn=r("tr"),eu=r("th"),I3=i("All parameters"),H3=c(),ih=r("th"),B3=c(),x=r("tbody"),Sn=r("tr"),In=r("td"),uh=r("strong"),C3=i("inputs"),G3=i(" (required)"),U3=c(),fh=r("td"),L3=c(),Hn=r("tr"),tu=r("td"),z3=i("text (required)"),M3=c(),su=r("td"),F3=i("The last input from the user in the conversation."),J3=c(),Bn=r("tr"),au=r("td"),K3=i("generated_responses"),W3=c(),nu=r("td"),Y3=i("A list of strings corresponding to the earlier replies from the model."),V3=c(),Cn=r("tr"),ru=r("td"),X3=i("past_user_inputs"),Q3=c(),Gt=r("td"),Z3=i("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),ch=r("code"),ej=i("generated_responses"),tj=i("."),sj=c(),Gn=r("tr"),ou=r("td"),ph=r("strong"),aj=i("parameters"),nj=c(),lu=r("td"),rj=i("a dict containing the following keys:"),oj=c(),Un=r("tr"),iu=r("td"),lj=i("min_length"),ij=c(),$e=r("td"),uj=i("(Default: "),dh=r("code"),fj=i("None"),cj=i("). Integer to define the minimum length "),hh=r("strong"),pj=i("in tokens"),dj=i(" of the output summary."),hj=c(),Ln=r("tr"),uu=r("td"),gj=i("max_length"),mj=c(),qe=r("td"),_j=i("(Default: "),gh=r("code"),$j=i("None"),qj=i("). Integer to define the maximum length "),mh=r("strong"),vj=i("in tokens"),yj=i(" of the output summary."),Ej=c(),zn=r("tr"),fu=r("td"),wj=i("top_k"),bj=c(),ve=r("td"),Tj=i("(Default: "),_h=r("code"),jj=i("None"),kj=i("). Integer to define the top tokens considered within the "),$h=r("code"),Aj=i("sample"),Dj=i(" operation to create new text."),Oj=c(),Mn=r("tr"),cu=r("td"),Pj=i("top_p"),Rj=c(),oe=r("td"),Nj=i("(Default: "),qh=r("code"),xj=i("None"),Sj=i("). Float to define the tokens that are within the "),vh=r("code"),Ij=i("sample"),Hj=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),yh=r("code"),Bj=i("top_p"),Cj=i("."),Gj=c(),Fn=r("tr"),pu=r("td"),Uj=i("temperature"),Lj=c(),ye=r("td"),zj=i("(Default: "),Eh=r("code"),Mj=i("1.0"),Fj=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),wh=r("code"),Jj=i("100.0"),Kj=i(" is getting closer to uniform probability."),Wj=c(),Jn=r("tr"),du=r("td"),Yj=i("repetition_penalty"),Vj=c(),Ut=r("td"),Xj=i("(Default: "),bh=r("code"),Qj=i("None"),Zj=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),e5=c(),Kn=r("tr"),hu=r("td"),t5=i("max_time"),s5=c(),Lt=r("td"),a5=i("(Default: "),Th=r("code"),n5=i("None"),r5=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),o5=c(),Wn=r("tr"),gu=r("td"),jh=r("strong"),l5=i("options"),i5=c(),mu=r("td"),u5=i("a dict containing the following keys:"),f5=c(),Yn=r("tr"),_u=r("td"),c5=i("use_gpu"),p5=c(),zt=r("td"),d5=i("(Default: "),kh=r("code"),h5=i("false"),g5=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),m5=c(),Vn=r("tr"),$u=r("td"),_5=i("use_cache"),$5=c(),Mt=r("td"),q5=i("(Default: "),Ah=r("code"),v5=i("true"),y5=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),E5=c(),Xn=r("tr"),qu=r("td"),w5=i("wait_for_model"),b5=c(),Ft=r("td"),T5=i("(Default: "),Dh=r("code"),j5=i("false"),k5=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Y$=c(),vu=r("p"),A5=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),V$=c(),Jt=r("table"),Oh=r("thead"),Qn=r("tr"),yu=r("th"),D5=i("Returned values"),O5=c(),Ph=r("th"),P5=c(),ie=r("tbody"),Zn=r("tr"),Eu=r("td"),Rh=r("strong"),R5=i("generated_text"),N5=c(),wu=r("td"),x5=i("The answer of the bot"),S5=c(),er=r("tr"),bu=r("td"),Nh=r("strong"),I5=i("conversation"),H5=c(),Tu=r("td"),B5=i("A facility dictionnary to send back for the next input (with the new user input addition)."),C5=c(),tr=r("tr"),ju=r("td"),G5=i("past_user_inputs"),U5=c(),ku=r("td"),L5=i("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),z5=c(),sr=r("tr"),Au=r("td"),M5=i("generated_responses"),F5=c(),Du=r("td"),J5=i("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),X$=c(),Ce=r("h2"),Kt=r("a"),xh=r("span"),q(ar.$$.fragment),K5=c(),Sh=r("span"),W5=i("Table question answering task"),Q$=c(),Ou=r("p"),Y5=i(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),Z$=c(),q(Wt.$$.fragment),eq=c(),nr=r("p"),V5=i("Available with: "),rr=r("a"),X5=i("\u{1F917} Transformers"),tq=c(),Pu=r("p"),Q5=i("Example:"),sq=c(),q(Yt.$$.fragment),aq=c(),Ru=r("p"),Z5=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),nq=c(),Vt=r("table"),Ih=r("thead"),or=r("tr"),Nu=r("th"),e4=i("All parameters"),t4=c(),Hh=r("th"),s4=c(),J=r("tbody"),lr=r("tr"),ir=r("td"),Bh=r("strong"),a4=i("inputs"),n4=i(" (required)"),r4=c(),Ch=r("td"),o4=c(),ur=r("tr"),xu=r("td"),l4=i("query (required)"),i4=c(),Su=r("td"),u4=i("The query in plain text that you want to ask the table"),f4=c(),fr=r("tr"),Iu=r("td"),c4=i("table (required)"),p4=c(),Hu=r("td"),d4=i("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),h4=c(),cr=r("tr"),Bu=r("td"),Gh=r("strong"),g4=i("options"),m4=c(),Cu=r("td"),_4=i("a dict containing the following keys:"),$4=c(),pr=r("tr"),Gu=r("td"),q4=i("use_gpu"),v4=c(),Xt=r("td"),y4=i("(Default: "),Uh=r("code"),E4=i("false"),w4=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),b4=c(),dr=r("tr"),Uu=r("td"),T4=i("use_cache"),j4=c(),Qt=r("td"),k4=i("(Default: "),Lh=r("code"),A4=i("true"),D4=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),O4=c(),hr=r("tr"),Lu=r("td"),P4=i("wait_for_model"),R4=c(),Zt=r("td"),N4=i("(Default: "),zh=r("code"),x4=i("false"),S4=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),rq=c(),zu=r("p"),I4=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),oq=c(),q(es.$$.fragment),lq=c(),ts=r("table"),Mh=r("thead"),gr=r("tr"),Mu=r("th"),H4=i("Returned values"),B4=c(),Fh=r("th"),C4=c(),ue=r("tbody"),mr=r("tr"),Fu=r("td"),Jh=r("strong"),G4=i("answer"),U4=c(),Ju=r("td"),L4=i("The plaintext answer"),z4=c(),_r=r("tr"),Ku=r("td"),Kh=r("strong"),M4=i("coordinates"),F4=c(),Wu=r("td"),J4=i("a list of coordinates of the cells references in the answer"),K4=c(),$r=r("tr"),Yu=r("td"),Wh=r("strong"),W4=i("cells"),Y4=c(),Vu=r("td"),V4=i("a list of coordinates of the cells contents"),X4=c(),qr=r("tr"),Xu=r("td"),Yh=r("strong"),Q4=i("aggregator"),Z4=c(),Qu=r("td"),ek=i("The aggregator used to get the answer"),iq=c(),Ge=r("h2"),ss=r("a"),Vh=r("span"),q(vr.$$.fragment),tk=c(),Xh=r("span"),sk=i("Question answering task"),uq=c(),Zu=r("p"),ak=i("Want to have a nice know-it-all bot that can answer any questions ?"),fq=c(),q(as.$$.fragment),cq=c(),Ue=r("p"),nk=i("Available with: "),yr=r("a"),rk=i("\u{1F917}Transformers"),ok=i(` and
`),Er=r("a"),lk=i("AllenNLP"),pq=c(),ef=r("p"),ik=i("Example:"),dq=c(),q(ns.$$.fragment),hq=c(),tf=r("p"),uk=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),gq=c(),sf=r("p"),fk=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),mq=c(),q(rs.$$.fragment),_q=c(),os=r("table"),Qh=r("thead"),wr=r("tr"),af=r("th"),ck=i("Returned values"),pk=c(),Zh=r("th"),dk=c(),fe=r("tbody"),br=r("tr"),nf=r("td"),eg=r("strong"),hk=i("answer"),gk=c(),rf=r("td"),mk=i("A string that\u2019s the answer within the text."),_k=c(),Tr=r("tr"),of=r("td"),tg=r("strong"),$k=i("score"),qk=c(),lf=r("td"),vk=i("A floats that represents how likely that the answer is correct"),yk=c(),jr=r("tr"),uf=r("td"),sg=r("strong"),Ek=i("start"),wk=c(),ls=r("td"),bk=i("The index (string wise) of the start of the answer within "),ag=r("code"),Tk=i("context"),jk=i("."),kk=c(),kr=r("tr"),ff=r("td"),ng=r("strong"),Ak=i("stop"),Dk=c(),is=r("td"),Ok=i("The index (string wise) of the stop of the answer within "),rg=r("code"),Pk=i("context"),Rk=i("."),$q=c(),Le=r("h2"),us=r("a"),og=r("span"),q(Ar.$$.fragment),Nk=c(),lg=r("span"),xk=i("Text-classification task"),qq=c(),cf=r("p"),Sk=i(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),vq=c(),q(fs.$$.fragment),yq=c(),Dr=r("p"),Ik=i("Available with: "),Or=r("a"),Hk=i("\u{1F917} Transformers"),Eq=c(),pf=r("p"),Bk=i("Example:"),wq=c(),q(cs.$$.fragment),bq=c(),df=r("p"),Ck=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Tq=c(),ps=r("table"),ig=r("thead"),Pr=r("tr"),hf=r("th"),Gk=i("All parameters"),Uk=c(),ug=r("th"),Lk=c(),ee=r("tbody"),Rr=r("tr"),Nr=r("td"),fg=r("strong"),zk=i("inputs"),Mk=i(" (required)"),Fk=c(),gf=r("td"),Jk=i("a string to be classified"),Kk=c(),xr=r("tr"),mf=r("td"),cg=r("strong"),Wk=i("options"),Yk=c(),_f=r("td"),Vk=i("a dict containing the following keys:"),Xk=c(),Sr=r("tr"),$f=r("td"),Qk=i("use_gpu"),Zk=c(),ds=r("td"),e7=i("(Default: "),pg=r("code"),t7=i("false"),s7=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),a7=c(),Ir=r("tr"),qf=r("td"),n7=i("use_cache"),r7=c(),hs=r("td"),o7=i("(Default: "),dg=r("code"),l7=i("true"),i7=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),u7=c(),Hr=r("tr"),vf=r("td"),f7=i("wait_for_model"),c7=c(),gs=r("td"),p7=i("(Default: "),hg=r("code"),d7=i("false"),h7=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),jq=c(),yf=r("p"),g7=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),kq=c(),q(ms.$$.fragment),Aq=c(),_s=r("table"),gg=r("thead"),Br=r("tr"),Ef=r("th"),m7=i("Returned values"),_7=c(),mg=r("th"),$7=c(),Cr=r("tbody"),Gr=r("tr"),wf=r("td"),_g=r("strong"),q7=i("label"),v7=c(),bf=r("td"),y7=i("The label for the class (model specific)"),E7=c(),Ur=r("tr"),Tf=r("td"),$g=r("strong"),w7=i("score"),b7=c(),jf=r("td"),T7=i("A floats that represents how likely is that the text belongs the this class."),Dq=c(),ze=r("h2"),$s=r("a"),qg=r("span"),q(Lr.$$.fragment),j7=c(),vg=r("span"),k7=i("Named Entity Recognition (NER) task"),Oq=c(),zr=r("p"),A7=i("See "),kf=r("a"),D7=i("Token-classification task"),Pq=c(),Me=r("h2"),qs=r("a"),yg=r("span"),q(Mr.$$.fragment),O7=c(),Eg=r("span"),P7=i("Token-classification task"),Rq=c(),Af=r("p"),R7=i(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),Nq=c(),q(vs.$$.fragment),xq=c(),Fe=r("p"),N7=i("Available with: "),Fr=r("a"),x7=i("\u{1F917} Transformers"),S7=i(`,
`),Jr=r("a"),I7=i("Flair"),Sq=c(),Df=r("p"),H7=i("Example:"),Iq=c(),q(ys.$$.fragment),Hq=c(),Of=r("p"),B7=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Bq=c(),Es=r("table"),wg=r("thead"),Kr=r("tr"),Pf=r("th"),C7=i("All parameters"),G7=c(),bg=r("th"),U7=c(),K=r("tbody"),Wr=r("tr"),Yr=r("td"),Tg=r("strong"),L7=i("inputs"),z7=i(" (required)"),M7=c(),Rf=r("td"),F7=i("a string to be classified"),J7=c(),Vr=r("tr"),Nf=r("td"),jg=r("strong"),K7=i("parameters"),W7=c(),xf=r("td"),Y7=i("a dict containing the following key:"),V7=c(),Xr=r("tr"),Sf=r("td"),X7=i("aggregation_strategy"),Q7=c(),S=r("td"),Z7=i("(Default: "),kg=r("code"),e6=i("simple"),t6=i("). There are several aggregation strategies: "),s6=r("br"),a6=c(),Ag=r("code"),n6=i("none"),r6=i(": Every token gets classified without further aggregation. "),o6=r("br"),l6=c(),Dg=r("code"),i6=i("simple"),u6=i(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),f6=r("br"),c6=c(),Og=r("code"),p6=i("first"),d6=i(": Same as the "),Pg=r("code"),h6=i("simple"),g6=i(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),m6=r("br"),_6=c(),Rg=r("code"),$6=i("average"),q6=i(": Same as the "),Ng=r("code"),v6=i("simple"),y6=i(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),E6=r("br"),w6=c(),xg=r("code"),b6=i("max"),T6=i(": Same as the "),Sg=r("code"),j6=i("simple"),k6=i(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),A6=c(),Qr=r("tr"),If=r("td"),Ig=r("strong"),D6=i("options"),O6=c(),Hf=r("td"),P6=i("a dict containing the following keys:"),R6=c(),Zr=r("tr"),Bf=r("td"),N6=i("use_gpu"),x6=c(),ws=r("td"),S6=i("(Default: "),Hg=r("code"),I6=i("false"),H6=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),B6=c(),eo=r("tr"),Cf=r("td"),C6=i("use_cache"),G6=c(),bs=r("td"),U6=i("(Default: "),Bg=r("code"),L6=i("true"),z6=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),M6=c(),to=r("tr"),Gf=r("td"),F6=i("wait_for_model"),J6=c(),Ts=r("td"),K6=i("(Default: "),Cg=r("code"),W6=i("false"),Y6=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Cq=c(),Uf=r("p"),V6=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Gq=c(),q(js.$$.fragment),Uq=c(),ks=r("table"),Gg=r("thead"),so=r("tr"),Lf=r("th"),X6=i("Returned values"),Q6=c(),Ug=r("th"),Z6=c(),te=r("tbody"),ao=r("tr"),zf=r("td"),Lg=r("strong"),e9=i("entity_group"),t9=c(),Mf=r("td"),s9=i("The type for the entity being recognized (model specific)."),a9=c(),no=r("tr"),Ff=r("td"),zg=r("strong"),n9=i("score"),r9=c(),Jf=r("td"),o9=i("How likely the entity was recognized."),l9=c(),ro=r("tr"),Kf=r("td"),Mg=r("strong"),i9=i("word"),u9=c(),Wf=r("td"),f9=i("The string that was captured"),c9=c(),oo=r("tr"),Yf=r("td"),Fg=r("strong"),p9=i("start"),d9=c(),As=r("td"),h9=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Jg=r("code"),g9=i("word"),m9=i(" occurs multiple times."),_9=c(),lo=r("tr"),Vf=r("td"),Kg=r("strong"),$9=i("end"),q9=c(),Ds=r("td"),v9=i("The offset stringwise where the answer is located. Useful to disambiguate if "),Wg=r("code"),y9=i("word"),E9=i(" occurs multiple times."),Lq=c(),Je=r("h2"),Os=r("a"),Yg=r("span"),q(io.$$.fragment),w9=c(),Vg=r("span"),b9=i("Text-generation task"),zq=c(),Xf=r("p"),T9=i("Use to continue text from a prompt. This is a very generic task."),Mq=c(),q(Ps.$$.fragment),Fq=c(),uo=r("p"),j9=i("Available with: "),fo=r("a"),k9=i("\u{1F917} Transformers"),Jq=c(),Qf=r("p"),A9=i("Example:"),Kq=c(),q(Rs.$$.fragment),Wq=c(),Zf=r("p"),D9=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Yq=c(),Ns=r("table"),Xg=r("thead"),co=r("tr"),ec=r("th"),O9=i("All parameters"),P9=c(),Qg=r("th"),R9=c(),I=r("tbody"),po=r("tr"),ho=r("td"),Zg=r("strong"),N9=i("inputs"),x9=i(" (required):"),S9=c(),tc=r("td"),I9=i("a string to be generated from"),H9=c(),go=r("tr"),sc=r("td"),em=r("strong"),B9=i("parameters"),C9=c(),ac=r("td"),G9=i("dict containing the following keys:"),U9=c(),mo=r("tr"),nc=r("td"),L9=i("top_k"),z9=c(),Ee=r("td"),M9=i("(Default: "),tm=r("code"),F9=i("None"),J9=i("). Integer to define the top tokens considered within the "),sm=r("code"),K9=i("sample"),W9=i(" operation to create new text."),Y9=c(),_o=r("tr"),rc=r("td"),V9=i("top_p"),X9=c(),le=r("td"),Q9=i("(Default: "),am=r("code"),Z9=i("None"),e8=i("). Float to define the tokens that are within the "),nm=r("code"),t8=i("sample"),s8=i(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),rm=r("code"),a8=i("top_p"),n8=i("."),r8=c(),$o=r("tr"),oc=r("td"),o8=i("temperature"),l8=c(),we=r("td"),i8=i("(Default: "),om=r("code"),u8=i("1.0"),f8=i("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),lm=r("code"),c8=i("100.0"),p8=i(" is getting closer to uniform probability."),d8=c(),qo=r("tr"),lc=r("td"),h8=i("repetition_penalty"),g8=c(),xs=r("td"),m8=i("(Default: "),im=r("code"),_8=i("None"),$8=i("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),q8=c(),vo=r("tr"),ic=r("td"),v8=i("max_new_tokens"),y8=c(),be=r("td"),E8=i("(Default: "),um=r("code"),w8=i("None"),b8=i("). Int (0-250). The amount of new tokens to be generated, this does "),fm=r("strong"),T8=i("not"),j8=i(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),k8=c(),yo=r("tr"),uc=r("td"),A8=i("max_time"),D8=c(),Te=r("td"),O8=i("(Default: "),cm=r("code"),P8=i("None"),R8=i("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),pm=r("code"),N8=i("max_new_tokens"),x8=i(" for best results."),S8=c(),Eo=r("tr"),fc=r("td"),I8=i("return_full_text"),H8=c(),je=r("td"),B8=i("(Default: "),dm=r("code"),C8=i("True"),G8=i("). Bool. If set to False, the return results will "),hm=r("strong"),U8=i("not"),L8=i(" contain the original query making it easier for prompting."),z8=c(),wo=r("tr"),cc=r("td"),M8=i("num_return_sequences"),F8=c(),Ss=r("td"),J8=i("(Default: "),gm=r("code"),K8=i("1"),W8=i("). Integer. The number of proposition you want to be returned."),Y8=c(),bo=r("tr"),pc=r("td"),V8=i("do_sample"),X8=c(),Is=r("td"),Q8=i("(Optional: "),mm=r("code"),Z8=i("True"),eA=i("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),tA=c(),To=r("tr"),dc=r("td"),_m=r("strong"),sA=i("options"),aA=c(),hc=r("td"),nA=i("a dict containing the following keys:"),rA=c(),jo=r("tr"),gc=r("td"),oA=i("use_gpu"),lA=c(),Hs=r("td"),iA=i("(Default: "),$m=r("code"),uA=i("false"),fA=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),cA=c(),ko=r("tr"),mc=r("td"),pA=i("use_cache"),dA=c(),Bs=r("td"),hA=i("(Default: "),qm=r("code"),gA=i("true"),mA=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),_A=c(),Ao=r("tr"),_c=r("td"),$A=i("wait_for_model"),qA=c(),Cs=r("td"),vA=i("(Default: "),vm=r("code"),yA=i("false"),EA=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Vq=c(),$c=r("p"),wA=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),Xq=c(),q(Gs.$$.fragment),Qq=c(),Us=r("table"),ym=r("thead"),Do=r("tr"),qc=r("th"),bA=i("Returned values"),TA=c(),Em=r("th"),jA=c(),wm=r("tbody"),Oo=r("tr"),vc=r("td"),bm=r("strong"),kA=i("generated_text"),AA=c(),yc=r("td"),DA=i("The continuated string"),Zq=c(),Ke=r("h2"),Ls=r("a"),Tm=r("span"),q(Po.$$.fragment),OA=c(),jm=r("span"),PA=i("Text2text-generation task"),e1=c(),zs=r("p"),RA=i("Essentially "),Ec=r("a"),NA=i("Text-generation task"),xA=i(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),t1=c(),We=r("h2"),Ms=r("a"),km=r("span"),q(Ro.$$.fragment),SA=c(),Am=r("span"),IA=i("Fill mask task"),s1=c(),wc=r("p"),HA=i(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),a1=c(),q(Fs.$$.fragment),n1=c(),No=r("p"),BA=i("Available with: "),xo=r("a"),CA=i("\u{1F917} Transformers"),r1=c(),bc=r("p"),GA=i("Example:"),o1=c(),q(Js.$$.fragment),l1=c(),Tc=r("p"),UA=i(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),i1=c(),Ks=r("table"),Dm=r("thead"),So=r("tr"),jc=r("th"),LA=i("All parameters"),zA=c(),Om=r("th"),MA=c(),se=r("tbody"),Io=r("tr"),Ho=r("td"),Pm=r("strong"),FA=i("inputs"),JA=i(" (required):"),KA=c(),kc=r("td"),WA=i("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),YA=c(),Bo=r("tr"),Ac=r("td"),Rm=r("strong"),VA=i("options"),XA=c(),Dc=r("td"),QA=i("a dict containing the following keys:"),ZA=c(),Co=r("tr"),Oc=r("td"),eD=i("use_gpu"),tD=c(),Ws=r("td"),sD=i("(Default: "),Nm=r("code"),aD=i("false"),nD=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),rD=c(),Go=r("tr"),Pc=r("td"),oD=i("use_cache"),lD=c(),Ys=r("td"),iD=i("(Default: "),xm=r("code"),uD=i("true"),fD=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),cD=c(),Uo=r("tr"),Rc=r("td"),pD=i("wait_for_model"),dD=c(),Vs=r("td"),hD=i("(Default: "),Sm=r("code"),gD=i("false"),mD=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),u1=c(),Nc=r("p"),_D=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),f1=c(),q(Xs.$$.fragment),c1=c(),Qs=r("table"),Im=r("thead"),Lo=r("tr"),xc=r("th"),$D=i("Returned values"),qD=c(),Hm=r("th"),vD=c(),ce=r("tbody"),zo=r("tr"),Sc=r("td"),Bm=r("strong"),yD=i("sequence"),ED=c(),Ic=r("td"),wD=i("The actual sequence of tokens that ran against the model (may contain special tokens)"),bD=c(),Mo=r("tr"),Hc=r("td"),Cm=r("strong"),TD=i("score"),jD=c(),Bc=r("td"),kD=i("The probability for this token."),AD=c(),Fo=r("tr"),Cc=r("td"),Gm=r("strong"),DD=i("token"),OD=c(),Gc=r("td"),PD=i("The id of the token"),RD=c(),Jo=r("tr"),Uc=r("td"),Um=r("strong"),ND=i("token_str"),xD=c(),Lc=r("td"),SD=i("The string representation of the token"),p1=c(),Ye=r("h2"),Zs=r("a"),Lm=r("span"),q(Ko.$$.fragment),ID=c(),zm=r("span"),HD=i("Automatic speech recognition task"),d1=c(),zc=r("p"),BD=i(`This task reads some audio input and outputs the said words within the
audio files.`),h1=c(),q(ea.$$.fragment),g1=c(),q(ta.$$.fragment),m1=c(),pe=r("p"),CD=i("Available with: "),Wo=r("a"),GD=i("\u{1F917} Transformers"),UD=c(),Yo=r("a"),LD=i("ESPnet"),zD=i(` and
`),Vo=r("a"),MD=i("SpeechBrain"),_1=c(),Mc=r("p"),FD=i("Request:"),$1=c(),q(sa.$$.fragment),q1=c(),Fc=r("p"),JD=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),v1=c(),aa=r("table"),Mm=r("thead"),Xo=r("tr"),Jc=r("th"),KD=i("All parameters"),WD=c(),Fm=r("th"),YD=c(),Jm=r("tbody"),Qo=r("tr"),Zo=r("td"),Km=r("strong"),VD=i("no parameter"),XD=i(" (required)"),QD=c(),Kc=r("td"),ZD=i("a binary representation of the audio file. No other parameters are currently allowed."),y1=c(),Wc=r("p"),eO=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),E1=c(),Yc=r("p"),tO=i("Response:"),w1=c(),q(na.$$.fragment),b1=c(),ra=r("table"),Wm=r("thead"),el=r("tr"),Vc=r("th"),sO=i("Returned values"),aO=c(),Ym=r("th"),nO=c(),Vm=r("tbody"),tl=r("tr"),Xc=r("td"),Xm=r("strong"),rO=i("text"),oO=c(),Qc=r("td"),lO=i("The string that was recognized within the audio file."),T1=c(),Ve=r("h2"),oa=r("a"),Qm=r("span"),q(sl.$$.fragment),iO=c(),Zm=r("span"),uO=i("Feature-extraction task"),j1=c(),Zc=r("p"),fO=i(`This task reads some text and outputs raw float values, that usually
consumed as part of a semantic database/semantic search.`),k1=c(),q(la.$$.fragment),A1=c(),Xe=r("p"),cO=i("Available with: "),al=r("a"),pO=i("\u{1F917} Transformers"),dO=c(),nl=r("a"),hO=i("Sentence-transformers"),D1=c(),ep=r("p"),gO=i("Request:"),O1=c(),ia=r("table"),e_=r("thead"),rl=r("tr"),tp=r("th"),mO=i("All parameters"),_O=c(),t_=r("th"),$O=c(),ae=r("tbody"),ol=r("tr"),ll=r("td"),s_=r("strong"),qO=i("inputs"),vO=i(" (required):"),yO=c(),sp=r("td"),EO=i("a string or a list of strings to get the features from."),wO=c(),il=r("tr"),ap=r("td"),a_=r("strong"),bO=i("options"),TO=c(),np=r("td"),jO=i("a dict containing the following keys:"),kO=c(),ul=r("tr"),rp=r("td"),AO=i("use_gpu"),DO=c(),ua=r("td"),OO=i("(Default: "),n_=r("code"),PO=i("false"),RO=i("). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),NO=c(),fl=r("tr"),op=r("td"),xO=i("use_cache"),SO=c(),fa=r("td"),IO=i("(Default: "),r_=r("code"),HO=i("true"),BO=i("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),CO=c(),cl=r("tr"),lp=r("td"),GO=i("wait_for_model"),UO=c(),ca=r("td"),LO=i("(Default: "),o_=r("code"),zO=i("false"),MO=i(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),P1=c(),ip=r("p"),FO=i("Return value is either a dict or a list of dicts if you sent a list of inputs"),R1=c(),pa=r("table"),l_=r("thead"),pl=r("tr"),up=r("th"),JO=i("Returned values"),KO=c(),i_=r("th"),WO=c(),u_=r("tbody"),dl=r("tr"),fp=r("td"),f_=r("strong"),YO=i("A list of float (or list of list of floats)"),VO=c(),cp=r("td"),XO=i("The numbers that are the representation features of the input."),N1=c(),pp=r("small"),QO=i(`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),x1=c(),Qe=r("h2"),da=r("a"),c_=r("span"),q(hl.$$.fragment),ZO=c(),p_=r("span"),eP=i("Audio-classification task"),S1=c(),dp=r("p"),tP=i("This task reads some audio input and outputs the likelihood of classes."),I1=c(),q(ha.$$.fragment),H1=c(),Ze=r("p"),sP=i("Available with: "),gl=r("a"),aP=i("\u{1F917} Transformers"),nP=c(),ml=r("a"),rP=i("SpeechBrain"),B1=c(),hp=r("p"),oP=i("Request:"),C1=c(),q(ga.$$.fragment),G1=c(),gp=r("p"),lP=i(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),U1=c(),ma=r("table"),d_=r("thead"),_l=r("tr"),mp=r("th"),iP=i("All parameters"),uP=c(),h_=r("th"),fP=c(),g_=r("tbody"),$l=r("tr"),ql=r("td"),m_=r("strong"),cP=i("no parameter"),pP=i(" (required)"),dP=c(),_p=r("td"),hP=i("a binary representation of the audio file. No other parameters are currently allowed."),L1=c(),$p=r("p"),gP=i("Return value is a dict"),z1=c(),q(_a.$$.fragment),M1=c(),$a=r("table"),__=r("thead"),vl=r("tr"),qp=r("th"),mP=i("Returned values"),_P=c(),$_=r("th"),$P=c(),yl=r("tbody"),El=r("tr"),vp=r("td"),q_=r("strong"),qP=i("label"),vP=c(),yp=r("td"),yP=i("The label for the class (model specific)"),EP=c(),wl=r("tr"),Ep=r("td"),v_=r("strong"),wP=i("score"),bP=c(),wp=r("td"),TP=i("A floats that represents how likely is that the audio file belongs the this class."),F1=c(),et=r("h2"),qa=r("a"),y_=r("span"),q(bl.$$.fragment),jP=c(),E_=r("span"),kP=i("Object-detection task"),J1=c(),bp=r("p"),AP=i(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),K1=c(),q(va.$$.fragment),W1=c(),Tl=r("p"),DP=i("Available with: "),jl=r("a"),OP=i("\u{1F917} Transformers"),Y1=c(),Tp=r("p"),PP=i("Request:"),V1=c(),q(ya.$$.fragment),X1=c(),Ea=r("p"),RP=i(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),kl=r("a"),NP=i(`Pillow
supports`),xP=i("."),Q1=c(),wa=r("table"),w_=r("thead"),Al=r("tr"),jp=r("th"),SP=i("All parameters"),IP=c(),b_=r("th"),HP=c(),T_=r("tbody"),Dl=r("tr"),Ol=r("td"),j_=r("strong"),BP=i("no parameter"),CP=i(" (required)"),GP=c(),kp=r("td"),UP=i("a binary representation of the image file. No other parameters are currently allowed."),Z1=c(),Ap=r("p"),LP=i("Return value is a dict"),ev=c(),q(ba.$$.fragment),tv=c(),Ta=r("table"),k_=r("thead"),Pl=r("tr"),Dp=r("th"),zP=i("Returned values"),MP=c(),A_=r("th"),FP=c(),tt=r("tbody"),Rl=r("tr"),Op=r("td"),D_=r("strong"),JP=i("label"),KP=c(),Pp=r("td"),WP=i("The label for the class (model specific) of a detected object."),YP=c(),Nl=r("tr"),Rp=r("td"),O_=r("strong"),VP=i("score"),XP=c(),Np=r("td"),QP=i("A float that represents how likely it is that the detected object belongs to the given class."),ZP=c(),xl=r("tr"),xp=r("td"),P_=r("strong"),eR=i("box"),tR=c(),Sp=r("td"),sR=i("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),this.h()},l(a){const g=LU('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(t),f=p(a),s=o(a,"H1",{class:!0});var Sl=l(s);h=o(Sl,"A",{id:!0,class:!0,href:!0});var R_=l(h);_=o(R_,"SPAN",{});var N_=l(_);v(k.$$.fragment,N_),N_.forEach(t),R_.forEach(t),A=p(Sl),j=o(Sl,"SPAN",{});var x_=l(j);T=u(x_,"Detailed parameters"),x_.forEach(t),Sl.forEach(t),O=p(a),D=o(a,"H2",{class:!0});var Il=l(D);ne=o(Il,"A",{id:!0,class:!0,href:!0});var S_=l(ne);Re=o(S_,"SPAN",{});var I_=l(Re);v(Q.$$.fragment,I_),I_.forEach(t),S_.forEach(t),W=p(Il),at=o(Il,"SPAN",{});var H_=l(at);Fl=u(H_,"Which task is used by this model ?"),H_.forEach(t),Il.forEach(t),Na=p(a),Ne=o(a,"P",{});var B_=l(Ne);bw=u(B_,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),B_.forEach(t),o$=p(a),Jl=o(a,"P",{});var C_=l(Jl);Tw=u(C_,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),C_.forEach(t),l$=p(a),nt=o(a,"IMG",{class:!0,src:!0,width:!0}),i$=p(a),rt=o(a,"IMG",{class:!0,src:!0,width:!0}),u$=p(a),xe=o(a,"H2",{class:!0});var Hl=l(xe);ot=o(Hl,"A",{id:!0,class:!0,href:!0});var G_=l(ot);ed=o(G_,"SPAN",{});var U_=l(ed);v(xa.$$.fragment,U_),U_.forEach(t),G_.forEach(t),jw=p(Hl),td=o(Hl,"SPAN",{});var L_=l(td);kw=u(L_,"Zero-shot classification task"),L_.forEach(t),Hl.forEach(t),f$=p(a),Kl=o(a,"P",{});var z_=l(Kl);Aw=u(z_,`This task is a super useful to try it out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence and you get a result.`),z_.forEach(t),c$=p(a),v(lt.$$.fragment,a),p$=p(a),Sa=o(a,"P",{});var Ip=l(Sa);Dw=u(Ip,"Available with: "),Ia=o(Ip,"A",{href:!0,rel:!0});var M_=l(Ia);Ow=u(M_,"\u{1F917} Transformers"),M_.forEach(t),Ip.forEach(t),d$=p(a),Wl=o(a,"P",{});var F_=l(Wl);Pw=u(F_,"Request:"),F_.forEach(t),h$=p(a),v(it.$$.fragment,a),g$=p(a),Yl=o(a,"P",{});var J_=l(Yl);Rw=u(J_,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),J_.forEach(t),m$=p(a),ut=o(a,"TABLE",{});var Bl=l(ut);sd=o(Bl,"THEAD",{});var K_=l(sd);Ha=o(K_,"TR",{});var Cl=l(Ha);Vl=o(Cl,"TH",{align:!0});var W_=l(Vl);Nw=u(W_,"All parameters"),W_.forEach(t),xw=p(Cl),ad=o(Cl,"TH",{align:!0}),l(ad).forEach(t),Cl.forEach(t),K_.forEach(t),Sw=p(Bl),z=o(Bl,"TBODY",{});var M=l(z);Ba=o(M,"TR",{});var Gl=l(Ba);Ca=o(Gl,"TD",{align:!0});var Hp=l(Ca);nd=o(Hp,"STRONG",{});var Y_=l(nd);Iw=u(Y_,"inputs"),Y_.forEach(t),Hw=u(Hp," (required)"),Hp.forEach(t),Bw=p(Gl),Xl=o(Gl,"TD",{align:!0});var V_=l(Xl);Cw=u(V_,"a string or list of strings"),V_.forEach(t),Gl.forEach(t),Gw=p(M),Ga=o(M,"TR",{});var Ul=l(Ga);Ua=o(Ul,"TD",{align:!0});var Bp=l(Ua);rd=o(Bp,"STRONG",{});var X_=l(rd);Uw=u(X_,"parameters"),X_.forEach(t),Lw=u(Bp," (required)"),Bp.forEach(t),zw=p(Ul),Ql=o(Ul,"TD",{align:!0});var Q_=l(Ql);Mw=u(Q_,"a dict containing the following keys:"),Q_.forEach(t),Ul.forEach(t),Fw=p(M),La=o(M,"TR",{});var Ll=l(La);Zl=o(Ll,"TD",{align:!0});var Z_=l(Zl);Jw=u(Z_,"candidate_labels (required)"),Z_.forEach(t),Kw=p(Ll),de=o(Ll,"TD",{align:!0});var st=l(de);Ww=u(st,"a list of strings that are potential classes for "),od=o(st,"CODE",{});var e$=l(od);Yw=u(e$,"inputs"),e$.forEach(t),Vw=u(st,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),ld=o(st,"CODE",{});var t$=l(ld);Xw=u(t$,"multi_label=True"),t$.forEach(t),Qw=u(st," and do the scaling on your end. )"),st.forEach(t),Ll.forEach(t),Zw=p(M),za=o(M,"TR",{});var zl=l(za);ei=o(zl,"TD",{align:!0});var jR=l(ei);eb=u(jR,"multi_label"),jR.forEach(t),tb=p(zl),ft=o(zl,"TD",{align:!0});var av=l(ft);sb=u(av,"(Default: "),id=o(av,"CODE",{});var kR=l(id);ab=u(kR,"false"),kR.forEach(t),nb=u(av,") Boolean that is set to True if classes can overlap"),av.forEach(t),zl.forEach(t),rb=p(M),Ma=o(M,"TR",{});var nv=l(Ma);ti=o(nv,"TD",{align:!0});var AR=l(ti);ud=o(AR,"STRONG",{});var DR=l(ud);ob=u(DR,"options"),DR.forEach(t),AR.forEach(t),lb=p(nv),si=o(nv,"TD",{align:!0});var OR=l(si);ib=u(OR,"a dict containing the following keys:"),OR.forEach(t),nv.forEach(t),ub=p(M),Fa=o(M,"TR",{});var rv=l(Fa);ai=o(rv,"TD",{align:!0});var PR=l(ai);fb=u(PR,"use_gpu"),PR.forEach(t),cb=p(rv),ct=o(rv,"TD",{align:!0});var ov=l(ct);pb=u(ov,"(Default: "),fd=o(ov,"CODE",{});var RR=l(fd);db=u(RR,"false"),RR.forEach(t),hb=u(ov,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),ov.forEach(t),rv.forEach(t),gb=p(M),Ja=o(M,"TR",{});var lv=l(Ja);ni=o(lv,"TD",{align:!0});var NR=l(ni);mb=u(NR,"use_cache"),NR.forEach(t),_b=p(lv),pt=o(lv,"TD",{align:!0});var iv=l(pt);$b=u(iv,"(Default: "),cd=o(iv,"CODE",{});var xR=l(cd);qb=u(xR,"true"),xR.forEach(t),vb=u(iv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),iv.forEach(t),lv.forEach(t),yb=p(M),Ka=o(M,"TR",{});var uv=l(Ka);ri=o(uv,"TD",{align:!0});var SR=l(ri);Eb=u(SR,"wait_for_model"),SR.forEach(t),wb=p(uv),dt=o(uv,"TD",{align:!0});var fv=l(dt);bb=u(fv,"(Default: "),pd=o(fv,"CODE",{});var IR=l(pd);Tb=u(IR,"false"),IR.forEach(t),jb=u(fv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),fv.forEach(t),uv.forEach(t),M.forEach(t),Bl.forEach(t),_$=p(a),oi=o(a,"P",{});var HR=l(oi);kb=u(HR,"Return value is either a dict or a list of dicts if you sent a list of inputs"),HR.forEach(t),$$=p(a),li=o(a,"P",{});var BR=l(li);Ab=u(BR,"Response:"),BR.forEach(t),q$=p(a),v(ht.$$.fragment,a),v$=p(a),gt=o(a,"TABLE",{});var cv=l(gt);dd=o(cv,"THEAD",{});var CR=l(dd);Wa=o(CR,"TR",{});var pv=l(Wa);ii=o(pv,"TH",{align:!0});var GR=l(ii);Db=u(GR,"Returned values"),GR.forEach(t),Ob=p(pv),hd=o(pv,"TH",{align:!0}),l(hd).forEach(t),pv.forEach(t),CR.forEach(t),Pb=p(cv),Se=o(cv,"TBODY",{});var Cp=l(Se);Ya=o(Cp,"TR",{});var dv=l(Ya);ui=o(dv,"TD",{align:!0});var UR=l(ui);gd=o(UR,"STRONG",{});var LR=l(gd);Rb=u(LR,"sequence"),LR.forEach(t),UR.forEach(t),Nb=p(dv),fi=o(dv,"TD",{align:!0});var zR=l(fi);xb=u(zR,"The string sent as an input"),zR.forEach(t),dv.forEach(t),Sb=p(Cp),Va=o(Cp,"TR",{});var hv=l(Va);ci=o(hv,"TD",{align:!0});var MR=l(ci);md=o(MR,"STRONG",{});var FR=l(md);Ib=u(FR,"labels"),FR.forEach(t),MR.forEach(t),Hb=p(hv),pi=o(hv,"TD",{align:!0});var JR=l(pi);Bb=u(JR,"The list of strings for labels that you sent (in order)"),JR.forEach(t),hv.forEach(t),Cb=p(Cp),Xa=o(Cp,"TR",{});var gv=l(Xa);di=o(gv,"TD",{align:!0});var KR=l(di);_d=o(KR,"STRONG",{});var WR=l(_d);Gb=u(WR,"scores"),WR.forEach(t),KR.forEach(t),Ub=p(gv),mt=o(gv,"TD",{align:!0});var mv=l(mt);Lb=u(mv,"a list of floats that correspond the the probability of label, in the same order as "),$d=o(mv,"CODE",{});var YR=l($d);zb=u(YR,"labels"),YR.forEach(t),Mb=u(mv,"."),mv.forEach(t),gv.forEach(t),Cp.forEach(t),cv.forEach(t),y$=p(a),Ie=o(a,"H2",{class:!0});var _v=l(Ie);_t=o(_v,"A",{id:!0,class:!0,href:!0});var VR=l(_t);qd=o(VR,"SPAN",{});var XR=l(qd);v(Qa.$$.fragment,XR),XR.forEach(t),VR.forEach(t),Fb=p(_v),vd=o(_v,"SPAN",{});var QR=l(vd);Jb=u(QR,"Translation task"),QR.forEach(t),_v.forEach(t),E$=p(a),hi=o(a,"P",{});var ZR=l(hi);Kb=u(ZR,"This task is well known to translate text from one language to another"),ZR.forEach(t),w$=p(a),v($t.$$.fragment,a),b$=p(a),Za=o(a,"P",{});var aR=l(Za);Wb=u(aR,"Available with: "),en=o(aR,"A",{href:!0,rel:!0});var eN=l(en);Yb=u(eN,"\u{1F917} Transformers"),eN.forEach(t),aR.forEach(t),T$=p(a),gi=o(a,"P",{});var tN=l(gi);Vb=u(tN,"Example:"),tN.forEach(t),j$=p(a),v(qt.$$.fragment,a),k$=p(a),mi=o(a,"P",{});var sN=l(mi);Xb=u(sN,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),sN.forEach(t),A$=p(a),vt=o(a,"TABLE",{});var $v=l(vt);yd=o($v,"THEAD",{});var aN=l(yd);tn=o(aN,"TR",{});var qv=l(tn);_i=o(qv,"TH",{align:!0});var nN=l(_i);Qb=u(nN,"All parameters"),nN.forEach(t),Zb=p(qv),Ed=o(qv,"TH",{align:!0}),l(Ed).forEach(t),qv.forEach(t),aN.forEach(t),e0=p($v),Z=o($v,"TBODY",{});var ke=l(Z);sn=o(ke,"TR",{});var vv=l(sn);an=o(vv,"TD",{align:!0});var nR=l(an);wd=o(nR,"STRONG",{});var rN=l(wd);t0=u(rN,"inputs"),rN.forEach(t),s0=u(nR," (required)"),nR.forEach(t),a0=p(vv),$i=o(vv,"TD",{align:!0});var oN=l($i);n0=u(oN,"a string to be translated in the original languages"),oN.forEach(t),vv.forEach(t),r0=p(ke),nn=o(ke,"TR",{});var yv=l(nn);qi=o(yv,"TD",{align:!0});var lN=l(qi);bd=o(lN,"STRONG",{});var iN=l(bd);o0=u(iN,"options"),iN.forEach(t),lN.forEach(t),l0=p(yv),vi=o(yv,"TD",{align:!0});var uN=l(vi);i0=u(uN,"a dict containing the following keys:"),uN.forEach(t),yv.forEach(t),u0=p(ke),rn=o(ke,"TR",{});var Ev=l(rn);yi=o(Ev,"TD",{align:!0});var fN=l(yi);f0=u(fN,"use_gpu"),fN.forEach(t),c0=p(Ev),yt=o(Ev,"TD",{align:!0});var wv=l(yt);p0=u(wv,"(Default: "),Td=o(wv,"CODE",{});var cN=l(Td);d0=u(cN,"false"),cN.forEach(t),h0=u(wv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),wv.forEach(t),Ev.forEach(t),g0=p(ke),on=o(ke,"TR",{});var bv=l(on);Ei=o(bv,"TD",{align:!0});var pN=l(Ei);m0=u(pN,"use_cache"),pN.forEach(t),_0=p(bv),Et=o(bv,"TD",{align:!0});var Tv=l(Et);$0=u(Tv,"(Default: "),jd=o(Tv,"CODE",{});var dN=l(jd);q0=u(dN,"true"),dN.forEach(t),v0=u(Tv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Tv.forEach(t),bv.forEach(t),y0=p(ke),ln=o(ke,"TR",{});var jv=l(ln);wi=o(jv,"TD",{align:!0});var hN=l(wi);E0=u(hN,"wait_for_model"),hN.forEach(t),w0=p(jv),wt=o(jv,"TD",{align:!0});var kv=l(wt);b0=u(kv,"(Default: "),kd=o(kv,"CODE",{});var gN=l(kd);T0=u(gN,"false"),gN.forEach(t),j0=u(kv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),kv.forEach(t),jv.forEach(t),ke.forEach(t),$v.forEach(t),D$=p(a),bi=o(a,"P",{});var mN=l(bi);k0=u(mN,"Return value is either a dict or a list of dicts if you sent a list of inputs"),mN.forEach(t),O$=p(a),bt=o(a,"TABLE",{});var Av=l(bt);Ad=o(Av,"THEAD",{});var _N=l(Ad);un=o(_N,"TR",{});var Dv=l(un);Ti=o(Dv,"TH",{align:!0});var $N=l(Ti);A0=u($N,"Returned values"),$N.forEach(t),D0=p(Dv),Dd=o(Dv,"TH",{align:!0}),l(Dd).forEach(t),Dv.forEach(t),_N.forEach(t),O0=p(Av),Od=o(Av,"TBODY",{});var qN=l(Od);fn=o(qN,"TR",{});var Ov=l(fn);ji=o(Ov,"TD",{align:!0});var vN=l(ji);Pd=o(vN,"STRONG",{});var yN=l(Pd);P0=u(yN,"translation_text"),yN.forEach(t),vN.forEach(t),R0=p(Ov),ki=o(Ov,"TD",{align:!0});var EN=l(ki);N0=u(EN,"The string after translation"),EN.forEach(t),Ov.forEach(t),qN.forEach(t),Av.forEach(t),P$=p(a),He=o(a,"H2",{class:!0});var Pv=l(He);Tt=o(Pv,"A",{id:!0,class:!0,href:!0});var wN=l(Tt);Rd=o(wN,"SPAN",{});var bN=l(Rd);v(cn.$$.fragment,bN),bN.forEach(t),wN.forEach(t),x0=p(Pv),Nd=o(Pv,"SPAN",{});var TN=l(Nd);S0=u(TN,"Summarization task"),TN.forEach(t),Pv.forEach(t),R$=p(a),jt=o(a,"P",{});var Rv=l(jt);I0=u(Rv,`This task is well known to summarize text a big text into a small text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss you summarization needs,
please get in touch <`),Ai=o(Rv,"A",{href:!0});var jN=l(Ai);H0=u(jN,"api-enterprise@huggingface.co"),jN.forEach(t),B0=u(Rv,">"),Rv.forEach(t),N$=p(a),v(kt.$$.fragment,a),x$=p(a),pn=o(a,"P",{});var rR=l(pn);C0=u(rR,"Available with: "),dn=o(rR,"A",{href:!0,rel:!0});var kN=l(dn);G0=u(kN,"\u{1F917} Transformers"),kN.forEach(t),rR.forEach(t),S$=p(a),Di=o(a,"P",{});var AN=l(Di);U0=u(AN,"Example:"),AN.forEach(t),I$=p(a),v(At.$$.fragment,a),H$=p(a),Oi=o(a,"P",{});var DN=l(Oi);L0=u(DN,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),DN.forEach(t),B$=p(a),Dt=o(a,"TABLE",{});var Nv=l(Dt);xd=o(Nv,"THEAD",{});var ON=l(xd);hn=o(ON,"TR",{});var xv=l(hn);Pi=o(xv,"TH",{align:!0});var PN=l(Pi);z0=u(PN,"All parameters"),PN.forEach(t),M0=p(xv),Sd=o(xv,"TH",{align:!0}),l(Sd).forEach(t),xv.forEach(t),ON.forEach(t),F0=p(Nv),G=o(Nv,"TBODY",{});var U=l(G);gn=o(U,"TR",{});var Sv=l(gn);mn=o(Sv,"TD",{align:!0});var oR=l(mn);Id=o(oR,"STRONG",{});var RN=l(Id);J0=u(RN,"inputs"),RN.forEach(t),K0=u(oR," (required)"),oR.forEach(t),W0=p(Sv),Ri=o(Sv,"TD",{align:!0});var NN=l(Ri);Y0=u(NN,"a string to be summarized"),NN.forEach(t),Sv.forEach(t),V0=p(U),_n=o(U,"TR",{});var Iv=l(_n);Ni=o(Iv,"TD",{align:!0});var xN=l(Ni);Hd=o(xN,"STRONG",{});var SN=l(Hd);X0=u(SN,"parameters"),SN.forEach(t),xN.forEach(t),Q0=p(Iv),xi=o(Iv,"TD",{align:!0});var IN=l(xi);Z0=u(IN,"a dict containing the following keys:"),IN.forEach(t),Iv.forEach(t),eT=p(U),$n=o(U,"TR",{});var Hv=l($n);Si=o(Hv,"TD",{align:!0});var HN=l(Si);tT=u(HN,"min_length"),HN.forEach(t),sT=p(Hv),he=o(Hv,"TD",{align:!0});var Gp=l(he);aT=u(Gp,"(Default: "),Bd=o(Gp,"CODE",{});var BN=l(Bd);nT=u(BN,"None"),BN.forEach(t),rT=u(Gp,"). Integer to define the minimum length "),Cd=o(Gp,"STRONG",{});var CN=l(Cd);oT=u(CN,"in tokens"),CN.forEach(t),lT=u(Gp," of the output summary."),Gp.forEach(t),Hv.forEach(t),iT=p(U),qn=o(U,"TR",{});var Bv=l(qn);Ii=o(Bv,"TD",{align:!0});var GN=l(Ii);uT=u(GN,"max_length"),GN.forEach(t),fT=p(Bv),ge=o(Bv,"TD",{align:!0});var Up=l(ge);cT=u(Up,"(Default: "),Gd=o(Up,"CODE",{});var UN=l(Gd);pT=u(UN,"None"),UN.forEach(t),dT=u(Up,"). Integer to define the maximum length "),Ud=o(Up,"STRONG",{});var LN=l(Ud);hT=u(LN,"in tokens"),LN.forEach(t),gT=u(Up," of the output summary."),Up.forEach(t),Bv.forEach(t),mT=p(U),vn=o(U,"TR",{});var Cv=l(vn);Hi=o(Cv,"TD",{align:!0});var zN=l(Hi);_T=u(zN,"top_k"),zN.forEach(t),$T=p(Cv),me=o(Cv,"TD",{align:!0});var Lp=l(me);qT=u(Lp,"(Default: "),Ld=o(Lp,"CODE",{});var MN=l(Ld);vT=u(MN,"None"),MN.forEach(t),yT=u(Lp,"). Integer to define the top tokens considered within the "),zd=o(Lp,"CODE",{});var FN=l(zd);ET=u(FN,"sample"),FN.forEach(t),wT=u(Lp," operation to create new text."),Lp.forEach(t),Cv.forEach(t),bT=p(U),yn=o(U,"TR",{});var Gv=l(yn);Bi=o(Gv,"TD",{align:!0});var JN=l(Bi);TT=u(JN,"top_p"),JN.forEach(t),jT=p(Gv),re=o(Gv,"TD",{align:!0});var ja=l(re);kT=u(ja,"(Default: "),Md=o(ja,"CODE",{});var KN=l(Md);AT=u(KN,"None"),KN.forEach(t),DT=u(ja,"). Float to define the tokens that are within the "),Fd=o(ja,"CODE",{});var WN=l(Fd);OT=u(WN,"sample"),WN.forEach(t),PT=u(ja," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Jd=o(ja,"CODE",{});var YN=l(Jd);RT=u(YN,"top_p"),YN.forEach(t),NT=u(ja,"."),ja.forEach(t),Gv.forEach(t),xT=p(U),En=o(U,"TR",{});var Uv=l(En);Ci=o(Uv,"TD",{align:!0});var VN=l(Ci);ST=u(VN,"temperature"),VN.forEach(t),IT=p(Uv),_e=o(Uv,"TD",{align:!0});var zp=l(_e);HT=u(zp,"(Default: "),Kd=o(zp,"CODE",{});var XN=l(Kd);BT=u(XN,"1.0"),XN.forEach(t),CT=u(zp,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),Wd=o(zp,"CODE",{});var QN=l(Wd);GT=u(QN,"100.0"),QN.forEach(t),UT=u(zp," is getting closer to uniform probability."),zp.forEach(t),Uv.forEach(t),LT=p(U),wn=o(U,"TR",{});var Lv=l(wn);Gi=o(Lv,"TD",{align:!0});var ZN=l(Gi);zT=u(ZN,"repetition_penalty"),ZN.forEach(t),MT=p(Lv),Ot=o(Lv,"TD",{align:!0});var zv=l(Ot);FT=u(zv,"(Default: "),Yd=o(zv,"CODE",{});var ex=l(Yd);JT=u(ex,"None"),ex.forEach(t),KT=u(zv,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),zv.forEach(t),Lv.forEach(t),WT=p(U),bn=o(U,"TR",{});var Mv=l(bn);Ui=o(Mv,"TD",{align:!0});var tx=l(Ui);YT=u(tx,"max_time"),tx.forEach(t),VT=p(Mv),Pt=o(Mv,"TD",{align:!0});var Fv=l(Pt);XT=u(Fv,"(Default: "),Vd=o(Fv,"CODE",{});var sx=l(Vd);QT=u(sx,"None"),sx.forEach(t),ZT=u(Fv,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),Fv.forEach(t),Mv.forEach(t),e3=p(U),Tn=o(U,"TR",{});var Jv=l(Tn);Li=o(Jv,"TD",{align:!0});var ax=l(Li);Xd=o(ax,"STRONG",{});var nx=l(Xd);t3=u(nx,"options"),nx.forEach(t),ax.forEach(t),s3=p(Jv),zi=o(Jv,"TD",{align:!0});var rx=l(zi);a3=u(rx,"a dict containing the following keys:"),rx.forEach(t),Jv.forEach(t),n3=p(U),jn=o(U,"TR",{});var Kv=l(jn);Mi=o(Kv,"TD",{align:!0});var ox=l(Mi);r3=u(ox,"use_gpu"),ox.forEach(t),o3=p(Kv),Rt=o(Kv,"TD",{align:!0});var Wv=l(Rt);l3=u(Wv,"(Default: "),Qd=o(Wv,"CODE",{});var lx=l(Qd);i3=u(lx,"false"),lx.forEach(t),u3=u(Wv,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Wv.forEach(t),Kv.forEach(t),f3=p(U),kn=o(U,"TR",{});var Yv=l(kn);Fi=o(Yv,"TD",{align:!0});var ix=l(Fi);c3=u(ix,"use_cache"),ix.forEach(t),p3=p(Yv),Nt=o(Yv,"TD",{align:!0});var Vv=l(Nt);d3=u(Vv,"(Default: "),Zd=o(Vv,"CODE",{});var ux=l(Zd);h3=u(ux,"true"),ux.forEach(t),g3=u(Vv,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Vv.forEach(t),Yv.forEach(t),m3=p(U),An=o(U,"TR",{});var Xv=l(An);Ji=o(Xv,"TD",{align:!0});var fx=l(Ji);_3=u(fx,"wait_for_model"),fx.forEach(t),$3=p(Xv),xt=o(Xv,"TD",{align:!0});var Qv=l(xt);q3=u(Qv,"(Default: "),eh=o(Qv,"CODE",{});var cx=l(eh);v3=u(cx,"false"),cx.forEach(t),y3=u(Qv,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Qv.forEach(t),Xv.forEach(t),U.forEach(t),Nv.forEach(t),C$=p(a),Ki=o(a,"P",{});var px=l(Ki);E3=u(px,"Return value is either a dict or a list of dicts if you sent a list of inputs"),px.forEach(t),G$=p(a),St=o(a,"TABLE",{});var Zv=l(St);th=o(Zv,"THEAD",{});var dx=l(th);Dn=o(dx,"TR",{});var e2=l(Dn);Wi=o(e2,"TH",{align:!0});var hx=l(Wi);w3=u(hx,"Returned values"),hx.forEach(t),b3=p(e2),sh=o(e2,"TH",{align:!0}),l(sh).forEach(t),e2.forEach(t),dx.forEach(t),T3=p(Zv),ah=o(Zv,"TBODY",{});var gx=l(ah);On=o(gx,"TR",{});var t2=l(On);Yi=o(t2,"TD",{align:!0});var mx=l(Yi);nh=o(mx,"STRONG",{});var _x=l(nh);j3=u(_x,"summarization_text"),_x.forEach(t),mx.forEach(t),k3=p(t2),Vi=o(t2,"TD",{align:!0});var $x=l(Vi);A3=u($x,"The string after translation"),$x.forEach(t),t2.forEach(t),gx.forEach(t),Zv.forEach(t),U$=p(a),Be=o(a,"H2",{class:!0});var s2=l(Be);It=o(s2,"A",{id:!0,class:!0,href:!0});var qx=l(It);rh=o(qx,"SPAN",{});var vx=l(rh);v(Pn.$$.fragment,vx),vx.forEach(t),qx.forEach(t),D3=p(s2),oh=o(s2,"SPAN",{});var yx=l(oh);O3=u(yx,"Conversational task"),yx.forEach(t),s2.forEach(t),L$=p(a),Xi=o(a,"P",{});var Ex=l(Xi);P3=u(Ex,`This task corresponds to any chatbot like structure. Models tend to have
shorted max_length, so please check with caution when using a given
model if you need long range dependency or not.`),Ex.forEach(t),z$=p(a),v(Ht.$$.fragment,a),M$=p(a),Rn=o(a,"P",{});var lR=l(Rn);R3=u(lR,"Available with: "),Nn=o(lR,"A",{href:!0,rel:!0});var wx=l(Nn);N3=u(wx,"\u{1F917} Transformers"),wx.forEach(t),lR.forEach(t),F$=p(a),Qi=o(a,"P",{});var bx=l(Qi);x3=u(bx,"Example:"),bx.forEach(t),J$=p(a),v(Bt.$$.fragment,a),K$=p(a),Zi=o(a,"P",{});var Tx=l(Zi);S3=u(Tx,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Tx.forEach(t),W$=p(a),Ct=o(a,"TABLE",{});var a2=l(Ct);lh=o(a2,"THEAD",{});var jx=l(lh);xn=o(jx,"TR",{});var n2=l(xn);eu=o(n2,"TH",{align:!0});var kx=l(eu);I3=u(kx,"All parameters"),kx.forEach(t),H3=p(n2),ih=o(n2,"TH",{align:!0}),l(ih).forEach(t),n2.forEach(t),jx.forEach(t),B3=p(a2),x=o(a2,"TBODY",{});var H=l(x);Sn=o(H,"TR",{});var r2=l(Sn);In=o(r2,"TD",{align:!0});var iR=l(In);uh=o(iR,"STRONG",{});var Ax=l(uh);C3=u(Ax,"inputs"),Ax.forEach(t),G3=u(iR," (required)"),iR.forEach(t),U3=p(r2),fh=o(r2,"TD",{align:!0}),l(fh).forEach(t),r2.forEach(t),L3=p(H),Hn=o(H,"TR",{});var o2=l(Hn);tu=o(o2,"TD",{align:!0});var Dx=l(tu);z3=u(Dx,"text (required)"),Dx.forEach(t),M3=p(o2),su=o(o2,"TD",{align:!0});var Ox=l(su);F3=u(Ox,"The last input from the user in the conversation."),Ox.forEach(t),o2.forEach(t),J3=p(H),Bn=o(H,"TR",{});var l2=l(Bn);au=o(l2,"TD",{align:!0});var Px=l(au);K3=u(Px,"generated_responses"),Px.forEach(t),W3=p(l2),nu=o(l2,"TD",{align:!0});var Rx=l(nu);Y3=u(Rx,"A list of strings corresponding to the earlier replies from the model."),Rx.forEach(t),l2.forEach(t),V3=p(H),Cn=o(H,"TR",{});var i2=l(Cn);ru=o(i2,"TD",{align:!0});var Nx=l(ru);X3=u(Nx,"past_user_inputs"),Nx.forEach(t),Q3=p(i2),Gt=o(i2,"TD",{align:!0});var u2=l(Gt);Z3=u(u2,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),ch=o(u2,"CODE",{});var xx=l(ch);ej=u(xx,"generated_responses"),xx.forEach(t),tj=u(u2,"."),u2.forEach(t),i2.forEach(t),sj=p(H),Gn=o(H,"TR",{});var f2=l(Gn);ou=o(f2,"TD",{align:!0});var Sx=l(ou);ph=o(Sx,"STRONG",{});var Ix=l(ph);aj=u(Ix,"parameters"),Ix.forEach(t),Sx.forEach(t),nj=p(f2),lu=o(f2,"TD",{align:!0});var Hx=l(lu);rj=u(Hx,"a dict containing the following keys:"),Hx.forEach(t),f2.forEach(t),oj=p(H),Un=o(H,"TR",{});var c2=l(Un);iu=o(c2,"TD",{align:!0});var Bx=l(iu);lj=u(Bx,"min_length"),Bx.forEach(t),ij=p(c2),$e=o(c2,"TD",{align:!0});var Mp=l($e);uj=u(Mp,"(Default: "),dh=o(Mp,"CODE",{});var Cx=l(dh);fj=u(Cx,"None"),Cx.forEach(t),cj=u(Mp,"). Integer to define the minimum length "),hh=o(Mp,"STRONG",{});var Gx=l(hh);pj=u(Gx,"in tokens"),Gx.forEach(t),dj=u(Mp," of the output summary."),Mp.forEach(t),c2.forEach(t),hj=p(H),Ln=o(H,"TR",{});var p2=l(Ln);uu=o(p2,"TD",{align:!0});var Ux=l(uu);gj=u(Ux,"max_length"),Ux.forEach(t),mj=p(p2),qe=o(p2,"TD",{align:!0});var Fp=l(qe);_j=u(Fp,"(Default: "),gh=o(Fp,"CODE",{});var Lx=l(gh);$j=u(Lx,"None"),Lx.forEach(t),qj=u(Fp,"). Integer to define the maximum length "),mh=o(Fp,"STRONG",{});var zx=l(mh);vj=u(zx,"in tokens"),zx.forEach(t),yj=u(Fp," of the output summary."),Fp.forEach(t),p2.forEach(t),Ej=p(H),zn=o(H,"TR",{});var d2=l(zn);fu=o(d2,"TD",{align:!0});var Mx=l(fu);wj=u(Mx,"top_k"),Mx.forEach(t),bj=p(d2),ve=o(d2,"TD",{align:!0});var Jp=l(ve);Tj=u(Jp,"(Default: "),_h=o(Jp,"CODE",{});var Fx=l(_h);jj=u(Fx,"None"),Fx.forEach(t),kj=u(Jp,"). Integer to define the top tokens considered within the "),$h=o(Jp,"CODE",{});var Jx=l($h);Aj=u(Jx,"sample"),Jx.forEach(t),Dj=u(Jp," operation to create new text."),Jp.forEach(t),d2.forEach(t),Oj=p(H),Mn=o(H,"TR",{});var h2=l(Mn);cu=o(h2,"TD",{align:!0});var Kx=l(cu);Pj=u(Kx,"top_p"),Kx.forEach(t),Rj=p(h2),oe=o(h2,"TD",{align:!0});var ka=l(oe);Nj=u(ka,"(Default: "),qh=o(ka,"CODE",{});var Wx=l(qh);xj=u(Wx,"None"),Wx.forEach(t),Sj=u(ka,"). Float to define the tokens that are within the "),vh=o(ka,"CODE",{});var Yx=l(vh);Ij=u(Yx,"sample"),Yx.forEach(t),Hj=u(ka," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),yh=o(ka,"CODE",{});var Vx=l(yh);Bj=u(Vx,"top_p"),Vx.forEach(t),Cj=u(ka,"."),ka.forEach(t),h2.forEach(t),Gj=p(H),Fn=o(H,"TR",{});var g2=l(Fn);pu=o(g2,"TD",{align:!0});var Xx=l(pu);Uj=u(Xx,"temperature"),Xx.forEach(t),Lj=p(g2),ye=o(g2,"TD",{align:!0});var Kp=l(ye);zj=u(Kp,"(Default: "),Eh=o(Kp,"CODE",{});var Qx=l(Eh);Mj=u(Qx,"1.0"),Qx.forEach(t),Fj=u(Kp,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),wh=o(Kp,"CODE",{});var Zx=l(wh);Jj=u(Zx,"100.0"),Zx.forEach(t),Kj=u(Kp," is getting closer to uniform probability."),Kp.forEach(t),g2.forEach(t),Wj=p(H),Jn=o(H,"TR",{});var m2=l(Jn);du=o(m2,"TD",{align:!0});var eS=l(du);Yj=u(eS,"repetition_penalty"),eS.forEach(t),Vj=p(m2),Ut=o(m2,"TD",{align:!0});var _2=l(Ut);Xj=u(_2,"(Default: "),bh=o(_2,"CODE",{});var tS=l(bh);Qj=u(tS,"None"),tS.forEach(t),Zj=u(_2,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),_2.forEach(t),m2.forEach(t),e5=p(H),Kn=o(H,"TR",{});var $2=l(Kn);hu=o($2,"TD",{align:!0});var sS=l(hu);t5=u(sS,"max_time"),sS.forEach(t),s5=p($2),Lt=o($2,"TD",{align:!0});var q2=l(Lt);a5=u(q2,"(Default: "),Th=o(q2,"CODE",{});var aS=l(Th);n5=u(aS,"None"),aS.forEach(t),r5=u(q2,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),q2.forEach(t),$2.forEach(t),o5=p(H),Wn=o(H,"TR",{});var v2=l(Wn);gu=o(v2,"TD",{align:!0});var nS=l(gu);jh=o(nS,"STRONG",{});var rS=l(jh);l5=u(rS,"options"),rS.forEach(t),nS.forEach(t),i5=p(v2),mu=o(v2,"TD",{align:!0});var oS=l(mu);u5=u(oS,"a dict containing the following keys:"),oS.forEach(t),v2.forEach(t),f5=p(H),Yn=o(H,"TR",{});var y2=l(Yn);_u=o(y2,"TD",{align:!0});var lS=l(_u);c5=u(lS,"use_gpu"),lS.forEach(t),p5=p(y2),zt=o(y2,"TD",{align:!0});var E2=l(zt);d5=u(E2,"(Default: "),kh=o(E2,"CODE",{});var iS=l(kh);h5=u(iS,"false"),iS.forEach(t),g5=u(E2,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),E2.forEach(t),y2.forEach(t),m5=p(H),Vn=o(H,"TR",{});var w2=l(Vn);$u=o(w2,"TD",{align:!0});var uS=l($u);_5=u(uS,"use_cache"),uS.forEach(t),$5=p(w2),Mt=o(w2,"TD",{align:!0});var b2=l(Mt);q5=u(b2,"(Default: "),Ah=o(b2,"CODE",{});var fS=l(Ah);v5=u(fS,"true"),fS.forEach(t),y5=u(b2,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),b2.forEach(t),w2.forEach(t),E5=p(H),Xn=o(H,"TR",{});var T2=l(Xn);qu=o(T2,"TD",{align:!0});var cS=l(qu);w5=u(cS,"wait_for_model"),cS.forEach(t),b5=p(T2),Ft=o(T2,"TD",{align:!0});var j2=l(Ft);T5=u(j2,"(Default: "),Dh=o(j2,"CODE",{});var pS=l(Dh);j5=u(pS,"false"),pS.forEach(t),k5=u(j2,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),j2.forEach(t),T2.forEach(t),H.forEach(t),a2.forEach(t),Y$=p(a),vu=o(a,"P",{});var dS=l(vu);A5=u(dS,"Return value is either a dict or a list of dicts if you sent a list of inputs"),dS.forEach(t),V$=p(a),Jt=o(a,"TABLE",{});var k2=l(Jt);Oh=o(k2,"THEAD",{});var hS=l(Oh);Qn=o(hS,"TR",{});var A2=l(Qn);yu=o(A2,"TH",{align:!0});var gS=l(yu);D5=u(gS,"Returned values"),gS.forEach(t),O5=p(A2),Ph=o(A2,"TH",{align:!0}),l(Ph).forEach(t),A2.forEach(t),hS.forEach(t),P5=p(k2),ie=o(k2,"TBODY",{});var Aa=l(ie);Zn=o(Aa,"TR",{});var D2=l(Zn);Eu=o(D2,"TD",{align:!0});var mS=l(Eu);Rh=o(mS,"STRONG",{});var _S=l(Rh);R5=u(_S,"generated_text"),_S.forEach(t),mS.forEach(t),N5=p(D2),wu=o(D2,"TD",{align:!0});var $S=l(wu);x5=u($S,"The answer of the bot"),$S.forEach(t),D2.forEach(t),S5=p(Aa),er=o(Aa,"TR",{});var O2=l(er);bu=o(O2,"TD",{align:!0});var qS=l(bu);Nh=o(qS,"STRONG",{});var vS=l(Nh);I5=u(vS,"conversation"),vS.forEach(t),qS.forEach(t),H5=p(O2),Tu=o(O2,"TD",{align:!0});var yS=l(Tu);B5=u(yS,"A facility dictionnary to send back for the next input (with the new user input addition)."),yS.forEach(t),O2.forEach(t),C5=p(Aa),tr=o(Aa,"TR",{});var P2=l(tr);ju=o(P2,"TD",{align:!0});var ES=l(ju);G5=u(ES,"past_user_inputs"),ES.forEach(t),U5=p(P2),ku=o(P2,"TD",{align:!0});var wS=l(ku);L5=u(wS,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),wS.forEach(t),P2.forEach(t),z5=p(Aa),sr=o(Aa,"TR",{});var R2=l(sr);Au=o(R2,"TD",{align:!0});var bS=l(Au);M5=u(bS,"generated_responses"),bS.forEach(t),F5=p(R2),Du=o(R2,"TD",{align:!0});var TS=l(Du);J5=u(TS,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),TS.forEach(t),R2.forEach(t),Aa.forEach(t),k2.forEach(t),X$=p(a),Ce=o(a,"H2",{class:!0});var N2=l(Ce);Kt=o(N2,"A",{id:!0,class:!0,href:!0});var jS=l(Kt);xh=o(jS,"SPAN",{});var kS=l(xh);v(ar.$$.fragment,kS),kS.forEach(t),jS.forEach(t),K5=p(N2),Sh=o(N2,"SPAN",{});var AS=l(Sh);W5=u(AS,"Table question answering task"),AS.forEach(t),N2.forEach(t),Q$=p(a),Ou=o(a,"P",{});var DS=l(Ou);Y5=u(DS,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),DS.forEach(t),Z$=p(a),v(Wt.$$.fragment,a),eq=p(a),nr=o(a,"P",{});var uR=l(nr);V5=u(uR,"Available with: "),rr=o(uR,"A",{href:!0,rel:!0});var OS=l(rr);X5=u(OS,"\u{1F917} Transformers"),OS.forEach(t),uR.forEach(t),tq=p(a),Pu=o(a,"P",{});var PS=l(Pu);Q5=u(PS,"Example:"),PS.forEach(t),sq=p(a),v(Yt.$$.fragment,a),aq=p(a),Ru=o(a,"P",{});var RS=l(Ru);Z5=u(RS,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),RS.forEach(t),nq=p(a),Vt=o(a,"TABLE",{});var x2=l(Vt);Ih=o(x2,"THEAD",{});var NS=l(Ih);or=o(NS,"TR",{});var S2=l(or);Nu=o(S2,"TH",{align:!0});var xS=l(Nu);e4=u(xS,"All parameters"),xS.forEach(t),t4=p(S2),Hh=o(S2,"TH",{align:!0}),l(Hh).forEach(t),S2.forEach(t),NS.forEach(t),s4=p(x2),J=o(x2,"TBODY",{});var V=l(J);lr=o(V,"TR",{});var I2=l(lr);ir=o(I2,"TD",{align:!0});var fR=l(ir);Bh=o(fR,"STRONG",{});var SS=l(Bh);a4=u(SS,"inputs"),SS.forEach(t),n4=u(fR," (required)"),fR.forEach(t),r4=p(I2),Ch=o(I2,"TD",{align:!0}),l(Ch).forEach(t),I2.forEach(t),o4=p(V),ur=o(V,"TR",{});var H2=l(ur);xu=o(H2,"TD",{align:!0});var IS=l(xu);l4=u(IS,"query (required)"),IS.forEach(t),i4=p(H2),Su=o(H2,"TD",{align:!0});var HS=l(Su);u4=u(HS,"The query in plain text that you want to ask the table"),HS.forEach(t),H2.forEach(t),f4=p(V),fr=o(V,"TR",{});var B2=l(fr);Iu=o(B2,"TD",{align:!0});var BS=l(Iu);c4=u(BS,"table (required)"),BS.forEach(t),p4=p(B2),Hu=o(B2,"TD",{align:!0});var CS=l(Hu);d4=u(CS,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),CS.forEach(t),B2.forEach(t),h4=p(V),cr=o(V,"TR",{});var C2=l(cr);Bu=o(C2,"TD",{align:!0});var GS=l(Bu);Gh=o(GS,"STRONG",{});var US=l(Gh);g4=u(US,"options"),US.forEach(t),GS.forEach(t),m4=p(C2),Cu=o(C2,"TD",{align:!0});var LS=l(Cu);_4=u(LS,"a dict containing the following keys:"),LS.forEach(t),C2.forEach(t),$4=p(V),pr=o(V,"TR",{});var G2=l(pr);Gu=o(G2,"TD",{align:!0});var zS=l(Gu);q4=u(zS,"use_gpu"),zS.forEach(t),v4=p(G2),Xt=o(G2,"TD",{align:!0});var U2=l(Xt);y4=u(U2,"(Default: "),Uh=o(U2,"CODE",{});var MS=l(Uh);E4=u(MS,"false"),MS.forEach(t),w4=u(U2,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),U2.forEach(t),G2.forEach(t),b4=p(V),dr=o(V,"TR",{});var L2=l(dr);Uu=o(L2,"TD",{align:!0});var FS=l(Uu);T4=u(FS,"use_cache"),FS.forEach(t),j4=p(L2),Qt=o(L2,"TD",{align:!0});var z2=l(Qt);k4=u(z2,"(Default: "),Lh=o(z2,"CODE",{});var JS=l(Lh);A4=u(JS,"true"),JS.forEach(t),D4=u(z2,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),z2.forEach(t),L2.forEach(t),O4=p(V),hr=o(V,"TR",{});var M2=l(hr);Lu=o(M2,"TD",{align:!0});var KS=l(Lu);P4=u(KS,"wait_for_model"),KS.forEach(t),R4=p(M2),Zt=o(M2,"TD",{align:!0});var F2=l(Zt);N4=u(F2,"(Default: "),zh=o(F2,"CODE",{});var WS=l(zh);x4=u(WS,"false"),WS.forEach(t),S4=u(F2,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),F2.forEach(t),M2.forEach(t),V.forEach(t),x2.forEach(t),rq=p(a),zu=o(a,"P",{});var YS=l(zu);I4=u(YS,"Return value is either a dict or a list of dicts if you sent a list of inputs"),YS.forEach(t),oq=p(a),v(es.$$.fragment,a),lq=p(a),ts=o(a,"TABLE",{});var J2=l(ts);Mh=o(J2,"THEAD",{});var VS=l(Mh);gr=o(VS,"TR",{});var K2=l(gr);Mu=o(K2,"TH",{align:!0});var XS=l(Mu);H4=u(XS,"Returned values"),XS.forEach(t),B4=p(K2),Fh=o(K2,"TH",{align:!0}),l(Fh).forEach(t),K2.forEach(t),VS.forEach(t),C4=p(J2),ue=o(J2,"TBODY",{});var Da=l(ue);mr=o(Da,"TR",{});var W2=l(mr);Fu=o(W2,"TD",{align:!0});var QS=l(Fu);Jh=o(QS,"STRONG",{});var ZS=l(Jh);G4=u(ZS,"answer"),ZS.forEach(t),QS.forEach(t),U4=p(W2),Ju=o(W2,"TD",{align:!0});var eI=l(Ju);L4=u(eI,"The plaintext answer"),eI.forEach(t),W2.forEach(t),z4=p(Da),_r=o(Da,"TR",{});var Y2=l(_r);Ku=o(Y2,"TD",{align:!0});var tI=l(Ku);Kh=o(tI,"STRONG",{});var sI=l(Kh);M4=u(sI,"coordinates"),sI.forEach(t),tI.forEach(t),F4=p(Y2),Wu=o(Y2,"TD",{align:!0});var aI=l(Wu);J4=u(aI,"a list of coordinates of the cells references in the answer"),aI.forEach(t),Y2.forEach(t),K4=p(Da),$r=o(Da,"TR",{});var V2=l($r);Yu=o(V2,"TD",{align:!0});var nI=l(Yu);Wh=o(nI,"STRONG",{});var rI=l(Wh);W4=u(rI,"cells"),rI.forEach(t),nI.forEach(t),Y4=p(V2),Vu=o(V2,"TD",{align:!0});var oI=l(Vu);V4=u(oI,"a list of coordinates of the cells contents"),oI.forEach(t),V2.forEach(t),X4=p(Da),qr=o(Da,"TR",{});var X2=l(qr);Xu=o(X2,"TD",{align:!0});var lI=l(Xu);Yh=o(lI,"STRONG",{});var iI=l(Yh);Q4=u(iI,"aggregator"),iI.forEach(t),lI.forEach(t),Z4=p(X2),Qu=o(X2,"TD",{align:!0});var uI=l(Qu);ek=u(uI,"The aggregator used to get the answer"),uI.forEach(t),X2.forEach(t),Da.forEach(t),J2.forEach(t),iq=p(a),Ge=o(a,"H2",{class:!0});var Q2=l(Ge);ss=o(Q2,"A",{id:!0,class:!0,href:!0});var fI=l(ss);Vh=o(fI,"SPAN",{});var cI=l(Vh);v(vr.$$.fragment,cI),cI.forEach(t),fI.forEach(t),tk=p(Q2),Xh=o(Q2,"SPAN",{});var pI=l(Xh);sk=u(pI,"Question answering task"),pI.forEach(t),Q2.forEach(t),uq=p(a),Zu=o(a,"P",{});var dI=l(Zu);ak=u(dI,"Want to have a nice know-it-all bot that can answer any questions ?"),dI.forEach(t),fq=p(a),v(as.$$.fragment,a),cq=p(a),Ue=o(a,"P",{});var s$=l(Ue);nk=u(s$,"Available with: "),yr=o(s$,"A",{href:!0,rel:!0});var hI=l(yr);rk=u(hI,"\u{1F917}Transformers"),hI.forEach(t),ok=u(s$,` and
`),Er=o(s$,"A",{href:!0,rel:!0});var gI=l(Er);lk=u(gI,"AllenNLP"),gI.forEach(t),s$.forEach(t),pq=p(a),ef=o(a,"P",{});var mI=l(ef);ik=u(mI,"Example:"),mI.forEach(t),dq=p(a),v(ns.$$.fragment,a),hq=p(a),tf=o(a,"P",{});var _I=l(tf);uk=u(_I,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),_I.forEach(t),gq=p(a),sf=o(a,"P",{});var $I=l(sf);fk=u($I,"Return value is either a dict or a list of dicts if you sent a list of inputs"),$I.forEach(t),mq=p(a),v(rs.$$.fragment,a),_q=p(a),os=o(a,"TABLE",{});var Z2=l(os);Qh=o(Z2,"THEAD",{});var qI=l(Qh);wr=o(qI,"TR",{});var ey=l(wr);af=o(ey,"TH",{align:!0});var vI=l(af);ck=u(vI,"Returned values"),vI.forEach(t),pk=p(ey),Zh=o(ey,"TH",{align:!0}),l(Zh).forEach(t),ey.forEach(t),qI.forEach(t),dk=p(Z2),fe=o(Z2,"TBODY",{});var Oa=l(fe);br=o(Oa,"TR",{});var ty=l(br);nf=o(ty,"TD",{align:!0});var yI=l(nf);eg=o(yI,"STRONG",{});var EI=l(eg);hk=u(EI,"answer"),EI.forEach(t),yI.forEach(t),gk=p(ty),rf=o(ty,"TD",{align:!0});var wI=l(rf);mk=u(wI,"A string that\u2019s the answer within the text."),wI.forEach(t),ty.forEach(t),_k=p(Oa),Tr=o(Oa,"TR",{});var sy=l(Tr);of=o(sy,"TD",{align:!0});var bI=l(of);tg=o(bI,"STRONG",{});var TI=l(tg);$k=u(TI,"score"),TI.forEach(t),bI.forEach(t),qk=p(sy),lf=o(sy,"TD",{align:!0});var jI=l(lf);vk=u(jI,"A floats that represents how likely that the answer is correct"),jI.forEach(t),sy.forEach(t),yk=p(Oa),jr=o(Oa,"TR",{});var ay=l(jr);uf=o(ay,"TD",{align:!0});var kI=l(uf);sg=o(kI,"STRONG",{});var AI=l(sg);Ek=u(AI,"start"),AI.forEach(t),kI.forEach(t),wk=p(ay),ls=o(ay,"TD",{align:!0});var ny=l(ls);bk=u(ny,"The index (string wise) of the start of the answer within "),ag=o(ny,"CODE",{});var DI=l(ag);Tk=u(DI,"context"),DI.forEach(t),jk=u(ny,"."),ny.forEach(t),ay.forEach(t),kk=p(Oa),kr=o(Oa,"TR",{});var ry=l(kr);ff=o(ry,"TD",{align:!0});var OI=l(ff);ng=o(OI,"STRONG",{});var PI=l(ng);Ak=u(PI,"stop"),PI.forEach(t),OI.forEach(t),Dk=p(ry),is=o(ry,"TD",{align:!0});var oy=l(is);Ok=u(oy,"The index (string wise) of the stop of the answer within "),rg=o(oy,"CODE",{});var RI=l(rg);Pk=u(RI,"context"),RI.forEach(t),Rk=u(oy,"."),oy.forEach(t),ry.forEach(t),Oa.forEach(t),Z2.forEach(t),$q=p(a),Le=o(a,"H2",{class:!0});var ly=l(Le);us=o(ly,"A",{id:!0,class:!0,href:!0});var NI=l(us);og=o(NI,"SPAN",{});var xI=l(og);v(Ar.$$.fragment,xI),xI.forEach(t),NI.forEach(t),Nk=p(ly),lg=o(ly,"SPAN",{});var SI=l(lg);xk=u(SI,"Text-classification task"),SI.forEach(t),ly.forEach(t),qq=p(a),cf=o(a,"P",{});var II=l(cf);Sk=u(II,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),II.forEach(t),vq=p(a),v(fs.$$.fragment,a),yq=p(a),Dr=o(a,"P",{});var cR=l(Dr);Ik=u(cR,"Available with: "),Or=o(cR,"A",{href:!0,rel:!0});var HI=l(Or);Hk=u(HI,"\u{1F917} Transformers"),HI.forEach(t),cR.forEach(t),Eq=p(a),pf=o(a,"P",{});var BI=l(pf);Bk=u(BI,"Example:"),BI.forEach(t),wq=p(a),v(cs.$$.fragment,a),bq=p(a),df=o(a,"P",{});var CI=l(df);Ck=u(CI,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),CI.forEach(t),Tq=p(a),ps=o(a,"TABLE",{});var iy=l(ps);ig=o(iy,"THEAD",{});var GI=l(ig);Pr=o(GI,"TR",{});var uy=l(Pr);hf=o(uy,"TH",{align:!0});var UI=l(hf);Gk=u(UI,"All parameters"),UI.forEach(t),Uk=p(uy),ug=o(uy,"TH",{align:!0}),l(ug).forEach(t),uy.forEach(t),GI.forEach(t),Lk=p(iy),ee=o(iy,"TBODY",{});var Ae=l(ee);Rr=o(Ae,"TR",{});var fy=l(Rr);Nr=o(fy,"TD",{align:!0});var pR=l(Nr);fg=o(pR,"STRONG",{});var LI=l(fg);zk=u(LI,"inputs"),LI.forEach(t),Mk=u(pR," (required)"),pR.forEach(t),Fk=p(fy),gf=o(fy,"TD",{align:!0});var zI=l(gf);Jk=u(zI,"a string to be classified"),zI.forEach(t),fy.forEach(t),Kk=p(Ae),xr=o(Ae,"TR",{});var cy=l(xr);mf=o(cy,"TD",{align:!0});var MI=l(mf);cg=o(MI,"STRONG",{});var FI=l(cg);Wk=u(FI,"options"),FI.forEach(t),MI.forEach(t),Yk=p(cy),_f=o(cy,"TD",{align:!0});var JI=l(_f);Vk=u(JI,"a dict containing the following keys:"),JI.forEach(t),cy.forEach(t),Xk=p(Ae),Sr=o(Ae,"TR",{});var py=l(Sr);$f=o(py,"TD",{align:!0});var KI=l($f);Qk=u(KI,"use_gpu"),KI.forEach(t),Zk=p(py),ds=o(py,"TD",{align:!0});var dy=l(ds);e7=u(dy,"(Default: "),pg=o(dy,"CODE",{});var WI=l(pg);t7=u(WI,"false"),WI.forEach(t),s7=u(dy,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),dy.forEach(t),py.forEach(t),a7=p(Ae),Ir=o(Ae,"TR",{});var hy=l(Ir);qf=o(hy,"TD",{align:!0});var YI=l(qf);n7=u(YI,"use_cache"),YI.forEach(t),r7=p(hy),hs=o(hy,"TD",{align:!0});var gy=l(hs);o7=u(gy,"(Default: "),dg=o(gy,"CODE",{});var VI=l(dg);l7=u(VI,"true"),VI.forEach(t),i7=u(gy,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),gy.forEach(t),hy.forEach(t),u7=p(Ae),Hr=o(Ae,"TR",{});var my=l(Hr);vf=o(my,"TD",{align:!0});var XI=l(vf);f7=u(XI,"wait_for_model"),XI.forEach(t),c7=p(my),gs=o(my,"TD",{align:!0});var _y=l(gs);p7=u(_y,"(Default: "),hg=o(_y,"CODE",{});var QI=l(hg);d7=u(QI,"false"),QI.forEach(t),h7=u(_y,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),_y.forEach(t),my.forEach(t),Ae.forEach(t),iy.forEach(t),jq=p(a),yf=o(a,"P",{});var ZI=l(yf);g7=u(ZI,"Return value is either a dict or a list of dicts if you sent a list of inputs"),ZI.forEach(t),kq=p(a),v(ms.$$.fragment,a),Aq=p(a),_s=o(a,"TABLE",{});var $y=l(_s);gg=o($y,"THEAD",{});var eH=l(gg);Br=o(eH,"TR",{});var qy=l(Br);Ef=o(qy,"TH",{align:!0});var tH=l(Ef);m7=u(tH,"Returned values"),tH.forEach(t),_7=p(qy),mg=o(qy,"TH",{align:!0}),l(mg).forEach(t),qy.forEach(t),eH.forEach(t),$7=p($y),Cr=o($y,"TBODY",{});var vy=l(Cr);Gr=o(vy,"TR",{});var yy=l(Gr);wf=o(yy,"TD",{align:!0});var sH=l(wf);_g=o(sH,"STRONG",{});var aH=l(_g);q7=u(aH,"label"),aH.forEach(t),sH.forEach(t),v7=p(yy),bf=o(yy,"TD",{align:!0});var nH=l(bf);y7=u(nH,"The label for the class (model specific)"),nH.forEach(t),yy.forEach(t),E7=p(vy),Ur=o(vy,"TR",{});var Ey=l(Ur);Tf=o(Ey,"TD",{align:!0});var rH=l(Tf);$g=o(rH,"STRONG",{});var oH=l($g);w7=u(oH,"score"),oH.forEach(t),rH.forEach(t),b7=p(Ey),jf=o(Ey,"TD",{align:!0});var lH=l(jf);T7=u(lH,"A floats that represents how likely is that the text belongs the this class."),lH.forEach(t),Ey.forEach(t),vy.forEach(t),$y.forEach(t),Dq=p(a),ze=o(a,"H2",{class:!0});var wy=l(ze);$s=o(wy,"A",{id:!0,class:!0,href:!0});var iH=l($s);qg=o(iH,"SPAN",{});var uH=l(qg);v(Lr.$$.fragment,uH),uH.forEach(t),iH.forEach(t),j7=p(wy),vg=o(wy,"SPAN",{});var fH=l(vg);k7=u(fH,"Named Entity Recognition (NER) task"),fH.forEach(t),wy.forEach(t),Oq=p(a),zr=o(a,"P",{});var dR=l(zr);A7=u(dR,"See "),kf=o(dR,"A",{href:!0});var cH=l(kf);D7=u(cH,"Token-classification task"),cH.forEach(t),dR.forEach(t),Pq=p(a),Me=o(a,"H2",{class:!0});var by=l(Me);qs=o(by,"A",{id:!0,class:!0,href:!0});var pH=l(qs);yg=o(pH,"SPAN",{});var dH=l(yg);v(Mr.$$.fragment,dH),dH.forEach(t),pH.forEach(t),O7=p(by),Eg=o(by,"SPAN",{});var hH=l(Eg);P7=u(hH,"Token-classification task"),hH.forEach(t),by.forEach(t),Rq=p(a),Af=o(a,"P",{});var gH=l(Af);R7=u(gH,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),gH.forEach(t),Nq=p(a),v(vs.$$.fragment,a),xq=p(a),Fe=o(a,"P",{});var a$=l(Fe);N7=u(a$,"Available with: "),Fr=o(a$,"A",{href:!0,rel:!0});var mH=l(Fr);x7=u(mH,"\u{1F917} Transformers"),mH.forEach(t),S7=u(a$,`,
`),Jr=o(a$,"A",{href:!0,rel:!0});var _H=l(Jr);I7=u(_H,"Flair"),_H.forEach(t),a$.forEach(t),Sq=p(a),Df=o(a,"P",{});var $H=l(Df);H7=u($H,"Example:"),$H.forEach(t),Iq=p(a),v(ys.$$.fragment,a),Hq=p(a),Of=o(a,"P",{});var qH=l(Of);B7=u(qH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),qH.forEach(t),Bq=p(a),Es=o(a,"TABLE",{});var Ty=l(Es);wg=o(Ty,"THEAD",{});var vH=l(wg);Kr=o(vH,"TR",{});var jy=l(Kr);Pf=o(jy,"TH",{align:!0});var yH=l(Pf);C7=u(yH,"All parameters"),yH.forEach(t),G7=p(jy),bg=o(jy,"TH",{align:!0}),l(bg).forEach(t),jy.forEach(t),vH.forEach(t),U7=p(Ty),K=o(Ty,"TBODY",{});var X=l(K);Wr=o(X,"TR",{});var ky=l(Wr);Yr=o(ky,"TD",{align:!0});var hR=l(Yr);Tg=o(hR,"STRONG",{});var EH=l(Tg);L7=u(EH,"inputs"),EH.forEach(t),z7=u(hR," (required)"),hR.forEach(t),M7=p(ky),Rf=o(ky,"TD",{align:!0});var wH=l(Rf);F7=u(wH,"a string to be classified"),wH.forEach(t),ky.forEach(t),J7=p(X),Vr=o(X,"TR",{});var Ay=l(Vr);Nf=o(Ay,"TD",{align:!0});var bH=l(Nf);jg=o(bH,"STRONG",{});var TH=l(jg);K7=u(TH,"parameters"),TH.forEach(t),bH.forEach(t),W7=p(Ay),xf=o(Ay,"TD",{align:!0});var jH=l(xf);Y7=u(jH,"a dict containing the following key:"),jH.forEach(t),Ay.forEach(t),V7=p(X),Xr=o(X,"TR",{});var Dy=l(Xr);Sf=o(Dy,"TD",{align:!0});var kH=l(Sf);X7=u(kH,"aggregation_strategy"),kH.forEach(t),Q7=p(Dy),S=o(Dy,"TD",{align:!0});var B=l(S);Z7=u(B,"(Default: "),kg=o(B,"CODE",{});var AH=l(kg);e6=u(AH,"simple"),AH.forEach(t),t6=u(B,"). There are several aggregation strategies: "),s6=o(B,"BR",{}),a6=p(B),Ag=o(B,"CODE",{});var DH=l(Ag);n6=u(DH,"none"),DH.forEach(t),r6=u(B,": Every token gets classified without further aggregation. "),o6=o(B,"BR",{}),l6=p(B),Dg=o(B,"CODE",{});var OH=l(Dg);i6=u(OH,"simple"),OH.forEach(t),u6=u(B,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),f6=o(B,"BR",{}),c6=p(B),Og=o(B,"CODE",{});var PH=l(Og);p6=u(PH,"first"),PH.forEach(t),d6=u(B,": Same as the "),Pg=o(B,"CODE",{});var RH=l(Pg);h6=u(RH,"simple"),RH.forEach(t),g6=u(B," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),m6=o(B,"BR",{}),_6=p(B),Rg=o(B,"CODE",{});var NH=l(Rg);$6=u(NH,"average"),NH.forEach(t),q6=u(B,": Same as the "),Ng=o(B,"CODE",{});var xH=l(Ng);v6=u(xH,"simple"),xH.forEach(t),y6=u(B," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),E6=o(B,"BR",{}),w6=p(B),xg=o(B,"CODE",{});var SH=l(xg);b6=u(SH,"max"),SH.forEach(t),T6=u(B,": Same as the "),Sg=o(B,"CODE",{});var IH=l(Sg);j6=u(IH,"simple"),IH.forEach(t),k6=u(B," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),B.forEach(t),Dy.forEach(t),A6=p(X),Qr=o(X,"TR",{});var Oy=l(Qr);If=o(Oy,"TD",{align:!0});var HH=l(If);Ig=o(HH,"STRONG",{});var BH=l(Ig);D6=u(BH,"options"),BH.forEach(t),HH.forEach(t),O6=p(Oy),Hf=o(Oy,"TD",{align:!0});var CH=l(Hf);P6=u(CH,"a dict containing the following keys:"),CH.forEach(t),Oy.forEach(t),R6=p(X),Zr=o(X,"TR",{});var Py=l(Zr);Bf=o(Py,"TD",{align:!0});var GH=l(Bf);N6=u(GH,"use_gpu"),GH.forEach(t),x6=p(Py),ws=o(Py,"TD",{align:!0});var Ry=l(ws);S6=u(Ry,"(Default: "),Hg=o(Ry,"CODE",{});var UH=l(Hg);I6=u(UH,"false"),UH.forEach(t),H6=u(Ry,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),Ry.forEach(t),Py.forEach(t),B6=p(X),eo=o(X,"TR",{});var Ny=l(eo);Cf=o(Ny,"TD",{align:!0});var LH=l(Cf);C6=u(LH,"use_cache"),LH.forEach(t),G6=p(Ny),bs=o(Ny,"TD",{align:!0});var xy=l(bs);U6=u(xy,"(Default: "),Bg=o(xy,"CODE",{});var zH=l(Bg);L6=u(zH,"true"),zH.forEach(t),z6=u(xy,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),xy.forEach(t),Ny.forEach(t),M6=p(X),to=o(X,"TR",{});var Sy=l(to);Gf=o(Sy,"TD",{align:!0});var MH=l(Gf);F6=u(MH,"wait_for_model"),MH.forEach(t),J6=p(Sy),Ts=o(Sy,"TD",{align:!0});var Iy=l(Ts);K6=u(Iy,"(Default: "),Cg=o(Iy,"CODE",{});var FH=l(Cg);W6=u(FH,"false"),FH.forEach(t),Y6=u(Iy,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Iy.forEach(t),Sy.forEach(t),X.forEach(t),Ty.forEach(t),Cq=p(a),Uf=o(a,"P",{});var JH=l(Uf);V6=u(JH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),JH.forEach(t),Gq=p(a),v(js.$$.fragment,a),Uq=p(a),ks=o(a,"TABLE",{});var Hy=l(ks);Gg=o(Hy,"THEAD",{});var KH=l(Gg);so=o(KH,"TR",{});var By=l(so);Lf=o(By,"TH",{align:!0});var WH=l(Lf);X6=u(WH,"Returned values"),WH.forEach(t),Q6=p(By),Ug=o(By,"TH",{align:!0}),l(Ug).forEach(t),By.forEach(t),KH.forEach(t),Z6=p(Hy),te=o(Hy,"TBODY",{});var De=l(te);ao=o(De,"TR",{});var Cy=l(ao);zf=o(Cy,"TD",{align:!0});var YH=l(zf);Lg=o(YH,"STRONG",{});var VH=l(Lg);e9=u(VH,"entity_group"),VH.forEach(t),YH.forEach(t),t9=p(Cy),Mf=o(Cy,"TD",{align:!0});var XH=l(Mf);s9=u(XH,"The type for the entity being recognized (model specific)."),XH.forEach(t),Cy.forEach(t),a9=p(De),no=o(De,"TR",{});var Gy=l(no);Ff=o(Gy,"TD",{align:!0});var QH=l(Ff);zg=o(QH,"STRONG",{});var ZH=l(zg);n9=u(ZH,"score"),ZH.forEach(t),QH.forEach(t),r9=p(Gy),Jf=o(Gy,"TD",{align:!0});var eB=l(Jf);o9=u(eB,"How likely the entity was recognized."),eB.forEach(t),Gy.forEach(t),l9=p(De),ro=o(De,"TR",{});var Uy=l(ro);Kf=o(Uy,"TD",{align:!0});var tB=l(Kf);Mg=o(tB,"STRONG",{});var sB=l(Mg);i9=u(sB,"word"),sB.forEach(t),tB.forEach(t),u9=p(Uy),Wf=o(Uy,"TD",{align:!0});var aB=l(Wf);f9=u(aB,"The string that was captured"),aB.forEach(t),Uy.forEach(t),c9=p(De),oo=o(De,"TR",{});var Ly=l(oo);Yf=o(Ly,"TD",{align:!0});var nB=l(Yf);Fg=o(nB,"STRONG",{});var rB=l(Fg);p9=u(rB,"start"),rB.forEach(t),nB.forEach(t),d9=p(Ly),As=o(Ly,"TD",{align:!0});var zy=l(As);h9=u(zy,"The offset stringwise where the answer is located. Useful to disambiguate if "),Jg=o(zy,"CODE",{});var oB=l(Jg);g9=u(oB,"word"),oB.forEach(t),m9=u(zy," occurs multiple times."),zy.forEach(t),Ly.forEach(t),_9=p(De),lo=o(De,"TR",{});var My=l(lo);Vf=o(My,"TD",{align:!0});var lB=l(Vf);Kg=o(lB,"STRONG",{});var iB=l(Kg);$9=u(iB,"end"),iB.forEach(t),lB.forEach(t),q9=p(My),Ds=o(My,"TD",{align:!0});var Fy=l(Ds);v9=u(Fy,"The offset stringwise where the answer is located. Useful to disambiguate if "),Wg=o(Fy,"CODE",{});var uB=l(Wg);y9=u(uB,"word"),uB.forEach(t),E9=u(Fy," occurs multiple times."),Fy.forEach(t),My.forEach(t),De.forEach(t),Hy.forEach(t),Lq=p(a),Je=o(a,"H2",{class:!0});var Jy=l(Je);Os=o(Jy,"A",{id:!0,class:!0,href:!0});var fB=l(Os);Yg=o(fB,"SPAN",{});var cB=l(Yg);v(io.$$.fragment,cB),cB.forEach(t),fB.forEach(t),w9=p(Jy),Vg=o(Jy,"SPAN",{});var pB=l(Vg);b9=u(pB,"Text-generation task"),pB.forEach(t),Jy.forEach(t),zq=p(a),Xf=o(a,"P",{});var dB=l(Xf);T9=u(dB,"Use to continue text from a prompt. This is a very generic task."),dB.forEach(t),Mq=p(a),v(Ps.$$.fragment,a),Fq=p(a),uo=o(a,"P",{});var gR=l(uo);j9=u(gR,"Available with: "),fo=o(gR,"A",{href:!0,rel:!0});var hB=l(fo);k9=u(hB,"\u{1F917} Transformers"),hB.forEach(t),gR.forEach(t),Jq=p(a),Qf=o(a,"P",{});var gB=l(Qf);A9=u(gB,"Example:"),gB.forEach(t),Kq=p(a),v(Rs.$$.fragment,a),Wq=p(a),Zf=o(a,"P",{});var mB=l(Zf);D9=u(mB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),mB.forEach(t),Yq=p(a),Ns=o(a,"TABLE",{});var Ky=l(Ns);Xg=o(Ky,"THEAD",{});var _B=l(Xg);co=o(_B,"TR",{});var Wy=l(co);ec=o(Wy,"TH",{align:!0});var $B=l(ec);O9=u($B,"All parameters"),$B.forEach(t),P9=p(Wy),Qg=o(Wy,"TH",{align:!0}),l(Qg).forEach(t),Wy.forEach(t),_B.forEach(t),R9=p(Ky),I=o(Ky,"TBODY",{});var C=l(I);po=o(C,"TR",{});var Yy=l(po);ho=o(Yy,"TD",{align:!0});var mR=l(ho);Zg=o(mR,"STRONG",{});var qB=l(Zg);N9=u(qB,"inputs"),qB.forEach(t),x9=u(mR," (required):"),mR.forEach(t),S9=p(Yy),tc=o(Yy,"TD",{align:!0});var vB=l(tc);I9=u(vB,"a string to be generated from"),vB.forEach(t),Yy.forEach(t),H9=p(C),go=o(C,"TR",{});var Vy=l(go);sc=o(Vy,"TD",{align:!0});var yB=l(sc);em=o(yB,"STRONG",{});var EB=l(em);B9=u(EB,"parameters"),EB.forEach(t),yB.forEach(t),C9=p(Vy),ac=o(Vy,"TD",{align:!0});var wB=l(ac);G9=u(wB,"dict containing the following keys:"),wB.forEach(t),Vy.forEach(t),U9=p(C),mo=o(C,"TR",{});var Xy=l(mo);nc=o(Xy,"TD",{align:!0});var bB=l(nc);L9=u(bB,"top_k"),bB.forEach(t),z9=p(Xy),Ee=o(Xy,"TD",{align:!0});var Wp=l(Ee);M9=u(Wp,"(Default: "),tm=o(Wp,"CODE",{});var TB=l(tm);F9=u(TB,"None"),TB.forEach(t),J9=u(Wp,"). Integer to define the top tokens considered within the "),sm=o(Wp,"CODE",{});var jB=l(sm);K9=u(jB,"sample"),jB.forEach(t),W9=u(Wp," operation to create new text."),Wp.forEach(t),Xy.forEach(t),Y9=p(C),_o=o(C,"TR",{});var Qy=l(_o);rc=o(Qy,"TD",{align:!0});var kB=l(rc);V9=u(kB,"top_p"),kB.forEach(t),X9=p(Qy),le=o(Qy,"TD",{align:!0});var Pa=l(le);Q9=u(Pa,"(Default: "),am=o(Pa,"CODE",{});var AB=l(am);Z9=u(AB,"None"),AB.forEach(t),e8=u(Pa,"). Float to define the tokens that are within the "),nm=o(Pa,"CODE",{});var DB=l(nm);t8=u(DB,"sample"),DB.forEach(t),s8=u(Pa," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),rm=o(Pa,"CODE",{});var OB=l(rm);a8=u(OB,"top_p"),OB.forEach(t),n8=u(Pa,"."),Pa.forEach(t),Qy.forEach(t),r8=p(C),$o=o(C,"TR",{});var Zy=l($o);oc=o(Zy,"TD",{align:!0});var PB=l(oc);o8=u(PB,"temperature"),PB.forEach(t),l8=p(Zy),we=o(Zy,"TD",{align:!0});var Yp=l(we);i8=u(Yp,"(Default: "),om=o(Yp,"CODE",{});var RB=l(om);u8=u(RB,"1.0"),RB.forEach(t),f8=u(Yp,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, 0 means "),lm=o(Yp,"CODE",{});var NB=l(lm);c8=u(NB,"100.0"),NB.forEach(t),p8=u(Yp," is getting closer to uniform probability."),Yp.forEach(t),Zy.forEach(t),d8=p(C),qo=o(C,"TR",{});var eE=l(qo);lc=o(eE,"TD",{align:!0});var xB=l(lc);h8=u(xB,"repetition_penalty"),xB.forEach(t),g8=p(eE),xs=o(eE,"TD",{align:!0});var tE=l(xs);m8=u(tE,"(Default: "),im=o(tE,"CODE",{});var SB=l(im);_8=u(SB,"None"),SB.forEach(t),$8=u(tE,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),tE.forEach(t),eE.forEach(t),q8=p(C),vo=o(C,"TR",{});var sE=l(vo);ic=o(sE,"TD",{align:!0});var IB=l(ic);v8=u(IB,"max_new_tokens"),IB.forEach(t),y8=p(sE),be=o(sE,"TD",{align:!0});var Vp=l(be);E8=u(Vp,"(Default: "),um=o(Vp,"CODE",{});var HB=l(um);w8=u(HB,"None"),HB.forEach(t),b8=u(Vp,"). Int (0-250). The amount of new tokens to be generated, this does "),fm=o(Vp,"STRONG",{});var BB=l(fm);T8=u(BB,"not"),BB.forEach(t),j8=u(Vp," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),Vp.forEach(t),sE.forEach(t),k8=p(C),yo=o(C,"TR",{});var aE=l(yo);uc=o(aE,"TD",{align:!0});var CB=l(uc);A8=u(CB,"max_time"),CB.forEach(t),D8=p(aE),Te=o(aE,"TD",{align:!0});var Xp=l(Te);O8=u(Xp,"(Default: "),cm=o(Xp,"CODE",{});var GB=l(cm);P8=u(GB,"None"),GB.forEach(t),R8=u(Xp,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),pm=o(Xp,"CODE",{});var UB=l(pm);N8=u(UB,"max_new_tokens"),UB.forEach(t),x8=u(Xp," for best results."),Xp.forEach(t),aE.forEach(t),S8=p(C),Eo=o(C,"TR",{});var nE=l(Eo);fc=o(nE,"TD",{align:!0});var LB=l(fc);I8=u(LB,"return_full_text"),LB.forEach(t),H8=p(nE),je=o(nE,"TD",{align:!0});var Qp=l(je);B8=u(Qp,"(Default: "),dm=o(Qp,"CODE",{});var zB=l(dm);C8=u(zB,"True"),zB.forEach(t),G8=u(Qp,"). Bool. If set to False, the return results will "),hm=o(Qp,"STRONG",{});var MB=l(hm);U8=u(MB,"not"),MB.forEach(t),L8=u(Qp," contain the original query making it easier for prompting."),Qp.forEach(t),nE.forEach(t),z8=p(C),wo=o(C,"TR",{});var rE=l(wo);cc=o(rE,"TD",{align:!0});var FB=l(cc);M8=u(FB,"num_return_sequences"),FB.forEach(t),F8=p(rE),Ss=o(rE,"TD",{align:!0});var oE=l(Ss);J8=u(oE,"(Default: "),gm=o(oE,"CODE",{});var JB=l(gm);K8=u(JB,"1"),JB.forEach(t),W8=u(oE,"). Integer. The number of proposition you want to be returned."),oE.forEach(t),rE.forEach(t),Y8=p(C),bo=o(C,"TR",{});var lE=l(bo);pc=o(lE,"TD",{align:!0});var KB=l(pc);V8=u(KB,"do_sample"),KB.forEach(t),X8=p(lE),Is=o(lE,"TD",{align:!0});var iE=l(Is);Q8=u(iE,"(Optional: "),mm=o(iE,"CODE",{});var WB=l(mm);Z8=u(WB,"True"),WB.forEach(t),eA=u(iE,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),iE.forEach(t),lE.forEach(t),tA=p(C),To=o(C,"TR",{});var uE=l(To);dc=o(uE,"TD",{align:!0});var YB=l(dc);_m=o(YB,"STRONG",{});var VB=l(_m);sA=u(VB,"options"),VB.forEach(t),YB.forEach(t),aA=p(uE),hc=o(uE,"TD",{align:!0});var XB=l(hc);nA=u(XB,"a dict containing the following keys:"),XB.forEach(t),uE.forEach(t),rA=p(C),jo=o(C,"TR",{});var fE=l(jo);gc=o(fE,"TD",{align:!0});var QB=l(gc);oA=u(QB,"use_gpu"),QB.forEach(t),lA=p(fE),Hs=o(fE,"TD",{align:!0});var cE=l(Hs);iA=u(cE,"(Default: "),$m=o(cE,"CODE",{});var ZB=l($m);uA=u(ZB,"false"),ZB.forEach(t),fA=u(cE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),cE.forEach(t),fE.forEach(t),cA=p(C),ko=o(C,"TR",{});var pE=l(ko);mc=o(pE,"TD",{align:!0});var eC=l(mc);pA=u(eC,"use_cache"),eC.forEach(t),dA=p(pE),Bs=o(pE,"TD",{align:!0});var dE=l(Bs);hA=u(dE,"(Default: "),qm=o(dE,"CODE",{});var tC=l(qm);gA=u(tC,"true"),tC.forEach(t),mA=u(dE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),dE.forEach(t),pE.forEach(t),_A=p(C),Ao=o(C,"TR",{});var hE=l(Ao);_c=o(hE,"TD",{align:!0});var sC=l(_c);$A=u(sC,"wait_for_model"),sC.forEach(t),qA=p(hE),Cs=o(hE,"TD",{align:!0});var gE=l(Cs);vA=u(gE,"(Default: "),vm=o(gE,"CODE",{});var aC=l(vm);yA=u(aC,"false"),aC.forEach(t),EA=u(gE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),gE.forEach(t),hE.forEach(t),C.forEach(t),Ky.forEach(t),Vq=p(a),$c=o(a,"P",{});var nC=l($c);wA=u(nC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),nC.forEach(t),Xq=p(a),v(Gs.$$.fragment,a),Qq=p(a),Us=o(a,"TABLE",{});var mE=l(Us);ym=o(mE,"THEAD",{});var rC=l(ym);Do=o(rC,"TR",{});var _E=l(Do);qc=o(_E,"TH",{align:!0});var oC=l(qc);bA=u(oC,"Returned values"),oC.forEach(t),TA=p(_E),Em=o(_E,"TH",{align:!0}),l(Em).forEach(t),_E.forEach(t),rC.forEach(t),jA=p(mE),wm=o(mE,"TBODY",{});var lC=l(wm);Oo=o(lC,"TR",{});var $E=l(Oo);vc=o($E,"TD",{align:!0});var iC=l(vc);bm=o(iC,"STRONG",{});var uC=l(bm);kA=u(uC,"generated_text"),uC.forEach(t),iC.forEach(t),AA=p($E),yc=o($E,"TD",{align:!0});var fC=l(yc);DA=u(fC,"The continuated string"),fC.forEach(t),$E.forEach(t),lC.forEach(t),mE.forEach(t),Zq=p(a),Ke=o(a,"H2",{class:!0});var qE=l(Ke);Ls=o(qE,"A",{id:!0,class:!0,href:!0});var cC=l(Ls);Tm=o(cC,"SPAN",{});var pC=l(Tm);v(Po.$$.fragment,pC),pC.forEach(t),cC.forEach(t),OA=p(qE),jm=o(qE,"SPAN",{});var dC=l(jm);PA=u(dC,"Text2text-generation task"),dC.forEach(t),qE.forEach(t),e1=p(a),zs=o(a,"P",{});var vE=l(zs);RA=u(vE,"Essentially "),Ec=o(vE,"A",{href:!0});var hC=l(Ec);NA=u(hC,"Text-generation task"),hC.forEach(t),xA=u(vE,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),vE.forEach(t),t1=p(a),We=o(a,"H2",{class:!0});var yE=l(We);Ms=o(yE,"A",{id:!0,class:!0,href:!0});var gC=l(Ms);km=o(gC,"SPAN",{});var mC=l(km);v(Ro.$$.fragment,mC),mC.forEach(t),gC.forEach(t),SA=p(yE),Am=o(yE,"SPAN",{});var _C=l(Am);IA=u(_C,"Fill mask task"),_C.forEach(t),yE.forEach(t),s1=p(a),wc=o(a,"P",{});var $C=l(wc);HA=u($C,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),$C.forEach(t),a1=p(a),v(Fs.$$.fragment,a),n1=p(a),No=o(a,"P",{});var _R=l(No);BA=u(_R,"Available with: "),xo=o(_R,"A",{href:!0,rel:!0});var qC=l(xo);CA=u(qC,"\u{1F917} Transformers"),qC.forEach(t),_R.forEach(t),r1=p(a),bc=o(a,"P",{});var vC=l(bc);GA=u(vC,"Example:"),vC.forEach(t),o1=p(a),v(Js.$$.fragment,a),l1=p(a),Tc=o(a,"P",{});var yC=l(Tc);UA=u(yC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),yC.forEach(t),i1=p(a),Ks=o(a,"TABLE",{});var EE=l(Ks);Dm=o(EE,"THEAD",{});var EC=l(Dm);So=o(EC,"TR",{});var wE=l(So);jc=o(wE,"TH",{align:!0});var wC=l(jc);LA=u(wC,"All parameters"),wC.forEach(t),zA=p(wE),Om=o(wE,"TH",{align:!0}),l(Om).forEach(t),wE.forEach(t),EC.forEach(t),MA=p(EE),se=o(EE,"TBODY",{});var Oe=l(se);Io=o(Oe,"TR",{});var bE=l(Io);Ho=o(bE,"TD",{align:!0});var $R=l(Ho);Pm=o($R,"STRONG",{});var bC=l(Pm);FA=u(bC,"inputs"),bC.forEach(t),JA=u($R," (required):"),$R.forEach(t),KA=p(bE),kc=o(bE,"TD",{align:!0});var TC=l(kc);WA=u(TC,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),TC.forEach(t),bE.forEach(t),YA=p(Oe),Bo=o(Oe,"TR",{});var TE=l(Bo);Ac=o(TE,"TD",{align:!0});var jC=l(Ac);Rm=o(jC,"STRONG",{});var kC=l(Rm);VA=u(kC,"options"),kC.forEach(t),jC.forEach(t),XA=p(TE),Dc=o(TE,"TD",{align:!0});var AC=l(Dc);QA=u(AC,"a dict containing the following keys:"),AC.forEach(t),TE.forEach(t),ZA=p(Oe),Co=o(Oe,"TR",{});var jE=l(Co);Oc=o(jE,"TD",{align:!0});var DC=l(Oc);eD=u(DC,"use_gpu"),DC.forEach(t),tD=p(jE),Ws=o(jE,"TD",{align:!0});var kE=l(Ws);sD=u(kE,"(Default: "),Nm=o(kE,"CODE",{});var OC=l(Nm);aD=u(OC,"false"),OC.forEach(t),nD=u(kE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),kE.forEach(t),jE.forEach(t),rD=p(Oe),Go=o(Oe,"TR",{});var AE=l(Go);Pc=o(AE,"TD",{align:!0});var PC=l(Pc);oD=u(PC,"use_cache"),PC.forEach(t),lD=p(AE),Ys=o(AE,"TD",{align:!0});var DE=l(Ys);iD=u(DE,"(Default: "),xm=o(DE,"CODE",{});var RC=l(xm);uD=u(RC,"true"),RC.forEach(t),fD=u(DE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),DE.forEach(t),AE.forEach(t),cD=p(Oe),Uo=o(Oe,"TR",{});var OE=l(Uo);Rc=o(OE,"TD",{align:!0});var NC=l(Rc);pD=u(NC,"wait_for_model"),NC.forEach(t),dD=p(OE),Vs=o(OE,"TD",{align:!0});var PE=l(Vs);hD=u(PE,"(Default: "),Sm=o(PE,"CODE",{});var xC=l(Sm);gD=u(xC,"false"),xC.forEach(t),mD=u(PE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),PE.forEach(t),OE.forEach(t),Oe.forEach(t),EE.forEach(t),u1=p(a),Nc=o(a,"P",{});var SC=l(Nc);_D=u(SC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),SC.forEach(t),f1=p(a),v(Xs.$$.fragment,a),c1=p(a),Qs=o(a,"TABLE",{});var RE=l(Qs);Im=o(RE,"THEAD",{});var IC=l(Im);Lo=o(IC,"TR",{});var NE=l(Lo);xc=o(NE,"TH",{align:!0});var HC=l(xc);$D=u(HC,"Returned values"),HC.forEach(t),qD=p(NE),Hm=o(NE,"TH",{align:!0}),l(Hm).forEach(t),NE.forEach(t),IC.forEach(t),vD=p(RE),ce=o(RE,"TBODY",{});var Ra=l(ce);zo=o(Ra,"TR",{});var xE=l(zo);Sc=o(xE,"TD",{align:!0});var BC=l(Sc);Bm=o(BC,"STRONG",{});var CC=l(Bm);yD=u(CC,"sequence"),CC.forEach(t),BC.forEach(t),ED=p(xE),Ic=o(xE,"TD",{align:!0});var GC=l(Ic);wD=u(GC,"The actual sequence of tokens that ran against the model (may contain special tokens)"),GC.forEach(t),xE.forEach(t),bD=p(Ra),Mo=o(Ra,"TR",{});var SE=l(Mo);Hc=o(SE,"TD",{align:!0});var UC=l(Hc);Cm=o(UC,"STRONG",{});var LC=l(Cm);TD=u(LC,"score"),LC.forEach(t),UC.forEach(t),jD=p(SE),Bc=o(SE,"TD",{align:!0});var zC=l(Bc);kD=u(zC,"The probability for this token."),zC.forEach(t),SE.forEach(t),AD=p(Ra),Fo=o(Ra,"TR",{});var IE=l(Fo);Cc=o(IE,"TD",{align:!0});var MC=l(Cc);Gm=o(MC,"STRONG",{});var FC=l(Gm);DD=u(FC,"token"),FC.forEach(t),MC.forEach(t),OD=p(IE),Gc=o(IE,"TD",{align:!0});var JC=l(Gc);PD=u(JC,"The id of the token"),JC.forEach(t),IE.forEach(t),RD=p(Ra),Jo=o(Ra,"TR",{});var HE=l(Jo);Uc=o(HE,"TD",{align:!0});var KC=l(Uc);Um=o(KC,"STRONG",{});var WC=l(Um);ND=u(WC,"token_str"),WC.forEach(t),KC.forEach(t),xD=p(HE),Lc=o(HE,"TD",{align:!0});var YC=l(Lc);SD=u(YC,"The string representation of the token"),YC.forEach(t),HE.forEach(t),Ra.forEach(t),RE.forEach(t),p1=p(a),Ye=o(a,"H2",{class:!0});var BE=l(Ye);Zs=o(BE,"A",{id:!0,class:!0,href:!0});var VC=l(Zs);Lm=o(VC,"SPAN",{});var XC=l(Lm);v(Ko.$$.fragment,XC),XC.forEach(t),VC.forEach(t),ID=p(BE),zm=o(BE,"SPAN",{});var QC=l(zm);HD=u(QC,"Automatic speech recognition task"),QC.forEach(t),BE.forEach(t),d1=p(a),zc=o(a,"P",{});var ZC=l(zc);BD=u(ZC,`This task reads some audio input and outputs the said words within the
audio files.`),ZC.forEach(t),h1=p(a),v(ea.$$.fragment,a),g1=p(a),v(ta.$$.fragment,a),m1=p(a),pe=o(a,"P",{});var Ml=l(pe);CD=u(Ml,"Available with: "),Wo=o(Ml,"A",{href:!0,rel:!0});var eG=l(Wo);GD=u(eG,"\u{1F917} Transformers"),eG.forEach(t),UD=p(Ml),Yo=o(Ml,"A",{href:!0,rel:!0});var tG=l(Yo);LD=u(tG,"ESPnet"),tG.forEach(t),zD=u(Ml,` and
`),Vo=o(Ml,"A",{href:!0,rel:!0});var sG=l(Vo);MD=u(sG,"SpeechBrain"),sG.forEach(t),Ml.forEach(t),_1=p(a),Mc=o(a,"P",{});var aG=l(Mc);FD=u(aG,"Request:"),aG.forEach(t),$1=p(a),v(sa.$$.fragment,a),q1=p(a),Fc=o(a,"P",{});var nG=l(Fc);JD=u(nG,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),nG.forEach(t),v1=p(a),aa=o(a,"TABLE",{});var CE=l(aa);Mm=o(CE,"THEAD",{});var rG=l(Mm);Xo=o(rG,"TR",{});var GE=l(Xo);Jc=o(GE,"TH",{align:!0});var oG=l(Jc);KD=u(oG,"All parameters"),oG.forEach(t),WD=p(GE),Fm=o(GE,"TH",{align:!0}),l(Fm).forEach(t),GE.forEach(t),rG.forEach(t),YD=p(CE),Jm=o(CE,"TBODY",{});var lG=l(Jm);Qo=o(lG,"TR",{});var UE=l(Qo);Zo=o(UE,"TD",{align:!0});var qR=l(Zo);Km=o(qR,"STRONG",{});var iG=l(Km);VD=u(iG,"no parameter"),iG.forEach(t),XD=u(qR," (required)"),qR.forEach(t),QD=p(UE),Kc=o(UE,"TD",{align:!0});var uG=l(Kc);ZD=u(uG,"a binary representation of the audio file. No other parameters are currently allowed."),uG.forEach(t),UE.forEach(t),lG.forEach(t),CE.forEach(t),y1=p(a),Wc=o(a,"P",{});var fG=l(Wc);eO=u(fG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),fG.forEach(t),E1=p(a),Yc=o(a,"P",{});var cG=l(Yc);tO=u(cG,"Response:"),cG.forEach(t),w1=p(a),v(na.$$.fragment,a),b1=p(a),ra=o(a,"TABLE",{});var LE=l(ra);Wm=o(LE,"THEAD",{});var pG=l(Wm);el=o(pG,"TR",{});var zE=l(el);Vc=o(zE,"TH",{align:!0});var dG=l(Vc);sO=u(dG,"Returned values"),dG.forEach(t),aO=p(zE),Ym=o(zE,"TH",{align:!0}),l(Ym).forEach(t),zE.forEach(t),pG.forEach(t),nO=p(LE),Vm=o(LE,"TBODY",{});var hG=l(Vm);tl=o(hG,"TR",{});var ME=l(tl);Xc=o(ME,"TD",{align:!0});var gG=l(Xc);Xm=o(gG,"STRONG",{});var mG=l(Xm);rO=u(mG,"text"),mG.forEach(t),gG.forEach(t),oO=p(ME),Qc=o(ME,"TD",{align:!0});var _G=l(Qc);lO=u(_G,"The string that was recognized within the audio file."),_G.forEach(t),ME.forEach(t),hG.forEach(t),LE.forEach(t),T1=p(a),Ve=o(a,"H2",{class:!0});var FE=l(Ve);oa=o(FE,"A",{id:!0,class:!0,href:!0});var $G=l(oa);Qm=o($G,"SPAN",{});var qG=l(Qm);v(sl.$$.fragment,qG),qG.forEach(t),$G.forEach(t),iO=p(FE),Zm=o(FE,"SPAN",{});var vG=l(Zm);uO=u(vG,"Feature-extraction task"),vG.forEach(t),FE.forEach(t),j1=p(a),Zc=o(a,"P",{});var yG=l(Zc);fO=u(yG,`This task reads some text and outputs raw float values, that usually
consumed as part of a semantic database/semantic search.`),yG.forEach(t),k1=p(a),v(la.$$.fragment,a),A1=p(a),Xe=o(a,"P",{});var n$=l(Xe);cO=u(n$,"Available with: "),al=o(n$,"A",{href:!0,rel:!0});var EG=l(al);pO=u(EG,"\u{1F917} Transformers"),EG.forEach(t),dO=p(n$),nl=o(n$,"A",{href:!0,rel:!0});var wG=l(nl);hO=u(wG,"Sentence-transformers"),wG.forEach(t),n$.forEach(t),D1=p(a),ep=o(a,"P",{});var bG=l(ep);gO=u(bG,"Request:"),bG.forEach(t),O1=p(a),ia=o(a,"TABLE",{});var JE=l(ia);e_=o(JE,"THEAD",{});var TG=l(e_);rl=o(TG,"TR",{});var KE=l(rl);tp=o(KE,"TH",{align:!0});var jG=l(tp);mO=u(jG,"All parameters"),jG.forEach(t),_O=p(KE),t_=o(KE,"TH",{align:!0}),l(t_).forEach(t),KE.forEach(t),TG.forEach(t),$O=p(JE),ae=o(JE,"TBODY",{});var Pe=l(ae);ol=o(Pe,"TR",{});var WE=l(ol);ll=o(WE,"TD",{align:!0});var vR=l(ll);s_=o(vR,"STRONG",{});var kG=l(s_);qO=u(kG,"inputs"),kG.forEach(t),vO=u(vR," (required):"),vR.forEach(t),yO=p(WE),sp=o(WE,"TD",{align:!0});var AG=l(sp);EO=u(AG,"a string or a list of strings to get the features from."),AG.forEach(t),WE.forEach(t),wO=p(Pe),il=o(Pe,"TR",{});var YE=l(il);ap=o(YE,"TD",{align:!0});var DG=l(ap);a_=o(DG,"STRONG",{});var OG=l(a_);bO=u(OG,"options"),OG.forEach(t),DG.forEach(t),TO=p(YE),np=o(YE,"TD",{align:!0});var PG=l(np);jO=u(PG,"a dict containing the following keys:"),PG.forEach(t),YE.forEach(t),kO=p(Pe),ul=o(Pe,"TR",{});var VE=l(ul);rp=o(VE,"TD",{align:!0});var RG=l(rp);AO=u(RG,"use_gpu"),RG.forEach(t),DO=p(VE),ua=o(VE,"TD",{align:!0});var XE=l(ua);OO=u(XE,"(Default: "),n_=o(XE,"CODE",{});var NG=l(n_);PO=u(NG,"false"),NG.forEach(t),RO=u(XE,"). Boolean to use GPU instead of CPU for inference (requires Startup plan at least)"),XE.forEach(t),VE.forEach(t),NO=p(Pe),fl=o(Pe,"TR",{});var QE=l(fl);op=o(QE,"TD",{align:!0});var xG=l(op);xO=u(xG,"use_cache"),xG.forEach(t),SO=p(QE),fa=o(QE,"TD",{align:!0});var ZE=l(fa);IO=u(ZE,"(Default: "),r_=o(ZE,"CODE",{});var SG=l(r_);HO=u(SG,"true"),SG.forEach(t),BO=u(ZE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),ZE.forEach(t),QE.forEach(t),CO=p(Pe),cl=o(Pe,"TR",{});var ew=l(cl);lp=o(ew,"TD",{align:!0});var IG=l(lp);GO=u(IG,"wait_for_model"),IG.forEach(t),UO=p(ew),ca=o(ew,"TD",{align:!0});var tw=l(ca);LO=u(tw,"(Default: "),o_=o(tw,"CODE",{});var HG=l(o_);zO=u(HG,"false"),HG.forEach(t),MO=u(tw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),tw.forEach(t),ew.forEach(t),Pe.forEach(t),JE.forEach(t),P1=p(a),ip=o(a,"P",{});var BG=l(ip);FO=u(BG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),BG.forEach(t),R1=p(a),pa=o(a,"TABLE",{});var sw=l(pa);l_=o(sw,"THEAD",{});var CG=l(l_);pl=o(CG,"TR",{});var aw=l(pl);up=o(aw,"TH",{align:!0});var GG=l(up);JO=u(GG,"Returned values"),GG.forEach(t),KO=p(aw),i_=o(aw,"TH",{align:!0}),l(i_).forEach(t),aw.forEach(t),CG.forEach(t),WO=p(sw),u_=o(sw,"TBODY",{});var UG=l(u_);dl=o(UG,"TR",{});var nw=l(dl);fp=o(nw,"TD",{align:!0});var LG=l(fp);f_=o(LG,"STRONG",{});var zG=l(f_);YO=u(zG,"A list of float (or list of list of floats)"),zG.forEach(t),LG.forEach(t),VO=p(nw),cp=o(nw,"TD",{align:!0});var MG=l(cp);XO=u(MG,"The numbers that are the representation features of the input."),MG.forEach(t),nw.forEach(t),UG.forEach(t),sw.forEach(t),N1=p(a),pp=o(a,"SMALL",{});var FG=l(pp);QO=u(FG,`Returned values are a list of floats, or a list of list of floats
(depending on if you sent a string or a list of string, and if the
automatic reduction, usually mean_pooling for instance was applied for
you or not. This should be explained on the model's README.`),FG.forEach(t),x1=p(a),Qe=o(a,"H2",{class:!0});var rw=l(Qe);da=o(rw,"A",{id:!0,class:!0,href:!0});var JG=l(da);c_=o(JG,"SPAN",{});var KG=l(c_);v(hl.$$.fragment,KG),KG.forEach(t),JG.forEach(t),ZO=p(rw),p_=o(rw,"SPAN",{});var WG=l(p_);eP=u(WG,"Audio-classification task"),WG.forEach(t),rw.forEach(t),S1=p(a),dp=o(a,"P",{});var YG=l(dp);tP=u(YG,"This task reads some audio input and outputs the likelihood of classes."),YG.forEach(t),I1=p(a),v(ha.$$.fragment,a),H1=p(a),Ze=o(a,"P",{});var r$=l(Ze);sP=u(r$,"Available with: "),gl=o(r$,"A",{href:!0,rel:!0});var VG=l(gl);aP=u(VG,"\u{1F917} Transformers"),VG.forEach(t),nP=p(r$),ml=o(r$,"A",{href:!0,rel:!0});var XG=l(ml);rP=u(XG,"SpeechBrain"),XG.forEach(t),r$.forEach(t),B1=p(a),hp=o(a,"P",{});var QG=l(hp);oP=u(QG,"Request:"),QG.forEach(t),C1=p(a),v(ga.$$.fragment,a),G1=p(a),gp=o(a,"P",{});var ZG=l(gp);lP=u(ZG,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),ZG.forEach(t),U1=p(a),ma=o(a,"TABLE",{});var ow=l(ma);d_=o(ow,"THEAD",{});var eU=l(d_);_l=o(eU,"TR",{});var lw=l(_l);mp=o(lw,"TH",{align:!0});var tU=l(mp);iP=u(tU,"All parameters"),tU.forEach(t),uP=p(lw),h_=o(lw,"TH",{align:!0}),l(h_).forEach(t),lw.forEach(t),eU.forEach(t),fP=p(ow),g_=o(ow,"TBODY",{});var sU=l(g_);$l=o(sU,"TR",{});var iw=l($l);ql=o(iw,"TD",{align:!0});var yR=l(ql);m_=o(yR,"STRONG",{});var aU=l(m_);cP=u(aU,"no parameter"),aU.forEach(t),pP=u(yR," (required)"),yR.forEach(t),dP=p(iw),_p=o(iw,"TD",{align:!0});var nU=l(_p);hP=u(nU,"a binary representation of the audio file. No other parameters are currently allowed."),nU.forEach(t),iw.forEach(t),sU.forEach(t),ow.forEach(t),L1=p(a),$p=o(a,"P",{});var rU=l($p);gP=u(rU,"Return value is a dict"),rU.forEach(t),z1=p(a),v(_a.$$.fragment,a),M1=p(a),$a=o(a,"TABLE",{});var uw=l($a);__=o(uw,"THEAD",{});var oU=l(__);vl=o(oU,"TR",{});var fw=l(vl);qp=o(fw,"TH",{align:!0});var lU=l(qp);mP=u(lU,"Returned values"),lU.forEach(t),_P=p(fw),$_=o(fw,"TH",{align:!0}),l($_).forEach(t),fw.forEach(t),oU.forEach(t),$P=p(uw),yl=o(uw,"TBODY",{});var cw=l(yl);El=o(cw,"TR",{});var pw=l(El);vp=o(pw,"TD",{align:!0});var iU=l(vp);q_=o(iU,"STRONG",{});var uU=l(q_);qP=u(uU,"label"),uU.forEach(t),iU.forEach(t),vP=p(pw),yp=o(pw,"TD",{align:!0});var fU=l(yp);yP=u(fU,"The label for the class (model specific)"),fU.forEach(t),pw.forEach(t),EP=p(cw),wl=o(cw,"TR",{});var dw=l(wl);Ep=o(dw,"TD",{align:!0});var cU=l(Ep);v_=o(cU,"STRONG",{});var pU=l(v_);wP=u(pU,"score"),pU.forEach(t),cU.forEach(t),bP=p(dw),wp=o(dw,"TD",{align:!0});var dU=l(wp);TP=u(dU,"A floats that represents how likely is that the audio file belongs the this class."),dU.forEach(t),dw.forEach(t),cw.forEach(t),uw.forEach(t),F1=p(a),et=o(a,"H2",{class:!0});var hw=l(et);qa=o(hw,"A",{id:!0,class:!0,href:!0});var hU=l(qa);y_=o(hU,"SPAN",{});var gU=l(y_);v(bl.$$.fragment,gU),gU.forEach(t),hU.forEach(t),jP=p(hw),E_=o(hw,"SPAN",{});var mU=l(E_);kP=u(mU,"Object-detection task"),mU.forEach(t),hw.forEach(t),J1=p(a),bp=o(a,"P",{});var _U=l(bp);AP=u(_U,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),_U.forEach(t),K1=p(a),v(va.$$.fragment,a),W1=p(a),Tl=o(a,"P",{});var ER=l(Tl);DP=u(ER,"Available with: "),jl=o(ER,"A",{href:!0,rel:!0});var $U=l(jl);OP=u($U,"\u{1F917} Transformers"),$U.forEach(t),ER.forEach(t),Y1=p(a),Tp=o(a,"P",{});var qU=l(Tp);PP=u(qU,"Request:"),qU.forEach(t),V1=p(a),v(ya.$$.fragment,a),X1=p(a),Ea=o(a,"P",{});var gw=l(Ea);RP=u(gw,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),kl=o(gw,"A",{href:!0,rel:!0});var vU=l(kl);NP=u(vU,`Pillow
supports`),vU.forEach(t),xP=u(gw,"."),gw.forEach(t),Q1=p(a),wa=o(a,"TABLE",{});var mw=l(wa);w_=o(mw,"THEAD",{});var yU=l(w_);Al=o(yU,"TR",{});var _w=l(Al);jp=o(_w,"TH",{align:!0});var EU=l(jp);SP=u(EU,"All parameters"),EU.forEach(t),IP=p(_w),b_=o(_w,"TH",{align:!0}),l(b_).forEach(t),_w.forEach(t),yU.forEach(t),HP=p(mw),T_=o(mw,"TBODY",{});var wU=l(T_);Dl=o(wU,"TR",{});var $w=l(Dl);Ol=o($w,"TD",{align:!0});var wR=l(Ol);j_=o(wR,"STRONG",{});var bU=l(j_);BP=u(bU,"no parameter"),bU.forEach(t),CP=u(wR," (required)"),wR.forEach(t),GP=p($w),kp=o($w,"TD",{align:!0});var TU=l(kp);UP=u(TU,"a binary representation of the image file. No other parameters are currently allowed."),TU.forEach(t),$w.forEach(t),wU.forEach(t),mw.forEach(t),Z1=p(a),Ap=o(a,"P",{});var jU=l(Ap);LP=u(jU,"Return value is a dict"),jU.forEach(t),ev=p(a),v(ba.$$.fragment,a),tv=p(a),Ta=o(a,"TABLE",{});var qw=l(Ta);k_=o(qw,"THEAD",{});var kU=l(k_);Pl=o(kU,"TR",{});var vw=l(Pl);Dp=o(vw,"TH",{align:!0});var AU=l(Dp);zP=u(AU,"Returned values"),AU.forEach(t),MP=p(vw),A_=o(vw,"TH",{align:!0}),l(A_).forEach(t),vw.forEach(t),kU.forEach(t),FP=p(qw),tt=o(qw,"TBODY",{});var Zp=l(tt);Rl=o(Zp,"TR",{});var yw=l(Rl);Op=o(yw,"TD",{align:!0});var DU=l(Op);D_=o(DU,"STRONG",{});var OU=l(D_);JP=u(OU,"label"),OU.forEach(t),DU.forEach(t),KP=p(yw),Pp=o(yw,"TD",{align:!0});var PU=l(Pp);WP=u(PU,"The label for the class (model specific) of a detected object."),PU.forEach(t),yw.forEach(t),YP=p(Zp),Nl=o(Zp,"TR",{});var Ew=l(Nl);Rp=o(Ew,"TD",{align:!0});var RU=l(Rp);O_=o(RU,"STRONG",{});var NU=l(O_);VP=u(NU,"score"),NU.forEach(t),RU.forEach(t),XP=p(Ew),Np=o(Ew,"TD",{align:!0});var xU=l(Np);QP=u(xU,"A float that represents how likely it is that the detected object belongs to the given class."),xU.forEach(t),Ew.forEach(t),ZP=p(Zp),xl=o(Zp,"TR",{});var ww=l(xl);xp=o(ww,"TD",{align:!0});var SU=l(xp);P_=o(SU,"STRONG",{});var IU=l(P_);eR=u(IU,"box"),IU.forEach(t),SU.forEach(t),tR=p(ww),Sp=o(ww,"TD",{align:!0});var HU=l(Sp);sR=u(HU,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),HU.forEach(t),ww.forEach(t),Zp.forEach(t),qw.forEach(t),this.h()},h(){d(n,"name","hf:doc:metadata"),d(n,"content",JSON.stringify(Vz)),d(h,"id","detailed-parameters"),d(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(h,"href","#detailed-parameters"),d(s,"class","relative group"),d(ne,"id","which-task-is-used-by-this-model"),d(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ne,"href","#which-task-is-used-by-this-model"),d(D,"class","relative group"),d(nt,"class","block dark:hidden"),BU(nt.src,bR="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||d(nt,"src",bR),d(nt,"width","300"),d(rt,"class","hidden dark:block invert"),BU(rt.src,TR="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||d(rt,"src",TR),d(rt,"width","300"),d(ot,"id","zeroshot-classification-task"),d(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ot,"href","#zeroshot-classification-task"),d(xe,"class","relative group"),d(Ia,"href","https://github.com/huggingface/transformers"),d(Ia,"rel","nofollow"),d(Vl,"align","left"),d(ad,"align","left"),d(Ca,"align","left"),d(Xl,"align","left"),d(Ua,"align","left"),d(Ql,"align","left"),d(Zl,"align","left"),d(de,"align","left"),d(ei,"align","left"),d(ft,"align","left"),d(ti,"align","left"),d(si,"align","left"),d(ai,"align","left"),d(ct,"align","left"),d(ni,"align","left"),d(pt,"align","left"),d(ri,"align","left"),d(dt,"align","left"),d(ii,"align","left"),d(hd,"align","left"),d(ui,"align","left"),d(fi,"align","left"),d(ci,"align","left"),d(pi,"align","left"),d(di,"align","left"),d(mt,"align","left"),d(_t,"id","translation-task"),d(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_t,"href","#translation-task"),d(Ie,"class","relative group"),d(en,"href","https://github.com/huggingface/transformers"),d(en,"rel","nofollow"),d(_i,"align","left"),d(Ed,"align","left"),d(an,"align","left"),d($i,"align","left"),d(qi,"align","left"),d(vi,"align","left"),d(yi,"align","left"),d(yt,"align","left"),d(Ei,"align","left"),d(Et,"align","left"),d(wi,"align","left"),d(wt,"align","left"),d(Ti,"align","left"),d(Dd,"align","left"),d(ji,"align","left"),d(ki,"align","left"),d(Tt,"id","summarization-task"),d(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Tt,"href","#summarization-task"),d(He,"class","relative group"),d(Ai,"href","mailto:api-enterprise@huggingface.co"),d(dn,"href","https://github.com/huggingface/transformers"),d(dn,"rel","nofollow"),d(Pi,"align","left"),d(Sd,"align","left"),d(mn,"align","left"),d(Ri,"align","left"),d(Ni,"align","left"),d(xi,"align","left"),d(Si,"align","left"),d(he,"align","left"),d(Ii,"align","left"),d(ge,"align","left"),d(Hi,"align","left"),d(me,"align","left"),d(Bi,"align","left"),d(re,"align","left"),d(Ci,"align","left"),d(_e,"align","left"),d(Gi,"align","left"),d(Ot,"align","left"),d(Ui,"align","left"),d(Pt,"align","left"),d(Li,"align","left"),d(zi,"align","left"),d(Mi,"align","left"),d(Rt,"align","left"),d(Fi,"align","left"),d(Nt,"align","left"),d(Ji,"align","left"),d(xt,"align","left"),d(Wi,"align","left"),d(sh,"align","left"),d(Yi,"align","left"),d(Vi,"align","left"),d(It,"id","conversational-task"),d(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(It,"href","#conversational-task"),d(Be,"class","relative group"),d(Nn,"href","https://github.com/huggingface/transformers"),d(Nn,"rel","nofollow"),d(eu,"align","left"),d(ih,"align","left"),d(In,"align","left"),d(fh,"align","left"),d(tu,"align","left"),d(su,"align","left"),d(au,"align","left"),d(nu,"align","left"),d(ru,"align","left"),d(Gt,"align","left"),d(ou,"align","left"),d(lu,"align","left"),d(iu,"align","left"),d($e,"align","left"),d(uu,"align","left"),d(qe,"align","left"),d(fu,"align","left"),d(ve,"align","left"),d(cu,"align","left"),d(oe,"align","left"),d(pu,"align","left"),d(ye,"align","left"),d(du,"align","left"),d(Ut,"align","left"),d(hu,"align","left"),d(Lt,"align","left"),d(gu,"align","left"),d(mu,"align","left"),d(_u,"align","left"),d(zt,"align","left"),d($u,"align","left"),d(Mt,"align","left"),d(qu,"align","left"),d(Ft,"align","left"),d(yu,"align","left"),d(Ph,"align","left"),d(Eu,"align","left"),d(wu,"align","left"),d(bu,"align","left"),d(Tu,"align","left"),d(ju,"align","left"),d(ku,"align","left"),d(Au,"align","left"),d(Du,"align","left"),d(Kt,"id","table-question-answering-task"),d(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Kt,"href","#table-question-answering-task"),d(Ce,"class","relative group"),d(rr,"href","https://github.com/huggingface/transformers"),d(rr,"rel","nofollow"),d(Nu,"align","left"),d(Hh,"align","left"),d(ir,"align","left"),d(Ch,"align","left"),d(xu,"align","left"),d(Su,"align","left"),d(Iu,"align","left"),d(Hu,"align","left"),d(Bu,"align","left"),d(Cu,"align","left"),d(Gu,"align","left"),d(Xt,"align","left"),d(Uu,"align","left"),d(Qt,"align","left"),d(Lu,"align","left"),d(Zt,"align","left"),d(Mu,"align","left"),d(Fh,"align","left"),d(Fu,"align","left"),d(Ju,"align","left"),d(Ku,"align","left"),d(Wu,"align","left"),d(Yu,"align","left"),d(Vu,"align","left"),d(Xu,"align","left"),d(Qu,"align","left"),d(ss,"id","question-answering-task"),d(ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ss,"href","#question-answering-task"),d(Ge,"class","relative group"),d(yr,"href","https://github.com/huggingface/transformers"),d(yr,"rel","nofollow"),d(Er,"href","https://github.com/allenai/allennlp"),d(Er,"rel","nofollow"),d(af,"align","left"),d(Zh,"align","left"),d(nf,"align","left"),d(rf,"align","left"),d(of,"align","left"),d(lf,"align","left"),d(uf,"align","left"),d(ls,"align","left"),d(ff,"align","left"),d(is,"align","left"),d(us,"id","textclassification-task"),d(us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(us,"href","#textclassification-task"),d(Le,"class","relative group"),d(Or,"href","https://github.com/huggingface/transformers"),d(Or,"rel","nofollow"),d(hf,"align","left"),d(ug,"align","left"),d(Nr,"align","left"),d(gf,"align","left"),d(mf,"align","left"),d(_f,"align","left"),d($f,"align","left"),d(ds,"align","left"),d(qf,"align","left"),d(hs,"align","left"),d(vf,"align","left"),d(gs,"align","left"),d(Ef,"align","left"),d(mg,"align","left"),d(wf,"align","left"),d(bf,"align","left"),d(Tf,"align","left"),d(jf,"align","left"),d($s,"id","named-entity-recognition-ner-task"),d($s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($s,"href","#named-entity-recognition-ner-task"),d(ze,"class","relative group"),d(kf,"href","#token-classification-task"),d(qs,"id","tokenclassification-task"),d(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qs,"href","#tokenclassification-task"),d(Me,"class","relative group"),d(Fr,"href","https://github.com/huggingface/transformers"),d(Fr,"rel","nofollow"),d(Jr,"href","https://github.com/flairNLP/flair"),d(Jr,"rel","nofollow"),d(Pf,"align","left"),d(bg,"align","left"),d(Yr,"align","left"),d(Rf,"align","left"),d(Nf,"align","left"),d(xf,"align","left"),d(Sf,"align","left"),d(S,"align","left"),d(If,"align","left"),d(Hf,"align","left"),d(Bf,"align","left"),d(ws,"align","left"),d(Cf,"align","left"),d(bs,"align","left"),d(Gf,"align","left"),d(Ts,"align","left"),d(Lf,"align","left"),d(Ug,"align","left"),d(zf,"align","left"),d(Mf,"align","left"),d(Ff,"align","left"),d(Jf,"align","left"),d(Kf,"align","left"),d(Wf,"align","left"),d(Yf,"align","left"),d(As,"align","left"),d(Vf,"align","left"),d(Ds,"align","left"),d(Os,"id","textgeneration-task"),d(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Os,"href","#textgeneration-task"),d(Je,"class","relative group"),d(fo,"href","https://github.com/huggingface/transformers"),d(fo,"rel","nofollow"),d(ec,"align","left"),d(Qg,"align","left"),d(ho,"align","left"),d(tc,"align","left"),d(sc,"align","left"),d(ac,"align","left"),d(nc,"align","left"),d(Ee,"align","left"),d(rc,"align","left"),d(le,"align","left"),d(oc,"align","left"),d(we,"align","left"),d(lc,"align","left"),d(xs,"align","left"),d(ic,"align","left"),d(be,"align","left"),d(uc,"align","left"),d(Te,"align","left"),d(fc,"align","left"),d(je,"align","left"),d(cc,"align","left"),d(Ss,"align","left"),d(pc,"align","left"),d(Is,"align","left"),d(dc,"align","left"),d(hc,"align","left"),d(gc,"align","left"),d(Hs,"align","left"),d(mc,"align","left"),d(Bs,"align","left"),d(_c,"align","left"),d(Cs,"align","left"),d(qc,"align","left"),d(Em,"align","left"),d(vc,"align","left"),d(yc,"align","left"),d(Ls,"id","text2textgeneration-task"),d(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ls,"href","#text2textgeneration-task"),d(Ke,"class","relative group"),d(Ec,"href","#text-generation-task"),d(Ms,"id","fill-mask-task"),d(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ms,"href","#fill-mask-task"),d(We,"class","relative group"),d(xo,"href","https://github.com/huggingface/transformers"),d(xo,"rel","nofollow"),d(jc,"align","left"),d(Om,"align","left"),d(Ho,"align","left"),d(kc,"align","left"),d(Ac,"align","left"),d(Dc,"align","left"),d(Oc,"align","left"),d(Ws,"align","left"),d(Pc,"align","left"),d(Ys,"align","left"),d(Rc,"align","left"),d(Vs,"align","left"),d(xc,"align","left"),d(Hm,"align","left"),d(Sc,"align","left"),d(Ic,"align","left"),d(Hc,"align","left"),d(Bc,"align","left"),d(Cc,"align","left"),d(Gc,"align","left"),d(Uc,"align","left"),d(Lc,"align","left"),d(Zs,"id","automatic-speech-recognition-task"),d(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Zs,"href","#automatic-speech-recognition-task"),d(Ye,"class","relative group"),d(Wo,"href","https://github.com/huggingface/transformers"),d(Wo,"rel","nofollow"),d(Yo,"href","https://github.com/espnet/espnet"),d(Yo,"rel","nofollow"),d(Vo,"href","https://github.com/speechbrain/speechbrain"),d(Vo,"rel","nofollow"),d(Jc,"align","left"),d(Fm,"align","left"),d(Zo,"align","left"),d(Kc,"align","left"),d(Vc,"align","left"),d(Ym,"align","left"),d(Xc,"align","left"),d(Qc,"align","left"),d(oa,"id","featureextraction-task"),d(oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(oa,"href","#featureextraction-task"),d(Ve,"class","relative group"),d(al,"href","https://github.com/huggingface/transformers"),d(al,"rel","nofollow"),d(nl,"href","https://github.com/UKPLab/sentence-transformers"),d(nl,"rel","nofollow"),d(tp,"align","left"),d(t_,"align","left"),d(ll,"align","left"),d(sp,"align","left"),d(ap,"align","left"),d(np,"align","left"),d(rp,"align","left"),d(ua,"align","left"),d(op,"align","left"),d(fa,"align","left"),d(lp,"align","left"),d(ca,"align","left"),d(up,"align","left"),d(i_,"align","left"),d(fp,"align","left"),d(cp,"align","left"),d(da,"id","audioclassification-task"),d(da,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(da,"href","#audioclassification-task"),d(Qe,"class","relative group"),d(gl,"href","https://github.com/huggingface/transformers"),d(gl,"rel","nofollow"),d(ml,"href","https://github.com/speechbrain/speechbrain"),d(ml,"rel","nofollow"),d(mp,"align","left"),d(h_,"align","left"),d(ql,"align","left"),d(_p,"align","left"),d(qp,"align","left"),d($_,"align","left"),d(vp,"align","left"),d(yp,"align","left"),d(Ep,"align","left"),d(wp,"align","left"),d(qa,"id","objectdetection-task"),d(qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qa,"href","#objectdetection-task"),d(et,"class","relative group"),d(jl,"href","https://github.com/huggingface/transformers"),d(jl,"rel","nofollow"),d(kl,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),d(kl,"rel","nofollow"),d(jp,"align","left"),d(b_,"align","left"),d(Ol,"align","left"),d(kp,"align","left"),d(Dp,"align","left"),d(A_,"align","left"),d(Op,"align","left"),d(Pp,"align","left"),d(Rp,"align","left"),d(Np,"align","left"),d(xp,"align","left"),d(Sp,"align","left")},m(a,g){e(document.head,n),m(a,f,g),m(a,s,g),e(s,h),e(h,_),y(k,_,null),e(s,A),e(s,j),e(j,T),m(a,O,g),m(a,D,g),e(D,ne),e(ne,Re),y(Q,Re,null),e(D,W),e(D,at),e(at,Fl),m(a,Na,g),m(a,Ne,g),e(Ne,bw),m(a,o$,g),m(a,Jl,g),e(Jl,Tw),m(a,l$,g),m(a,nt,g),m(a,i$,g),m(a,rt,g),m(a,u$,g),m(a,xe,g),e(xe,ot),e(ot,ed),y(xa,ed,null),e(xe,jw),e(xe,td),e(td,kw),m(a,f$,g),m(a,Kl,g),e(Kl,Aw),m(a,c$,g),y(lt,a,g),m(a,p$,g),m(a,Sa,g),e(Sa,Dw),e(Sa,Ia),e(Ia,Ow),m(a,d$,g),m(a,Wl,g),e(Wl,Pw),m(a,h$,g),y(it,a,g),m(a,g$,g),m(a,Yl,g),e(Yl,Rw),m(a,m$,g),m(a,ut,g),e(ut,sd),e(sd,Ha),e(Ha,Vl),e(Vl,Nw),e(Ha,xw),e(Ha,ad),e(ut,Sw),e(ut,z),e(z,Ba),e(Ba,Ca),e(Ca,nd),e(nd,Iw),e(Ca,Hw),e(Ba,Bw),e(Ba,Xl),e(Xl,Cw),e(z,Gw),e(z,Ga),e(Ga,Ua),e(Ua,rd),e(rd,Uw),e(Ua,Lw),e(Ga,zw),e(Ga,Ql),e(Ql,Mw),e(z,Fw),e(z,La),e(La,Zl),e(Zl,Jw),e(La,Kw),e(La,de),e(de,Ww),e(de,od),e(od,Yw),e(de,Vw),e(de,ld),e(ld,Xw),e(de,Qw),e(z,Zw),e(z,za),e(za,ei),e(ei,eb),e(za,tb),e(za,ft),e(ft,sb),e(ft,id),e(id,ab),e(ft,nb),e(z,rb),e(z,Ma),e(Ma,ti),e(ti,ud),e(ud,ob),e(Ma,lb),e(Ma,si),e(si,ib),e(z,ub),e(z,Fa),e(Fa,ai),e(ai,fb),e(Fa,cb),e(Fa,ct),e(ct,pb),e(ct,fd),e(fd,db),e(ct,hb),e(z,gb),e(z,Ja),e(Ja,ni),e(ni,mb),e(Ja,_b),e(Ja,pt),e(pt,$b),e(pt,cd),e(cd,qb),e(pt,vb),e(z,yb),e(z,Ka),e(Ka,ri),e(ri,Eb),e(Ka,wb),e(Ka,dt),e(dt,bb),e(dt,pd),e(pd,Tb),e(dt,jb),m(a,_$,g),m(a,oi,g),e(oi,kb),m(a,$$,g),m(a,li,g),e(li,Ab),m(a,q$,g),y(ht,a,g),m(a,v$,g),m(a,gt,g),e(gt,dd),e(dd,Wa),e(Wa,ii),e(ii,Db),e(Wa,Ob),e(Wa,hd),e(gt,Pb),e(gt,Se),e(Se,Ya),e(Ya,ui),e(ui,gd),e(gd,Rb),e(Ya,Nb),e(Ya,fi),e(fi,xb),e(Se,Sb),e(Se,Va),e(Va,ci),e(ci,md),e(md,Ib),e(Va,Hb),e(Va,pi),e(pi,Bb),e(Se,Cb),e(Se,Xa),e(Xa,di),e(di,_d),e(_d,Gb),e(Xa,Ub),e(Xa,mt),e(mt,Lb),e(mt,$d),e($d,zb),e(mt,Mb),m(a,y$,g),m(a,Ie,g),e(Ie,_t),e(_t,qd),y(Qa,qd,null),e(Ie,Fb),e(Ie,vd),e(vd,Jb),m(a,E$,g),m(a,hi,g),e(hi,Kb),m(a,w$,g),y($t,a,g),m(a,b$,g),m(a,Za,g),e(Za,Wb),e(Za,en),e(en,Yb),m(a,T$,g),m(a,gi,g),e(gi,Vb),m(a,j$,g),y(qt,a,g),m(a,k$,g),m(a,mi,g),e(mi,Xb),m(a,A$,g),m(a,vt,g),e(vt,yd),e(yd,tn),e(tn,_i),e(_i,Qb),e(tn,Zb),e(tn,Ed),e(vt,e0),e(vt,Z),e(Z,sn),e(sn,an),e(an,wd),e(wd,t0),e(an,s0),e(sn,a0),e(sn,$i),e($i,n0),e(Z,r0),e(Z,nn),e(nn,qi),e(qi,bd),e(bd,o0),e(nn,l0),e(nn,vi),e(vi,i0),e(Z,u0),e(Z,rn),e(rn,yi),e(yi,f0),e(rn,c0),e(rn,yt),e(yt,p0),e(yt,Td),e(Td,d0),e(yt,h0),e(Z,g0),e(Z,on),e(on,Ei),e(Ei,m0),e(on,_0),e(on,Et),e(Et,$0),e(Et,jd),e(jd,q0),e(Et,v0),e(Z,y0),e(Z,ln),e(ln,wi),e(wi,E0),e(ln,w0),e(ln,wt),e(wt,b0),e(wt,kd),e(kd,T0),e(wt,j0),m(a,D$,g),m(a,bi,g),e(bi,k0),m(a,O$,g),m(a,bt,g),e(bt,Ad),e(Ad,un),e(un,Ti),e(Ti,A0),e(un,D0),e(un,Dd),e(bt,O0),e(bt,Od),e(Od,fn),e(fn,ji),e(ji,Pd),e(Pd,P0),e(fn,R0),e(fn,ki),e(ki,N0),m(a,P$,g),m(a,He,g),e(He,Tt),e(Tt,Rd),y(cn,Rd,null),e(He,x0),e(He,Nd),e(Nd,S0),m(a,R$,g),m(a,jt,g),e(jt,I0),e(jt,Ai),e(Ai,H0),e(jt,B0),m(a,N$,g),y(kt,a,g),m(a,x$,g),m(a,pn,g),e(pn,C0),e(pn,dn),e(dn,G0),m(a,S$,g),m(a,Di,g),e(Di,U0),m(a,I$,g),y(At,a,g),m(a,H$,g),m(a,Oi,g),e(Oi,L0),m(a,B$,g),m(a,Dt,g),e(Dt,xd),e(xd,hn),e(hn,Pi),e(Pi,z0),e(hn,M0),e(hn,Sd),e(Dt,F0),e(Dt,G),e(G,gn),e(gn,mn),e(mn,Id),e(Id,J0),e(mn,K0),e(gn,W0),e(gn,Ri),e(Ri,Y0),e(G,V0),e(G,_n),e(_n,Ni),e(Ni,Hd),e(Hd,X0),e(_n,Q0),e(_n,xi),e(xi,Z0),e(G,eT),e(G,$n),e($n,Si),e(Si,tT),e($n,sT),e($n,he),e(he,aT),e(he,Bd),e(Bd,nT),e(he,rT),e(he,Cd),e(Cd,oT),e(he,lT),e(G,iT),e(G,qn),e(qn,Ii),e(Ii,uT),e(qn,fT),e(qn,ge),e(ge,cT),e(ge,Gd),e(Gd,pT),e(ge,dT),e(ge,Ud),e(Ud,hT),e(ge,gT),e(G,mT),e(G,vn),e(vn,Hi),e(Hi,_T),e(vn,$T),e(vn,me),e(me,qT),e(me,Ld),e(Ld,vT),e(me,yT),e(me,zd),e(zd,ET),e(me,wT),e(G,bT),e(G,yn),e(yn,Bi),e(Bi,TT),e(yn,jT),e(yn,re),e(re,kT),e(re,Md),e(Md,AT),e(re,DT),e(re,Fd),e(Fd,OT),e(re,PT),e(re,Jd),e(Jd,RT),e(re,NT),e(G,xT),e(G,En),e(En,Ci),e(Ci,ST),e(En,IT),e(En,_e),e(_e,HT),e(_e,Kd),e(Kd,BT),e(_e,CT),e(_e,Wd),e(Wd,GT),e(_e,UT),e(G,LT),e(G,wn),e(wn,Gi),e(Gi,zT),e(wn,MT),e(wn,Ot),e(Ot,FT),e(Ot,Yd),e(Yd,JT),e(Ot,KT),e(G,WT),e(G,bn),e(bn,Ui),e(Ui,YT),e(bn,VT),e(bn,Pt),e(Pt,XT),e(Pt,Vd),e(Vd,QT),e(Pt,ZT),e(G,e3),e(G,Tn),e(Tn,Li),e(Li,Xd),e(Xd,t3),e(Tn,s3),e(Tn,zi),e(zi,a3),e(G,n3),e(G,jn),e(jn,Mi),e(Mi,r3),e(jn,o3),e(jn,Rt),e(Rt,l3),e(Rt,Qd),e(Qd,i3),e(Rt,u3),e(G,f3),e(G,kn),e(kn,Fi),e(Fi,c3),e(kn,p3),e(kn,Nt),e(Nt,d3),e(Nt,Zd),e(Zd,h3),e(Nt,g3),e(G,m3),e(G,An),e(An,Ji),e(Ji,_3),e(An,$3),e(An,xt),e(xt,q3),e(xt,eh),e(eh,v3),e(xt,y3),m(a,C$,g),m(a,Ki,g),e(Ki,E3),m(a,G$,g),m(a,St,g),e(St,th),e(th,Dn),e(Dn,Wi),e(Wi,w3),e(Dn,b3),e(Dn,sh),e(St,T3),e(St,ah),e(ah,On),e(On,Yi),e(Yi,nh),e(nh,j3),e(On,k3),e(On,Vi),e(Vi,A3),m(a,U$,g),m(a,Be,g),e(Be,It),e(It,rh),y(Pn,rh,null),e(Be,D3),e(Be,oh),e(oh,O3),m(a,L$,g),m(a,Xi,g),e(Xi,P3),m(a,z$,g),y(Ht,a,g),m(a,M$,g),m(a,Rn,g),e(Rn,R3),e(Rn,Nn),e(Nn,N3),m(a,F$,g),m(a,Qi,g),e(Qi,x3),m(a,J$,g),y(Bt,a,g),m(a,K$,g),m(a,Zi,g),e(Zi,S3),m(a,W$,g),m(a,Ct,g),e(Ct,lh),e(lh,xn),e(xn,eu),e(eu,I3),e(xn,H3),e(xn,ih),e(Ct,B3),e(Ct,x),e(x,Sn),e(Sn,In),e(In,uh),e(uh,C3),e(In,G3),e(Sn,U3),e(Sn,fh),e(x,L3),e(x,Hn),e(Hn,tu),e(tu,z3),e(Hn,M3),e(Hn,su),e(su,F3),e(x,J3),e(x,Bn),e(Bn,au),e(au,K3),e(Bn,W3),e(Bn,nu),e(nu,Y3),e(x,V3),e(x,Cn),e(Cn,ru),e(ru,X3),e(Cn,Q3),e(Cn,Gt),e(Gt,Z3),e(Gt,ch),e(ch,ej),e(Gt,tj),e(x,sj),e(x,Gn),e(Gn,ou),e(ou,ph),e(ph,aj),e(Gn,nj),e(Gn,lu),e(lu,rj),e(x,oj),e(x,Un),e(Un,iu),e(iu,lj),e(Un,ij),e(Un,$e),e($e,uj),e($e,dh),e(dh,fj),e($e,cj),e($e,hh),e(hh,pj),e($e,dj),e(x,hj),e(x,Ln),e(Ln,uu),e(uu,gj),e(Ln,mj),e(Ln,qe),e(qe,_j),e(qe,gh),e(gh,$j),e(qe,qj),e(qe,mh),e(mh,vj),e(qe,yj),e(x,Ej),e(x,zn),e(zn,fu),e(fu,wj),e(zn,bj),e(zn,ve),e(ve,Tj),e(ve,_h),e(_h,jj),e(ve,kj),e(ve,$h),e($h,Aj),e(ve,Dj),e(x,Oj),e(x,Mn),e(Mn,cu),e(cu,Pj),e(Mn,Rj),e(Mn,oe),e(oe,Nj),e(oe,qh),e(qh,xj),e(oe,Sj),e(oe,vh),e(vh,Ij),e(oe,Hj),e(oe,yh),e(yh,Bj),e(oe,Cj),e(x,Gj),e(x,Fn),e(Fn,pu),e(pu,Uj),e(Fn,Lj),e(Fn,ye),e(ye,zj),e(ye,Eh),e(Eh,Mj),e(ye,Fj),e(ye,wh),e(wh,Jj),e(ye,Kj),e(x,Wj),e(x,Jn),e(Jn,du),e(du,Yj),e(Jn,Vj),e(Jn,Ut),e(Ut,Xj),e(Ut,bh),e(bh,Qj),e(Ut,Zj),e(x,e5),e(x,Kn),e(Kn,hu),e(hu,t5),e(Kn,s5),e(Kn,Lt),e(Lt,a5),e(Lt,Th),e(Th,n5),e(Lt,r5),e(x,o5),e(x,Wn),e(Wn,gu),e(gu,jh),e(jh,l5),e(Wn,i5),e(Wn,mu),e(mu,u5),e(x,f5),e(x,Yn),e(Yn,_u),e(_u,c5),e(Yn,p5),e(Yn,zt),e(zt,d5),e(zt,kh),e(kh,h5),e(zt,g5),e(x,m5),e(x,Vn),e(Vn,$u),e($u,_5),e(Vn,$5),e(Vn,Mt),e(Mt,q5),e(Mt,Ah),e(Ah,v5),e(Mt,y5),e(x,E5),e(x,Xn),e(Xn,qu),e(qu,w5),e(Xn,b5),e(Xn,Ft),e(Ft,T5),e(Ft,Dh),e(Dh,j5),e(Ft,k5),m(a,Y$,g),m(a,vu,g),e(vu,A5),m(a,V$,g),m(a,Jt,g),e(Jt,Oh),e(Oh,Qn),e(Qn,yu),e(yu,D5),e(Qn,O5),e(Qn,Ph),e(Jt,P5),e(Jt,ie),e(ie,Zn),e(Zn,Eu),e(Eu,Rh),e(Rh,R5),e(Zn,N5),e(Zn,wu),e(wu,x5),e(ie,S5),e(ie,er),e(er,bu),e(bu,Nh),e(Nh,I5),e(er,H5),e(er,Tu),e(Tu,B5),e(ie,C5),e(ie,tr),e(tr,ju),e(ju,G5),e(tr,U5),e(tr,ku),e(ku,L5),e(ie,z5),e(ie,sr),e(sr,Au),e(Au,M5),e(sr,F5),e(sr,Du),e(Du,J5),m(a,X$,g),m(a,Ce,g),e(Ce,Kt),e(Kt,xh),y(ar,xh,null),e(Ce,K5),e(Ce,Sh),e(Sh,W5),m(a,Q$,g),m(a,Ou,g),e(Ou,Y5),m(a,Z$,g),y(Wt,a,g),m(a,eq,g),m(a,nr,g),e(nr,V5),e(nr,rr),e(rr,X5),m(a,tq,g),m(a,Pu,g),e(Pu,Q5),m(a,sq,g),y(Yt,a,g),m(a,aq,g),m(a,Ru,g),e(Ru,Z5),m(a,nq,g),m(a,Vt,g),e(Vt,Ih),e(Ih,or),e(or,Nu),e(Nu,e4),e(or,t4),e(or,Hh),e(Vt,s4),e(Vt,J),e(J,lr),e(lr,ir),e(ir,Bh),e(Bh,a4),e(ir,n4),e(lr,r4),e(lr,Ch),e(J,o4),e(J,ur),e(ur,xu),e(xu,l4),e(ur,i4),e(ur,Su),e(Su,u4),e(J,f4),e(J,fr),e(fr,Iu),e(Iu,c4),e(fr,p4),e(fr,Hu),e(Hu,d4),e(J,h4),e(J,cr),e(cr,Bu),e(Bu,Gh),e(Gh,g4),e(cr,m4),e(cr,Cu),e(Cu,_4),e(J,$4),e(J,pr),e(pr,Gu),e(Gu,q4),e(pr,v4),e(pr,Xt),e(Xt,y4),e(Xt,Uh),e(Uh,E4),e(Xt,w4),e(J,b4),e(J,dr),e(dr,Uu),e(Uu,T4),e(dr,j4),e(dr,Qt),e(Qt,k4),e(Qt,Lh),e(Lh,A4),e(Qt,D4),e(J,O4),e(J,hr),e(hr,Lu),e(Lu,P4),e(hr,R4),e(hr,Zt),e(Zt,N4),e(Zt,zh),e(zh,x4),e(Zt,S4),m(a,rq,g),m(a,zu,g),e(zu,I4),m(a,oq,g),y(es,a,g),m(a,lq,g),m(a,ts,g),e(ts,Mh),e(Mh,gr),e(gr,Mu),e(Mu,H4),e(gr,B4),e(gr,Fh),e(ts,C4),e(ts,ue),e(ue,mr),e(mr,Fu),e(Fu,Jh),e(Jh,G4),e(mr,U4),e(mr,Ju),e(Ju,L4),e(ue,z4),e(ue,_r),e(_r,Ku),e(Ku,Kh),e(Kh,M4),e(_r,F4),e(_r,Wu),e(Wu,J4),e(ue,K4),e(ue,$r),e($r,Yu),e(Yu,Wh),e(Wh,W4),e($r,Y4),e($r,Vu),e(Vu,V4),e(ue,X4),e(ue,qr),e(qr,Xu),e(Xu,Yh),e(Yh,Q4),e(qr,Z4),e(qr,Qu),e(Qu,ek),m(a,iq,g),m(a,Ge,g),e(Ge,ss),e(ss,Vh),y(vr,Vh,null),e(Ge,tk),e(Ge,Xh),e(Xh,sk),m(a,uq,g),m(a,Zu,g),e(Zu,ak),m(a,fq,g),y(as,a,g),m(a,cq,g),m(a,Ue,g),e(Ue,nk),e(Ue,yr),e(yr,rk),e(Ue,ok),e(Ue,Er),e(Er,lk),m(a,pq,g),m(a,ef,g),e(ef,ik),m(a,dq,g),y(ns,a,g),m(a,hq,g),m(a,tf,g),e(tf,uk),m(a,gq,g),m(a,sf,g),e(sf,fk),m(a,mq,g),y(rs,a,g),m(a,_q,g),m(a,os,g),e(os,Qh),e(Qh,wr),e(wr,af),e(af,ck),e(wr,pk),e(wr,Zh),e(os,dk),e(os,fe),e(fe,br),e(br,nf),e(nf,eg),e(eg,hk),e(br,gk),e(br,rf),e(rf,mk),e(fe,_k),e(fe,Tr),e(Tr,of),e(of,tg),e(tg,$k),e(Tr,qk),e(Tr,lf),e(lf,vk),e(fe,yk),e(fe,jr),e(jr,uf),e(uf,sg),e(sg,Ek),e(jr,wk),e(jr,ls),e(ls,bk),e(ls,ag),e(ag,Tk),e(ls,jk),e(fe,kk),e(fe,kr),e(kr,ff),e(ff,ng),e(ng,Ak),e(kr,Dk),e(kr,is),e(is,Ok),e(is,rg),e(rg,Pk),e(is,Rk),m(a,$q,g),m(a,Le,g),e(Le,us),e(us,og),y(Ar,og,null),e(Le,Nk),e(Le,lg),e(lg,xk),m(a,qq,g),m(a,cf,g),e(cf,Sk),m(a,vq,g),y(fs,a,g),m(a,yq,g),m(a,Dr,g),e(Dr,Ik),e(Dr,Or),e(Or,Hk),m(a,Eq,g),m(a,pf,g),e(pf,Bk),m(a,wq,g),y(cs,a,g),m(a,bq,g),m(a,df,g),e(df,Ck),m(a,Tq,g),m(a,ps,g),e(ps,ig),e(ig,Pr),e(Pr,hf),e(hf,Gk),e(Pr,Uk),e(Pr,ug),e(ps,Lk),e(ps,ee),e(ee,Rr),e(Rr,Nr),e(Nr,fg),e(fg,zk),e(Nr,Mk),e(Rr,Fk),e(Rr,gf),e(gf,Jk),e(ee,Kk),e(ee,xr),e(xr,mf),e(mf,cg),e(cg,Wk),e(xr,Yk),e(xr,_f),e(_f,Vk),e(ee,Xk),e(ee,Sr),e(Sr,$f),e($f,Qk),e(Sr,Zk),e(Sr,ds),e(ds,e7),e(ds,pg),e(pg,t7),e(ds,s7),e(ee,a7),e(ee,Ir),e(Ir,qf),e(qf,n7),e(Ir,r7),e(Ir,hs),e(hs,o7),e(hs,dg),e(dg,l7),e(hs,i7),e(ee,u7),e(ee,Hr),e(Hr,vf),e(vf,f7),e(Hr,c7),e(Hr,gs),e(gs,p7),e(gs,hg),e(hg,d7),e(gs,h7),m(a,jq,g),m(a,yf,g),e(yf,g7),m(a,kq,g),y(ms,a,g),m(a,Aq,g),m(a,_s,g),e(_s,gg),e(gg,Br),e(Br,Ef),e(Ef,m7),e(Br,_7),e(Br,mg),e(_s,$7),e(_s,Cr),e(Cr,Gr),e(Gr,wf),e(wf,_g),e(_g,q7),e(Gr,v7),e(Gr,bf),e(bf,y7),e(Cr,E7),e(Cr,Ur),e(Ur,Tf),e(Tf,$g),e($g,w7),e(Ur,b7),e(Ur,jf),e(jf,T7),m(a,Dq,g),m(a,ze,g),e(ze,$s),e($s,qg),y(Lr,qg,null),e(ze,j7),e(ze,vg),e(vg,k7),m(a,Oq,g),m(a,zr,g),e(zr,A7),e(zr,kf),e(kf,D7),m(a,Pq,g),m(a,Me,g),e(Me,qs),e(qs,yg),y(Mr,yg,null),e(Me,O7),e(Me,Eg),e(Eg,P7),m(a,Rq,g),m(a,Af,g),e(Af,R7),m(a,Nq,g),y(vs,a,g),m(a,xq,g),m(a,Fe,g),e(Fe,N7),e(Fe,Fr),e(Fr,x7),e(Fe,S7),e(Fe,Jr),e(Jr,I7),m(a,Sq,g),m(a,Df,g),e(Df,H7),m(a,Iq,g),y(ys,a,g),m(a,Hq,g),m(a,Of,g),e(Of,B7),m(a,Bq,g),m(a,Es,g),e(Es,wg),e(wg,Kr),e(Kr,Pf),e(Pf,C7),e(Kr,G7),e(Kr,bg),e(Es,U7),e(Es,K),e(K,Wr),e(Wr,Yr),e(Yr,Tg),e(Tg,L7),e(Yr,z7),e(Wr,M7),e(Wr,Rf),e(Rf,F7),e(K,J7),e(K,Vr),e(Vr,Nf),e(Nf,jg),e(jg,K7),e(Vr,W7),e(Vr,xf),e(xf,Y7),e(K,V7),e(K,Xr),e(Xr,Sf),e(Sf,X7),e(Xr,Q7),e(Xr,S),e(S,Z7),e(S,kg),e(kg,e6),e(S,t6),e(S,s6),e(S,a6),e(S,Ag),e(Ag,n6),e(S,r6),e(S,o6),e(S,l6),e(S,Dg),e(Dg,i6),e(S,u6),e(S,f6),e(S,c6),e(S,Og),e(Og,p6),e(S,d6),e(S,Pg),e(Pg,h6),e(S,g6),e(S,m6),e(S,_6),e(S,Rg),e(Rg,$6),e(S,q6),e(S,Ng),e(Ng,v6),e(S,y6),e(S,E6),e(S,w6),e(S,xg),e(xg,b6),e(S,T6),e(S,Sg),e(Sg,j6),e(S,k6),e(K,A6),e(K,Qr),e(Qr,If),e(If,Ig),e(Ig,D6),e(Qr,O6),e(Qr,Hf),e(Hf,P6),e(K,R6),e(K,Zr),e(Zr,Bf),e(Bf,N6),e(Zr,x6),e(Zr,ws),e(ws,S6),e(ws,Hg),e(Hg,I6),e(ws,H6),e(K,B6),e(K,eo),e(eo,Cf),e(Cf,C6),e(eo,G6),e(eo,bs),e(bs,U6),e(bs,Bg),e(Bg,L6),e(bs,z6),e(K,M6),e(K,to),e(to,Gf),e(Gf,F6),e(to,J6),e(to,Ts),e(Ts,K6),e(Ts,Cg),e(Cg,W6),e(Ts,Y6),m(a,Cq,g),m(a,Uf,g),e(Uf,V6),m(a,Gq,g),y(js,a,g),m(a,Uq,g),m(a,ks,g),e(ks,Gg),e(Gg,so),e(so,Lf),e(Lf,X6),e(so,Q6),e(so,Ug),e(ks,Z6),e(ks,te),e(te,ao),e(ao,zf),e(zf,Lg),e(Lg,e9),e(ao,t9),e(ao,Mf),e(Mf,s9),e(te,a9),e(te,no),e(no,Ff),e(Ff,zg),e(zg,n9),e(no,r9),e(no,Jf),e(Jf,o9),e(te,l9),e(te,ro),e(ro,Kf),e(Kf,Mg),e(Mg,i9),e(ro,u9),e(ro,Wf),e(Wf,f9),e(te,c9),e(te,oo),e(oo,Yf),e(Yf,Fg),e(Fg,p9),e(oo,d9),e(oo,As),e(As,h9),e(As,Jg),e(Jg,g9),e(As,m9),e(te,_9),e(te,lo),e(lo,Vf),e(Vf,Kg),e(Kg,$9),e(lo,q9),e(lo,Ds),e(Ds,v9),e(Ds,Wg),e(Wg,y9),e(Ds,E9),m(a,Lq,g),m(a,Je,g),e(Je,Os),e(Os,Yg),y(io,Yg,null),e(Je,w9),e(Je,Vg),e(Vg,b9),m(a,zq,g),m(a,Xf,g),e(Xf,T9),m(a,Mq,g),y(Ps,a,g),m(a,Fq,g),m(a,uo,g),e(uo,j9),e(uo,fo),e(fo,k9),m(a,Jq,g),m(a,Qf,g),e(Qf,A9),m(a,Kq,g),y(Rs,a,g),m(a,Wq,g),m(a,Zf,g),e(Zf,D9),m(a,Yq,g),m(a,Ns,g),e(Ns,Xg),e(Xg,co),e(co,ec),e(ec,O9),e(co,P9),e(co,Qg),e(Ns,R9),e(Ns,I),e(I,po),e(po,ho),e(ho,Zg),e(Zg,N9),e(ho,x9),e(po,S9),e(po,tc),e(tc,I9),e(I,H9),e(I,go),e(go,sc),e(sc,em),e(em,B9),e(go,C9),e(go,ac),e(ac,G9),e(I,U9),e(I,mo),e(mo,nc),e(nc,L9),e(mo,z9),e(mo,Ee),e(Ee,M9),e(Ee,tm),e(tm,F9),e(Ee,J9),e(Ee,sm),e(sm,K9),e(Ee,W9),e(I,Y9),e(I,_o),e(_o,rc),e(rc,V9),e(_o,X9),e(_o,le),e(le,Q9),e(le,am),e(am,Z9),e(le,e8),e(le,nm),e(nm,t8),e(le,s8),e(le,rm),e(rm,a8),e(le,n8),e(I,r8),e(I,$o),e($o,oc),e(oc,o8),e($o,l8),e($o,we),e(we,i8),e(we,om),e(om,u8),e(we,f8),e(we,lm),e(lm,c8),e(we,p8),e(I,d8),e(I,qo),e(qo,lc),e(lc,h8),e(qo,g8),e(qo,xs),e(xs,m8),e(xs,im),e(im,_8),e(xs,$8),e(I,q8),e(I,vo),e(vo,ic),e(ic,v8),e(vo,y8),e(vo,be),e(be,E8),e(be,um),e(um,w8),e(be,b8),e(be,fm),e(fm,T8),e(be,j8),e(I,k8),e(I,yo),e(yo,uc),e(uc,A8),e(yo,D8),e(yo,Te),e(Te,O8),e(Te,cm),e(cm,P8),e(Te,R8),e(Te,pm),e(pm,N8),e(Te,x8),e(I,S8),e(I,Eo),e(Eo,fc),e(fc,I8),e(Eo,H8),e(Eo,je),e(je,B8),e(je,dm),e(dm,C8),e(je,G8),e(je,hm),e(hm,U8),e(je,L8),e(I,z8),e(I,wo),e(wo,cc),e(cc,M8),e(wo,F8),e(wo,Ss),e(Ss,J8),e(Ss,gm),e(gm,K8),e(Ss,W8),e(I,Y8),e(I,bo),e(bo,pc),e(pc,V8),e(bo,X8),e(bo,Is),e(Is,Q8),e(Is,mm),e(mm,Z8),e(Is,eA),e(I,tA),e(I,To),e(To,dc),e(dc,_m),e(_m,sA),e(To,aA),e(To,hc),e(hc,nA),e(I,rA),e(I,jo),e(jo,gc),e(gc,oA),e(jo,lA),e(jo,Hs),e(Hs,iA),e(Hs,$m),e($m,uA),e(Hs,fA),e(I,cA),e(I,ko),e(ko,mc),e(mc,pA),e(ko,dA),e(ko,Bs),e(Bs,hA),e(Bs,qm),e(qm,gA),e(Bs,mA),e(I,_A),e(I,Ao),e(Ao,_c),e(_c,$A),e(Ao,qA),e(Ao,Cs),e(Cs,vA),e(Cs,vm),e(vm,yA),e(Cs,EA),m(a,Vq,g),m(a,$c,g),e($c,wA),m(a,Xq,g),y(Gs,a,g),m(a,Qq,g),m(a,Us,g),e(Us,ym),e(ym,Do),e(Do,qc),e(qc,bA),e(Do,TA),e(Do,Em),e(Us,jA),e(Us,wm),e(wm,Oo),e(Oo,vc),e(vc,bm),e(bm,kA),e(Oo,AA),e(Oo,yc),e(yc,DA),m(a,Zq,g),m(a,Ke,g),e(Ke,Ls),e(Ls,Tm),y(Po,Tm,null),e(Ke,OA),e(Ke,jm),e(jm,PA),m(a,e1,g),m(a,zs,g),e(zs,RA),e(zs,Ec),e(Ec,NA),e(zs,xA),m(a,t1,g),m(a,We,g),e(We,Ms),e(Ms,km),y(Ro,km,null),e(We,SA),e(We,Am),e(Am,IA),m(a,s1,g),m(a,wc,g),e(wc,HA),m(a,a1,g),y(Fs,a,g),m(a,n1,g),m(a,No,g),e(No,BA),e(No,xo),e(xo,CA),m(a,r1,g),m(a,bc,g),e(bc,GA),m(a,o1,g),y(Js,a,g),m(a,l1,g),m(a,Tc,g),e(Tc,UA),m(a,i1,g),m(a,Ks,g),e(Ks,Dm),e(Dm,So),e(So,jc),e(jc,LA),e(So,zA),e(So,Om),e(Ks,MA),e(Ks,se),e(se,Io),e(Io,Ho),e(Ho,Pm),e(Pm,FA),e(Ho,JA),e(Io,KA),e(Io,kc),e(kc,WA),e(se,YA),e(se,Bo),e(Bo,Ac),e(Ac,Rm),e(Rm,VA),e(Bo,XA),e(Bo,Dc),e(Dc,QA),e(se,ZA),e(se,Co),e(Co,Oc),e(Oc,eD),e(Co,tD),e(Co,Ws),e(Ws,sD),e(Ws,Nm),e(Nm,aD),e(Ws,nD),e(se,rD),e(se,Go),e(Go,Pc),e(Pc,oD),e(Go,lD),e(Go,Ys),e(Ys,iD),e(Ys,xm),e(xm,uD),e(Ys,fD),e(se,cD),e(se,Uo),e(Uo,Rc),e(Rc,pD),e(Uo,dD),e(Uo,Vs),e(Vs,hD),e(Vs,Sm),e(Sm,gD),e(Vs,mD),m(a,u1,g),m(a,Nc,g),e(Nc,_D),m(a,f1,g),y(Xs,a,g),m(a,c1,g),m(a,Qs,g),e(Qs,Im),e(Im,Lo),e(Lo,xc),e(xc,$D),e(Lo,qD),e(Lo,Hm),e(Qs,vD),e(Qs,ce),e(ce,zo),e(zo,Sc),e(Sc,Bm),e(Bm,yD),e(zo,ED),e(zo,Ic),e(Ic,wD),e(ce,bD),e(ce,Mo),e(Mo,Hc),e(Hc,Cm),e(Cm,TD),e(Mo,jD),e(Mo,Bc),e(Bc,kD),e(ce,AD),e(ce,Fo),e(Fo,Cc),e(Cc,Gm),e(Gm,DD),e(Fo,OD),e(Fo,Gc),e(Gc,PD),e(ce,RD),e(ce,Jo),e(Jo,Uc),e(Uc,Um),e(Um,ND),e(Jo,xD),e(Jo,Lc),e(Lc,SD),m(a,p1,g),m(a,Ye,g),e(Ye,Zs),e(Zs,Lm),y(Ko,Lm,null),e(Ye,ID),e(Ye,zm),e(zm,HD),m(a,d1,g),m(a,zc,g),e(zc,BD),m(a,h1,g),y(ea,a,g),m(a,g1,g),y(ta,a,g),m(a,m1,g),m(a,pe,g),e(pe,CD),e(pe,Wo),e(Wo,GD),e(pe,UD),e(pe,Yo),e(Yo,LD),e(pe,zD),e(pe,Vo),e(Vo,MD),m(a,_1,g),m(a,Mc,g),e(Mc,FD),m(a,$1,g),y(sa,a,g),m(a,q1,g),m(a,Fc,g),e(Fc,JD),m(a,v1,g),m(a,aa,g),e(aa,Mm),e(Mm,Xo),e(Xo,Jc),e(Jc,KD),e(Xo,WD),e(Xo,Fm),e(aa,YD),e(aa,Jm),e(Jm,Qo),e(Qo,Zo),e(Zo,Km),e(Km,VD),e(Zo,XD),e(Qo,QD),e(Qo,Kc),e(Kc,ZD),m(a,y1,g),m(a,Wc,g),e(Wc,eO),m(a,E1,g),m(a,Yc,g),e(Yc,tO),m(a,w1,g),y(na,a,g),m(a,b1,g),m(a,ra,g),e(ra,Wm),e(Wm,el),e(el,Vc),e(Vc,sO),e(el,aO),e(el,Ym),e(ra,nO),e(ra,Vm),e(Vm,tl),e(tl,Xc),e(Xc,Xm),e(Xm,rO),e(tl,oO),e(tl,Qc),e(Qc,lO),m(a,T1,g),m(a,Ve,g),e(Ve,oa),e(oa,Qm),y(sl,Qm,null),e(Ve,iO),e(Ve,Zm),e(Zm,uO),m(a,j1,g),m(a,Zc,g),e(Zc,fO),m(a,k1,g),y(la,a,g),m(a,A1,g),m(a,Xe,g),e(Xe,cO),e(Xe,al),e(al,pO),e(Xe,dO),e(Xe,nl),e(nl,hO),m(a,D1,g),m(a,ep,g),e(ep,gO),m(a,O1,g),m(a,ia,g),e(ia,e_),e(e_,rl),e(rl,tp),e(tp,mO),e(rl,_O),e(rl,t_),e(ia,$O),e(ia,ae),e(ae,ol),e(ol,ll),e(ll,s_),e(s_,qO),e(ll,vO),e(ol,yO),e(ol,sp),e(sp,EO),e(ae,wO),e(ae,il),e(il,ap),e(ap,a_),e(a_,bO),e(il,TO),e(il,np),e(np,jO),e(ae,kO),e(ae,ul),e(ul,rp),e(rp,AO),e(ul,DO),e(ul,ua),e(ua,OO),e(ua,n_),e(n_,PO),e(ua,RO),e(ae,NO),e(ae,fl),e(fl,op),e(op,xO),e(fl,SO),e(fl,fa),e(fa,IO),e(fa,r_),e(r_,HO),e(fa,BO),e(ae,CO),e(ae,cl),e(cl,lp),e(lp,GO),e(cl,UO),e(cl,ca),e(ca,LO),e(ca,o_),e(o_,zO),e(ca,MO),m(a,P1,g),m(a,ip,g),e(ip,FO),m(a,R1,g),m(a,pa,g),e(pa,l_),e(l_,pl),e(pl,up),e(up,JO),e(pl,KO),e(pl,i_),e(pa,WO),e(pa,u_),e(u_,dl),e(dl,fp),e(fp,f_),e(f_,YO),e(dl,VO),e(dl,cp),e(cp,XO),m(a,N1,g),m(a,pp,g),e(pp,QO),m(a,x1,g),m(a,Qe,g),e(Qe,da),e(da,c_),y(hl,c_,null),e(Qe,ZO),e(Qe,p_),e(p_,eP),m(a,S1,g),m(a,dp,g),e(dp,tP),m(a,I1,g),y(ha,a,g),m(a,H1,g),m(a,Ze,g),e(Ze,sP),e(Ze,gl),e(gl,aP),e(Ze,nP),e(Ze,ml),e(ml,rP),m(a,B1,g),m(a,hp,g),e(hp,oP),m(a,C1,g),y(ga,a,g),m(a,G1,g),m(a,gp,g),e(gp,lP),m(a,U1,g),m(a,ma,g),e(ma,d_),e(d_,_l),e(_l,mp),e(mp,iP),e(_l,uP),e(_l,h_),e(ma,fP),e(ma,g_),e(g_,$l),e($l,ql),e(ql,m_),e(m_,cP),e(ql,pP),e($l,dP),e($l,_p),e(_p,hP),m(a,L1,g),m(a,$p,g),e($p,gP),m(a,z1,g),y(_a,a,g),m(a,M1,g),m(a,$a,g),e($a,__),e(__,vl),e(vl,qp),e(qp,mP),e(vl,_P),e(vl,$_),e($a,$P),e($a,yl),e(yl,El),e(El,vp),e(vp,q_),e(q_,qP),e(El,vP),e(El,yp),e(yp,yP),e(yl,EP),e(yl,wl),e(wl,Ep),e(Ep,v_),e(v_,wP),e(wl,bP),e(wl,wp),e(wp,TP),m(a,F1,g),m(a,et,g),e(et,qa),e(qa,y_),y(bl,y_,null),e(et,jP),e(et,E_),e(E_,kP),m(a,J1,g),m(a,bp,g),e(bp,AP),m(a,K1,g),y(va,a,g),m(a,W1,g),m(a,Tl,g),e(Tl,DP),e(Tl,jl),e(jl,OP),m(a,Y1,g),m(a,Tp,g),e(Tp,PP),m(a,V1,g),y(ya,a,g),m(a,X1,g),m(a,Ea,g),e(Ea,RP),e(Ea,kl),e(kl,NP),e(Ea,xP),m(a,Q1,g),m(a,wa,g),e(wa,w_),e(w_,Al),e(Al,jp),e(jp,SP),e(Al,IP),e(Al,b_),e(wa,HP),e(wa,T_),e(T_,Dl),e(Dl,Ol),e(Ol,j_),e(j_,BP),e(Ol,CP),e(Dl,GP),e(Dl,kp),e(kp,UP),m(a,Z1,g),m(a,Ap,g),e(Ap,LP),m(a,ev,g),y(ba,a,g),m(a,tv,g),m(a,Ta,g),e(Ta,k_),e(k_,Pl),e(Pl,Dp),e(Dp,zP),e(Pl,MP),e(Pl,A_),e(Ta,FP),e(Ta,tt),e(tt,Rl),e(Rl,Op),e(Op,D_),e(D_,JP),e(Rl,KP),e(Rl,Pp),e(Pp,WP),e(tt,YP),e(tt,Nl),e(Nl,Rp),e(Rp,O_),e(O_,VP),e(Nl,XP),e(Nl,Np),e(Np,QP),e(tt,ZP),e(tt,xl),e(xl,xp),e(xp,P_),e(P_,eR),e(xl,tR),e(xl,Sp),e(Sp,sR),sv=!0},p(a,[g]){const Sl={};g&2&&(Sl.$$scope={dirty:g,ctx:a}),lt.$set(Sl);const R_={};g&2&&(R_.$$scope={dirty:g,ctx:a}),it.$set(R_);const N_={};g&2&&(N_.$$scope={dirty:g,ctx:a}),ht.$set(N_);const x_={};g&2&&(x_.$$scope={dirty:g,ctx:a}),$t.$set(x_);const Il={};g&2&&(Il.$$scope={dirty:g,ctx:a}),qt.$set(Il);const S_={};g&2&&(S_.$$scope={dirty:g,ctx:a}),kt.$set(S_);const I_={};g&2&&(I_.$$scope={dirty:g,ctx:a}),At.$set(I_);const H_={};g&2&&(H_.$$scope={dirty:g,ctx:a}),Ht.$set(H_);const B_={};g&2&&(B_.$$scope={dirty:g,ctx:a}),Bt.$set(B_);const C_={};g&2&&(C_.$$scope={dirty:g,ctx:a}),Wt.$set(C_);const Hl={};g&2&&(Hl.$$scope={dirty:g,ctx:a}),Yt.$set(Hl);const G_={};g&2&&(G_.$$scope={dirty:g,ctx:a}),es.$set(G_);const U_={};g&2&&(U_.$$scope={dirty:g,ctx:a}),as.$set(U_);const L_={};g&2&&(L_.$$scope={dirty:g,ctx:a}),ns.$set(L_);const z_={};g&2&&(z_.$$scope={dirty:g,ctx:a}),rs.$set(z_);const Ip={};g&2&&(Ip.$$scope={dirty:g,ctx:a}),fs.$set(Ip);const M_={};g&2&&(M_.$$scope={dirty:g,ctx:a}),cs.$set(M_);const F_={};g&2&&(F_.$$scope={dirty:g,ctx:a}),ms.$set(F_);const J_={};g&2&&(J_.$$scope={dirty:g,ctx:a}),vs.$set(J_);const Bl={};g&2&&(Bl.$$scope={dirty:g,ctx:a}),ys.$set(Bl);const K_={};g&2&&(K_.$$scope={dirty:g,ctx:a}),js.$set(K_);const Cl={};g&2&&(Cl.$$scope={dirty:g,ctx:a}),Ps.$set(Cl);const W_={};g&2&&(W_.$$scope={dirty:g,ctx:a}),Rs.$set(W_);const M={};g&2&&(M.$$scope={dirty:g,ctx:a}),Gs.$set(M);const Gl={};g&2&&(Gl.$$scope={dirty:g,ctx:a}),Fs.$set(Gl);const Hp={};g&2&&(Hp.$$scope={dirty:g,ctx:a}),Js.$set(Hp);const Y_={};g&2&&(Y_.$$scope={dirty:g,ctx:a}),Xs.$set(Y_);const V_={};g&2&&(V_.$$scope={dirty:g,ctx:a}),ea.$set(V_);const Ul={};g&2&&(Ul.$$scope={dirty:g,ctx:a}),ta.$set(Ul);const Bp={};g&2&&(Bp.$$scope={dirty:g,ctx:a}),sa.$set(Bp);const X_={};g&2&&(X_.$$scope={dirty:g,ctx:a}),na.$set(X_);const Q_={};g&2&&(Q_.$$scope={dirty:g,ctx:a}),la.$set(Q_);const Ll={};g&2&&(Ll.$$scope={dirty:g,ctx:a}),ha.$set(Ll);const Z_={};g&2&&(Z_.$$scope={dirty:g,ctx:a}),ga.$set(Z_);const st={};g&2&&(st.$$scope={dirty:g,ctx:a}),_a.$set(st);const e$={};g&2&&(e$.$$scope={dirty:g,ctx:a}),va.$set(e$);const t$={};g&2&&(t$.$$scope={dirty:g,ctx:a}),ya.$set(t$);const zl={};g&2&&(zl.$$scope={dirty:g,ctx:a}),ba.$set(zl)},i(a){sv||(E(k.$$.fragment,a),E(Q.$$.fragment,a),E(xa.$$.fragment,a),E(lt.$$.fragment,a),E(it.$$.fragment,a),E(ht.$$.fragment,a),E(Qa.$$.fragment,a),E($t.$$.fragment,a),E(qt.$$.fragment,a),E(cn.$$.fragment,a),E(kt.$$.fragment,a),E(At.$$.fragment,a),E(Pn.$$.fragment,a),E(Ht.$$.fragment,a),E(Bt.$$.fragment,a),E(ar.$$.fragment,a),E(Wt.$$.fragment,a),E(Yt.$$.fragment,a),E(es.$$.fragment,a),E(vr.$$.fragment,a),E(as.$$.fragment,a),E(ns.$$.fragment,a),E(rs.$$.fragment,a),E(Ar.$$.fragment,a),E(fs.$$.fragment,a),E(cs.$$.fragment,a),E(ms.$$.fragment,a),E(Lr.$$.fragment,a),E(Mr.$$.fragment,a),E(vs.$$.fragment,a),E(ys.$$.fragment,a),E(js.$$.fragment,a),E(io.$$.fragment,a),E(Ps.$$.fragment,a),E(Rs.$$.fragment,a),E(Gs.$$.fragment,a),E(Po.$$.fragment,a),E(Ro.$$.fragment,a),E(Fs.$$.fragment,a),E(Js.$$.fragment,a),E(Xs.$$.fragment,a),E(Ko.$$.fragment,a),E(ea.$$.fragment,a),E(ta.$$.fragment,a),E(sa.$$.fragment,a),E(na.$$.fragment,a),E(sl.$$.fragment,a),E(la.$$.fragment,a),E(hl.$$.fragment,a),E(ha.$$.fragment,a),E(ga.$$.fragment,a),E(_a.$$.fragment,a),E(bl.$$.fragment,a),E(va.$$.fragment,a),E(ya.$$.fragment,a),E(ba.$$.fragment,a),sv=!0)},o(a){w(k.$$.fragment,a),w(Q.$$.fragment,a),w(xa.$$.fragment,a),w(lt.$$.fragment,a),w(it.$$.fragment,a),w(ht.$$.fragment,a),w(Qa.$$.fragment,a),w($t.$$.fragment,a),w(qt.$$.fragment,a),w(cn.$$.fragment,a),w(kt.$$.fragment,a),w(At.$$.fragment,a),w(Pn.$$.fragment,a),w(Ht.$$.fragment,a),w(Bt.$$.fragment,a),w(ar.$$.fragment,a),w(Wt.$$.fragment,a),w(Yt.$$.fragment,a),w(es.$$.fragment,a),w(vr.$$.fragment,a),w(as.$$.fragment,a),w(ns.$$.fragment,a),w(rs.$$.fragment,a),w(Ar.$$.fragment,a),w(fs.$$.fragment,a),w(cs.$$.fragment,a),w(ms.$$.fragment,a),w(Lr.$$.fragment,a),w(Mr.$$.fragment,a),w(vs.$$.fragment,a),w(ys.$$.fragment,a),w(js.$$.fragment,a),w(io.$$.fragment,a),w(Ps.$$.fragment,a),w(Rs.$$.fragment,a),w(Gs.$$.fragment,a),w(Po.$$.fragment,a),w(Ro.$$.fragment,a),w(Fs.$$.fragment,a),w(Js.$$.fragment,a),w(Xs.$$.fragment,a),w(Ko.$$.fragment,a),w(ea.$$.fragment,a),w(ta.$$.fragment,a),w(sa.$$.fragment,a),w(na.$$.fragment,a),w(sl.$$.fragment,a),w(la.$$.fragment,a),w(hl.$$.fragment,a),w(ha.$$.fragment,a),w(ga.$$.fragment,a),w(_a.$$.fragment,a),w(bl.$$.fragment,a),w(va.$$.fragment,a),w(ya.$$.fragment,a),w(ba.$$.fragment,a),sv=!1},d(a){t(n),a&&t(f),a&&t(s),b(k),a&&t(O),a&&t(D),b(Q),a&&t(Na),a&&t(Ne),a&&t(o$),a&&t(Jl),a&&t(l$),a&&t(nt),a&&t(i$),a&&t(rt),a&&t(u$),a&&t(xe),b(xa),a&&t(f$),a&&t(Kl),a&&t(c$),b(lt,a),a&&t(p$),a&&t(Sa),a&&t(d$),a&&t(Wl),a&&t(h$),b(it,a),a&&t(g$),a&&t(Yl),a&&t(m$),a&&t(ut),a&&t(_$),a&&t(oi),a&&t($$),a&&t(li),a&&t(q$),b(ht,a),a&&t(v$),a&&t(gt),a&&t(y$),a&&t(Ie),b(Qa),a&&t(E$),a&&t(hi),a&&t(w$),b($t,a),a&&t(b$),a&&t(Za),a&&t(T$),a&&t(gi),a&&t(j$),b(qt,a),a&&t(k$),a&&t(mi),a&&t(A$),a&&t(vt),a&&t(D$),a&&t(bi),a&&t(O$),a&&t(bt),a&&t(P$),a&&t(He),b(cn),a&&t(R$),a&&t(jt),a&&t(N$),b(kt,a),a&&t(x$),a&&t(pn),a&&t(S$),a&&t(Di),a&&t(I$),b(At,a),a&&t(H$),a&&t(Oi),a&&t(B$),a&&t(Dt),a&&t(C$),a&&t(Ki),a&&t(G$),a&&t(St),a&&t(U$),a&&t(Be),b(Pn),a&&t(L$),a&&t(Xi),a&&t(z$),b(Ht,a),a&&t(M$),a&&t(Rn),a&&t(F$),a&&t(Qi),a&&t(J$),b(Bt,a),a&&t(K$),a&&t(Zi),a&&t(W$),a&&t(Ct),a&&t(Y$),a&&t(vu),a&&t(V$),a&&t(Jt),a&&t(X$),a&&t(Ce),b(ar),a&&t(Q$),a&&t(Ou),a&&t(Z$),b(Wt,a),a&&t(eq),a&&t(nr),a&&t(tq),a&&t(Pu),a&&t(sq),b(Yt,a),a&&t(aq),a&&t(Ru),a&&t(nq),a&&t(Vt),a&&t(rq),a&&t(zu),a&&t(oq),b(es,a),a&&t(lq),a&&t(ts),a&&t(iq),a&&t(Ge),b(vr),a&&t(uq),a&&t(Zu),a&&t(fq),b(as,a),a&&t(cq),a&&t(Ue),a&&t(pq),a&&t(ef),a&&t(dq),b(ns,a),a&&t(hq),a&&t(tf),a&&t(gq),a&&t(sf),a&&t(mq),b(rs,a),a&&t(_q),a&&t(os),a&&t($q),a&&t(Le),b(Ar),a&&t(qq),a&&t(cf),a&&t(vq),b(fs,a),a&&t(yq),a&&t(Dr),a&&t(Eq),a&&t(pf),a&&t(wq),b(cs,a),a&&t(bq),a&&t(df),a&&t(Tq),a&&t(ps),a&&t(jq),a&&t(yf),a&&t(kq),b(ms,a),a&&t(Aq),a&&t(_s),a&&t(Dq),a&&t(ze),b(Lr),a&&t(Oq),a&&t(zr),a&&t(Pq),a&&t(Me),b(Mr),a&&t(Rq),a&&t(Af),a&&t(Nq),b(vs,a),a&&t(xq),a&&t(Fe),a&&t(Sq),a&&t(Df),a&&t(Iq),b(ys,a),a&&t(Hq),a&&t(Of),a&&t(Bq),a&&t(Es),a&&t(Cq),a&&t(Uf),a&&t(Gq),b(js,a),a&&t(Uq),a&&t(ks),a&&t(Lq),a&&t(Je),b(io),a&&t(zq),a&&t(Xf),a&&t(Mq),b(Ps,a),a&&t(Fq),a&&t(uo),a&&t(Jq),a&&t(Qf),a&&t(Kq),b(Rs,a),a&&t(Wq),a&&t(Zf),a&&t(Yq),a&&t(Ns),a&&t(Vq),a&&t($c),a&&t(Xq),b(Gs,a),a&&t(Qq),a&&t(Us),a&&t(Zq),a&&t(Ke),b(Po),a&&t(e1),a&&t(zs),a&&t(t1),a&&t(We),b(Ro),a&&t(s1),a&&t(wc),a&&t(a1),b(Fs,a),a&&t(n1),a&&t(No),a&&t(r1),a&&t(bc),a&&t(o1),b(Js,a),a&&t(l1),a&&t(Tc),a&&t(i1),a&&t(Ks),a&&t(u1),a&&t(Nc),a&&t(f1),b(Xs,a),a&&t(c1),a&&t(Qs),a&&t(p1),a&&t(Ye),b(Ko),a&&t(d1),a&&t(zc),a&&t(h1),b(ea,a),a&&t(g1),b(ta,a),a&&t(m1),a&&t(pe),a&&t(_1),a&&t(Mc),a&&t($1),b(sa,a),a&&t(q1),a&&t(Fc),a&&t(v1),a&&t(aa),a&&t(y1),a&&t(Wc),a&&t(E1),a&&t(Yc),a&&t(w1),b(na,a),a&&t(b1),a&&t(ra),a&&t(T1),a&&t(Ve),b(sl),a&&t(j1),a&&t(Zc),a&&t(k1),b(la,a),a&&t(A1),a&&t(Xe),a&&t(D1),a&&t(ep),a&&t(O1),a&&t(ia),a&&t(P1),a&&t(ip),a&&t(R1),a&&t(pa),a&&t(N1),a&&t(pp),a&&t(x1),a&&t(Qe),b(hl),a&&t(S1),a&&t(dp),a&&t(I1),b(ha,a),a&&t(H1),a&&t(Ze),a&&t(B1),a&&t(hp),a&&t(C1),b(ga,a),a&&t(G1),a&&t(gp),a&&t(U1),a&&t(ma),a&&t(L1),a&&t($p),a&&t(z1),b(_a,a),a&&t(M1),a&&t($a),a&&t(F1),a&&t(et),b(bl),a&&t(J1),a&&t(bp),a&&t(K1),b(va,a),a&&t(W1),a&&t(Tl),a&&t(Y1),a&&t(Tp),a&&t(V1),b(ya,a),a&&t(X1),a&&t(Ea),a&&t(Q1),a&&t(wa),a&&t(Z1),a&&t(Ap),a&&t(ev),b(ba,a),a&&t(tv),a&&t(Ta)}}}const Vz={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"zeroshot-classification-task",title:"Zero-shot classification task"},{local:"translation-task",title:"Translation task"},{local:"summarization-task",title:"Summarization task"},{local:"conversational-task",title:"Conversational task"},{local:"table-question-answering-task",title:"Table question answering task"},{local:"question-answering-task",title:"Question answering task"},{local:"textclassification-task",title:"Text-classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"tokenclassification-task",title:"Token-classification task"},{local:"textgeneration-task",title:"Text-generation task"},{local:"text2textgeneration-task",title:"Text2text-generation task"},{local:"fill-mask-task",title:"Fill mask task"},{local:"automatic-speech-recognition-task",title:"Automatic speech recognition task"},{local:"featureextraction-task",title:"Feature-extraction task"},{local:"audioclassification-task",title:"Audio-classification task"},{local:"objectdetection-task",title:"Object-detection task"}],title:"Detailed parameters"};function Xz($){return zU(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sM extends CU{constructor(n){super();GU(this,n,Xz,Yz,UU,{})}}export{sM as default,Vz as metadata};
