import{S as Ms,i as Hs,s as Ds,e as o,k as h,w,t as r,M as Ks,c as n,d as a,m as d,a as s,x as $,h as l,b as p,G as t,g as f,y as _,q as P,o as b,B as A,v as Js,L as Un}from"../chunks/vendor-hf-doc-builder.js";import{I as X}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as jt,I as Fs,M as Nn}from"../chunks/InferenceApi-hf-doc-builder.js";function Ws(k){let c,m;return c=new jt({props:{code:`import json
import requests
API_URL = "https://api-inference.huggingface.co/models/gpt2"
headers = {"Authorization": f"Bearer {API_TOKEN}"}
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("Can you please let us know more details about your ")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;Can you please let us know more details about your &quot;</span>)`}}),{c(){w(c.$$.fragment)},l(i){$(c.$$.fragment,i)},m(i,g){_(c,i,g),m=!0},p:Un,i(i){m||(P(c.$$.fragment,i),m=!0)},o(i){b(c.$$.fragment,i),m=!1},d(i){A(c,i)}}}function Qs(k){let c,m;return c=new Nn({props:{$$slots:{default:[Ws]},$$scope:{ctx:k}}}),{c(){w(c.$$.fragment)},l(i){$(c.$$.fragment,i)},m(i,g){_(c,i,g),m=!0},p(i,g){const E={};g&2&&(E.$$scope={dirty:g,ctx:i}),c.$set(E)},i(i){m||(P(c.$$.fragment,i),m=!0)},o(i){b(c.$$.fragment,i),m=!1},d(i){A(c,i)}}}function Ys(k){let c,m;return c=new jt({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query("Can you please let us know more details about your ").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"Can you please let us know more details about your ids as a subscriber or other related project? Be sure to update your username and password or it will be stolen via email. Our information is only accessible through our website, and the payment support services"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;Can you please let us know more details about your &quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;Can you please let us know more details about your ids as a subscriber or other related project? Be sure to update your username and password or it will be stolen via email. Our information is only accessible through our website, and the payment support services&quot;}]</span>`}}),{c(){w(c.$$.fragment)},l(i){$(c.$$.fragment,i)},m(i,g){_(c,i,g),m=!0},p:Un,i(i){m||(P(c.$$.fragment,i),m=!0)},o(i){b(c.$$.fragment,i),m=!1},d(i){A(c,i)}}}function Vs(k){let c,m;return c=new Nn({props:{$$slots:{default:[Ys]},$$scope:{ctx:k}}}),{c(){w(c.$$.fragment)},l(i){$(c.$$.fragment,i)},m(i,g){_(c,i,g),m=!0},p(i,g){const E={};g&2&&(E.$$scope={dirty:g,ctx:i}),c.$set(E)},i(i){m||(P(c.$$.fragment,i),m=!0)},o(i){b(c.$$.fragment,i),m=!1},d(i){A(c,i)}}}function Zs(k){let c,m;return c=new jt({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '"Can you please let us know more details about your "' \\
        -H "Authorization: Bearer \${API_TOKEN}"
# [{"generated_text":"Can you please let us know more details about your ids as a subscriber or other related project? Be sure to update your username and password or it will be stolen via email. Our information is only accessible through our website, and the payment support services"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;&quot;Can you please let us know more details about your &quot;&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;Can you please let us know more details about your ids as a subscriber or other related project? Be sure to update your username and password or it will be stolen via email. Our information is only accessible through our website, and the payment support services&quot;}]</span>`}}),{c(){w(c.$$.fragment)},l(i){$(c.$$.fragment,i)},m(i,g){_(c,i,g),m=!0},p:Un,i(i){m||(P(c.$$.fragment,i),m=!0)},o(i){b(c.$$.fragment,i),m=!1},d(i){A(c,i)}}}function er(k){let c,m;return c=new Nn({props:{$$slots:{default:[Zs]},$$scope:{ctx:k}}}),{c(){w(c.$$.fragment)},l(i){$(c.$$.fragment,i)},m(i,g){_(c,i,g),m=!0},p(i,g){const E={};g&2&&(E.$$scope={dirty:g,ctx:i}),c.$set(E)},i(i){m||(P(c.$$.fragment,i),m=!0)},o(i){b(c.$$.fragment,i),m=!1},d(i){A(c,i)}}}function tr(k){let c,m,i,g,E,le,Ia,Qe,ja,Tt,Le,Ta,xt,U,z,Ye,ie,xa,Ve,Ua,Ut,y,ue,Na,Ze,Oa,La,Sa,Se,Ga,et,Ca,Ra,tt,Xa,za,pe,Ba,at,Ma,Ha,Da,I,Ka,ot,Ja,Fa,nt,Wa,Qa,ce,Ya,Va,Za,fe,eo,st,to,ao,oo,he,no,rt,so,ro,lo,Ge,lt,io,uo,po,it,co,Nt,N,B,ut,de,fo,pt,ho,Ot,Ce,mo,Lt,M,H,me,go,yo,ge,vo,wo,$o,ye,_o,ve,Po,bo,St,q,Ao,ct,Eo,ko,ft,qo,Io,ht,jo,To,Gt,Re,xo,Ct,O,D,dt,we,Uo,mt,No,Rt,j,Oo,$e,Lo,So,_e,Go,Co,Xt,Pe,zt,K,Ro,be,Xo,zo,Bt,J,Mt,L,F,gt,Ae,Bo,yt,Mo,Ht,T,Ho,Ee,Do,Ko,vt,Jo,Fo,Dt,S,W,wt,ke,Wo,$t,Qo,Kt,Q,Yo,_t,Vo,Zo,Jt,Xe,en,Ft,Y,tn,ze,an,on,Wt,G,V,Pt,qe,nn,bt,sn,Qt,Z,rn,Ie,ln,un,Yt,je,Vt,Be,pn,Zt,Me,cn,ea,ee,fn,He,hn,dn,ta,C,te,At,Te,mn,Et,gn,aa,ae,yn,De,vn,wn,oa,R,oe,kt,xe,$n,qt,_n,na,ne,Pn,Ue,bn,An,sa,x,En,Ne,kn,qn,Ke,In,jn,ra;return le=new X({}),ie=new X({}),de=new X({}),we=new X({}),Pe=new jt({props:{code:"ENDPOINT = https://api-inference.huggingface.co/models/<MODEL_ID>",highlighted:'ENDPOINT = https:<span class="hljs-regexp">//</span>api-inference.huggingface.co<span class="hljs-regexp">/models/</span>&lt;MODEL_ID&gt;'}}),J=new Fs({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[er],js:[Vs],python:[Qs]},$$scope:{ctx:k}}}),Ae=new X({}),ke=new X({}),qe=new X({}),je=new jt({props:{code:'{"inputs": "...REGULAR INPUT...", "options": {"use_gpu": true}}',highlighted:'<span class="hljs-punctuation">{</span><span class="hljs-attr">&quot;inputs&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;...REGULAR INPUT...&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;options&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span><span class="hljs-attr">&quot;use_gpu&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-keyword">true</span><span class="hljs-punctuation">}</span><span class="hljs-punctuation">}</span>'}}),Te=new X({}),xe=new X({}),{c(){c=o("meta"),m=h(),i=o("h1"),g=o("a"),E=o("span"),w(le.$$.fragment),Ia=h(),Qe=o("span"),ja=r("Overview"),Tt=h(),Le=o("p"),Ta=r("Let\u2019s have a quick look at the \u{1F917} Accelerated Inference API."),xt=h(),U=o("h2"),z=o("a"),Ye=o("span"),w(ie.$$.fragment),xa=h(),Ve=o("span"),Ua=r("Main features:"),Ut=h(),y=o("ul"),ue=o("li"),Na=r("Leverage "),Ze=o("strong"),Oa=r("10,000+ Transformer models"),La=r(" (T5, Blenderbot, Bart, GPT-2, Pegasus...)"),Sa=h(),Se=o("li"),Ga=r("Upload, manage and serve your "),et=o("strong"),Ca=r("own models privately"),Ra=h(),tt=o("li"),Xa=r("Run Classification, NER, Conversational, Summarization, Translation, Question-Answering, Embeddings Extraction tasks"),za=h(),pe=o("li"),Ba=r("Get up to "),at=o("strong"),Ma=r("10x inference speedup"),Ha=r(" to reduce user latency"),Da=h(),I=o("li"),Ka=r("Accelerated inference for a number of supported models on "),ot=o("strong"),Ja=r("CPU"),Fa=r(" and "),nt=o("strong"),Wa=r("GPU"),Qa=r(" (GPU requires a "),ce=o("a"),Ya=r("Community Pro or Organization Lab plan"),Va=r(")"),Za=h(),fe=o("li"),eo=r("Run "),st=o("strong"),to=r("large models"),ao=r(" that are challenging to deploy in production"),oo=h(),he=o("li"),no=r("Scale up to 1,000 requests per second with "),rt=o("strong"),so=r("automatic scaling"),ro=r(" built-in"),lo=h(),Ge=o("li"),lt=o("strong"),io=r("Ship new NLP features faster"),uo=r(" as new models become available"),po=h(),it=o("li"),co=r("Build your business on a platform powered by the reference open source project in NLP"),Nt=h(),N=o("h2"),B=o("a"),ut=o("span"),w(de.$$.fragment),fo=h(),pt=o("span"),ho=r("Get your API Token"),Ot=h(),Ce=o("p"),mo=r("To get started you need to:"),Lt=h(),M=o("ul"),H=o("li"),me=o("a"),go=r("Register"),yo=r(" or "),ge=o("a"),vo=r("Login"),wo=r("."),$o=h(),ye=o("li"),_o=r("Get your API token "),ve=o("a"),Po=r("in your Hugging Face profile"),bo=r("."),St=h(),q=o("p"),Ao=r("You should see a token "),ct=o("code"),Eo=r("hf_xxxxx"),ko=r(" (old tokens are "),ft=o("code"),qo=r("api_XXXXXXXX"),Io=r(" or "),ht=o("code"),jo=r("api_org_XXXXXXX"),To=r(")."),Gt=h(),Re=o("p"),xo=r(`If you do not submit your API token when sending requests to the API,
you will not be able to run inference on your private models, or
benefit from the model pinning and acceleration features of the API.`),Ct=h(),O=o("h2"),D=o("a"),dt=o("span"),w(we.$$.fragment),Uo=h(),mt=o("span"),No=r("Running Inference with API Requests"),Rt=h(),j=o("p"),Oo=r(`The first step is to choose which model you are going to run. Go to the
`),$e=o("a"),Lo=r("Model Hub"),So=r(` and select the model you want
to use. If you are unsure where to start, make sure to check our
`),_e=o("a"),Go=r(`recommended models for each NLP
task`),Co=r(`
available.`),Xt=h(),w(Pe.$$.fragment),zt=h(),K=o("p"),Ro=r("Let\u2019s use "),be=o("a"),Xo=r("gpt2"),zo=r(` as an example. To run
inference, simply use this code:`),Bt=h(),w(J.$$.fragment),Mt=h(),L=o("h2"),F=o("a"),gt=o("span"),w(Ae.$$.fragment),Bo=h(),yt=o("span"),Mo=r("API Options and Parameters"),Ht=h(),T=o("p"),Ho=r(`Depending on the task (aka pipeline) the model is configured for, the
request will accept specific parameters. When sending requests to run
any model, API options allow you to specify the caching and model
loading behavior, and inference on GPU (`),Ee=o("a"),Do=r("Community Pro or Organization Lab plan"),Ko=r(` required) All API options and
parameters are detailed here `),vt=o("code"),Jo=r("detailed_parameters"),Fo=r("."),Dt=h(),S=o("h2"),W=o("a"),wt=o("span"),w(ke.$$.fragment),Wo=h(),$t=o("span"),Qo=r("Using CPU-Accelerated Inference (~up to 10x speedup)"),Kt=h(),Q=o("p"),Yo=r(`As an API customer, your API token will automatically enable CPU-Accelerated inference on your requests if the model type is supported. For instance, if you compare
gpt2 model inference through our API with
CPU-Acceleration, compared to running inference on the model out of the
box on a local setup, you should measure a `),_t=o("strong"),Vo=r("~10x speedup"),Zo=r(`. The
specific performance boost depends on the model and input payload (and
your local hardware).`),Jt=h(),Xe=o("p"),en=r(`To verify you are using the CPU-Accelerated version of a model you can
check the x-compute-type header of your requests, which
should be cpu+optimized. If you do not see it, it simply
means not all optimizations are turned on. This can be for various
factors; the model might have been added recently to transformers, or
the model can be optimized in several different ways and the best one
depends on your use case.`),Ft=h(),Y=o("p"),tn=r("If you contact us at "),ze=o("a"),an=r("api-enterprise@huggingface.co"),on=r(`, we\u2019ll be able to
increase the inference speed for you, depending on your actual use case.`),Wt=h(),G=o("h2"),V=o("a"),Pt=o("span"),w(qe.$$.fragment),nn=h(),bt=o("span"),sn=r("Using GPU-Accelerated Inference"),Qt=h(),Z=o("p"),rn=r("In order to use GPU-Accelerated inference, you need a "),Ie=o("a"),ln=r("Community Pro or Organization Lab plan"),un=r(`. To run any model on a
GPU, you need to specify it via an option in your request:`),Yt=h(),w(je.$$.fragment),Vt=h(),Be=o("p"),pn=r(`Using GPU-Accelerated inference should produce a significant speedup for
all models.`),Zt=h(),Me=o("p"),cn=r(`To verify you are using the GPU-Accelerated version of the model, you can
check the x-compute-type header of your requests, which
should be gpu.`),ea=h(),ee=o("p"),fn=r("Please note: Contact us at "),He=o("a"),hn=r("api-enterprise@huggingface.co"),dn=r(` to discuss
your use case and usage profile when running GPU-Accelerated inference
on many models or large models, so we can optimize the infrastructure
accordingly.`),ta=h(),C=o("h2"),te=o("a"),At=o("span"),w(Te.$$.fragment),mn=h(),Et=o("span"),gn=r("Using Large Models (>10 GB)"),aa=h(),ae=o("p"),yn=r(`Large models do not get loaded automatically to protect quality of
service. Contact us at `),De=o("a"),vn=r("api-enterprise@huggingface.co"),wn=r(` so we can
configure large models for your endpoints.`),oa=h(),R=o("h2"),oe=o("a"),kt=o("span"),w(xe.$$.fragment),$n=h(),qt=o("span"),_n=r("Model Pinning / Preloading"),na=h(),ne=o("p"),Pn=r("With over 10,000 models available in the "),Ue=o("a"),bn=r(`Model
Hub`),An=r(`, not all can be loaded in compute
memory to be instantly available for inference. To guarantee model
availability for API customers who integrate them in production
applications, we offer to pin frequently used model(s) to their API
endpoints, so these models are always instantly available for inference.`),sa=h(),x=o("p"),En=r("The number of models that can be pinned depends on the selected "),Ne=o("a"),kn=r(`API
plan`),qn=r(`. To get a model pinned to your
account, please contact us at `),Ke=o("a"),In=r("api-enterprise@huggingface.co"),jn=r("."),this.h()},l(e){const u=Ks('[data-svelte="svelte-1phssyn"]',document.head);c=n(u,"META",{name:!0,content:!0}),u.forEach(a),m=d(e),i=n(e,"H1",{class:!0});var Oe=s(i);g=n(Oe,"A",{id:!0,class:!0,href:!0});var On=s(g);E=n(On,"SPAN",{});var Ln=s(E);$(le.$$.fragment,Ln),Ln.forEach(a),On.forEach(a),Ia=d(Oe),Qe=n(Oe,"SPAN",{});var Sn=s(Qe);ja=l(Sn,"Overview"),Sn.forEach(a),Oe.forEach(a),Tt=d(e),Le=n(e,"P",{});var Gn=s(Le);Ta=l(Gn,"Let\u2019s have a quick look at the \u{1F917} Accelerated Inference API."),Gn.forEach(a),xt=d(e),U=n(e,"H2",{class:!0});var la=s(U);z=n(la,"A",{id:!0,class:!0,href:!0});var Cn=s(z);Ye=n(Cn,"SPAN",{});var Rn=s(Ye);$(ie.$$.fragment,Rn),Rn.forEach(a),Cn.forEach(a),xa=d(la),Ve=n(la,"SPAN",{});var Xn=s(Ve);Ua=l(Xn,"Main features:"),Xn.forEach(a),la.forEach(a),Ut=d(e),y=n(e,"UL",{});var v=s(y);ue=n(v,"LI",{});var ia=s(ue);Na=l(ia,"Leverage "),Ze=n(ia,"STRONG",{});var zn=s(Ze);Oa=l(zn,"10,000+ Transformer models"),zn.forEach(a),La=l(ia," (T5, Blenderbot, Bart, GPT-2, Pegasus...)"),ia.forEach(a),Sa=d(v),Se=n(v,"LI",{});var Tn=s(Se);Ga=l(Tn,"Upload, manage and serve your "),et=n(Tn,"STRONG",{});var Bn=s(et);Ca=l(Bn,"own models privately"),Bn.forEach(a),Tn.forEach(a),Ra=d(v),tt=n(v,"LI",{});var Mn=s(tt);Xa=l(Mn,"Run Classification, NER, Conversational, Summarization, Translation, Question-Answering, Embeddings Extraction tasks"),Mn.forEach(a),za=d(v),pe=n(v,"LI",{});var ua=s(pe);Ba=l(ua,"Get up to "),at=n(ua,"STRONG",{});var Hn=s(at);Ma=l(Hn,"10x inference speedup"),Hn.forEach(a),Ha=l(ua," to reduce user latency"),ua.forEach(a),Da=d(v),I=n(v,"LI",{});var se=s(I);Ka=l(se,"Accelerated inference for a number of supported models on "),ot=n(se,"STRONG",{});var Dn=s(ot);Ja=l(Dn,"CPU"),Dn.forEach(a),Fa=l(se," and "),nt=n(se,"STRONG",{});var Kn=s(nt);Wa=l(Kn,"GPU"),Kn.forEach(a),Qa=l(se," (GPU requires a "),ce=n(se,"A",{href:!0,rel:!0});var Jn=s(ce);Ya=l(Jn,"Community Pro or Organization Lab plan"),Jn.forEach(a),Va=l(se,")"),se.forEach(a),Za=d(v),fe=n(v,"LI",{});var pa=s(fe);eo=l(pa,"Run "),st=n(pa,"STRONG",{});var Fn=s(st);to=l(Fn,"large models"),Fn.forEach(a),ao=l(pa," that are challenging to deploy in production"),pa.forEach(a),oo=d(v),he=n(v,"LI",{});var ca=s(he);no=l(ca,"Scale up to 1,000 requests per second with "),rt=n(ca,"STRONG",{});var Wn=s(rt);so=l(Wn,"automatic scaling"),Wn.forEach(a),ro=l(ca," built-in"),ca.forEach(a),lo=d(v),Ge=n(v,"LI",{});var xn=s(Ge);lt=n(xn,"STRONG",{});var Qn=s(lt);io=l(Qn,"Ship new NLP features faster"),Qn.forEach(a),uo=l(xn," as new models become available"),xn.forEach(a),po=d(v),it=n(v,"LI",{});var Yn=s(it);co=l(Yn,"Build your business on a platform powered by the reference open source project in NLP"),Yn.forEach(a),v.forEach(a),Nt=d(e),N=n(e,"H2",{class:!0});var fa=s(N);B=n(fa,"A",{id:!0,class:!0,href:!0});var Vn=s(B);ut=n(Vn,"SPAN",{});var Zn=s(ut);$(de.$$.fragment,Zn),Zn.forEach(a),Vn.forEach(a),fo=d(fa),pt=n(fa,"SPAN",{});var es=s(pt);ho=l(es,"Get your API Token"),es.forEach(a),fa.forEach(a),Ot=d(e),Ce=n(e,"P",{});var ts=s(Ce);mo=l(ts,"To get started you need to:"),ts.forEach(a),Lt=d(e),M=n(e,"UL",{});var ha=s(M);H=n(ha,"LI",{});var It=s(H);me=n(It,"A",{href:!0,rel:!0});var as=s(me);go=l(as,"Register"),as.forEach(a),yo=l(It," or "),ge=n(It,"A",{href:!0,rel:!0});var os=s(ge);vo=l(os,"Login"),os.forEach(a),wo=l(It,"."),It.forEach(a),$o=d(ha),ye=n(ha,"LI",{});var da=s(ye);_o=l(da,"Get your API token "),ve=n(da,"A",{href:!0,rel:!0});var ns=s(ve);Po=l(ns,"in your Hugging Face profile"),ns.forEach(a),bo=l(da,"."),da.forEach(a),ha.forEach(a),St=d(e),q=n(e,"P",{});var re=s(q);Ao=l(re,"You should see a token "),ct=n(re,"CODE",{});var ss=s(ct);Eo=l(ss,"hf_xxxxx"),ss.forEach(a),ko=l(re," (old tokens are "),ft=n(re,"CODE",{});var rs=s(ft);qo=l(rs,"api_XXXXXXXX"),rs.forEach(a),Io=l(re," or "),ht=n(re,"CODE",{});var ls=s(ht);jo=l(ls,"api_org_XXXXXXX"),ls.forEach(a),To=l(re,")."),re.forEach(a),Gt=d(e),Re=n(e,"P",{});var is=s(Re);xo=l(is,`If you do not submit your API token when sending requests to the API,
you will not be able to run inference on your private models, or
benefit from the model pinning and acceleration features of the API.`),is.forEach(a),Ct=d(e),O=n(e,"H2",{class:!0});var ma=s(O);D=n(ma,"A",{id:!0,class:!0,href:!0});var us=s(D);dt=n(us,"SPAN",{});var ps=s(dt);$(we.$$.fragment,ps),ps.forEach(a),us.forEach(a),Uo=d(ma),mt=n(ma,"SPAN",{});var cs=s(mt);No=l(cs,"Running Inference with API Requests"),cs.forEach(a),ma.forEach(a),Rt=d(e),j=n(e,"P",{});var Je=s(j);Oo=l(Je,`The first step is to choose which model you are going to run. Go to the
`),$e=n(Je,"A",{href:!0,rel:!0});var fs=s($e);Lo=l(fs,"Model Hub"),fs.forEach(a),So=l(Je,` and select the model you want
to use. If you are unsure where to start, make sure to check our
`),_e=n(Je,"A",{href:!0,rel:!0});var hs=s(_e);Go=l(hs,`recommended models for each NLP
task`),hs.forEach(a),Co=l(Je,`
available.`),Je.forEach(a),Xt=d(e),$(Pe.$$.fragment,e),zt=d(e),K=n(e,"P",{});var ga=s(K);Ro=l(ga,"Let\u2019s use "),be=n(ga,"A",{href:!0,rel:!0});var ds=s(be);Xo=l(ds,"gpt2"),ds.forEach(a),zo=l(ga,` as an example. To run
inference, simply use this code:`),ga.forEach(a),Bt=d(e),$(J.$$.fragment,e),Mt=d(e),L=n(e,"H2",{class:!0});var ya=s(L);F=n(ya,"A",{id:!0,class:!0,href:!0});var ms=s(F);gt=n(ms,"SPAN",{});var gs=s(gt);$(Ae.$$.fragment,gs),gs.forEach(a),ms.forEach(a),Bo=d(ya),yt=n(ya,"SPAN",{});var ys=s(yt);Mo=l(ys,"API Options and Parameters"),ys.forEach(a),ya.forEach(a),Ht=d(e),T=n(e,"P",{});var Fe=s(T);Ho=l(Fe,`Depending on the task (aka pipeline) the model is configured for, the
request will accept specific parameters. When sending requests to run
any model, API options allow you to specify the caching and model
loading behavior, and inference on GPU (`),Ee=n(Fe,"A",{href:!0,rel:!0});var vs=s(Ee);Do=l(vs,"Community Pro or Organization Lab plan"),vs.forEach(a),Ko=l(Fe,` required) All API options and
parameters are detailed here `),vt=n(Fe,"CODE",{});var ws=s(vt);Jo=l(ws,"detailed_parameters"),ws.forEach(a),Fo=l(Fe,"."),Fe.forEach(a),Dt=d(e),S=n(e,"H2",{class:!0});var va=s(S);W=n(va,"A",{id:!0,class:!0,href:!0});var $s=s(W);wt=n($s,"SPAN",{});var _s=s(wt);$(ke.$$.fragment,_s),_s.forEach(a),$s.forEach(a),Wo=d(va),$t=n(va,"SPAN",{});var Ps=s($t);Qo=l(Ps,"Using CPU-Accelerated Inference (~up to 10x speedup)"),Ps.forEach(a),va.forEach(a),Kt=d(e),Q=n(e,"P",{});var wa=s(Q);Yo=l(wa,`As an API customer, your API token will automatically enable CPU-Accelerated inference on your requests if the model type is supported. For instance, if you compare
gpt2 model inference through our API with
CPU-Acceleration, compared to running inference on the model out of the
box on a local setup, you should measure a `),_t=n(wa,"STRONG",{});var bs=s(_t);Vo=l(bs,"~10x speedup"),bs.forEach(a),Zo=l(wa,`. The
specific performance boost depends on the model and input payload (and
your local hardware).`),wa.forEach(a),Jt=d(e),Xe=n(e,"P",{});var As=s(Xe);en=l(As,`To verify you are using the CPU-Accelerated version of a model you can
check the x-compute-type header of your requests, which
should be cpu+optimized. If you do not see it, it simply
means not all optimizations are turned on. This can be for various
factors; the model might have been added recently to transformers, or
the model can be optimized in several different ways and the best one
depends on your use case.`),As.forEach(a),Ft=d(e),Y=n(e,"P",{});var $a=s(Y);tn=l($a,"If you contact us at "),ze=n($a,"A",{href:!0});var Es=s(ze);an=l(Es,"api-enterprise@huggingface.co"),Es.forEach(a),on=l($a,`, we\u2019ll be able to
increase the inference speed for you, depending on your actual use case.`),$a.forEach(a),Wt=d(e),G=n(e,"H2",{class:!0});var _a=s(G);V=n(_a,"A",{id:!0,class:!0,href:!0});var ks=s(V);Pt=n(ks,"SPAN",{});var qs=s(Pt);$(qe.$$.fragment,qs),qs.forEach(a),ks.forEach(a),nn=d(_a),bt=n(_a,"SPAN",{});var Is=s(bt);sn=l(Is,"Using GPU-Accelerated Inference"),Is.forEach(a),_a.forEach(a),Qt=d(e),Z=n(e,"P",{});var Pa=s(Z);rn=l(Pa,"In order to use GPU-Accelerated inference, you need a "),Ie=n(Pa,"A",{href:!0,rel:!0});var js=s(Ie);ln=l(js,"Community Pro or Organization Lab plan"),js.forEach(a),un=l(Pa,`. To run any model on a
GPU, you need to specify it via an option in your request:`),Pa.forEach(a),Yt=d(e),$(je.$$.fragment,e),Vt=d(e),Be=n(e,"P",{});var Ts=s(Be);pn=l(Ts,`Using GPU-Accelerated inference should produce a significant speedup for
all models.`),Ts.forEach(a),Zt=d(e),Me=n(e,"P",{});var xs=s(Me);cn=l(xs,`To verify you are using the GPU-Accelerated version of the model, you can
check the x-compute-type header of your requests, which
should be gpu.`),xs.forEach(a),ea=d(e),ee=n(e,"P",{});var ba=s(ee);fn=l(ba,"Please note: Contact us at "),He=n(ba,"A",{href:!0});var Us=s(He);hn=l(Us,"api-enterprise@huggingface.co"),Us.forEach(a),dn=l(ba,` to discuss
your use case and usage profile when running GPU-Accelerated inference
on many models or large models, so we can optimize the infrastructure
accordingly.`),ba.forEach(a),ta=d(e),C=n(e,"H2",{class:!0});var Aa=s(C);te=n(Aa,"A",{id:!0,class:!0,href:!0});var Ns=s(te);At=n(Ns,"SPAN",{});var Os=s(At);$(Te.$$.fragment,Os),Os.forEach(a),Ns.forEach(a),mn=d(Aa),Et=n(Aa,"SPAN",{});var Ls=s(Et);gn=l(Ls,"Using Large Models (>10 GB)"),Ls.forEach(a),Aa.forEach(a),aa=d(e),ae=n(e,"P",{});var Ea=s(ae);yn=l(Ea,`Large models do not get loaded automatically to protect quality of
service. Contact us at `),De=n(Ea,"A",{href:!0});var Ss=s(De);vn=l(Ss,"api-enterprise@huggingface.co"),Ss.forEach(a),wn=l(Ea,` so we can
configure large models for your endpoints.`),Ea.forEach(a),oa=d(e),R=n(e,"H2",{class:!0});var ka=s(R);oe=n(ka,"A",{id:!0,class:!0,href:!0});var Gs=s(oe);kt=n(Gs,"SPAN",{});var Cs=s(kt);$(xe.$$.fragment,Cs),Cs.forEach(a),Gs.forEach(a),$n=d(ka),qt=n(ka,"SPAN",{});var Rs=s(qt);_n=l(Rs,"Model Pinning / Preloading"),Rs.forEach(a),ka.forEach(a),na=d(e),ne=n(e,"P",{});var qa=s(ne);Pn=l(qa,"With over 10,000 models available in the "),Ue=n(qa,"A",{href:!0,rel:!0});var Xs=s(Ue);bn=l(Xs,`Model
Hub`),Xs.forEach(a),An=l(qa,`, not all can be loaded in compute
memory to be instantly available for inference. To guarantee model
availability for API customers who integrate them in production
applications, we offer to pin frequently used model(s) to their API
endpoints, so these models are always instantly available for inference.`),qa.forEach(a),sa=d(e),x=n(e,"P",{});var We=s(x);En=l(We,"The number of models that can be pinned depends on the selected "),Ne=n(We,"A",{href:!0,rel:!0});var zs=s(Ne);kn=l(zs,`API
plan`),zs.forEach(a),qn=l(We,`. To get a model pinned to your
account, please contact us at `),Ke=n(We,"A",{href:!0});var Bs=s(Ke);In=l(Bs,"api-enterprise@huggingface.co"),Bs.forEach(a),jn=l(We,"."),We.forEach(a),this.h()},h(){p(c,"name","hf:doc:metadata"),p(c,"content",JSON.stringify(ar)),p(g,"id","overview"),p(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(g,"href","#overview"),p(i,"class","relative group"),p(z,"id","main-features"),p(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(z,"href","#main-features"),p(U,"class","relative group"),p(ce,"href","https://huggingface.co/pricing"),p(ce,"rel","nofollow"),p(B,"id","get-your-api-token"),p(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(B,"href","#get-your-api-token"),p(N,"class","relative group"),p(me,"href","https://huggingface.co/join"),p(me,"rel","nofollow"),p(ge,"href","https://huggingface.co/login"),p(ge,"rel","nofollow"),p(ve,"href","https://huggingface.co/settings/tokens"),p(ve,"rel","nofollow"),p(D,"id","running-inference-with-api-requests"),p(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(D,"href","#running-inference-with-api-requests"),p(O,"class","relative group"),p($e,"href","https://huggingface.co/models"),p($e,"rel","nofollow"),p(_e,"href","https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html#detailed-parameters"),p(_e,"rel","nofollow"),p(be,"href","https://huggingface.co/gpt2"),p(be,"rel","nofollow"),p(F,"id","api-options-and-parameters"),p(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(F,"href","#api-options-and-parameters"),p(L,"class","relative group"),p(Ee,"href","https://huggingface.co/pricing"),p(Ee,"rel","nofollow"),p(W,"id","using-cpuaccelerated-inference-up-to-10x-speedup"),p(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(W,"href","#using-cpuaccelerated-inference-up-to-10x-speedup"),p(S,"class","relative group"),p(ze,"href","mailto:api-enterprise@huggingface.co"),p(V,"id","using-gpuaccelerated-inference"),p(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(V,"href","#using-gpuaccelerated-inference"),p(G,"class","relative group"),p(Ie,"href","https://huggingface.co/pricing"),p(Ie,"rel","nofollow"),p(He,"href","mailto:api-enterprise@huggingface.co"),p(te,"id","using-large-models-10-gb"),p(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(te,"href","#using-large-models-10-gb"),p(C,"class","relative group"),p(De,"href","mailto:api-enterprise@huggingface.co"),p(oe,"id","model-pinning-preloading"),p(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(oe,"href","#model-pinning-preloading"),p(R,"class","relative group"),p(Ue,"href","https://huggingface.co/models"),p(Ue,"rel","nofollow"),p(Ne,"href","https://huggingface.co/pricing"),p(Ne,"rel","nofollow"),p(Ke,"href","mailto:api-enterprise@huggingface.co")},m(e,u){t(document.head,c),f(e,m,u),f(e,i,u),t(i,g),t(g,E),_(le,E,null),t(i,Ia),t(i,Qe),t(Qe,ja),f(e,Tt,u),f(e,Le,u),t(Le,Ta),f(e,xt,u),f(e,U,u),t(U,z),t(z,Ye),_(ie,Ye,null),t(U,xa),t(U,Ve),t(Ve,Ua),f(e,Ut,u),f(e,y,u),t(y,ue),t(ue,Na),t(ue,Ze),t(Ze,Oa),t(ue,La),t(y,Sa),t(y,Se),t(Se,Ga),t(Se,et),t(et,Ca),t(y,Ra),t(y,tt),t(tt,Xa),t(y,za),t(y,pe),t(pe,Ba),t(pe,at),t(at,Ma),t(pe,Ha),t(y,Da),t(y,I),t(I,Ka),t(I,ot),t(ot,Ja),t(I,Fa),t(I,nt),t(nt,Wa),t(I,Qa),t(I,ce),t(ce,Ya),t(I,Va),t(y,Za),t(y,fe),t(fe,eo),t(fe,st),t(st,to),t(fe,ao),t(y,oo),t(y,he),t(he,no),t(he,rt),t(rt,so),t(he,ro),t(y,lo),t(y,Ge),t(Ge,lt),t(lt,io),t(Ge,uo),t(y,po),t(y,it),t(it,co),f(e,Nt,u),f(e,N,u),t(N,B),t(B,ut),_(de,ut,null),t(N,fo),t(N,pt),t(pt,ho),f(e,Ot,u),f(e,Ce,u),t(Ce,mo),f(e,Lt,u),f(e,M,u),t(M,H),t(H,me),t(me,go),t(H,yo),t(H,ge),t(ge,vo),t(H,wo),t(M,$o),t(M,ye),t(ye,_o),t(ye,ve),t(ve,Po),t(ye,bo),f(e,St,u),f(e,q,u),t(q,Ao),t(q,ct),t(ct,Eo),t(q,ko),t(q,ft),t(ft,qo),t(q,Io),t(q,ht),t(ht,jo),t(q,To),f(e,Gt,u),f(e,Re,u),t(Re,xo),f(e,Ct,u),f(e,O,u),t(O,D),t(D,dt),_(we,dt,null),t(O,Uo),t(O,mt),t(mt,No),f(e,Rt,u),f(e,j,u),t(j,Oo),t(j,$e),t($e,Lo),t(j,So),t(j,_e),t(_e,Go),t(j,Co),f(e,Xt,u),_(Pe,e,u),f(e,zt,u),f(e,K,u),t(K,Ro),t(K,be),t(be,Xo),t(K,zo),f(e,Bt,u),_(J,e,u),f(e,Mt,u),f(e,L,u),t(L,F),t(F,gt),_(Ae,gt,null),t(L,Bo),t(L,yt),t(yt,Mo),f(e,Ht,u),f(e,T,u),t(T,Ho),t(T,Ee),t(Ee,Do),t(T,Ko),t(T,vt),t(vt,Jo),t(T,Fo),f(e,Dt,u),f(e,S,u),t(S,W),t(W,wt),_(ke,wt,null),t(S,Wo),t(S,$t),t($t,Qo),f(e,Kt,u),f(e,Q,u),t(Q,Yo),t(Q,_t),t(_t,Vo),t(Q,Zo),f(e,Jt,u),f(e,Xe,u),t(Xe,en),f(e,Ft,u),f(e,Y,u),t(Y,tn),t(Y,ze),t(ze,an),t(Y,on),f(e,Wt,u),f(e,G,u),t(G,V),t(V,Pt),_(qe,Pt,null),t(G,nn),t(G,bt),t(bt,sn),f(e,Qt,u),f(e,Z,u),t(Z,rn),t(Z,Ie),t(Ie,ln),t(Z,un),f(e,Yt,u),_(je,e,u),f(e,Vt,u),f(e,Be,u),t(Be,pn),f(e,Zt,u),f(e,Me,u),t(Me,cn),f(e,ea,u),f(e,ee,u),t(ee,fn),t(ee,He),t(He,hn),t(ee,dn),f(e,ta,u),f(e,C,u),t(C,te),t(te,At),_(Te,At,null),t(C,mn),t(C,Et),t(Et,gn),f(e,aa,u),f(e,ae,u),t(ae,yn),t(ae,De),t(De,vn),t(ae,wn),f(e,oa,u),f(e,R,u),t(R,oe),t(oe,kt),_(xe,kt,null),t(R,$n),t(R,qt),t(qt,_n),f(e,na,u),f(e,ne,u),t(ne,Pn),t(ne,Ue),t(Ue,bn),t(ne,An),f(e,sa,u),f(e,x,u),t(x,En),t(x,Ne),t(Ne,kn),t(x,qn),t(x,Ke),t(Ke,In),t(x,jn),ra=!0},p(e,[u]){const Oe={};u&2&&(Oe.$$scope={dirty:u,ctx:e}),J.$set(Oe)},i(e){ra||(P(le.$$.fragment,e),P(ie.$$.fragment,e),P(de.$$.fragment,e),P(we.$$.fragment,e),P(Pe.$$.fragment,e),P(J.$$.fragment,e),P(Ae.$$.fragment,e),P(ke.$$.fragment,e),P(qe.$$.fragment,e),P(je.$$.fragment,e),P(Te.$$.fragment,e),P(xe.$$.fragment,e),ra=!0)},o(e){b(le.$$.fragment,e),b(ie.$$.fragment,e),b(de.$$.fragment,e),b(we.$$.fragment,e),b(Pe.$$.fragment,e),b(J.$$.fragment,e),b(Ae.$$.fragment,e),b(ke.$$.fragment,e),b(qe.$$.fragment,e),b(je.$$.fragment,e),b(Te.$$.fragment,e),b(xe.$$.fragment,e),ra=!1},d(e){a(c),e&&a(m),e&&a(i),A(le),e&&a(Tt),e&&a(Le),e&&a(xt),e&&a(U),A(ie),e&&a(Ut),e&&a(y),e&&a(Nt),e&&a(N),A(de),e&&a(Ot),e&&a(Ce),e&&a(Lt),e&&a(M),e&&a(St),e&&a(q),e&&a(Gt),e&&a(Re),e&&a(Ct),e&&a(O),A(we),e&&a(Rt),e&&a(j),e&&a(Xt),A(Pe,e),e&&a(zt),e&&a(K),e&&a(Bt),A(J,e),e&&a(Mt),e&&a(L),A(Ae),e&&a(Ht),e&&a(T),e&&a(Dt),e&&a(S),A(ke),e&&a(Kt),e&&a(Q),e&&a(Jt),e&&a(Xe),e&&a(Ft),e&&a(Y),e&&a(Wt),e&&a(G),A(qe),e&&a(Qt),e&&a(Z),e&&a(Yt),A(je,e),e&&a(Vt),e&&a(Be),e&&a(Zt),e&&a(Me),e&&a(ea),e&&a(ee),e&&a(ta),e&&a(C),A(Te),e&&a(aa),e&&a(ae),e&&a(oa),e&&a(R),A(xe),e&&a(na),e&&a(ne),e&&a(sa),e&&a(x)}}}const ar={local:"overview",sections:[{local:"main-features",title:"Main features:"},{local:"get-your-api-token",title:"Get your API Token"},{local:"running-inference-with-api-requests",title:"Running Inference with API Requests"},{local:"api-options-and-parameters",title:"API Options and Parameters"},{local:"using-cpuaccelerated-inference-up-to-10x-speedup",title:"Using CPU-Accelerated Inference (~up to 10x speedup)"},{local:"using-gpuaccelerated-inference",title:"Using GPU-Accelerated Inference"},{local:"using-large-models-10-gb",title:"Using Large Models (>10 GB)"},{local:"model-pinning-preloading",title:"Model Pinning / Preloading"}],title:"Overview"};function or(k){return Js(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class lr extends Ms{constructor(c){super();Hs(this,c,or,tr,Ds,{})}}export{lr as default,ar as metadata};
