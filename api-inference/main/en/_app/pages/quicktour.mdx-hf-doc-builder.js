import{S as $s,i as _s,s as bs,e as s,k as h,w,t as l,M as Ps,c as n,d as a,m as d,a as r,x as $,h as i,b as c,G as t,g as f,y as _,q as b,o as P,B as A,v as As,L as bo}from"../chunks/vendor-hf-doc-builder.js";import{I as ve}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ht,I as ks,M as Po}from"../chunks/InferenceApi-hf-doc-builder.js";function Es(E){let u,m;return u=new Ht({props:{code:`import json
import requests
API_URL = "https://api-inference.huggingface.co/models/gpt2"
headers = {"Authorization": f"Bearer {API_TOKEN}"}
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("Can you please let us know more details about your ")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;Can you please let us know more details about your &quot;</span>)`}}),{c(){w(u.$$.fragment)},l(o){$(u.$$.fragment,o)},m(o,g){_(u,o,g),m=!0},p:bo,i(o){m||(b(u.$$.fragment,o),m=!0)},o(o){P(u.$$.fragment,o),m=!1},d(o){A(u,o)}}}function Is(E){let u,m;return u=new Po({props:{$$slots:{default:[Es]},$$scope:{ctx:E}}}),{c(){w(u.$$.fragment)},l(o){$(u.$$.fragment,o)},m(o,g){_(u,o,g),m=!0},p(o,g){const k={};g&2&&(k.$$scope={dirty:g,ctx:o}),u.$set(k)},i(o){m||(b(u.$$.fragment,o),m=!0)},o(o){P(u.$$.fragment,o),m=!1},d(o){A(u,o)}}}function js(E){let u,m;return u=new Ht({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query("Can you please let us know more details about your ").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"Can you please let us know more details about your ids as a subscriber or other related project? Be sure to update your username and password or it will be stolen via email. Our information is only accessible through our website, and the payment support services"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;Can you please let us know more details about your &quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;Can you please let us know more details about your ids as a subscriber or other related project? Be sure to update your username and password or it will be stolen via email. Our information is only accessible through our website, and the payment support services&quot;}]</span>`}}),{c(){w(u.$$.fragment)},l(o){$(u.$$.fragment,o)},m(o,g){_(u,o,g),m=!0},p:bo,i(o){m||(b(u.$$.fragment,o),m=!0)},o(o){P(u.$$.fragment,o),m=!1},d(o){A(u,o)}}}function qs(E){let u,m;return u=new Po({props:{$$slots:{default:[js]},$$scope:{ctx:E}}}),{c(){w(u.$$.fragment)},l(o){$(u.$$.fragment,o)},m(o,g){_(u,o,g),m=!0},p(o,g){const k={};g&2&&(k.$$scope={dirty:g,ctx:o}),u.$set(k)},i(o){m||(b(u.$$.fragment,o),m=!0)},o(o){P(u.$$.fragment,o),m=!1},d(o){A(u,o)}}}function xs(E){let u,m;return u=new Ht({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '"Can you please let us know more details about your "' \\
        -H "Authorization: Bearer \${API_TOKEN}"
# [{"generated_text":"Can you please let us know more details about your ids as a subscriber or other related project? Be sure to update your username and password or it will be stolen via email. Our information is only accessible through our website, and the payment support services"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;&quot;Can you please let us know more details about your &quot;&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;Can you please let us know more details about your ids as a subscriber or other related project? Be sure to update your username and password or it will be stolen via email. Our information is only accessible through our website, and the payment support services&quot;}]</span>`}}),{c(){w(u.$$.fragment)},l(o){$(u.$$.fragment,o)},m(o,g){_(u,o,g),m=!0},p:bo,i(o){m||(b(u.$$.fragment,o),m=!0)},o(o){P(u.$$.fragment,o),m=!1},d(o){A(u,o)}}}function Ts(E){let u,m;return u=new Po({props:{$$slots:{default:[xs]},$$scope:{ctx:E}}}),{c(){w(u.$$.fragment)},l(o){$(u.$$.fragment,o)},m(o,g){_(u,o,g),m=!0},p(o,g){const k={};g&2&&(k.$$scope={dirty:g,ctx:o}),u.$set(k)},i(o){m||(b(u.$$.fragment,o),m=!0)},o(o){P(u.$$.fragment,o),m=!1},d(o){A(u,o)}}}function Os(E){let u,m,o,g,k,V,zt,xe,Dt,st,we,Ft,nt,x,C,Te,W,Kt,Oe,Jt,rt,y,Y,Qt,Se,Vt,Wt,Yt,$e,Zt,Ne,ea,ta,Le,aa,oa,Z,sa,Ce,na,ra,la,Xe,ia,ua,ee,pa,Re,ca,fa,ha,te,da,Ue,ma,ga,ya,_e,Ge,va,wa,$a,Me,_a,lt,T,X,Be,ae,ba,He,Pa,it,be,Aa,ut,R,U,oe,ka,Ea,se,Ia,ja,qa,ne,xa,re,Ta,Oa,pt,I,Sa,ze,Na,La,De,Ca,Xa,Fe,Ra,Ua,ct,Pe,Ga,ft,O,G,Ke,le,Ma,Je,Ba,ht,j,Ha,ie,za,Da,ue,Fa,Ka,dt,pe,mt,M,Ja,ce,Qa,Va,gt,B,yt,S,H,Qe,fe,Wa,Ve,Ya,vt,z,Za,Ae,We,eo,to,wt,N,D,Ye,he,ao,Ze,oo,$t,F,so,et,no,ro,_t,ke,lo,bt,K,io,Ee,uo,po,Pt,L,J,tt,de,co,at,fo,At,Ie,ho,kt,q,mo,me,go,yo,ge,vo,wo,Et;return V=new ve({}),W=new ve({}),ae=new ve({}),le=new ve({}),pe=new Ht({props:{code:"ENDPOINT = https://api-inference.huggingface.co/models/<MODEL_ID>",highlighted:'ENDPOINT = https:<span class="hljs-regexp">//</span>api-inference.huggingface.co<span class="hljs-regexp">/models/</span>&lt;MODEL_ID&gt;'}}),B=new ks({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[Ts],js:[qs],python:[Is]},$$scope:{ctx:E}}}),fe=new ve({}),he=new ve({}),de=new ve({}),{c(){u=s("meta"),m=h(),o=s("h1"),g=s("a"),k=s("span"),w(V.$$.fragment),zt=h(),xe=s("span"),Dt=l("Overview"),st=h(),we=s("p"),Ft=l("Let\u2019s have a quick look at the \u{1F917} Accelerated Inference API."),nt=h(),x=s("h2"),C=s("a"),Te=s("span"),w(W.$$.fragment),Kt=h(),Oe=s("span"),Jt=l("Main features:"),rt=h(),y=s("ul"),Y=s("li"),Qt=l("Leverage "),Se=s("strong"),Vt=l("10,000+ Transformer models"),Wt=l(" (T5, Blenderbot, Bart, GPT-2, Pegasus...)"),Yt=h(),$e=s("li"),Zt=l("Upload, manage and serve your "),Ne=s("strong"),ea=l("own models privately"),ta=h(),Le=s("li"),aa=l("Run Classification, NER, Conversational, Summarization, Translation, Question-Answering, Embeddings Extraction tasks"),oa=h(),Z=s("li"),sa=l("Get up to "),Ce=s("strong"),na=l("10x inference speedup"),ra=l(" to reduce user latency"),la=h(),Xe=s("li"),ia=l("Accelerated inference for a number of supported models on CPU"),ua=h(),ee=s("li"),pa=l("Run "),Re=s("strong"),ca=l("large models"),fa=l(" that are challenging to deploy in production"),ha=h(),te=s("li"),da=l("Scale up to 1,000 requests per second with "),Ue=s("strong"),ma=l("automatic scaling"),ga=l(" built-in"),ya=h(),_e=s("li"),Ge=s("strong"),va=l("Ship new NLP, CV, Audio, or RL features faster"),wa=l(" as new models become available"),$a=h(),Me=s("li"),_a=l("Build your business on a platform powered by the reference open source project in ML"),lt=h(),T=s("h2"),X=s("a"),Be=s("span"),w(ae.$$.fragment),ba=h(),He=s("span"),Pa=l("Get your API Token"),it=h(),be=s("p"),Aa=l("To get started you need to:"),ut=h(),R=s("ul"),U=s("li"),oe=s("a"),ka=l("Register"),Ea=l(" or "),se=s("a"),Ia=l("Login"),ja=l("."),qa=h(),ne=s("li"),xa=l("Get your API token "),re=s("a"),Ta=l("in your Hugging Face profile"),Oa=l("."),pt=h(),I=s("p"),Sa=l("You should see a token "),ze=s("code"),Na=l("hf_xxxxx"),La=l(" (old tokens are "),De=s("code"),Ca=l("api_XXXXXXXX"),Xa=l(" or "),Fe=s("code"),Ra=l("api_org_XXXXXXX"),Ua=l(")."),ct=h(),Pe=s("p"),Ga=l(`If you do not submit your API token when sending requests to the API,
you will not be able to run inference on your private models, or
benefit from the model pinning and acceleration features of the API.`),ft=h(),O=s("h2"),G=s("a"),Ke=s("span"),w(le.$$.fragment),Ma=h(),Je=s("span"),Ba=l("Running Inference with API Requests"),ht=h(),j=s("p"),Ha=l(`The first step is to choose which model you are going to run. Go to the
`),ie=s("a"),za=l("Model Hub"),Da=l(` and select the model you want
to use. If you are unsure where to start, make sure to check our
`),ue=s("a"),Fa=l(`recommended models for each ML
task`),Ka=l(`
available.`),dt=h(),w(pe.$$.fragment),mt=h(),M=s("p"),Ja=l("Let\u2019s use "),ce=s("a"),Qa=l("gpt2"),Va=l(` as an example. To run
inference, simply use this code:`),gt=h(),w(B.$$.fragment),yt=h(),S=s("h2"),H=s("a"),Qe=s("span"),w(fe.$$.fragment),Wa=h(),Ve=s("span"),Ya=l("API Options and Parameters"),vt=h(),z=s("p"),Za=l(`Depending on the task (aka pipeline) the model is configured for, the
request will accept specific parameters. When sending requests to run
any model, API options allow you to specify the caching and model
loading behavior. All API options and
parameters are detailed here `),Ae=s("a"),We=s("code"),eo=l("detailed_parameters"),to=l("."),wt=h(),N=s("h2"),D=s("a"),Ye=s("span"),w(he.$$.fragment),ao=h(),Ze=s("span"),oo=l("Using CPU-Accelerated Inference (~up to 10x speedup)"),$t=h(),F=s("p"),so=l(`As an API customer, your API token will automatically enable CPU-Accelerated inference on your requests if the model type is supported. For instance, if you compare
gpt2 model inference through our API with
CPU-Acceleration, compared to running inference on the model out of the
box on a local setup, you should measure a `),et=s("strong"),no=l("~10x speedup"),ro=l(`. The
specific performance boost depends on the model and input payload (and
your local hardware).`),_t=h(),ke=s("p"),lo=l(`To verify you are using the CPU-Accelerated version of a model you can
check the x-compute-type header of your requests, which
should be cpu+optimized. If you do not see it, it simply
means not all optimizations are turned on. This can be for various
factors; the model might have been added recently to transformers, or
the model can be optimized in several different ways and the best one
depends on your use case.`),bt=h(),K=s("p"),io=l("If you contact us at "),Ee=s("a"),uo=l("api-enterprise@huggingface.co"),po=l(`, we\u2019ll be able to
increase the inference speed for you, depending on your actual use case.`),Pt=h(),L=s("h2"),J=s("a"),tt=s("span"),w(de.$$.fragment),co=h(),at=s("span"),fo=l("Model Pinning / Preloading"),At=h(),Ie=s("p"),ho=l("Model pinning is only supported for existing customers."),kt=h(),q=s("p"),mo=l("If you\u2019re interested in having a model that you can "),me=s("a"),go=l(`readily deploy for
inference`),yo=l(", take a look at our "),ge=s("a"),vo=l(`Inference
Endpoints`),wo=l(` solution! It is a secure production environment with dedicated
and autoscaling infrastructure managed by Hugging Face, and you have the flexibility to choose between CPU and GPU
resources.`),this.h()},l(e){const p=Ps('[data-svelte="svelte-1phssyn"]',document.head);u=n(p,"META",{name:!0,content:!0}),p.forEach(a),m=d(e),o=n(e,"H1",{class:!0});var ye=r(o);g=n(ye,"A",{id:!0,class:!0,href:!0});var Ao=r(g);k=n(Ao,"SPAN",{});var ko=r(k);$(V.$$.fragment,ko),ko.forEach(a),Ao.forEach(a),zt=d(ye),xe=n(ye,"SPAN",{});var Eo=r(xe);Dt=i(Eo,"Overview"),Eo.forEach(a),ye.forEach(a),st=d(e),we=n(e,"P",{});var Io=r(we);Ft=i(Io,"Let\u2019s have a quick look at the \u{1F917} Accelerated Inference API."),Io.forEach(a),nt=d(e),x=n(e,"H2",{class:!0});var It=r(x);C=n(It,"A",{id:!0,class:!0,href:!0});var jo=r(C);Te=n(jo,"SPAN",{});var qo=r(Te);$(W.$$.fragment,qo),qo.forEach(a),jo.forEach(a),Kt=d(It),Oe=n(It,"SPAN",{});var xo=r(Oe);Jt=i(xo,"Main features:"),xo.forEach(a),It.forEach(a),rt=d(e),y=n(e,"UL",{});var v=r(y);Y=n(v,"LI",{});var jt=r(Y);Qt=i(jt,"Leverage "),Se=n(jt,"STRONG",{});var To=r(Se);Vt=i(To,"10,000+ Transformer models"),To.forEach(a),Wt=i(jt," (T5, Blenderbot, Bart, GPT-2, Pegasus...)"),jt.forEach(a),Yt=d(v),$e=n(v,"LI",{});var $o=r($e);Zt=i($o,"Upload, manage and serve your "),Ne=n($o,"STRONG",{});var Oo=r(Ne);ea=i(Oo,"own models privately"),Oo.forEach(a),$o.forEach(a),ta=d(v),Le=n(v,"LI",{});var So=r(Le);aa=i(So,"Run Classification, NER, Conversational, Summarization, Translation, Question-Answering, Embeddings Extraction tasks"),So.forEach(a),oa=d(v),Z=n(v,"LI",{});var qt=r(Z);sa=i(qt,"Get up to "),Ce=n(qt,"STRONG",{});var No=r(Ce);na=i(No,"10x inference speedup"),No.forEach(a),ra=i(qt," to reduce user latency"),qt.forEach(a),la=d(v),Xe=n(v,"LI",{});var Lo=r(Xe);ia=i(Lo,"Accelerated inference for a number of supported models on CPU"),Lo.forEach(a),ua=d(v),ee=n(v,"LI",{});var xt=r(ee);pa=i(xt,"Run "),Re=n(xt,"STRONG",{});var Co=r(Re);ca=i(Co,"large models"),Co.forEach(a),fa=i(xt," that are challenging to deploy in production"),xt.forEach(a),ha=d(v),te=n(v,"LI",{});var Tt=r(te);da=i(Tt,"Scale up to 1,000 requests per second with "),Ue=n(Tt,"STRONG",{});var Xo=r(Ue);ma=i(Xo,"automatic scaling"),Xo.forEach(a),ga=i(Tt," built-in"),Tt.forEach(a),ya=d(v),_e=n(v,"LI",{});var _o=r(_e);Ge=n(_o,"STRONG",{});var Ro=r(Ge);va=i(Ro,"Ship new NLP, CV, Audio, or RL features faster"),Ro.forEach(a),wa=i(_o," as new models become available"),_o.forEach(a),$a=d(v),Me=n(v,"LI",{});var Uo=r(Me);_a=i(Uo,"Build your business on a platform powered by the reference open source project in ML"),Uo.forEach(a),v.forEach(a),lt=d(e),T=n(e,"H2",{class:!0});var Ot=r(T);X=n(Ot,"A",{id:!0,class:!0,href:!0});var Go=r(X);Be=n(Go,"SPAN",{});var Mo=r(Be);$(ae.$$.fragment,Mo),Mo.forEach(a),Go.forEach(a),ba=d(Ot),He=n(Ot,"SPAN",{});var Bo=r(He);Pa=i(Bo,"Get your API Token"),Bo.forEach(a),Ot.forEach(a),it=d(e),be=n(e,"P",{});var Ho=r(be);Aa=i(Ho,"To get started you need to:"),Ho.forEach(a),ut=d(e),R=n(e,"UL",{});var St=r(R);U=n(St,"LI",{});var ot=r(U);oe=n(ot,"A",{href:!0,rel:!0});var zo=r(oe);ka=i(zo,"Register"),zo.forEach(a),Ea=i(ot," or "),se=n(ot,"A",{href:!0,rel:!0});var Do=r(se);Ia=i(Do,"Login"),Do.forEach(a),ja=i(ot,"."),ot.forEach(a),qa=d(St),ne=n(St,"LI",{});var Nt=r(ne);xa=i(Nt,"Get your API token "),re=n(Nt,"A",{href:!0,rel:!0});var Fo=r(re);Ta=i(Fo,"in your Hugging Face profile"),Fo.forEach(a),Oa=i(Nt,"."),Nt.forEach(a),St.forEach(a),pt=d(e),I=n(e,"P",{});var Q=r(I);Sa=i(Q,"You should see a token "),ze=n(Q,"CODE",{});var Ko=r(ze);Na=i(Ko,"hf_xxxxx"),Ko.forEach(a),La=i(Q," (old tokens are "),De=n(Q,"CODE",{});var Jo=r(De);Ca=i(Jo,"api_XXXXXXXX"),Jo.forEach(a),Xa=i(Q," or "),Fe=n(Q,"CODE",{});var Qo=r(Fe);Ra=i(Qo,"api_org_XXXXXXX"),Qo.forEach(a),Ua=i(Q,")."),Q.forEach(a),ct=d(e),Pe=n(e,"P",{});var Vo=r(Pe);Ga=i(Vo,`If you do not submit your API token when sending requests to the API,
you will not be able to run inference on your private models, or
benefit from the model pinning and acceleration features of the API.`),Vo.forEach(a),ft=d(e),O=n(e,"H2",{class:!0});var Lt=r(O);G=n(Lt,"A",{id:!0,class:!0,href:!0});var Wo=r(G);Ke=n(Wo,"SPAN",{});var Yo=r(Ke);$(le.$$.fragment,Yo),Yo.forEach(a),Wo.forEach(a),Ma=d(Lt),Je=n(Lt,"SPAN",{});var Zo=r(Je);Ba=i(Zo,"Running Inference with API Requests"),Zo.forEach(a),Lt.forEach(a),ht=d(e),j=n(e,"P",{});var je=r(j);Ha=i(je,`The first step is to choose which model you are going to run. Go to the
`),ie=n(je,"A",{href:!0,rel:!0});var es=r(ie);za=i(es,"Model Hub"),es.forEach(a),Da=i(je,` and select the model you want
to use. If you are unsure where to start, make sure to check our
`),ue=n(je,"A",{href:!0,rel:!0});var ts=r(ue);Fa=i(ts,`recommended models for each ML
task`),ts.forEach(a),Ka=i(je,`
available.`),je.forEach(a),dt=d(e),$(pe.$$.fragment,e),mt=d(e),M=n(e,"P",{});var Ct=r(M);Ja=i(Ct,"Let\u2019s use "),ce=n(Ct,"A",{href:!0,rel:!0});var as=r(ce);Qa=i(as,"gpt2"),as.forEach(a),Va=i(Ct,` as an example. To run
inference, simply use this code:`),Ct.forEach(a),gt=d(e),$(B.$$.fragment,e),yt=d(e),S=n(e,"H2",{class:!0});var Xt=r(S);H=n(Xt,"A",{id:!0,class:!0,href:!0});var os=r(H);Qe=n(os,"SPAN",{});var ss=r(Qe);$(fe.$$.fragment,ss),ss.forEach(a),os.forEach(a),Wa=d(Xt),Ve=n(Xt,"SPAN",{});var ns=r(Ve);Ya=i(ns,"API Options and Parameters"),ns.forEach(a),Xt.forEach(a),vt=d(e),z=n(e,"P",{});var Rt=r(z);Za=i(Rt,`Depending on the task (aka pipeline) the model is configured for, the
request will accept specific parameters. When sending requests to run
any model, API options allow you to specify the caching and model
loading behavior. All API options and
parameters are detailed here `),Ae=n(Rt,"A",{href:!0});var rs=r(Ae);We=n(rs,"CODE",{});var ls=r(We);eo=i(ls,"detailed_parameters"),ls.forEach(a),rs.forEach(a),to=i(Rt,"."),Rt.forEach(a),wt=d(e),N=n(e,"H2",{class:!0});var Ut=r(N);D=n(Ut,"A",{id:!0,class:!0,href:!0});var is=r(D);Ye=n(is,"SPAN",{});var us=r(Ye);$(he.$$.fragment,us),us.forEach(a),is.forEach(a),ao=d(Ut),Ze=n(Ut,"SPAN",{});var ps=r(Ze);oo=i(ps,"Using CPU-Accelerated Inference (~up to 10x speedup)"),ps.forEach(a),Ut.forEach(a),$t=d(e),F=n(e,"P",{});var Gt=r(F);so=i(Gt,`As an API customer, your API token will automatically enable CPU-Accelerated inference on your requests if the model type is supported. For instance, if you compare
gpt2 model inference through our API with
CPU-Acceleration, compared to running inference on the model out of the
box on a local setup, you should measure a `),et=n(Gt,"STRONG",{});var cs=r(et);no=i(cs,"~10x speedup"),cs.forEach(a),ro=i(Gt,`. The
specific performance boost depends on the model and input payload (and
your local hardware).`),Gt.forEach(a),_t=d(e),ke=n(e,"P",{});var fs=r(ke);lo=i(fs,`To verify you are using the CPU-Accelerated version of a model you can
check the x-compute-type header of your requests, which
should be cpu+optimized. If you do not see it, it simply
means not all optimizations are turned on. This can be for various
factors; the model might have been added recently to transformers, or
the model can be optimized in several different ways and the best one
depends on your use case.`),fs.forEach(a),bt=d(e),K=n(e,"P",{});var Mt=r(K);io=i(Mt,"If you contact us at "),Ee=n(Mt,"A",{href:!0});var hs=r(Ee);uo=i(hs,"api-enterprise@huggingface.co"),hs.forEach(a),po=i(Mt,`, we\u2019ll be able to
increase the inference speed for you, depending on your actual use case.`),Mt.forEach(a),Pt=d(e),L=n(e,"H2",{class:!0});var Bt=r(L);J=n(Bt,"A",{id:!0,class:!0,href:!0});var ds=r(J);tt=n(ds,"SPAN",{});var ms=r(tt);$(de.$$.fragment,ms),ms.forEach(a),ds.forEach(a),co=d(Bt),at=n(Bt,"SPAN",{});var gs=r(at);fo=i(gs,"Model Pinning / Preloading"),gs.forEach(a),Bt.forEach(a),At=d(e),Ie=n(e,"P",{});var ys=r(Ie);ho=i(ys,"Model pinning is only supported for existing customers."),ys.forEach(a),kt=d(e),q=n(e,"P",{});var qe=r(q);mo=i(qe,"If you\u2019re interested in having a model that you can "),me=n(qe,"A",{href:!0,rel:!0});var vs=r(me);go=i(vs,`readily deploy for
inference`),vs.forEach(a),yo=i(qe,", take a look at our "),ge=n(qe,"A",{href:!0,rel:!0});var ws=r(ge);vo=i(ws,`Inference
Endpoints`),ws.forEach(a),wo=i(qe,` solution! It is a secure production environment with dedicated
and autoscaling infrastructure managed by Hugging Face, and you have the flexibility to choose between CPU and GPU
resources.`),qe.forEach(a),this.h()},h(){c(u,"name","hf:doc:metadata"),c(u,"content",JSON.stringify(Ss)),c(g,"id","overview"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#overview"),c(o,"class","relative group"),c(C,"id","main-features"),c(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(C,"href","#main-features"),c(x,"class","relative group"),c(X,"id","get-your-api-token"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#get-your-api-token"),c(T,"class","relative group"),c(oe,"href","https://huggingface.co/join"),c(oe,"rel","nofollow"),c(se,"href","https://huggingface.co/login"),c(se,"rel","nofollow"),c(re,"href","https://huggingface.co/settings/tokens"),c(re,"rel","nofollow"),c(G,"id","running-inference-with-api-requests"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#running-inference-with-api-requests"),c(O,"class","relative group"),c(ie,"href","https://huggingface.co/models"),c(ie,"rel","nofollow"),c(ue,"href","https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html#detailed-parameters"),c(ue,"rel","nofollow"),c(ce,"href","https://huggingface.co/gpt2"),c(ce,"rel","nofollow"),c(H,"id","api-options-and-parameters"),c(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H,"href","#api-options-and-parameters"),c(S,"class","relative group"),c(Ae,"href","detailed_parameters"),c(D,"id","using-cpuaccelerated-inference-up-to-10x-speedup"),c(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(D,"href","#using-cpuaccelerated-inference-up-to-10x-speedup"),c(N,"class","relative group"),c(Ee,"href","mailto:api-enterprise@huggingface.co"),c(J,"id","model-pinning-preloading"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#model-pinning-preloading"),c(L,"class","relative group"),c(me,"href","https://ui.endpoints.huggingface.co/new"),c(me,"rel","nofollow"),c(ge,"href","https://huggingface.co/inference-endpoints"),c(ge,"rel","nofollow")},m(e,p){t(document.head,u),f(e,m,p),f(e,o,p),t(o,g),t(g,k),_(V,k,null),t(o,zt),t(o,xe),t(xe,Dt),f(e,st,p),f(e,we,p),t(we,Ft),f(e,nt,p),f(e,x,p),t(x,C),t(C,Te),_(W,Te,null),t(x,Kt),t(x,Oe),t(Oe,Jt),f(e,rt,p),f(e,y,p),t(y,Y),t(Y,Qt),t(Y,Se),t(Se,Vt),t(Y,Wt),t(y,Yt),t(y,$e),t($e,Zt),t($e,Ne),t(Ne,ea),t(y,ta),t(y,Le),t(Le,aa),t(y,oa),t(y,Z),t(Z,sa),t(Z,Ce),t(Ce,na),t(Z,ra),t(y,la),t(y,Xe),t(Xe,ia),t(y,ua),t(y,ee),t(ee,pa),t(ee,Re),t(Re,ca),t(ee,fa),t(y,ha),t(y,te),t(te,da),t(te,Ue),t(Ue,ma),t(te,ga),t(y,ya),t(y,_e),t(_e,Ge),t(Ge,va),t(_e,wa),t(y,$a),t(y,Me),t(Me,_a),f(e,lt,p),f(e,T,p),t(T,X),t(X,Be),_(ae,Be,null),t(T,ba),t(T,He),t(He,Pa),f(e,it,p),f(e,be,p),t(be,Aa),f(e,ut,p),f(e,R,p),t(R,U),t(U,oe),t(oe,ka),t(U,Ea),t(U,se),t(se,Ia),t(U,ja),t(R,qa),t(R,ne),t(ne,xa),t(ne,re),t(re,Ta),t(ne,Oa),f(e,pt,p),f(e,I,p),t(I,Sa),t(I,ze),t(ze,Na),t(I,La),t(I,De),t(De,Ca),t(I,Xa),t(I,Fe),t(Fe,Ra),t(I,Ua),f(e,ct,p),f(e,Pe,p),t(Pe,Ga),f(e,ft,p),f(e,O,p),t(O,G),t(G,Ke),_(le,Ke,null),t(O,Ma),t(O,Je),t(Je,Ba),f(e,ht,p),f(e,j,p),t(j,Ha),t(j,ie),t(ie,za),t(j,Da),t(j,ue),t(ue,Fa),t(j,Ka),f(e,dt,p),_(pe,e,p),f(e,mt,p),f(e,M,p),t(M,Ja),t(M,ce),t(ce,Qa),t(M,Va),f(e,gt,p),_(B,e,p),f(e,yt,p),f(e,S,p),t(S,H),t(H,Qe),_(fe,Qe,null),t(S,Wa),t(S,Ve),t(Ve,Ya),f(e,vt,p),f(e,z,p),t(z,Za),t(z,Ae),t(Ae,We),t(We,eo),t(z,to),f(e,wt,p),f(e,N,p),t(N,D),t(D,Ye),_(he,Ye,null),t(N,ao),t(N,Ze),t(Ze,oo),f(e,$t,p),f(e,F,p),t(F,so),t(F,et),t(et,no),t(F,ro),f(e,_t,p),f(e,ke,p),t(ke,lo),f(e,bt,p),f(e,K,p),t(K,io),t(K,Ee),t(Ee,uo),t(K,po),f(e,Pt,p),f(e,L,p),t(L,J),t(J,tt),_(de,tt,null),t(L,co),t(L,at),t(at,fo),f(e,At,p),f(e,Ie,p),t(Ie,ho),f(e,kt,p),f(e,q,p),t(q,mo),t(q,me),t(me,go),t(q,yo),t(q,ge),t(ge,vo),t(q,wo),Et=!0},p(e,[p]){const ye={};p&2&&(ye.$$scope={dirty:p,ctx:e}),B.$set(ye)},i(e){Et||(b(V.$$.fragment,e),b(W.$$.fragment,e),b(ae.$$.fragment,e),b(le.$$.fragment,e),b(pe.$$.fragment,e),b(B.$$.fragment,e),b(fe.$$.fragment,e),b(he.$$.fragment,e),b(de.$$.fragment,e),Et=!0)},o(e){P(V.$$.fragment,e),P(W.$$.fragment,e),P(ae.$$.fragment,e),P(le.$$.fragment,e),P(pe.$$.fragment,e),P(B.$$.fragment,e),P(fe.$$.fragment,e),P(he.$$.fragment,e),P(de.$$.fragment,e),Et=!1},d(e){a(u),e&&a(m),e&&a(o),A(V),e&&a(st),e&&a(we),e&&a(nt),e&&a(x),A(W),e&&a(rt),e&&a(y),e&&a(lt),e&&a(T),A(ae),e&&a(it),e&&a(be),e&&a(ut),e&&a(R),e&&a(pt),e&&a(I),e&&a(ct),e&&a(Pe),e&&a(ft),e&&a(O),A(le),e&&a(ht),e&&a(j),e&&a(dt),A(pe,e),e&&a(mt),e&&a(M),e&&a(gt),A(B,e),e&&a(yt),e&&a(S),A(fe),e&&a(vt),e&&a(z),e&&a(wt),e&&a(N),A(he),e&&a($t),e&&a(F),e&&a(_t),e&&a(ke),e&&a(bt),e&&a(K),e&&a(Pt),e&&a(L),A(de),e&&a(At),e&&a(Ie),e&&a(kt),e&&a(q)}}}const Ss={local:"overview",sections:[{local:"main-features",title:"Main features:"},{local:"get-your-api-token",title:"Get your API Token"},{local:"running-inference-with-api-requests",title:"Running Inference with API Requests"},{local:"api-options-and-parameters",title:"API Options and Parameters"},{local:"using-cpuaccelerated-inference-up-to-10x-speedup",title:"Using CPU-Accelerated Inference (~up to 10x speedup)"},{local:"model-pinning-preloading",title:"Model Pinning / Preloading"}],title:"Overview"};function Ns(E){return As(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rs extends $s{constructor(u){super();_s(this,u,Ns,Os,bs,{})}}export{Rs as default,Ss as metadata};
