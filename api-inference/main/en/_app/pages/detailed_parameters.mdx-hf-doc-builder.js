import{S as lJ,i as iJ,s as uJ,e as r,k as f,w as _,t as u,M as cJ,c as o,d as s,m as h,a as l,x as v,h as c,b as p,N as oJ,G as e,g as m,y,q as E,o as w,B as b,v as pJ,L as O}from"../chunks/vendor-hf-doc-builder.js";import{T as K}from"../chunks/Tip-hf-doc-builder.js";import{I as z}from"../chunks/IconCopyLink-hf-doc-builder.js";import{I as C,M as R,C as P}from"../chunks/InferenceApi-hf-doc-builder.js";function fJ(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("bert-base-uncased"),A=u(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"bert-base-uncased"),D.forEach(s),A=c(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/bert-base-uncased"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function hJ(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/bert-base-uncased"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is [MASK]."})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is [MASK].&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function dJ(q){let n,i;return n=new R({props:{$$slots:{default:[hJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function gJ(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/bert-base-uncased",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is [MASK]."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/bert-base-uncased&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;The answer to the universe is [MASK].&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:<span class="hljs-number">0.16963955760002136</span>,&quot;token&quot;:<span class="hljs-number">2053</span>,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:<span class="hljs-number">0.07344776391983032</span>,&quot;token&quot;:<span class="hljs-number">2498</span>,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:<span class="hljs-number">0.05803241208195686</span>,&quot;token&quot;:<span class="hljs-number">2748</span>,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:<span class="hljs-number">0.043957844376564026</span>,&quot;token&quot;:<span class="hljs-number">4242</span>,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:<span class="hljs-number">0.04015745222568512</span>,&quot;token&quot;:<span class="hljs-number">3722</span>,&quot;token_str&quot;:&quot;simple&quot;}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function mJ(q){let n,i;return n=new R({props:{$$slots:{default:[gJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function $J(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is [MASK]."}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"sequence":"the answer to the universe is no.","score":0.16963955760002136,"token":2053,"token_str":"no"},{"sequence":"the answer to the universe is nothing.","score":0.07344776391983032,"token":2498,"token_str":"nothing"},{"sequence":"the answer to the universe is yes.","score":0.05803241208195686,"token":2748,"token_str":"yes"},{"sequence":"the answer to the universe is unknown.","score":0.043957844376564026,"token":4242,"token_str":"unknown"},{"sequence":"the answer to the universe is simple.","score":0.04015745222568512,"token":3722,"token_str":"simple"}]`,highlighted:`curl https://api-inference.huggingface.co/models/bert-base-uncased \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is [MASK].&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;sequence&quot;:&quot;the answer to the universe is no.&quot;,&quot;score&quot;:0.16963955760002136,&quot;token&quot;:2053,&quot;token_str&quot;:&quot;no&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is nothing.&quot;,&quot;score&quot;:0.07344776391983032,&quot;token&quot;:2498,&quot;token_str&quot;:&quot;nothing&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is yes.&quot;,&quot;score&quot;:0.05803241208195686,&quot;token&quot;:2748,&quot;token_str&quot;:&quot;yes&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is unknown.&quot;,&quot;score&quot;:0.043957844376564026,&quot;token&quot;:4242,&quot;token_str&quot;:&quot;unknown&quot;},{&quot;sequence&quot;:&quot;the answer to the universe is simple.&quot;,&quot;score&quot;:0.04015745222568512,&quot;token&quot;:3722,&quot;token_str&quot;:&quot;simple&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function qJ(q){let n,i;return n=new R({props:{$$slots:{default:[$J]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function _J(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "sequence": "the answer to the universe is no.",
            "score": 0.1696,
            "token": 2053,
            "token_str": "no",
        },
        {
            "sequence": "the answer to the universe is nothing.",
            "score": 0.0734,
            "token": 2498,
            "token_str": "nothing",
        },
        {
            "sequence": "the answer to the universe is yes.",
            "score": 0.0580,
            "token": 2748,
            "token_str": "yes",
        },
        {
            "sequence": "the answer to the universe is unknown.",
            "score": 0.044,
            "token": 4242,
            "token_str": "unknown",
        },
        {
            "sequence": "the answer to the universe is simple.",
            "score": 0.0402,
            "token": 3722,
            "token_str": "simple",
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is no.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.1696</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2053</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;no&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is nothing.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0734</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2498</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;nothing&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is yes.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0580</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">2748</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;yes&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is unknown.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.044</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">4242</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;unknown&quot;</span>,
        },
        {
            <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;the answer to the universe is simple.&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0402</span>,
            <span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-number">3722</span>,
            <span class="hljs-string">&quot;token_str&quot;</span>: <span class="hljs-string">&quot;simple&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function vJ(q){let n,i;return n=new R({props:{$$slots:{default:[_J]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function yJ(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("facebook/bart-large-cnn"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"facebook/bart-large-cnn"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/facebook/bart-large-cnn"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function EJ(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.",
        "parameters": {"do_sample": False},
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "summary_text": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;do_sample&quot;</span>: <span class="hljs-literal">False</span>},
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;summary_text&quot;</span>: <span class="hljs-string">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function wJ(q){let n,i;return n=new R({props:{$$slots:{default:[EJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function bJ(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-cnn",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres."}]`,highlighted:`import fetch from <span class="hljs-comment">&quot;node-fetch&quot;</span>;
async function query(data) {
    const response = await fetch(
        <span class="hljs-comment">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-cnn&quot;</span>,
        {
            headers: { <span class="hljs-type">Authorization</span>: \`<span class="hljs-type">Bearer</span> <span class="hljs-string">\${</span><span class="hljs-type">API_TOKEN</span>}\` },
            method: <span class="hljs-comment">&quot;POST&quot;</span>,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: <span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;</span>}).then((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{<span class="hljs-comment">&quot;summary_text&quot;</span>:<span class="hljs-comment">&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. It was the first structure to reach a height of 300 metres.&quot;</span>}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function TJ(q){let n,i;return n=new R({props:{$$slots:{default:[bJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function jJ(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d '{"inputs": "The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.", "parameters": {"do_sample": false}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"summary_text":"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world."}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-cnn \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.&quot;, &quot;parameters&quot;: {&quot;do_sample&quot;: false}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;summary_text&quot;:&quot;The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function kJ(q){let n,i;return n=new R({props:{$$slots:{default:[jJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function AJ(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("deepset/roberta-base-squad2"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"deepset/roberta-base-squad2"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/deepset/roberta-base-squad2"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function DJ(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "question": "What's my name?",
            "context": "My name is Clara and I live in Berkeley.",
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;question&quot;</span>: <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>,
            <span class="hljs-string">&quot;context&quot;</span>: <span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>,
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function OJ(q){let n,i;return n=new R({props:{$$slots:{default:[DJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function RJ(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/deepset/roberta-base-squad2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{question:"What is my name?",context:"My name is Clara and I live in Berkeley."}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/deepset/roberta-base-squad2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{<span class="hljs-attr">question</span>:<span class="hljs-string">&quot;What is my name?&quot;</span>,<span class="hljs-attr">context</span>:<span class="hljs-string">&quot;My name is Clara and I live in Berkeley.&quot;</span>}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function PJ(q){let n,i;return n=new R({props:{$$slots:{default:[RJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function SJ(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d '{"inputs":{"question":"What is my name?","context":"My name is Clara and I live in Berkeley."}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"score":0.933128833770752,"start":11,"end":16,"answer":"Clara"}`,highlighted:`curl https://api-inference.huggingface.co/models/deepset/roberta-base-squad2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;question&quot;:&quot;What is my name?&quot;,&quot;context&quot;:&quot;My name is Clara and I live in Berkeley.&quot;}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;score&quot;:0.933128833770752,&quot;start&quot;:11,&quot;end&quot;:16,&quot;answer&quot;:&quot;Clara&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function NJ(q){let n,i;return n=new R({props:{$$slots:{default:[SJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function xJ(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    deep_round(data),
    {"score": 0.9327, "start": 11, "end": 16, "answer": "Clara"},
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9327</span>, <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>, <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">16</span>, <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;Clara&quot;</span>},
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function IJ(q){let n,i;return n=new R({props:{$$slots:{default:[xJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function HJ(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("google/tapas-base-finetuned-wtq"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"google/tapas-base-finetuned-wtq"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/google/tapas-base-finetuned-wtq"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function BJ(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "query": "How many stars does the transformers repository have?",
            "table": {
                "Repository": ["Transformers", "Datasets", "Tokenizers"],
                "Stars": ["36542", "4512", "3934"],
                "Contributors": ["651", "77", "34"],
                "Programming language": [
                    "Python",
                    "Python",
                    "Rust, Python and NodeJS",
                ],
            },
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;query&quot;</span>: <span class="hljs-string">&quot;How many stars does the transformers repository have?&quot;</span>,
            <span class="hljs-string">&quot;table&quot;</span>: {
                <span class="hljs-string">&quot;Repository&quot;</span>: [<span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;Datasets&quot;</span>, <span class="hljs-string">&quot;Tokenizers&quot;</span>],
                <span class="hljs-string">&quot;Stars&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>, <span class="hljs-string">&quot;4512&quot;</span>, <span class="hljs-string">&quot;3934&quot;</span>],
                <span class="hljs-string">&quot;Contributors&quot;</span>: [<span class="hljs-string">&quot;651&quot;</span>, <span class="hljs-string">&quot;77&quot;</span>, <span class="hljs-string">&quot;34&quot;</span>],
                <span class="hljs-string">&quot;Programming language&quot;</span>: [
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Python&quot;</span>,
                    <span class="hljs-string">&quot;Rust, Python and NodeJS&quot;</span>,
                ],
            },
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function CJ(q){let n,i;return n=new R({props:{$$slots:{default:[BJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function LJ(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{query:"How many stars does the transformers repository have?",table:{Repository:["Transformers","Datasets","Tokenizers"],Stars:["36542","4512","3934"],Contributors:["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:{query:&quot;How many stars does the transformers repository have?&quot;,<span class="hljs-keyword">table</span>:{Repository:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],Stars:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],Contributors:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function GJ(q){let n,i;return n=new R({props:{$$slots:{default:[LJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function MJ(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d '{"inputs":{"query":"How many stars does the transformers repository have?","table":{"Repository":["Transformers","Datasets","Tokenizers"],"Stars":["36542","4512","3934"],"Contributors":["651","77","34"],"Programming language":["Python","Python","Rust, Python and NodeJS"]}}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"answer":"AVERAGE > 36542","coordinates":[[0,1]],"cells":["36542"],"aggregator":"AVERAGE"}`,highlighted:`curl https://api-inference.huggingface.co/models/google/tapas-base-finetuned-wtq \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;query&quot;:&quot;How many stars does the transformers repository have?&quot;,&quot;table&quot;:{&quot;Repository&quot;:[&quot;Transformers&quot;,&quot;Datasets&quot;,&quot;Tokenizers&quot;],&quot;Stars&quot;:[&quot;36542&quot;,&quot;4512&quot;,&quot;3934&quot;],&quot;Contributors&quot;:[&quot;651&quot;,&quot;77&quot;,&quot;34&quot;],&quot;Programming language&quot;:[&quot;Python&quot;,&quot;Python&quot;,&quot;Rust, Python and NodeJS&quot;]}}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;answer&quot;:&quot;AVERAGE &gt; 36542&quot;,&quot;coordinates&quot;:[[0,1]],&quot;cells&quot;:[&quot;36542&quot;],&quot;aggregator&quot;:&quot;AVERAGE&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function zJ(q){let n,i;return n=new R({props:{$$slots:{default:[MJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function KJ(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    data,
    {
        "answer": "AVERAGE > 36542",
        "coordinates": [[0, 1]],
        "cells": ["36542"],
        "aggregator": "AVERAGE",
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;answer&quot;</span>: <span class="hljs-string">&quot;AVERAGE &gt; 36542&quot;</span>,
        <span class="hljs-string">&quot;coordinates&quot;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]],
        <span class="hljs-string">&quot;cells&quot;</span>: [<span class="hljs-string">&quot;36542&quot;</span>],
        <span class="hljs-string">&quot;aggregator&quot;</span>: <span class="hljs-string">&quot;AVERAGE&quot;</span>,
    },
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function FJ(q){let n,i;return n=new R({props:{$$slots:{default:[KJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function UJ(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("sentence-transformers/all-MiniLM-L6-v2"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"sentence-transformers/all-MiniLM-L6-v2"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function JJ(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "source_sentence": "That is a happy person",
            "sentences": ["That is a happy dog", "That is a very happy person", "Today is a sunny day"],
        }
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;source_sentence&quot;</span>: <span class="hljs-string">&quot;That is a happy person&quot;</span>,
            <span class="hljs-string">&quot;sentences&quot;</span>: [<span class="hljs-string">&quot;That is a happy dog&quot;</span>, <span class="hljs-string">&quot;That is a very happy person&quot;</span>, <span class="hljs-string">&quot;Today is a sunny day&quot;</span>],
        }
    }
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function WJ(q){let n,i;return n=new R({props:{$$slots:{default:[JJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function YJ(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:{
    source_sentence: "That is a happy person",
    sentences: [
        "That is a happy dog",
        "That is a very happy person",
        "Today is a sunny day"
    ]
}}).then((response) => {
    console.log(JSON.stringify(response));
});
// [0.6945773363113403,0.9429150819778442,0.2568760812282562]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:{
    <span class="hljs-attr">source_sentence</span>: <span class="hljs-string">&quot;That is a happy person&quot;</span>,
    <span class="hljs-attr">sentences</span>: [
        <span class="hljs-string">&quot;That is a happy dog&quot;</span>,
        <span class="hljs-string">&quot;That is a very happy person&quot;</span>,
        <span class="hljs-string">&quot;Today is a sunny day&quot;</span>
    ]
}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [0.6945773363113403,0.9429150819778442,0.2568760812282562]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function VJ(q){let n,i;return n=new R({props:{$$slots:{default:[YJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function XJ(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2 \\
        -X POST \\
        -d '{"inputs":{"source_sentence": "That is a happy person", "sentences": ["That is a happy dog","That is a very happy person","Today is a sunny day"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [0.6945773363113403,0.9429150819778442,0.2568760812282562]`,highlighted:`curl https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:{&quot;source_sentence&quot;: &quot;That is a happy person&quot;, &quot;sentences&quot;: [&quot;That is a happy dog&quot;,&quot;That is a very happy person&quot;,&quot;Today is a sunny day&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [0.6945773363113403,0.9429150819778442,0.2568760812282562]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function QJ(q){let n,i;return n=new R({props:{$$slots:{default:[XJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function ZJ(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    data,
    [0.6945773363113403, 0.9429150819778442, 0.2568760812282562],
)`,highlighted:`self.assertEqual(
    data,
    [<span class="hljs-number">0.6945773363113403</span>, <span class="hljs-number">0.9429150819778442</span>, <span class="hljs-number">0.2568760812282562</span>],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function eW(q){let n,i;return n=new R({props:{$$slots:{default:[ZJ]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function tW(q){let n,i,t,d,$,k;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("distilbert-base-uncased-finetuned-sst-2-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);i=o(j,"STRONG",{});var T=l(i);t=c(T,"Recommended model"),T.forEach(s),d=c(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=c(S,"distilbert-base-uncased-finetuned-sst-2-english"),S.forEach(s),j.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),p($,"rel","nofollow")},m(A,j){m(A,n,j),e(n,i),e(i,t),e(n,d),e(n,$),e($,k)},d(A){A&&s(n)}}}function sW(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "I like you. I love you"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;I like you. I love you&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function aW(q){let n,i;return n=new R({props:{$$slots:{default:[sW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function nW(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"I like you. I love you"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [[{"label":"POSITIVE","score":0.9998738765716553},{"label":"NEGATIVE","score":0.0001261125144083053}]]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;I like you. I love you&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [[{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553},{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053}]]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function rW(q){let n,i;return n=new R({props:{$$slots:{default:[nW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function oW(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d '{"inputs":"I like you. I love you"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [[{"label":"POSITIVE","score":0.9998738765716553},{"label":"NEGATIVE","score":0.0001261125144083053}]]`,highlighted:`curl https://api-inference.huggingface.co/models/distilbert-base-uncased-finetuned-sst-2-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;I like you. I love you&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [[{&quot;label&quot;:&quot;POSITIVE&quot;,&quot;score&quot;:0.9998738765716553},{&quot;label&quot;:&quot;NEGATIVE&quot;,&quot;score&quot;:0.0001261125144083053}]]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function lW(q){let n,i;return n=new R({props:{$$slots:{default:[oW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function iW(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    deep_round(data),
    [
        [
            {"label": "POSITIVE", "score": 0.9999},
            {"label": "NEGATIVE", "score": 0.0001},
        ]
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        [
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9999</span>},
            {<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0001</span>},
        ]
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function uW(q){let n,i;return n=new R({props:{$$slots:{default:[iW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function cW(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(": "),$=r("a"),k=u("gpt2"),A=u(" (it\u2019s a simple model, but fun to play with)."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"gpt2"),D.forEach(s),A=c(T," (it\u2019s a simple model, but fun to play with)."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/gpt2"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function pW(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/gpt2"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "The answer to the universe is"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;The answer to the universe is&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function fW(q){let n,i;return n=new R({props:{$$slots:{default:[pW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function hW(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/gpt2",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"The answer to the universe is"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/gpt2&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>:<span class="hljs-string">&quot;The answer to the universe is&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function dW(q){let n,i;return n=new R({props:{$$slots:{default:[hW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function gW(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d '{"inputs":"The answer to the universe is"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"generated_text":"The answer to the universe is in a different shape (or shapeless) than"}]`,highlighted:`curl https://api-inference.huggingface.co/models/gpt2 \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;The answer to the universe is&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;generated_text&quot;:&quot;The answer to the universe is in a different shape (or shapeless) than&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function mW(q){let n,i;return n=new R({props:{$$slots:{default:[gW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function $W(q){let n,i;return n=new P({props:{code:`data == [
    {
        "generated_text": 'The answer to the universe is that we are the creation of the entire universe," says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.'
    }
]`,highlighted:`data == [
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&#x27;The answer to the universe is that we are the creation of the entire universe,&quot; says Fitch.\\n\\nAs of the 1960s, six times as many Americans still make fewer than six bucks ($17) per year on their way to retirement.&#x27;</span>
    }
]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function qW(q){let n,i;return n=new R({props:{$$slots:{default:[$W]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function _W(q){let n,i,t,d,$,k;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("dbmdz/bert-large-cased-finetuned-conll03-english"),this.h()},l(A){n=o(A,"P",{});var j=l(n);i=o(j,"STRONG",{});var T=l(i);t=c(T,"Recommended model"),T.forEach(s),d=c(j,`:
`),$=o(j,"A",{href:!0,rel:!0});var S=l($);k=c(S,"dbmdz/bert-large-cased-finetuned-conll03-english"),S.forEach(s),j.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english"),p($,"rel","nofollow")},m(A,j){m(A,n,j),e(n,i),e(i,t),e(n,d),e(n,$),e($,k)},d(A){A&&s(n)}}}function vW(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query({"inputs": "My name is Sarah Jessica Parker but you can call me Jessica"})`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query({<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;</span>})`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function yW(q){let n,i;return n=new R({props:{$$slots:{default:[vW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function EW(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs:"My name is Sarah Jessica Parker but you can call me Jessica"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9991337060928345</span>,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:<span class="hljs-number">11</span>,&quot;end&quot;:<span class="hljs-number">31</span>},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:<span class="hljs-number">0.9979912042617798</span>,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:<span class="hljs-number">52</span>,&quot;end&quot;:<span class="hljs-number">59</span>}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function wW(q){let n,i;return n=new R({props:{$$slots:{default:[EW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function bW(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d '{"inputs":"My name is Sarah Jessica Parker but you can call me Jessica"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"entity_group":"PER","score":0.9991337060928345,"word":"Sarah Jessica Parker","start":11,"end":31},{"entity_group":"PER","score":0.9979912042617798,"word":"Jessica","start":52,"end":59}]`,highlighted:`curl https://api-inference.huggingface.co/models/dbmdz/bert-large-cased-finetuned-conll03-english \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;:&quot;My name is Sarah Jessica Parker but you can call me Jessica&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9991337060928345,&quot;word&quot;:&quot;Sarah Jessica Parker&quot;,&quot;start&quot;:11,&quot;end&quot;:31},{&quot;entity_group&quot;:&quot;PER&quot;,&quot;score&quot;:0.9979912042617798,&quot;word&quot;:&quot;Jessica&quot;,&quot;start&quot;:52,&quot;end&quot;:59}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function TW(q){let n,i;return n=new R({props:{$$slots:{default:[bW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function jW(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    deep_round(data),
    [
        {
            "entity_group": "PER",
            "score": 0.9991,
            "word": "Sarah Jessica Parker",
            "start": 11,
            "end": 31,
        },
        {
            "entity_group": "PER",
            "score": 0.998,
            "word": "Jessica",
            "start": 52,
            "end": 59,
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data),
    [
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9991</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Sarah Jessica Parker&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">11</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">31</span>,
        },
        {
            <span class="hljs-string">&quot;entity_group&quot;</span>: <span class="hljs-string">&quot;PER&quot;</span>,
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.998</span>,
            <span class="hljs-string">&quot;word&quot;</span>: <span class="hljs-string">&quot;Jessica&quot;</span>,
            <span class="hljs-string">&quot;start&quot;</span>: <span class="hljs-number">52</span>,
            <span class="hljs-string">&quot;end&quot;</span>: <span class="hljs-number">59</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function kW(q){let n,i;return n=new R({props:{$$slots:{default:[jW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function AW(q){let n,i,t,d,$,k,A,j,T,S,D,Z,Re;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("Helsinki-NLP/opus-mt-ru-en"),A=u(`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=r("strong"),T=u("Recommended model"),S=u(": "),D=r("a"),Z=u("t5-base"),Re=u("."),this.h()},l(X){n=o(X,"P",{});var U=l(n);i=o(U,"STRONG",{});var it=l(i);t=c(it,"Recommended model"),it.forEach(s),d=c(U,`:
`),$=o(U,"A",{href:!0,rel:!0});var Yi=l($);k=c(Yi,"Helsinki-NLP/opus-mt-ru-en"),Yi.forEach(s),A=c(U,`.
Helsinki-NLP uploaded many models with many language pairs.
`),j=o(U,"STRONG",{});var rn=l(j);T=c(rn,"Recommended model"),rn.forEach(s),S=c(U,": "),D=o(U,"A",{href:!0,rel:!0});var Pe=l(D);Z=c(Pe,"t5-base"),Pe.forEach(s),Re=c(U,"."),U.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/Helsinki-NLP/opus-mt-ru-en"),p($,"rel","nofollow"),p(D,"href","https://huggingface.co/t5-base"),p(D,"rel","nofollow")},m(X,U){m(X,n,U),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A),e(n,j),e(j,T),e(n,S),e(n,D),e(D,Z),e(n,Re)},d(X){X&&s(n)}}}function DW(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435",
    }
)
# Response
self.assertEqual(
    data,
    [
        {
            "translation_text": "My name is Wolfgang and I live in Berlin.",
        },
    ],
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>,
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    [
        {
            <span class="hljs-string">&quot;translation_text&quot;</span>: <span class="hljs-string">&quot;My name is Wolfgang and I live in Berlin.&quot;</span>,
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function OW(q){let n,i;return n=new R({props:{$$slots:{default:[DW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function RW(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}).then((response) => {
    console.log(JSON.stringify(response));
});
// [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;</span>}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function PW(q){let n,i;return n=new R({props:{$$slots:{default:[RW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function SW(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d '{"inputs": "\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435"}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"translation_text":"My name is Wolfgang and I live in Berlin."}]`,highlighted:`curl https://api-inference.huggingface.co/models/Helsinki-NLP/opus-mt-ru-en \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;\u041C\u0435\u043D\u044F \u0437\u043E\u0432\u0443\u0442 \u0412\u043E\u043B\u044C\u0444\u0433\u0430\u043D\u0433 \u0438 \u044F \u0436\u0438\u0432\u0443 \u0432 \u0411\u0435\u0440\u043B\u0438\u043D\u0435&quot;}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;translation_text&quot;:&quot;My name is Wolfgang and I live in Berlin.&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function NW(q){let n,i;return n=new R({props:{$$slots:{default:[SW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function xW(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("facebook/bart-large-mnli"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"facebook/bart-large-mnli"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/facebook/bart-large-mnli"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function IW(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-mnli"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "parameters": {"candidate_labels": ["refund", "legal", "faq"]},
    }
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;parameters&quot;</span>: {<span class="hljs-string">&quot;candidate_labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]},
    }
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function HW(q){let n,i;return n=new R({props:{$$slots:{default:[IW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function BW(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/bart-large-mnli",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", parameters: {candidate_labels: ["refund", "legal", "faq"]}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">data</span>) {
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/bart-large-mnli&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: <span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(data),
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>({<span class="hljs-attr">inputs</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>, <span class="hljs-attr">parameters</span>: {<span class="hljs-attr">candidate_labels</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>]}}).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function CW(q){let n,i;return n=new R({props:{$$slots:{default:[BW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function LW(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d '{"inputs": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!", "parameters": {"candidate_labels": ["refund", "legal", "faq"]}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"sequence":"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!","labels":["refund","faq","legal"],"scores":[0.8778, 0.1052, 0.017]}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/bart-large-mnli \\
        -X POST \\
        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: &quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;, &quot;parameters&quot;: {&quot;candidate_labels&quot;: [&quot;refund&quot;, &quot;legal&quot;, &quot;faq&quot;]}}&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;sequence&quot;:&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;,&quot;labels&quot;:[&quot;refund&quot;,&quot;faq&quot;,&quot;legal&quot;],&quot;scores&quot;:[0.8778, 0.1052, 0.017]}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function GW(q){let n,i;return n=new R({props:{$$slots:{default:[LW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function MW(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    deep_round(data),
    {
        "sequence": "Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!",
        "labels": ["refund", "faq", "legal"],
        "scores": [
            # 88% refund
            0.8778,
            0.1052,
            0.017,
        ],
    },
)`,highlighted:`self.assertEqual(
    deep_round(data),
    {
        <span class="hljs-string">&quot;sequence&quot;</span>: <span class="hljs-string">&quot;Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!&quot;</span>,
        <span class="hljs-string">&quot;labels&quot;</span>: [<span class="hljs-string">&quot;refund&quot;</span>, <span class="hljs-string">&quot;faq&quot;</span>, <span class="hljs-string">&quot;legal&quot;</span>],
        <span class="hljs-string">&quot;scores&quot;</span>: [
            <span class="hljs-comment"># 88% refund</span>
            <span class="hljs-number">0.8778</span>,
            <span class="hljs-number">0.1052</span>,
            <span class="hljs-number">0.017</span>,
        ],
    },
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function zW(q){let n,i;return n=new R({props:{$$slots:{default:[MW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function KW(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("microsoft/DialoGPT-large"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"microsoft/DialoGPT-large"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/microsoft/DialoGPT-large"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function FW(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large"
def query(payload):
    data = json.dumps(payload)
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query(
    {
        "inputs": {
            "past_user_inputs": ["Which movie is the best ?"],
            "generated_responses": ["It's Die Hard for sure."],
            "text": "Can you explain why ?",
        },
    }
)
# Response
self.assertEqual(
    data,
    {
        "generated_text": "It's the best movie ever.",
        "conversation": {
            "past_user_inputs": [
                "Which movie is the best ?",
                "Can you explain why ?",
            ],
            "generated_responses": [
                "It's Die Hard for sure.",
                "It's the best movie ever.",
            ],
        },
        "warnings": ["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."],
    },
)`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">payload</span>):
    data = json.dumps(payload)
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(
    {
        <span class="hljs-string">&quot;inputs&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [<span class="hljs-string">&quot;Which movie is the best ?&quot;</span>],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [<span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>],
            <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
        },
    }
)
<span class="hljs-comment"># Response</span>
self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;generated_text&quot;</span>: <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
        <span class="hljs-string">&quot;conversation&quot;</span>: {
            <span class="hljs-string">&quot;past_user_inputs&quot;</span>: [
                <span class="hljs-string">&quot;Which movie is the best ?&quot;</span>,
                <span class="hljs-string">&quot;Can you explain why ?&quot;</span>,
            ],
            <span class="hljs-string">&quot;generated_responses&quot;</span>: [
                <span class="hljs-string">&quot;It&#x27;s Die Hard for sure.&quot;</span>,
                <span class="hljs-string">&quot;It&#x27;s the best movie ever.&quot;</span>,
            ],
        },
        <span class="hljs-string">&quot;warnings&quot;</span>: [<span class="hljs-string">&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;</span>],
    },
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function UW(q){let n,i;return n=new R({props:{$$slots:{default:[FW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function JW(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
async function query(data) {
    const response = await fetch(
        "https://api-inference.huggingface.co/models/microsoft/DialoGPT-large",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: JSON.stringify(data),
        }
    );
    const result = await response.json();
    return result;
}
query({inputs: {past_user_inputs: ["Which movie is the best ?"], generated_responses: ["It is Die Hard for sure."], text:"Can you explain why ?"}}).then((response) => {
    console.log(JSON.stringify(response));
});
// {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
async <span class="hljs-keyword">function</span> query(data) {
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/microsoft/DialoGPT-large&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: <span class="hljs-type">JSON</span>.stringify(data),
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query({inputs: {past_user_inputs: [&quot;Which movie is the best ?&quot;], generated_responses: [&quot;It is Die Hard for sure.&quot;], <span class="hljs-type">text</span>:&quot;Can you explain why ?&quot;}}).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation.&quot;]}`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function WW(q){let n,i;return n=new R({props:{$$slots:{default:[JW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function YW(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\
        -X POST \\
        -d '{"inputs": {"past_user_inputs": ["Which movie is the best ?"], "generated_responses": ["It is Die Hard for sure."], "text":"Can you explain why ?"}}' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"generated_text":"It's the best movie ever.","conversation":{"past_user_inputs":["Which movie is the best ?","Can you explain why ?"],"generated_responses":["It is Die Hard for sure.","It's the best movie ever."]},"warnings":["Setting \`pad_token_id\` to \`eos_token_id\`:50256 for open-end generation."]}`,highlighted:'curl https://api-inference.huggingface.co/models/microsoft/DialoGPT-large \\\n        -X POST \\\n        -d <span class="hljs-string">&#x27;{&quot;inputs&quot;: {&quot;past_user_inputs&quot;: [&quot;Which movie is the best ?&quot;], &quot;generated_responses&quot;: [&quot;It is Die Hard for sure.&quot;], &quot;text&quot;:&quot;Can you explain why ?&quot;}}&#x27;</span> \\\n        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">${HF_API_TOKEN}</span>&quot;</span>\n<span class="hljs-comment"># {&quot;generated_text&quot;:&quot;It&#x27;s the best movie ever.&quot;,&quot;conversation&quot;:{&quot;past_user_inputs&quot;:[&quot;Which movie is the best ?&quot;,&quot;Can you explain why ?&quot;],&quot;generated_responses&quot;:[&quot;It is Die Hard for sure.&quot;,&quot;It&#x27;s the best movie ever.&quot;]},&quot;warnings&quot;:[&quot;Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.&quot;]}</span>'}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function VW(q){let n,i;return n=new R({props:{$$slots:{default:[YW]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function XW(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("Sentence-transformers"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"Sentence-transformers"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function QW(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(": "),$=r("a"),k=u(`Check your
langage`),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,": "),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,`Check your
langage`),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/models?pipeline_tag=automatic-speech-recognition"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function ZW(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("English"),d=u(`:
`),$=r("a"),k=u("facebook/wav2vec2-large-960h-lv60-self"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"English"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"facebook/wav2vec2-large-960h-lv60-self"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function eY(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function tY(q){let n,i;return n=new R({props:{$$slots:{default:[eY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function sY(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`<span class="hljs-keyword">import</span> fetch <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;node-fetch&quot;</span>;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> <span class="hljs-string">&quot;fs&quot;</span>;
<span class="hljs-keyword">async</span> <span class="hljs-keyword">function</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>) {
    <span class="hljs-keyword">const</span> data = fs.<span class="hljs-title function_">readFileSync</span>(filename);
    <span class="hljs-keyword">const</span> response = <span class="hljs-keyword">await</span> <span class="hljs-title function_">fetch</span>(
        <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h&quot;</span>,
        {
            <span class="hljs-attr">headers</span>: { <span class="hljs-title class_">Authorization</span>: <span class="hljs-string">\`Bearer <span class="hljs-subst">\${API_TOKEN}</span>\`</span> },
            <span class="hljs-attr">method</span>: <span class="hljs-string">&quot;POST&quot;</span>,
            <span class="hljs-attr">body</span>: data,
        }
    );
    <span class="hljs-keyword">const</span> result = <span class="hljs-keyword">await</span> response.<span class="hljs-title function_">json</span>();
    <span class="hljs-keyword">return</span> result;
}
<span class="hljs-title function_">query</span>(<span class="hljs-string">&quot;sample1.flac&quot;</span>).<span class="hljs-title function_">then</span>(<span class="hljs-function">(<span class="hljs-params">response</span>) =&gt;</span> {
    <span class="hljs-variable language_">console</span>.<span class="hljs-title function_">log</span>(<span class="hljs-title class_">JSON</span>.<span class="hljs-title function_">stringify</span>(response));
});
<span class="hljs-comment">// {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function aY(q){let n,i;return n=new R({props:{$$slots:{default:[sY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function nY(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# {"text":"GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"}`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/wav2vec2-base-960h \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># {&quot;text&quot;:&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;}</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function rY(q){let n,i;return n=new R({props:{$$slots:{default:[nY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function oY(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    data,
    {
        "text": "GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE'LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS"
    },
)`,highlighted:`self.assertEqual(
    data,
    {
        <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;GOING ALONG SLUSHY COUNTRY ROADS AND SPEAKING TO DAMP AUDIENCES IN DRAUGHTY SCHOOL ROOMS DAY AFTER DAY FOR A FORTNIGHT HE&#x27;LL HAVE TO PUT IN AN APPEARANCE AT SOME PLACE OF WORSHIP ON SUNDAY MORNING AND HE CAN COME TO US IMMEDIATELY AFTERWARDS&quot;</span>
    },
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function lY(q){let n,i;return n=new R({props:{$$slots:{default:[oY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function iY(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("superb/hubert-large-superb-er"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"superb/hubert-large-superb-er"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/superb/hubert-large-superb-er"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function uY(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("sample1.flac")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;sample1.flac&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function cY(q){let n,i;return n=new R({props:{$$slots:{default:[uY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function pY(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/superb/hubert-large-superb-er",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("sample1.flac").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/superb/hubert-large-superb-er&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;sample1.flac&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.5927661657333374</span>,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:<span class="hljs-number">0.2002529799938202</span>,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:<span class="hljs-number">0.12795612215995789</span>,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:<span class="hljs-number">0.07902472466230392</span>,&quot;label&quot;:&quot;sad&quot;}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function fY(q){let n,i;return n=new R({props:{$$slots:{default:[pY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function hY(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary '@sample1.flac' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.5927661657333374,"label":"neu"},{"score":0.2002529799938202,"label":"hap"},{"score":0.12795612215995789,"label":"ang"},{"score":0.07902472466230392,"label":"sad"}]`,highlighted:`curl https://api-inference.huggingface.co/models/superb/hubert-large-superb-er \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@sample1.flac&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.5927661657333374,&quot;label&quot;:&quot;neu&quot;},{&quot;score&quot;:0.2002529799938202,&quot;label&quot;:&quot;hap&quot;},{&quot;score&quot;:0.12795612215995789,&quot;label&quot;:&quot;ang&quot;},{&quot;score&quot;:0.07902472466230392,&quot;label&quot;:&quot;sad&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function dY(q){let n,i;return n=new R({props:{$$slots:{default:[hY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function gY(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.5928, "label": "neu"},
        {"score": 0.2003, "label": "hap"},
        {"score": 0.128, "label": "ang"},
        {"score": 0.079, "label": "sad"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.5928</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;neu&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.2003</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;hap&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.128</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;ang&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.079</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;sad&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function mY(q){let n,i;return n=new R({props:{$$slots:{default:[gY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function $Y(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("google/vit-base-patch16-224"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"google/vit-base-patch16-224"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/google/vit-base-patch16-224"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function qY(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/google/vit-base-patch16-224"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function _Y(q){let n,i;return n=new R({props:{$$slots:{default:[qY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function vY(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/google/vit-base-patch16-224",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/google/vit-base-patch16-224&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9374412894248962</span>,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.03844260051846504</span>,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.014411412179470062</span>,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:<span class="hljs-number">0.003274323185905814</span>,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:<span class="hljs-number">0.0006795919616706669</span>,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function yY(q){let n,i;return n=new R({props:{$$slots:{default:[vY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function EY(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9374412894248962,"label":"Egyptian cat"},{"score":0.03844260051846504,"label":"tabby, tabby cat"},{"score":0.014411412179470062,"label":"tiger cat"},{"score":0.003274323185905814,"label":"lynx, catamount"},{"score":0.0006795919616706669,"label":"Siamese cat, Siamese"}]`,highlighted:`curl https://api-inference.huggingface.co/models/google/vit-base-patch16-224 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9374412894248962,&quot;label&quot;:&quot;Egyptian cat&quot;},{&quot;score&quot;:0.03844260051846504,&quot;label&quot;:&quot;tabby, tabby cat&quot;},{&quot;score&quot;:0.014411412179470062,&quot;label&quot;:&quot;tiger cat&quot;},{&quot;score&quot;:0.003274323185905814,&quot;label&quot;:&quot;lynx, catamount&quot;},{&quot;score&quot;:0.0006795919616706669,&quot;label&quot;:&quot;Siamese cat, Siamese&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function wY(q){let n,i;return n=new R({props:{$$slots:{default:[EY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function bY(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {"score": 0.9374, "label": "Egyptian cat"},
        {"score": 0.0384, "label": "tabby, tabby cat"},
        {"score": 0.0144, "label": "tiger cat"},
        {"score": 0.0033, "label": "lynx, catamount"},
        {"score": 0.0007, "label": "Siamese cat, Siamese"},
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9374</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Egyptian cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0384</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tabby, tabby cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0144</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;tiger cat&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0033</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;lynx, catamount&quot;</span>},
        {<span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.0007</span>, <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;Siamese cat, Siamese&quot;</span>},
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function TY(q){let n,i;return n=new R({props:{$$slots:{default:[bY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function jY(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("facebook/detr-resnet-50"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"facebook/detr-resnet-50"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/facebook/detr-resnet-50"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function kY(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function AY(q){let n,i;return n=new R({props:{$$slots:{default:[kY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function DY(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;:<span class="hljs-number">0.9982201457023621</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">40</span>,&quot;ymin&quot;:<span class="hljs-number">70</span>,&quot;xmax&quot;:<span class="hljs-number">175</span>,&quot;ymax&quot;:<span class="hljs-number">117</span>}},{&quot;score&quot;:<span class="hljs-number">0.9960021376609802</span>,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">333</span>,&quot;ymin&quot;:<span class="hljs-number">72</span>,&quot;xmax&quot;:<span class="hljs-number">368</span>,&quot;ymax&quot;:<span class="hljs-number">187</span>}},{&quot;score&quot;:<span class="hljs-number">0.9954745173454285</span>,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">0</span>,&quot;ymin&quot;:<span class="hljs-number">1</span>,&quot;xmax&quot;:<span class="hljs-number">639</span>,&quot;ymax&quot;:<span class="hljs-number">473</span>}},{&quot;score&quot;:<span class="hljs-number">0.9988006353378296</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">13</span>,&quot;ymin&quot;:<span class="hljs-number">52</span>,&quot;xmax&quot;:<span class="hljs-number">314</span>,&quot;ymax&quot;:<span class="hljs-number">470</span>}},{&quot;score&quot;:<span class="hljs-number">0.9986783862113953</span>,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:<span class="hljs-number">345</span>,&quot;ymin&quot;:<span class="hljs-number">23</span>,&quot;xmax&quot;:<span class="hljs-number">640</span>,&quot;ymax&quot;:<span class="hljs-number">368</span>}}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function OY(q){let n,i;return n=new R({props:{$$slots:{default:[DY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function RY(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score":0.9982201457023621,"label":"remote","box":{"xmin":40,"ymin":70,"xmax":175,"ymax":117}},{"score":0.9960021376609802,"label":"remote","box":{"xmin":333,"ymin":72,"xmax":368,"ymax":187}},{"score":0.9954745173454285,"label":"couch","box":{"xmin":0,"ymin":1,"xmax":639,"ymax":473}},{"score":0.9988006353378296,"label":"cat","box":{"xmin":13,"ymin":52,"xmax":314,"ymax":470}},{"score":0.9986783862113953,"label":"cat","box":{"xmin":345,"ymin":23,"xmax":640,"ymax":368}}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50 \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;:0.9982201457023621,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:40,&quot;ymin&quot;:70,&quot;xmax&quot;:175,&quot;ymax&quot;:117}},{&quot;score&quot;:0.9960021376609802,&quot;label&quot;:&quot;remote&quot;,&quot;box&quot;:{&quot;xmin&quot;:333,&quot;ymin&quot;:72,&quot;xmax&quot;:368,&quot;ymax&quot;:187}},{&quot;score&quot;:0.9954745173454285,&quot;label&quot;:&quot;couch&quot;,&quot;box&quot;:{&quot;xmin&quot;:0,&quot;ymin&quot;:1,&quot;xmax&quot;:639,&quot;ymax&quot;:473}},{&quot;score&quot;:0.9988006353378296,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:13,&quot;ymin&quot;:52,&quot;xmax&quot;:314,&quot;ymax&quot;:470}},{&quot;score&quot;:0.9986783862113953,&quot;label&quot;:&quot;cat&quot;,&quot;box&quot;:{&quot;xmin&quot;:345,&quot;ymin&quot;:23,&quot;xmax&quot;:640,&quot;ymax&quot;:368}}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function PY(q){let n,i;return n=new R({props:{$$slots:{default:[RY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function SY(q){let n,i;return n=new P({props:{code:`self.assertEqual(
    deep_round(data, 4),
    [
        {
            "score": 0.9982,
            "label": "remote",
            "box": {"xmin": 40, "ymin": 70, "xmax": 175, "ymax": 117},
        },
        {
            "score": 0.9960,
            "label": "remote",
            "box": {"xmin": 333, "ymin": 72, "xmax": 368, "ymax": 187},
        },
        {
            "score": 0.9955,
            "label": "couch",
            "box": {"xmin": 0, "ymin": 1, "xmax": 639, "ymax": 473},
        },
        {
            "score": 0.9988,
            "label": "cat",
            "box": {"xmin": 13, "ymin": 52, "xmax": 314, "ymax": 470},
        },
        {
            "score": 0.9987,
            "label": "cat",
            "box": {"xmin": 345, "ymin": 23, "xmax": 640, "ymax": 368},
        },
    ],
)`,highlighted:`self.assertEqual(
    deep_round(data, <span class="hljs-number">4</span>),
    [
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9982</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">40</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">70</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">175</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">117</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9960</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;remote&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">333</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">72</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">368</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">187</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9955</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;couch&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">639</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">473</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9988</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">13</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">52</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">314</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">470</span>},
        },
        {
            <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9987</span>,
            <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;cat&quot;</span>,
            <span class="hljs-string">&quot;box&quot;</span>: {<span class="hljs-string">&quot;xmin&quot;</span>: <span class="hljs-number">345</span>, <span class="hljs-string">&quot;ymin&quot;</span>: <span class="hljs-number">23</span>, <span class="hljs-string">&quot;xmax&quot;</span>: <span class="hljs-number">640</span>, <span class="hljs-string">&quot;ymax&quot;</span>: <span class="hljs-number">368</span>},
        },
    ],
)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function NY(q){let n,i;return n=new R({props:{$$slots:{default:[SY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function xY(q){let n,i,t,d,$,k,A;return{c(){n=r("p"),i=r("strong"),t=u("Recommended model"),d=u(`:
`),$=r("a"),k=u("facebook/detr-resnet-50-panoptic"),A=u("."),this.h()},l(j){n=o(j,"P",{});var T=l(n);i=o(T,"STRONG",{});var S=l(i);t=c(S,"Recommended model"),S.forEach(s),d=c(T,`:
`),$=o(T,"A",{href:!0,rel:!0});var D=l($);k=c(D,"facebook/detr-resnet-50-panoptic"),D.forEach(s),A=c(T,"."),T.forEach(s),this.h()},h(){p($,"href","https://huggingface.co/facebook/detr-resnet-50-panoptic"),p($,"rel","nofollow")},m(j,T){m(j,n,T),e(n,i),e(i,t),e(n,d),e(n,$),e($,k),e(n,A)},d(j){j&&s(n)}}}function IY(q){let n,i;return n=new P({props:{code:`import json
import requests
headers = {"Authorization": f"Bearer {API_TOKEN}"}
API_URL = "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic"
def query(filename):
    with open(filename, "rb") as f:
        data = f.read()
    response = requests.request("POST", API_URL, headers=headers, data=data)
    return json.loads(response.content.decode("utf-8"))
data = query("cats.jpg")`,highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-keyword">import</span> requests
headers = {<span class="hljs-string">&quot;Authorization&quot;</span>: <span class="hljs-string">f&quot;Bearer <span class="hljs-subst">{API_TOKEN}</span>&quot;</span>}
API_URL = <span class="hljs-string">&quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">query</span>(<span class="hljs-params">filename</span>):
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">&quot;rb&quot;</span>) <span class="hljs-keyword">as</span> f:
        data = f.read()
    response = requests.request(<span class="hljs-string">&quot;POST&quot;</span>, API_URL, headers=headers, data=data)
    <span class="hljs-keyword">return</span> json.loads(response.content.decode(<span class="hljs-string">&quot;utf-8&quot;</span>))
data = query(<span class="hljs-string">&quot;cats.jpg&quot;</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function HY(q){let n,i;return n=new R({props:{$$slots:{default:[IY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function BY(q){let n,i;return n=new P({props:{code:`import fetch from "node-fetch";
import fs from "fs";
async function query(filename) {
    const data = fs.readFileSync(filename);
    const response = await fetch(
        "https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic",
        {
            headers: { Authorization: \`Bearer \${API_TOKEN}\` },
            method: "POST",
            body: data,
        }
    );
    const result = await response.json();
    return result;
}
query("cats.jpg").then((response) => {
    console.log(JSON.stringify(response));
});
// [{"score": 0.9094279408454895, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722069501876831, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`<span class="hljs-keyword">import</span> <span class="hljs-keyword">fetch</span> <span class="hljs-keyword">from</span> &quot;node-fetch&quot;;
<span class="hljs-keyword">import</span> fs <span class="hljs-keyword">from</span> &quot;fs&quot;;
async <span class="hljs-keyword">function</span> query(filename) {
    const data = fs.readFileSync(filename);
    const response = await <span class="hljs-keyword">fetch</span>(
        &quot;https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic&quot;,
        {
            headers: { <span class="hljs-keyword">Authorization</span>: \`Bearer \${API_TOKEN}\` },
            <span class="hljs-keyword">method</span>: &quot;POST&quot;,
            body: data,
        }
    );
    const result = await response.json();
    <span class="hljs-keyword">return</span> result;
}
query(&quot;cats.jpg&quot;).<span class="hljs-keyword">then</span>((response) =&gt; {
    console.log(<span class="hljs-type">JSON</span>.stringify(response));
});
// [{&quot;score&quot;: <span class="hljs-number">0.9094279408454895</span>, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9940965175628662</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9986692667007446</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994757771492004</span>, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9722069501876831</span>, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: <span class="hljs-number">0.9994235038757324</span>, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function CY(q){let n,i;return n=new R({props:{$$slots:{default:[BY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function LY(q){let n,i;return n=new P({props:{code:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary '@cats.jpg' \\
        -H "Authorization: Bearer \${HF_API_TOKEN}"
# [{"score": 0.9094279408454895, "label": "blanket", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9940965175628662, "label": "cat", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9986692667007446, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994757771492004, "label": "remote", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9722069501876831, "label": "couch", "mask": "BASE64ENCODED_MASK"}, {"score": 0.9994235038757324, "label": "cat", "mask": "BASE64ENCODED_MASK"}]`,highlighted:`curl https://api-inference.huggingface.co/models/facebook/detr-resnet-50-panoptic \\
        -X POST \\
        --data-binary <span class="hljs-string">&#x27;@cats.jpg&#x27;</span> \\
        -H <span class="hljs-string">&quot;Authorization: Bearer <span class="hljs-variable">\${HF_API_TOKEN}</span>&quot;</span>
<span class="hljs-comment"># [{&quot;score&quot;: 0.9094279408454895, &quot;label&quot;: &quot;blanket&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9940965175628662, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9986692667007446, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994757771492004, &quot;label&quot;: &quot;remote&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9722069501876831, &quot;label&quot;: &quot;couch&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}, {&quot;score&quot;: 0.9994235038757324, &quot;label&quot;: &quot;cat&quot;, &quot;mask&quot;: &quot;BASE64ENCODED_MASK&quot;}]</span>`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function GY(q){let n,i;return n=new R({props:{$$slots:{default:[LY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function MY(q){let n,i;return n=new P({props:{code:`import base64
from io import BytesIO
from PIL import Image
with Image.open("cats.jpg") as img:
    masks = [d["mask"] for d in data]
    self.assertEqual(img.size, (640, 480))
    mask_imgs = [Image.open(BytesIO(base64.b64decode(mask))) for mask in masks]
    for mask_img in mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, "L")  # L (8-bit pixels, black and white)
    first_mask_img = mask_imgs[0]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, 0)
    self.assertLessEqual(max_pxl_val, 255)`,highlighted:`<span class="hljs-keyword">import</span> base64
<span class="hljs-keyword">from</span> io <span class="hljs-keyword">import</span> BytesIO
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">with</span> Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;cats.jpg&quot;</span>) <span class="hljs-keyword">as</span> img:
    masks = [d[<span class="hljs-string">&quot;mask&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> data]
    self.assertEqual(img.size, (<span class="hljs-number">640</span>, <span class="hljs-number">480</span>))
    mask_imgs = [Image.<span class="hljs-built_in">open</span>(BytesIO(base64.b64decode(mask))) <span class="hljs-keyword">for</span> mask <span class="hljs-keyword">in</span> masks]
    <span class="hljs-keyword">for</span> mask_img <span class="hljs-keyword">in</span> mask_imgs:
        self.assertEqual(mask_img.size, img.size)
        self.assertEqual(mask_img.mode, <span class="hljs-string">&quot;L&quot;</span>)  <span class="hljs-comment"># L (8-bit pixels, black and white)</span>
    first_mask_img = mask_imgs[<span class="hljs-number">0</span>]
    min_pxl_val, max_pxl_val = first_mask_img.getextrema()
    self.assertGreaterEqual(min_pxl_val, <span class="hljs-number">0</span>)
    self.assertLessEqual(max_pxl_val, <span class="hljs-number">255</span>)`}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p:O,i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function zY(q){let n,i;return n=new R({props:{$$slots:{default:[MY]},$$scope:{ctx:q}}}),{c(){_(n.$$.fragment)},l(t){v(n.$$.fragment,t)},m(t,d){y(n,t,d),i=!0},p(t,d){const $={};d&2&&($.$$scope={dirty:d,ctx:t}),n.$set($)},i(t){i||(E(n.$$.fragment,t),i=!0)},o(t){w(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function KY(q){let n,i,t,d,$,k,A,j,T,S,D,Z,Re,X,U,it,Yi,rn,Pe,x3,E1,Vi,I3,w1,ut,MI,b1,ct,zI,T1,Se,pt,Od,on,H3,Rd,B3,j1,Ne,ft,Pd,ln,C3,Sd,L3,k1,Xi,G3,A1,ht,D1,un,M3,cn,z3,O1,Qi,K3,R1,dt,P1,Zi,F3,S1,gt,Nd,pn,eu,U3,J3,xd,W3,ue,fn,hn,Id,Y3,V3,X3,tu,Q3,Z3,dn,su,Hd,eT,tT,au,sT,aT,gn,nu,nT,rT,mt,oT,Bd,lT,iT,uT,mn,ru,cT,pT,$t,fT,Cd,hT,dT,N1,ou,gT,x1,qt,I1,_t,Ld,$n,lu,mT,$T,Gd,qT,ce,qn,iu,Md,_T,vT,uu,yT,ET,_n,cu,zd,wT,bT,pu,TT,jT,vn,fu,Kd,kT,AT,hu,DT,OT,yn,du,Fd,RT,PT,gu,ST,H1,xe,vt,Ud,En,NT,Jd,xT,B1,yt,IT,mu,HT,BT,C1,Et,L1,wn,CT,bn,LT,G1,$u,GT,M1,wt,z1,qu,MT,K1,bt,Wd,Tn,_u,zT,KT,Yd,FT,G,jn,kn,Vd,UT,JT,WT,vu,YT,VT,An,yu,Xd,XT,QT,Eu,ZT,ej,Dn,wu,tj,sj,_e,aj,Qd,nj,rj,Zd,oj,lj,ij,On,bu,uj,cj,ve,pj,eg,fj,hj,tg,dj,gj,mj,Rn,Tu,$j,qj,ye,_j,sg,vj,yj,ag,Ej,wj,bj,Pn,ju,Tj,jj,ee,kj,ng,Aj,Dj,rg,Oj,Rj,og,Pj,Sj,Nj,Sn,ku,xj,Ij,te,Hj,lg,Bj,Cj,ig,Lj,Gj,ug,Mj,zj,Kj,Nn,Au,Fj,Uj,Tt,Jj,cg,Wj,Yj,Vj,xn,Du,Xj,Qj,jt,Zj,pg,e4,t4,s4,In,Ou,fg,a4,n4,Ru,r4,o4,Hn,Pu,l4,i4,kt,u4,hg,c4,p4,f4,Bn,Su,h4,d4,At,g4,dg,m4,$4,F1,Nu,q4,U1,Dt,gg,Cn,xu,_4,v4,mg,y4,$g,Ln,Iu,qg,E4,w4,Hu,b4,J1,Ie,Ot,_g,Gn,T4,vg,j4,W1,Bu,k4,Y1,Rt,V1,He,A4,Mn,D4,O4,zn,R4,X1,Cu,P4,Q1,Pt,Z1,Lu,S4,ev,Gu,N4,tv,St,sv,Nt,yg,Kn,Mu,x4,I4,Eg,H4,pe,Fn,zu,wg,B4,C4,Ku,L4,G4,Un,Fu,bg,M4,z4,Uu,K4,F4,Jn,Ju,Tg,U4,J4,xt,W4,jg,Y4,V4,X4,Wn,Wu,kg,Q4,Z4,It,e5,Ag,t5,s5,av,Be,Ht,Dg,Yn,a5,Og,n5,nv,Yu,r5,rv,Bt,ov,Vn,o5,Xn,l5,lv,Vu,i5,iv,Ct,uv,Xu,u5,cv,Lt,Rg,Qn,Qu,c5,p5,Pg,f5,J,Zn,er,Sg,h5,d5,g5,Ng,m5,tr,Zu,$5,q5,ec,_5,v5,sr,tc,y5,E5,sc,w5,b5,ar,ac,xg,T5,j5,nc,k5,A5,nr,rc,D5,O5,Gt,R5,Ig,P5,S5,N5,rr,oc,x5,I5,Mt,H5,Hg,B5,C5,pv,lc,L5,fv,zt,hv,Kt,Bg,or,ic,G5,M5,Cg,z5,fe,lr,uc,Lg,K5,F5,cc,U5,J5,ir,pc,Gg,W5,Y5,fc,V5,X5,ur,hc,Mg,Q5,Z5,dc,ek,tk,cr,gc,zg,sk,ak,mc,nk,dv,Ce,Ft,Kg,pr,rk,Fg,ok,gv,$c,lk,mv,Ut,$v,fr,ik,hr,uk,qv,qc,ck,_v,Jt,vv,_c,pk,yv,Wt,Ug,dr,vc,fk,hk,Jg,dk,W,gr,mr,Wg,gk,mk,$k,Yg,qk,$r,yc,_k,vk,Ec,yk,Ek,qr,wc,wk,bk,bc,Tk,jk,_r,Tc,Vg,kk,Ak,jc,Dk,Ok,vr,kc,Rk,Pk,Yt,Sk,Xg,Nk,xk,Ik,yr,Ac,Hk,Bk,Vt,Ck,Qg,Lk,Gk,Ev,Dc,Mk,wv,Xt,bv,Qt,Zg,Er,Oc,zk,Kk,em,Fk,tm,wr,Rc,sm,Uk,Jk,Pc,Wk,Tv,Le,Zt,am,br,Yk,nm,Vk,jv,Sc,Xk,kv,es,Av,Tr,Qk,jr,Zk,Dv,Nc,e6,Ov,ts,Rv,xc,t6,Pv,ss,rm,kr,Ic,s6,a6,om,n6,he,Ar,Dr,lm,r6,o6,l6,Hc,i6,u6,Or,Bc,im,c6,p6,Cc,f6,h6,Rr,Lc,d6,g6,as,m6,um,$6,q6,_6,Pr,Gc,v6,y6,ns,E6,cm,w6,b6,Sv,Mc,T6,Nv,rs,xv,os,pm,Sr,zc,j6,k6,fm,A6,Nr,xr,Kc,hm,D6,O6,Fc,R6,P6,Ir,Uc,dm,S6,N6,Jc,x6,Iv,Ge,ls,gm,Hr,I6,mm,H6,Hv,Wc,B6,Bv,is,Cv,Br,C6,Cr,L6,Lv,Yc,G6,Gv,us,Mv,Vc,M6,zv,cs,$m,Lr,Xc,z6,K6,qm,F6,I,Gr,Mr,_m,U6,J6,W6,Qc,Y6,V6,zr,Zc,vm,X6,Q6,ep,Z6,e7,Kr,tp,t7,s7,Ee,a7,ym,n7,r7,Em,o7,l7,i7,Fr,sp,u7,c7,se,p7,wm,f7,h7,bm,d7,g7,Tm,m7,$7,q7,Ur,ap,_7,v7,ae,y7,jm,E7,w7,km,b7,T7,Am,j7,k7,A7,Jr,np,D7,O7,ps,R7,Dm,P7,S7,N7,Wr,rp,x7,I7,we,H7,Om,B7,C7,Rm,L7,G7,M7,Yr,op,z7,K7,be,F7,Pm,U7,J7,Sm,W7,Y7,V7,Vr,lp,X7,Q7,Te,Z7,Nm,e9,t9,xm,s9,a9,n9,Xr,ip,r9,o9,fs,l9,Im,i9,u9,c9,Qr,up,p9,f9,hs,h9,Hm,d9,g9,m9,Zr,cp,Bm,$9,q9,pp,_9,v9,eo,fp,y9,E9,ds,w9,Cm,b9,T9,j9,to,hp,k9,A9,gs,D9,Lm,O9,R9,Kv,dp,P9,Fv,ms,Uv,$s,Gm,so,gp,S9,N9,Mm,x9,zm,ao,mp,Km,I9,H9,$p,B9,Jv,Me,qs,Fm,no,C9,Um,L9,Wv,_s,G9,qp,M9,z9,Yv,ze,vs,Jm,ro,K9,Wm,F9,Vv,_p,U9,Xv,ys,Qv,Ke,J9,oo,W9,Y9,lo,V9,Zv,vp,X9,ey,Es,ty,yp,Q9,sy,ws,Ym,io,Ep,Z9,e8,Vm,t8,Y,uo,co,Xm,s8,a8,n8,wp,r8,o8,po,bp,Qm,l8,i8,Tp,u8,c8,fo,jp,p8,f8,N,h8,Zm,d8,g8,m8,$8,e$,q8,_8,v8,y8,t$,E8,w8,b8,T8,s$,j8,k8,a$,A8,D8,O8,R8,n$,P8,S8,r$,N8,x8,I8,H8,o$,B8,C8,l$,L8,G8,M8,ho,kp,i$,z8,K8,Ap,F8,U8,go,Dp,J8,W8,bs,Y8,u$,V8,X8,Q8,mo,Op,Z8,eA,Ts,tA,c$,sA,aA,ay,Rp,nA,ny,js,ry,ks,p$,$o,Pp,rA,oA,f$,lA,Q,qo,Sp,h$,iA,uA,Np,cA,pA,_o,xp,d$,fA,hA,Ip,dA,gA,vo,Hp,g$,mA,$A,Bp,qA,_A,yo,Cp,m$,vA,yA,As,EA,$$,wA,bA,TA,Eo,Lp,q$,jA,kA,Ds,AA,_$,DA,OA,oy,Fe,Os,v$,wo,RA,y$,PA,ly,bo,SA,Gp,NA,iy,Ue,Rs,E$,To,xA,w$,IA,uy,Mp,HA,cy,Ps,py,jo,BA,ko,CA,fy,zp,LA,hy,Ss,dy,Kp,GA,gy,Ns,b$,Ao,Fp,MA,zA,T$,KA,de,Do,Oo,j$,FA,UA,JA,Up,WA,YA,Ro,Jp,k$,VA,XA,Wp,QA,ZA,Po,Yp,eD,tD,xs,sD,A$,aD,nD,rD,So,Vp,oD,lD,Is,iD,D$,uD,cD,my,Xp,pD,$y,Hs,O$,No,Qp,fD,hD,R$,dD,P$,xo,Zp,S$,gD,mD,ef,$D,qy,Je,Bs,N$,Io,qD,x$,_D,_y,tf,vD,vy,Cs,yy,Ho,yD,Bo,ED,Ey,sf,wD,wy,Ls,by,af,bD,Ty,Gs,I$,Co,nf,TD,jD,H$,kD,F,Lo,Go,B$,AD,DD,OD,rf,RD,PD,Mo,zo,C$,SD,ND,xD,of,ID,HD,Ko,lf,BD,CD,je,LD,L$,GD,MD,G$,zD,KD,FD,Fo,uf,UD,JD,Ms,WD,M$,YD,VD,XD,Uo,cf,z$,QD,ZD,pf,eO,tO,Jo,ff,sO,aO,zs,nO,K$,rO,oO,lO,Wo,hf,iO,uO,Ks,cO,F$,pO,fO,jy,df,hO,ky,gf,dO,Ay,Fs,Dy,Us,U$,Yo,mf,gO,mO,J$,$O,We,Vo,$f,W$,qO,_O,qf,vO,yO,Xo,_f,Y$,EO,wO,vf,bO,TO,Qo,yf,V$,jO,kO,Js,AO,X$,DO,OO,Oy,Ye,Ws,Q$,Zo,RO,Z$,PO,Ry,Ef,SO,Py,Ys,Sy,el,NO,tl,xO,Ny,wf,IO,xy,Vs,Iy,bf,HO,Hy,Xs,eq,sl,Tf,BO,CO,tq,LO,x,al,nl,sq,GO,MO,zO,aq,KO,rl,jf,FO,UO,kf,JO,WO,ol,Af,YO,VO,Df,XO,QO,ll,Of,ZO,eR,Qs,tR,nq,sR,aR,nR,il,Rf,rq,rR,oR,Pf,lR,iR,ul,Sf,uR,cR,ke,pR,oq,fR,hR,lq,dR,gR,mR,cl,Nf,$R,qR,Ae,_R,iq,vR,yR,uq,ER,wR,bR,pl,xf,TR,jR,De,kR,cq,AR,DR,pq,OR,RR,PR,fl,If,SR,NR,ne,xR,fq,IR,HR,hq,BR,CR,dq,LR,GR,MR,hl,Hf,zR,KR,re,FR,gq,UR,JR,mq,WR,YR,$q,VR,XR,QR,dl,Bf,ZR,eP,Zs,tP,qq,sP,aP,nP,gl,Cf,rP,oP,ea,lP,_q,iP,uP,cP,ml,Lf,vq,pP,fP,Gf,hP,dP,$l,Mf,gP,mP,ta,$P,yq,qP,_P,vP,ql,zf,yP,EP,sa,wP,Eq,bP,TP,By,Kf,jP,Cy,aa,wq,_l,Ff,kP,AP,bq,DP,ge,vl,Uf,Tq,OP,RP,Jf,PP,SP,yl,Wf,jq,NP,xP,Yf,IP,HP,El,Vf,BP,CP,Xf,LP,GP,wl,Qf,MP,zP,Zf,KP,Ly,Ve,na,kq,bl,FP,Aq,UP,Gy,eh,JP,My,ra,zy,Xe,WP,Tl,YP,VP,jl,XP,Ky,th,QP,Fy,oa,Dq,kl,sh,ZP,eS,Oq,tS,me,Al,Dl,Rq,sS,aS,nS,ah,rS,oS,Ol,nh,Pq,lS,iS,rh,uS,cS,Rl,oh,pS,fS,la,hS,Sq,dS,gS,mS,Pl,lh,$S,qS,ia,_S,Nq,vS,yS,Uy,ih,ES,Jy,ua,xq,Sl,uh,wS,bS,Iq,TS,Hq,Nl,ch,Bq,jS,kS,ph,AS,Wy,fh,DS,Yy,Qe,ca,Cq,xl,OS,Lq,RS,Vy,Ze,pa,Gq,Il,PS,Mq,SS,Xy,hh,NS,Qy,fa,Zy,ha,e2,$e,xS,Hl,IS,HS,Bl,BS,CS,Cl,LS,t2,dh,GS,s2,da,a2,gh,MS,n2,ga,zq,Ll,mh,zS,KS,Kq,FS,Fq,Gl,Ml,Uq,US,JS,WS,$h,YS,r2,qh,VS,o2,_h,XS,l2,ma,i2,$a,Jq,zl,vh,QS,ZS,Wq,eN,Yq,Kl,yh,Vq,tN,sN,Eh,aN,u2,et,qa,Xq,Fl,nN,Qq,rN,c2,wh,oN,p2,_a,f2,tt,lN,Ul,iN,uN,Jl,cN,h2,bh,pN,d2,va,g2,Th,fN,m2,ya,Zq,Wl,jh,hN,dN,e_,gN,t_,Yl,Vl,s_,mN,$N,qN,kh,_N,$2,Ah,vN,q2,Ea,_2,wa,a_,Xl,Dh,yN,EN,n_,wN,Ql,Zl,Oh,r_,bN,TN,Rh,jN,kN,ei,Ph,o_,AN,DN,Sh,ON,v2,st,ba,l_,ti,RN,i_,PN,y2,at,Ta,u_,si,SN,c_,NN,E2,Nh,xN,w2,ja,b2,ai,IN,ni,HN,T2,xh,BN,j2,ka,k2,Aa,CN,ri,LN,GN,A2,Da,p_,oi,Ih,MN,zN,f_,KN,h_,li,ii,d_,FN,UN,JN,Hh,WN,D2,Bh,YN,O2,Oa,R2,Ra,g_,ui,Ch,VN,XN,m_,QN,ci,pi,Lh,$_,ZN,ex,Gh,tx,sx,fi,Mh,q_,ax,nx,zh,rx,P2,nt,Pa,__,hi,ox,v_,lx,S2,Kh,ix,N2,Sa,x2,di,ux,gi,cx,I2,Fh,px,H2,Na,B2,xa,fx,mi,hx,dx,C2,Ia,y_,$i,Uh,gx,mx,E_,$x,w_,qi,_i,b_,qx,_x,vx,Jh,yx,L2,Wh,Ex,G2,Ha,M2,Ba,T_,vi,Yh,wx,bx,j_,Tx,rt,yi,Vh,k_,jx,kx,Xh,Ax,Dx,Ei,Qh,A_,Ox,Rx,Zh,Px,Sx,wi,ed,D_,Nx,xx,td,Ix,z2,ot,Ca,O_,bi,Hx,R_,Bx,K2,sd,Cx,F2,La,U2,Ti,Lx,ji,Gx,J2,ad,Mx,W2,Ga,Y2,Ma,zx,ki,Kx,Fx,V2,za,P_,Ai,nd,Ux,Jx,S_,Wx,N_,Di,Oi,x_,Yx,Vx,Xx,rd,Qx,X2,od,Zx,Q2,Ka,Z2,Fa,I_,Ri,ld,eI,tI,H_,sI,lt,Pi,id,B_,aI,nI,ud,rI,oI,Si,cd,C_,lI,iI,pd,uI,cI,Ni,fd,L_,pI,fI,hd,hI,eE;return k=new z({}),X=new z({}),on=new z({}),ln=new z({}),ht=new K({props:{$$slots:{default:[fJ]},$$scope:{ctx:q}}}),dt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[qJ],js:[mJ],python:[dJ]},$$scope:{ctx:q}}}),qt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[vJ]},$$scope:{ctx:q}}}),En=new z({}),Et=new K({props:{$$slots:{default:[yJ]},$$scope:{ctx:q}}}),wt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[kJ],js:[TJ],python:[wJ]},$$scope:{ctx:q}}}),Gn=new z({}),Rt=new K({props:{$$slots:{default:[AJ]},$$scope:{ctx:q}}}),Pt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[NJ],js:[PJ],python:[OJ]},$$scope:{ctx:q}}}),St=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[IJ]},$$scope:{ctx:q}}}),Yn=new z({}),Bt=new K({props:{$$slots:{default:[HJ]},$$scope:{ctx:q}}}),Ct=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[zJ],js:[GJ],python:[CJ]},$$scope:{ctx:q}}}),zt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[FJ]},$$scope:{ctx:q}}}),pr=new z({}),Ut=new K({props:{$$slots:{default:[UJ]},$$scope:{ctx:q}}}),Jt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[QJ],js:[VJ],python:[WJ]},$$scope:{ctx:q}}}),Xt=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[eW]},$$scope:{ctx:q}}}),br=new z({}),es=new K({props:{$$slots:{default:[tW]},$$scope:{ctx:q}}}),ts=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[lW],js:[rW],python:[aW]},$$scope:{ctx:q}}}),rs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[uW]},$$scope:{ctx:q}}}),Hr=new z({}),is=new K({props:{$$slots:{default:[cW]},$$scope:{ctx:q}}}),us=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[mW],js:[dW],python:[fW]},$$scope:{ctx:q}}}),ms=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[qW]},$$scope:{ctx:q}}}),no=new z({}),ro=new z({}),ys=new K({props:{$$slots:{default:[_W]},$$scope:{ctx:q}}}),Es=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[TW],js:[wW],python:[yW]},$$scope:{ctx:q}}}),js=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[kW]},$$scope:{ctx:q}}}),wo=new z({}),To=new z({}),Ps=new K({props:{$$slots:{default:[AW]},$$scope:{ctx:q}}}),Ss=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[NW],js:[PW],python:[OW]},$$scope:{ctx:q}}}),Io=new z({}),Cs=new K({props:{$$slots:{default:[xW]},$$scope:{ctx:q}}}),Ls=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[GW],js:[CW],python:[HW]},$$scope:{ctx:q}}}),Fs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[zW]},$$scope:{ctx:q}}}),Zo=new z({}),Ys=new K({props:{$$slots:{default:[KW]},$$scope:{ctx:q}}}),Vs=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[VW],js:[WW],python:[UW]},$$scope:{ctx:q}}}),bl=new z({}),ra=new K({props:{$$slots:{default:[XW]},$$scope:{ctx:q}}}),xl=new z({}),Il=new z({}),fa=new K({props:{$$slots:{default:[QW]},$$scope:{ctx:q}}}),ha=new K({props:{$$slots:{default:[ZW]},$$scope:{ctx:q}}}),da=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[rY],js:[aY],python:[tY]},$$scope:{ctx:q}}}),ma=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[lY]},$$scope:{ctx:q}}}),Fl=new z({}),_a=new K({props:{$$slots:{default:[iY]},$$scope:{ctx:q}}}),va=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[dY],js:[fY],python:[cY]},$$scope:{ctx:q}}}),Ea=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[mY]},$$scope:{ctx:q}}}),ti=new z({}),si=new z({}),ja=new K({props:{$$slots:{default:[$Y]},$$scope:{ctx:q}}}),ka=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[wY],js:[yY],python:[_Y]},$$scope:{ctx:q}}}),Oa=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[TY]},$$scope:{ctx:q}}}),hi=new z({}),Sa=new K({props:{$$slots:{default:[jY]},$$scope:{ctx:q}}}),Na=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[PY],js:[OY],python:[AY]},$$scope:{ctx:q}}}),Ha=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[NY]},$$scope:{ctx:q}}}),bi=new z({}),La=new K({props:{$$slots:{default:[xY]},$$scope:{ctx:q}}}),Ga=new C({props:{python:!0,js:!0,curl:!0,$$slots:{curl:[GY],js:[CY],python:[HY]},$$scope:{ctx:q}}}),Ka=new C({props:{python:!0,js:!0,curl:!0,$$slots:{python:[zY]},$$scope:{ctx:q}}}),{c(){n=r("meta"),i=f(),t=r("h1"),d=r("a"),$=r("span"),_(k.$$.fragment),A=f(),j=r("span"),T=u("Detailed parameters"),S=f(),D=r("h2"),Z=r("a"),Re=r("span"),_(X.$$.fragment),U=f(),it=r("span"),Yi=u("Which task is used by this model ?"),rn=f(),Pe=r("p"),x3=u(`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),E1=f(),Vi=r("p"),I3=u("The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),w1=f(),ut=r("img"),b1=f(),ct=r("img"),T1=f(),Se=r("h2"),pt=r("a"),Od=r("span"),_(on.$$.fragment),H3=f(),Rd=r("span"),B3=u("Natural Language Processing"),j1=f(),Ne=r("h3"),ft=r("a"),Pd=r("span"),_(ln.$$.fragment),C3=f(),Sd=r("span"),L3=u("Fill Mask task"),k1=f(),Xi=r("p"),G3=u(`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),A1=f(),_(ht.$$.fragment),D1=f(),un=r("p"),M3=u("Available with: "),cn=r("a"),z3=u("\u{1F917} Transformers"),O1=f(),Qi=r("p"),K3=u("Example:"),R1=f(),_(dt.$$.fragment),P1=f(),Zi=r("p"),F3=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),S1=f(),gt=r("table"),Nd=r("thead"),pn=r("tr"),eu=r("th"),U3=u("All parameters"),J3=f(),xd=r("th"),W3=f(),ue=r("tbody"),fn=r("tr"),hn=r("td"),Id=r("strong"),Y3=u("inputs"),V3=u(" (required):"),X3=f(),tu=r("td"),Q3=u("a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),Z3=f(),dn=r("tr"),su=r("td"),Hd=r("strong"),eT=u("options"),tT=f(),au=r("td"),sT=u("a dict containing the following keys:"),aT=f(),gn=r("tr"),nu=r("td"),nT=u("use_cache"),rT=f(),mt=r("td"),oT=u("(Default: "),Bd=r("code"),lT=u("true"),iT=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),uT=f(),mn=r("tr"),ru=r("td"),cT=u("wait_for_model"),pT=f(),$t=r("td"),fT=u("(Default: "),Cd=r("code"),hT=u("false"),dT=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),N1=f(),ou=r("p"),gT=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),x1=f(),_(qt.$$.fragment),I1=f(),_t=r("table"),Ld=r("thead"),$n=r("tr"),lu=r("th"),mT=u("Returned values"),$T=f(),Gd=r("th"),qT=f(),ce=r("tbody"),qn=r("tr"),iu=r("td"),Md=r("strong"),_T=u("sequence"),vT=f(),uu=r("td"),yT=u("The actual sequence of tokens that ran against the model (may contain special tokens)"),ET=f(),_n=r("tr"),cu=r("td"),zd=r("strong"),wT=u("score"),bT=f(),pu=r("td"),TT=u("The probability for this token."),jT=f(),vn=r("tr"),fu=r("td"),Kd=r("strong"),kT=u("token"),AT=f(),hu=r("td"),DT=u("The id of the token"),OT=f(),yn=r("tr"),du=r("td"),Fd=r("strong"),RT=u("token_str"),PT=f(),gu=r("td"),ST=u("The string representation of the token"),H1=f(),xe=r("h3"),vt=r("a"),Ud=r("span"),_(En.$$.fragment),NT=f(),Jd=r("span"),xT=u("Summarization task"),B1=f(),yt=r("p"),IT=u(`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),mu=r("a"),HT=u("api-enterprise@huggingface.co"),BT=u(">"),C1=f(),_(Et.$$.fragment),L1=f(),wn=r("p"),CT=u("Available with: "),bn=r("a"),LT=u("\u{1F917} Transformers"),G1=f(),$u=r("p"),GT=u("Example:"),M1=f(),_(wt.$$.fragment),z1=f(),qu=r("p"),MT=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),K1=f(),bt=r("table"),Wd=r("thead"),Tn=r("tr"),_u=r("th"),zT=u("All parameters"),KT=f(),Yd=r("th"),FT=f(),G=r("tbody"),jn=r("tr"),kn=r("td"),Vd=r("strong"),UT=u("inputs"),JT=u(" (required)"),WT=f(),vu=r("td"),YT=u("a string to be summarized"),VT=f(),An=r("tr"),yu=r("td"),Xd=r("strong"),XT=u("parameters"),QT=f(),Eu=r("td"),ZT=u("a dict containing the following keys:"),ej=f(),Dn=r("tr"),wu=r("td"),tj=u("min_length"),sj=f(),_e=r("td"),aj=u("(Default: "),Qd=r("code"),nj=u("None"),rj=u("). Integer to define the minimum length "),Zd=r("strong"),oj=u("in tokens"),lj=u(" of the output summary."),ij=f(),On=r("tr"),bu=r("td"),uj=u("max_length"),cj=f(),ve=r("td"),pj=u("(Default: "),eg=r("code"),fj=u("None"),hj=u("). Integer to define the maximum length "),tg=r("strong"),dj=u("in tokens"),gj=u(" of the output summary."),mj=f(),Rn=r("tr"),Tu=r("td"),$j=u("top_k"),qj=f(),ye=r("td"),_j=u("(Default: "),sg=r("code"),vj=u("None"),yj=u("). Integer to define the top tokens considered within the "),ag=r("code"),Ej=u("sample"),wj=u(" operation to create new text."),bj=f(),Pn=r("tr"),ju=r("td"),Tj=u("top_p"),jj=f(),ee=r("td"),kj=u("(Default: "),ng=r("code"),Aj=u("None"),Dj=u("). Float to define the tokens that are within the "),rg=r("code"),Oj=u("sample"),Rj=u(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),og=r("code"),Pj=u("top_p"),Sj=u("."),Nj=f(),Sn=r("tr"),ku=r("td"),xj=u("temperature"),Ij=f(),te=r("td"),Hj=u("(Default: "),lg=r("code"),Bj=u("1.0"),Cj=u("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),ig=r("code"),Lj=u("0"),Gj=u(" means always take the highest score, "),ug=r("code"),Mj=u("100.0"),zj=u(" is getting closer to uniform probability."),Kj=f(),Nn=r("tr"),Au=r("td"),Fj=u("repetition_penalty"),Uj=f(),Tt=r("td"),Jj=u("(Default: "),cg=r("code"),Wj=u("None"),Yj=u("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Vj=f(),xn=r("tr"),Du=r("td"),Xj=u("max_time"),Qj=f(),jt=r("td"),Zj=u("(Default: "),pg=r("code"),e4=u("None"),t4=u("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),s4=f(),In=r("tr"),Ou=r("td"),fg=r("strong"),a4=u("options"),n4=f(),Ru=r("td"),r4=u("a dict containing the following keys:"),o4=f(),Hn=r("tr"),Pu=r("td"),l4=u("use_cache"),i4=f(),kt=r("td"),u4=u("(Default: "),hg=r("code"),c4=u("true"),p4=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),f4=f(),Bn=r("tr"),Su=r("td"),h4=u("wait_for_model"),d4=f(),At=r("td"),g4=u("(Default: "),dg=r("code"),m4=u("false"),$4=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),F1=f(),Nu=r("p"),q4=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),U1=f(),Dt=r("table"),gg=r("thead"),Cn=r("tr"),xu=r("th"),_4=u("Returned values"),v4=f(),mg=r("th"),y4=f(),$g=r("tbody"),Ln=r("tr"),Iu=r("td"),qg=r("strong"),E4=u("summarization_text"),w4=f(),Hu=r("td"),b4=u("The string after translation"),J1=f(),Ie=r("h3"),Ot=r("a"),_g=r("span"),_(Gn.$$.fragment),T4=f(),vg=r("span"),j4=u("Question Answering task"),W1=f(),Bu=r("p"),k4=u("Want to have a nice know-it-all bot that can answer any question?"),Y1=f(),_(Rt.$$.fragment),V1=f(),He=r("p"),A4=u("Available with: "),Mn=r("a"),D4=u("\u{1F917}Transformers"),O4=u(` and
`),zn=r("a"),R4=u("AllenNLP"),X1=f(),Cu=r("p"),P4=u("Example:"),Q1=f(),_(Pt.$$.fragment),Z1=f(),Lu=r("p"),S4=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),ev=f(),Gu=r("p"),N4=u("Return value is a dict."),tv=f(),_(St.$$.fragment),sv=f(),Nt=r("table"),yg=r("thead"),Kn=r("tr"),Mu=r("th"),x4=u("Returned values"),I4=f(),Eg=r("th"),H4=f(),pe=r("tbody"),Fn=r("tr"),zu=r("td"),wg=r("strong"),B4=u("answer"),C4=f(),Ku=r("td"),L4=u("A string that\u2019s the answer within the text."),G4=f(),Un=r("tr"),Fu=r("td"),bg=r("strong"),M4=u("score"),z4=f(),Uu=r("td"),K4=u("A float that represents how likely that the answer is correct"),F4=f(),Jn=r("tr"),Ju=r("td"),Tg=r("strong"),U4=u("start"),J4=f(),xt=r("td"),W4=u("The index (string wise) of the start of the answer within "),jg=r("code"),Y4=u("context"),V4=u("."),X4=f(),Wn=r("tr"),Wu=r("td"),kg=r("strong"),Q4=u("stop"),Z4=f(),It=r("td"),e5=u("The index (string wise) of the stop of the answer within "),Ag=r("code"),t5=u("context"),s5=u("."),av=f(),Be=r("h3"),Ht=r("a"),Dg=r("span"),_(Yn.$$.fragment),a5=f(),Og=r("span"),n5=u("Table Question Answering task"),nv=f(),Yu=r("p"),r5=u(`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),rv=f(),_(Bt.$$.fragment),ov=f(),Vn=r("p"),o5=u("Available with: "),Xn=r("a"),l5=u("\u{1F917} Transformers"),lv=f(),Vu=r("p"),i5=u("Example:"),iv=f(),_(Ct.$$.fragment),uv=f(),Xu=r("p"),u5=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),cv=f(),Lt=r("table"),Rg=r("thead"),Qn=r("tr"),Qu=r("th"),c5=u("All parameters"),p5=f(),Pg=r("th"),f5=f(),J=r("tbody"),Zn=r("tr"),er=r("td"),Sg=r("strong"),h5=u("inputs"),d5=u(" (required)"),g5=f(),Ng=r("td"),m5=f(),tr=r("tr"),Zu=r("td"),$5=u("query (required)"),q5=f(),ec=r("td"),_5=u("The query in plain text that you want to ask the table"),v5=f(),sr=r("tr"),tc=r("td"),y5=u("table (required)"),E5=f(),sc=r("td"),w5=u("A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),b5=f(),ar=r("tr"),ac=r("td"),xg=r("strong"),T5=u("options"),j5=f(),nc=r("td"),k5=u("a dict containing the following keys:"),A5=f(),nr=r("tr"),rc=r("td"),D5=u("use_cache"),O5=f(),Gt=r("td"),R5=u("(Default: "),Ig=r("code"),P5=u("true"),S5=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),N5=f(),rr=r("tr"),oc=r("td"),x5=u("wait_for_model"),I5=f(),Mt=r("td"),H5=u("(Default: "),Hg=r("code"),B5=u("false"),C5=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),pv=f(),lc=r("p"),L5=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),fv=f(),_(zt.$$.fragment),hv=f(),Kt=r("table"),Bg=r("thead"),or=r("tr"),ic=r("th"),G5=u("Returned values"),M5=f(),Cg=r("th"),z5=f(),fe=r("tbody"),lr=r("tr"),uc=r("td"),Lg=r("strong"),K5=u("answer"),F5=f(),cc=r("td"),U5=u("The plaintext answer"),J5=f(),ir=r("tr"),pc=r("td"),Gg=r("strong"),W5=u("coordinates"),Y5=f(),fc=r("td"),V5=u("a list of coordinates of the cells referenced in the answer"),X5=f(),ur=r("tr"),hc=r("td"),Mg=r("strong"),Q5=u("cells"),Z5=f(),dc=r("td"),ek=u("a list of coordinates of the cells contents"),tk=f(),cr=r("tr"),gc=r("td"),zg=r("strong"),sk=u("aggregator"),ak=f(),mc=r("td"),nk=u("The aggregator used to get the answer"),dv=f(),Ce=r("h3"),Ft=r("a"),Kg=r("span"),_(pr.$$.fragment),rk=f(),Fg=r("span"),ok=u("Sentence Similarity task"),gv=f(),$c=r("p"),lk=u("Calculate the semantic similarity between one text and a list of other sentences by comparing their embeddings."),mv=f(),_(Ut.$$.fragment),$v=f(),fr=r("p"),ik=u("Available with: "),hr=r("a"),uk=u("Sentence Transformers"),qv=f(),qc=r("p"),ck=u("Example:"),_v=f(),_(Jt.$$.fragment),vv=f(),_c=r("p"),pk=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),yv=f(),Wt=r("table"),Ug=r("thead"),dr=r("tr"),vc=r("th"),fk=u("All parameters"),hk=f(),Jg=r("th"),dk=f(),W=r("tbody"),gr=r("tr"),mr=r("td"),Wg=r("strong"),gk=u("inputs"),mk=u(" (required)"),$k=f(),Yg=r("td"),qk=f(),$r=r("tr"),yc=r("td"),_k=u("source_sentence (required)"),vk=f(),Ec=r("td"),yk=u("The string that you wish to compare the other strings with. This can be a phrase, sentence, or longer passage, depending on the model being used."),Ek=f(),qr=r("tr"),wc=r("td"),wk=u("sentences (required)"),bk=f(),bc=r("td"),Tk=u("A list of strings which will be compared against the source_sentence."),jk=f(),_r=r("tr"),Tc=r("td"),Vg=r("strong"),kk=u("options"),Ak=f(),jc=r("td"),Dk=u("a dict containing the following keys:"),Ok=f(),vr=r("tr"),kc=r("td"),Rk=u("use_cache"),Pk=f(),Yt=r("td"),Sk=u("(Default: "),Xg=r("code"),Nk=u("true"),xk=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ik=f(),yr=r("tr"),Ac=r("td"),Hk=u("wait_for_model"),Bk=f(),Vt=r("td"),Ck=u("(Default: "),Qg=r("code"),Lk=u("false"),Gk=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ev=f(),Dc=r("p"),Mk=u("The return value is a list of similarity scores, given as floats."),wv=f(),_(Xt.$$.fragment),bv=f(),Qt=r("table"),Zg=r("thead"),Er=r("tr"),Oc=r("th"),zk=u("Returned values"),Kk=f(),em=r("th"),Fk=f(),tm=r("tbody"),wr=r("tr"),Rc=r("td"),sm=r("strong"),Uk=u("Scores"),Jk=f(),Pc=r("td"),Wk=u("The associated similarity score for each of the given strings"),Tv=f(),Le=r("h3"),Zt=r("a"),am=r("span"),_(br.$$.fragment),Yk=f(),nm=r("span"),Vk=u("Text Classification task"),jv=f(),Sc=r("p"),Xk=u(`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),kv=f(),_(es.$$.fragment),Av=f(),Tr=r("p"),Qk=u("Available with: "),jr=r("a"),Zk=u("\u{1F917} Transformers"),Dv=f(),Nc=r("p"),e6=u("Example:"),Ov=f(),_(ts.$$.fragment),Rv=f(),xc=r("p"),t6=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Pv=f(),ss=r("table"),rm=r("thead"),kr=r("tr"),Ic=r("th"),s6=u("All parameters"),a6=f(),om=r("th"),n6=f(),he=r("tbody"),Ar=r("tr"),Dr=r("td"),lm=r("strong"),r6=u("inputs"),o6=u(" (required)"),l6=f(),Hc=r("td"),i6=u("a string to be classified"),u6=f(),Or=r("tr"),Bc=r("td"),im=r("strong"),c6=u("options"),p6=f(),Cc=r("td"),f6=u("a dict containing the following keys:"),h6=f(),Rr=r("tr"),Lc=r("td"),d6=u("use_cache"),g6=f(),as=r("td"),m6=u("(Default: "),um=r("code"),$6=u("true"),q6=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),_6=f(),Pr=r("tr"),Gc=r("td"),v6=u("wait_for_model"),y6=f(),ns=r("td"),E6=u("(Default: "),cm=r("code"),w6=u("false"),b6=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Sv=f(),Mc=r("p"),T6=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),Nv=f(),_(rs.$$.fragment),xv=f(),os=r("table"),pm=r("thead"),Sr=r("tr"),zc=r("th"),j6=u("Returned values"),k6=f(),fm=r("th"),A6=f(),Nr=r("tbody"),xr=r("tr"),Kc=r("td"),hm=r("strong"),D6=u("label"),O6=f(),Fc=r("td"),R6=u("The label for the class (model specific)"),P6=f(),Ir=r("tr"),Uc=r("td"),dm=r("strong"),S6=u("score"),N6=f(),Jc=r("td"),x6=u("A floats that represents how likely is that the text belongs the this class."),Iv=f(),Ge=r("h3"),ls=r("a"),gm=r("span"),_(Hr.$$.fragment),I6=f(),mm=r("span"),H6=u("Text Generation task"),Hv=f(),Wc=r("p"),B6=u("Use to continue text from a prompt. This is a very generic task."),Bv=f(),_(is.$$.fragment),Cv=f(),Br=r("p"),C6=u("Available with: "),Cr=r("a"),L6=u("\u{1F917} Transformers"),Lv=f(),Yc=r("p"),G6=u("Example:"),Gv=f(),_(us.$$.fragment),Mv=f(),Vc=r("p"),M6=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),zv=f(),cs=r("table"),$m=r("thead"),Lr=r("tr"),Xc=r("th"),z6=u("All parameters"),K6=f(),qm=r("th"),F6=f(),I=r("tbody"),Gr=r("tr"),Mr=r("td"),_m=r("strong"),U6=u("inputs"),J6=u(" (required):"),W6=f(),Qc=r("td"),Y6=u("a string to be generated from"),V6=f(),zr=r("tr"),Zc=r("td"),vm=r("strong"),X6=u("parameters"),Q6=f(),ep=r("td"),Z6=u("dict containing the following keys:"),e7=f(),Kr=r("tr"),tp=r("td"),t7=u("top_k"),s7=f(),Ee=r("td"),a7=u("(Default: "),ym=r("code"),n7=u("None"),r7=u("). Integer to define the top tokens considered within the "),Em=r("code"),o7=u("sample"),l7=u(" operation to create new text."),i7=f(),Fr=r("tr"),sp=r("td"),u7=u("top_p"),c7=f(),se=r("td"),p7=u("(Default: "),wm=r("code"),f7=u("None"),h7=u("). Float to define the tokens that are within the "),bm=r("code"),d7=u("sample"),g7=u(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Tm=r("code"),m7=u("top_p"),$7=u("."),q7=f(),Ur=r("tr"),ap=r("td"),_7=u("temperature"),v7=f(),ae=r("td"),y7=u("(Default: "),jm=r("code"),E7=u("1.0"),w7=u("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),km=r("code"),b7=u("0"),T7=u(" means always take the highest score, "),Am=r("code"),j7=u("100.0"),k7=u(" is getting closer to uniform probability."),A7=f(),Jr=r("tr"),np=r("td"),D7=u("repetition_penalty"),O7=f(),ps=r("td"),R7=u("(Default: "),Dm=r("code"),P7=u("None"),S7=u("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),N7=f(),Wr=r("tr"),rp=r("td"),x7=u("max_new_tokens"),I7=f(),we=r("td"),H7=u("(Default: "),Om=r("code"),B7=u("None"),C7=u("). Int (0-250). The amount of new tokens to be generated, this does "),Rm=r("strong"),L7=u("not"),G7=u(" include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),M7=f(),Yr=r("tr"),op=r("td"),z7=u("max_time"),K7=f(),be=r("td"),F7=u("(Default: "),Pm=r("code"),U7=u("None"),J7=u("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),Sm=r("code"),W7=u("max_new_tokens"),Y7=u(" for best results."),V7=f(),Vr=r("tr"),lp=r("td"),X7=u("return_full_text"),Q7=f(),Te=r("td"),Z7=u("(Default: "),Nm=r("code"),e9=u("True"),t9=u("). Bool. If set to False, the return results will "),xm=r("strong"),s9=u("not"),a9=u(" contain the original query making it easier for prompting."),n9=f(),Xr=r("tr"),ip=r("td"),r9=u("num_return_sequences"),o9=f(),fs=r("td"),l9=u("(Default: "),Im=r("code"),i9=u("1"),u9=u("). Integer. The number of proposition you want to be returned."),c9=f(),Qr=r("tr"),up=r("td"),p9=u("do_sample"),f9=f(),hs=r("td"),h9=u("(Optional: "),Hm=r("code"),d9=u("True"),g9=u("). Bool. Whether or not to use sampling, use greedy decoding otherwise."),m9=f(),Zr=r("tr"),cp=r("td"),Bm=r("strong"),$9=u("options"),q9=f(),pp=r("td"),_9=u("a dict containing the following keys:"),v9=f(),eo=r("tr"),fp=r("td"),y9=u("use_cache"),E9=f(),ds=r("td"),w9=u("(Default: "),Cm=r("code"),b9=u("true"),T9=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),j9=f(),to=r("tr"),hp=r("td"),k9=u("wait_for_model"),A9=f(),gs=r("td"),D9=u("(Default: "),Lm=r("code"),O9=u("false"),R9=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Kv=f(),dp=r("p"),P9=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),Fv=f(),_(ms.$$.fragment),Uv=f(),$s=r("table"),Gm=r("thead"),so=r("tr"),gp=r("th"),S9=u("Returned values"),N9=f(),Mm=r("th"),x9=f(),zm=r("tbody"),ao=r("tr"),mp=r("td"),Km=r("strong"),I9=u("generated_text"),H9=f(),$p=r("td"),B9=u("The continuated string"),Jv=f(),Me=r("h3"),qs=r("a"),Fm=r("span"),_(no.$$.fragment),C9=f(),Um=r("span"),L9=u("Text2Text Generation task"),Wv=f(),_s=r("p"),G9=u("Essentially "),qp=r("a"),M9=u("Text-generation task"),z9=u(`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),Yv=f(),ze=r("h3"),vs=r("a"),Jm=r("span"),_(ro.$$.fragment),K9=f(),Wm=r("span"),F9=u("Token Classification task"),Vv=f(),_p=r("p"),U9=u(`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),Xv=f(),_(ys.$$.fragment),Qv=f(),Ke=r("p"),J9=u("Available with: "),oo=r("a"),W9=u("\u{1F917} Transformers"),Y9=u(`,
`),lo=r("a"),V9=u("Flair"),Zv=f(),vp=r("p"),X9=u("Example:"),ey=f(),_(Es.$$.fragment),ty=f(),yp=r("p"),Q9=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),sy=f(),ws=r("table"),Ym=r("thead"),io=r("tr"),Ep=r("th"),Z9=u("All parameters"),e8=f(),Vm=r("th"),t8=f(),Y=r("tbody"),uo=r("tr"),co=r("td"),Xm=r("strong"),s8=u("inputs"),a8=u(" (required)"),n8=f(),wp=r("td"),r8=u("a string to be classified"),o8=f(),po=r("tr"),bp=r("td"),Qm=r("strong"),l8=u("parameters"),i8=f(),Tp=r("td"),u8=u("a dict containing the following key:"),c8=f(),fo=r("tr"),jp=r("td"),p8=u("aggregation_strategy"),f8=f(),N=r("td"),h8=u("(Default: "),Zm=r("code"),d8=u("simple"),g8=u("). There are several aggregation strategies: "),m8=r("br"),$8=f(),e$=r("code"),q8=u("none"),_8=u(": Every token gets classified without further aggregation. "),v8=r("br"),y8=f(),t$=r("code"),E8=u("simple"),w8=u(": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),b8=r("br"),T8=f(),s$=r("code"),j8=u("first"),k8=u(": Same as the "),a$=r("code"),A8=u("simple"),D8=u(" strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),O8=r("br"),R8=f(),n$=r("code"),P8=u("average"),S8=u(": Same as the "),r$=r("code"),N8=u("simple"),x8=u(" strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),I8=r("br"),H8=f(),o$=r("code"),B8=u("max"),C8=u(": Same as the "),l$=r("code"),L8=u("simple"),G8=u(" strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),M8=f(),ho=r("tr"),kp=r("td"),i$=r("strong"),z8=u("options"),K8=f(),Ap=r("td"),F8=u("a dict containing the following keys:"),U8=f(),go=r("tr"),Dp=r("td"),J8=u("use_cache"),W8=f(),bs=r("td"),Y8=u("(Default: "),u$=r("code"),V8=u("true"),X8=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Q8=f(),mo=r("tr"),Op=r("td"),Z8=u("wait_for_model"),eA=f(),Ts=r("td"),tA=u("(Default: "),c$=r("code"),sA=u("false"),aA=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ay=f(),Rp=r("p"),nA=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),ny=f(),_(js.$$.fragment),ry=f(),ks=r("table"),p$=r("thead"),$o=r("tr"),Pp=r("th"),rA=u("Returned values"),oA=f(),f$=r("th"),lA=f(),Q=r("tbody"),qo=r("tr"),Sp=r("td"),h$=r("strong"),iA=u("entity_group"),uA=f(),Np=r("td"),cA=u("The type for the entity being recognized (model specific)."),pA=f(),_o=r("tr"),xp=r("td"),d$=r("strong"),fA=u("score"),hA=f(),Ip=r("td"),dA=u("How likely the entity was recognized."),gA=f(),vo=r("tr"),Hp=r("td"),g$=r("strong"),mA=u("word"),$A=f(),Bp=r("td"),qA=u("The string that was captured"),_A=f(),yo=r("tr"),Cp=r("td"),m$=r("strong"),vA=u("start"),yA=f(),As=r("td"),EA=u("The offset stringwise where the answer is located. Useful to disambiguate if "),$$=r("code"),wA=u("word"),bA=u(" occurs multiple times."),TA=f(),Eo=r("tr"),Lp=r("td"),q$=r("strong"),jA=u("end"),kA=f(),Ds=r("td"),AA=u("The offset stringwise where the answer is located. Useful to disambiguate if "),_$=r("code"),DA=u("word"),OA=u(" occurs multiple times."),oy=f(),Fe=r("h3"),Os=r("a"),v$=r("span"),_(wo.$$.fragment),RA=f(),y$=r("span"),PA=u("Named Entity Recognition (NER) task"),ly=f(),bo=r("p"),SA=u("See "),Gp=r("a"),NA=u("Token-classification task"),iy=f(),Ue=r("h3"),Rs=r("a"),E$=r("span"),_(To.$$.fragment),xA=f(),w$=r("span"),IA=u("Translation task"),uy=f(),Mp=r("p"),HA=u("This task is well known to translate text from one language to another"),cy=f(),_(Ps.$$.fragment),py=f(),jo=r("p"),BA=u("Available with: "),ko=r("a"),CA=u("\u{1F917} Transformers"),fy=f(),zp=r("p"),LA=u("Example:"),hy=f(),_(Ss.$$.fragment),dy=f(),Kp=r("p"),GA=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),gy=f(),Ns=r("table"),b$=r("thead"),Ao=r("tr"),Fp=r("th"),MA=u("All parameters"),zA=f(),T$=r("th"),KA=f(),de=r("tbody"),Do=r("tr"),Oo=r("td"),j$=r("strong"),FA=u("inputs"),UA=u(" (required)"),JA=f(),Up=r("td"),WA=u("a string to be translated in the original languages"),YA=f(),Ro=r("tr"),Jp=r("td"),k$=r("strong"),VA=u("options"),XA=f(),Wp=r("td"),QA=u("a dict containing the following keys:"),ZA=f(),Po=r("tr"),Yp=r("td"),eD=u("use_cache"),tD=f(),xs=r("td"),sD=u("(Default: "),A$=r("code"),aD=u("true"),nD=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),rD=f(),So=r("tr"),Vp=r("td"),oD=u("wait_for_model"),lD=f(),Is=r("td"),iD=u("(Default: "),D$=r("code"),uD=u("false"),cD=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),my=f(),Xp=r("p"),pD=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),$y=f(),Hs=r("table"),O$=r("thead"),No=r("tr"),Qp=r("th"),fD=u("Returned values"),hD=f(),R$=r("th"),dD=f(),P$=r("tbody"),xo=r("tr"),Zp=r("td"),S$=r("strong"),gD=u("translation_text"),mD=f(),ef=r("td"),$D=u("The string after translation"),qy=f(),Je=r("h3"),Bs=r("a"),N$=r("span"),_(Io.$$.fragment),qD=f(),x$=r("span"),_D=u("Zero-Shot Classification task"),_y=f(),tf=r("p"),vD=u(`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),vy=f(),_(Cs.$$.fragment),yy=f(),Ho=r("p"),yD=u("Available with: "),Bo=r("a"),ED=u("\u{1F917} Transformers"),Ey=f(),sf=r("p"),wD=u("Request:"),wy=f(),_(Ls.$$.fragment),by=f(),af=r("p"),bD=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Ty=f(),Gs=r("table"),I$=r("thead"),Co=r("tr"),nf=r("th"),TD=u("All parameters"),jD=f(),H$=r("th"),kD=f(),F=r("tbody"),Lo=r("tr"),Go=r("td"),B$=r("strong"),AD=u("inputs"),DD=u(" (required)"),OD=f(),rf=r("td"),RD=u("a string or list of strings"),PD=f(),Mo=r("tr"),zo=r("td"),C$=r("strong"),SD=u("parameters"),ND=u(" (required)"),xD=f(),of=r("td"),ID=u("a dict containing the following keys:"),HD=f(),Ko=r("tr"),lf=r("td"),BD=u("candidate_labels (required)"),CD=f(),je=r("td"),LD=u("a list of strings that are potential classes for "),L$=r("code"),GD=u("inputs"),MD=u(". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),G$=r("code"),zD=u("multi_label=True"),KD=u(" and do the scaling on your end. )"),FD=f(),Fo=r("tr"),uf=r("td"),UD=u("multi_label"),JD=f(),Ms=r("td"),WD=u("(Default: "),M$=r("code"),YD=u("false"),VD=u(") Boolean that is set to True if classes can overlap"),XD=f(),Uo=r("tr"),cf=r("td"),z$=r("strong"),QD=u("options"),ZD=f(),pf=r("td"),eO=u("a dict containing the following keys:"),tO=f(),Jo=r("tr"),ff=r("td"),sO=u("use_cache"),aO=f(),zs=r("td"),nO=u("(Default: "),K$=r("code"),rO=u("true"),oO=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),lO=f(),Wo=r("tr"),hf=r("td"),iO=u("wait_for_model"),uO=f(),Ks=r("td"),cO=u("(Default: "),F$=r("code"),pO=u("false"),fO=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),jy=f(),df=r("p"),hO=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),ky=f(),gf=r("p"),dO=u("Response:"),Ay=f(),_(Fs.$$.fragment),Dy=f(),Us=r("table"),U$=r("thead"),Yo=r("tr"),mf=r("th"),gO=u("Returned values"),mO=f(),J$=r("th"),$O=f(),We=r("tbody"),Vo=r("tr"),$f=r("td"),W$=r("strong"),qO=u("sequence"),_O=f(),qf=r("td"),vO=u("The string sent as an input"),yO=f(),Xo=r("tr"),_f=r("td"),Y$=r("strong"),EO=u("labels"),wO=f(),vf=r("td"),bO=u("The list of strings for labels that you sent (in order)"),TO=f(),Qo=r("tr"),yf=r("td"),V$=r("strong"),jO=u("scores"),kO=f(),Js=r("td"),AO=u("a list of floats that correspond the the probability of label, in the same order as "),X$=r("code"),DO=u("labels"),OO=u("."),Oy=f(),Ye=r("h3"),Ws=r("a"),Q$=r("span"),_(Zo.$$.fragment),RO=f(),Z$=r("span"),PO=u("Conversational task"),Ry=f(),Ef=r("p"),SO=u(`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),Py=f(),_(Ys.$$.fragment),Sy=f(),el=r("p"),NO=u("Available with: "),tl=r("a"),xO=u("\u{1F917} Transformers"),Ny=f(),wf=r("p"),IO=u("Example:"),xy=f(),_(Vs.$$.fragment),Iy=f(),bf=r("p"),HO=u(`When sending your request, you should send a JSON encoded payload. Here
are all the options`),Hy=f(),Xs=r("table"),eq=r("thead"),sl=r("tr"),Tf=r("th"),BO=u("All parameters"),CO=f(),tq=r("th"),LO=f(),x=r("tbody"),al=r("tr"),nl=r("td"),sq=r("strong"),GO=u("inputs"),MO=u(" (required)"),zO=f(),aq=r("td"),KO=f(),rl=r("tr"),jf=r("td"),FO=u("text (required)"),UO=f(),kf=r("td"),JO=u("The last input from the user in the conversation."),WO=f(),ol=r("tr"),Af=r("td"),YO=u("generated_responses"),VO=f(),Df=r("td"),XO=u("A list of strings corresponding to the earlier replies from the model."),QO=f(),ll=r("tr"),Of=r("td"),ZO=u("past_user_inputs"),eR=f(),Qs=r("td"),tR=u("A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),nq=r("code"),sR=u("generated_responses"),aR=u("."),nR=f(),il=r("tr"),Rf=r("td"),rq=r("strong"),rR=u("parameters"),oR=f(),Pf=r("td"),lR=u("a dict containing the following keys:"),iR=f(),ul=r("tr"),Sf=r("td"),uR=u("min_length"),cR=f(),ke=r("td"),pR=u("(Default: "),oq=r("code"),fR=u("None"),hR=u("). Integer to define the minimum length "),lq=r("strong"),dR=u("in tokens"),gR=u(" of the output summary."),mR=f(),cl=r("tr"),Nf=r("td"),$R=u("max_length"),qR=f(),Ae=r("td"),_R=u("(Default: "),iq=r("code"),vR=u("None"),yR=u("). Integer to define the maximum length "),uq=r("strong"),ER=u("in tokens"),wR=u(" of the output summary."),bR=f(),pl=r("tr"),xf=r("td"),TR=u("top_k"),jR=f(),De=r("td"),kR=u("(Default: "),cq=r("code"),AR=u("None"),DR=u("). Integer to define the top tokens considered within the "),pq=r("code"),OR=u("sample"),RR=u(" operation to create new text."),PR=f(),fl=r("tr"),If=r("td"),SR=u("top_p"),NR=f(),ne=r("td"),xR=u("(Default: "),fq=r("code"),IR=u("None"),HR=u("). Float to define the tokens that are within the "),hq=r("code"),BR=u("sample"),CR=u(" operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),dq=r("code"),LR=u("top_p"),GR=u("."),MR=f(),hl=r("tr"),Hf=r("td"),zR=u("temperature"),KR=f(),re=r("td"),FR=u("(Default: "),gq=r("code"),UR=u("1.0"),JR=u("). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),mq=r("code"),WR=u("0"),YR=u(" means always take the highest score, "),$q=r("code"),VR=u("100.0"),XR=u(" is getting closer to uniform probability."),QR=f(),dl=r("tr"),Bf=r("td"),ZR=u("repetition_penalty"),eP=f(),Zs=r("td"),tP=u("(Default: "),qq=r("code"),sP=u("None"),aP=u("). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),nP=f(),gl=r("tr"),Cf=r("td"),rP=u("max_time"),oP=f(),ea=r("td"),lP=u("(Default: "),_q=r("code"),iP=u("None"),uP=u("). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),cP=f(),ml=r("tr"),Lf=r("td"),vq=r("strong"),pP=u("options"),fP=f(),Gf=r("td"),hP=u("a dict containing the following keys:"),dP=f(),$l=r("tr"),Mf=r("td"),gP=u("use_cache"),mP=f(),ta=r("td"),$P=u("(Default: "),yq=r("code"),qP=u("true"),_P=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),vP=f(),ql=r("tr"),zf=r("td"),yP=u("wait_for_model"),EP=f(),sa=r("td"),wP=u("(Default: "),Eq=r("code"),bP=u("false"),TP=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),By=f(),Kf=r("p"),jP=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),Cy=f(),aa=r("table"),wq=r("thead"),_l=r("tr"),Ff=r("th"),kP=u("Returned values"),AP=f(),bq=r("th"),DP=f(),ge=r("tbody"),vl=r("tr"),Uf=r("td"),Tq=r("strong"),OP=u("generated_text"),RP=f(),Jf=r("td"),PP=u("The answer of the bot"),SP=f(),yl=r("tr"),Wf=r("td"),jq=r("strong"),NP=u("conversation"),xP=f(),Yf=r("td"),IP=u("A facility dictionnary to send back for the next input (with the new user input addition)."),HP=f(),El=r("tr"),Vf=r("td"),BP=u("past_user_inputs"),CP=f(),Xf=r("td"),LP=u("List of strings. The last inputs from the user in the conversation, <em>after the model has run."),GP=f(),wl=r("tr"),Qf=r("td"),MP=u("generated_responses"),zP=f(),Zf=r("td"),KP=u("List of strings. The last outputs from the model in the conversation, <em>after the model has run."),Ly=f(),Ve=r("h3"),na=r("a"),kq=r("span"),_(bl.$$.fragment),FP=f(),Aq=r("span"),UP=u("Feature Extraction task"),Gy=f(),eh=r("p"),JP=u(`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),My=f(),_(ra.$$.fragment),zy=f(),Xe=r("p"),WP=u("Available with: "),Tl=r("a"),YP=u("\u{1F917} Transformers"),VP=f(),jl=r("a"),XP=u("Sentence-transformers"),Ky=f(),th=r("p"),QP=u("Request:"),Fy=f(),oa=r("table"),Dq=r("thead"),kl=r("tr"),sh=r("th"),ZP=u("All parameters"),eS=f(),Oq=r("th"),tS=f(),me=r("tbody"),Al=r("tr"),Dl=r("td"),Rq=r("strong"),sS=u("inputs"),aS=u(" (required):"),nS=f(),ah=r("td"),rS=u("a string or a list of strings to get the features from."),oS=f(),Ol=r("tr"),nh=r("td"),Pq=r("strong"),lS=u("options"),iS=f(),rh=r("td"),uS=u("a dict containing the following keys:"),cS=f(),Rl=r("tr"),oh=r("td"),pS=u("use_cache"),fS=f(),la=r("td"),hS=u("(Default: "),Sq=r("code"),dS=u("true"),gS=u("). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),mS=f(),Pl=r("tr"),lh=r("td"),$S=u("wait_for_model"),qS=f(),ia=r("td"),_S=u("(Default: "),Nq=r("code"),vS=u("false"),yS=u(") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Uy=f(),ih=r("p"),ES=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),Jy=f(),ua=r("table"),xq=r("thead"),Sl=r("tr"),uh=r("th"),wS=u("Returned values"),bS=f(),Iq=r("th"),TS=f(),Hq=r("tbody"),Nl=r("tr"),ch=r("td"),Bq=r("strong"),jS=u("A list of float (or list of list of floats)"),kS=f(),ph=r("td"),AS=u("The numbers that are the representation features of the input."),Wy=f(),fh=r("small"),DS=u(`Returned values are a list of floats, or a list of list of floats (depending
  on if you sent a string or a list of string, and if the automatic reduction,
  usually mean_pooling for instance was applied for you or not. This should be
  explained on the model's README.`),Yy=f(),Qe=r("h2"),ca=r("a"),Cq=r("span"),_(xl.$$.fragment),OS=f(),Lq=r("span"),RS=u("Audio"),Vy=f(),Ze=r("h3"),pa=r("a"),Gq=r("span"),_(Il.$$.fragment),PS=f(),Mq=r("span"),SS=u("Automatic Speech Recognition task"),Xy=f(),hh=r("p"),NS=u(`This task reads some audio input and outputs the said words within the
audio files.`),Qy=f(),_(fa.$$.fragment),Zy=f(),_(ha.$$.fragment),e2=f(),$e=r("p"),xS=u("Available with: "),Hl=r("a"),IS=u("\u{1F917} Transformers"),HS=f(),Bl=r("a"),BS=u("ESPnet"),CS=u(` and
`),Cl=r("a"),LS=u("SpeechBrain"),t2=f(),dh=r("p"),GS=u("Request:"),s2=f(),_(da.$$.fragment),a2=f(),gh=r("p"),MS=u(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),n2=f(),ga=r("table"),zq=r("thead"),Ll=r("tr"),mh=r("th"),zS=u("All parameters"),KS=f(),Kq=r("th"),FS=f(),Fq=r("tbody"),Gl=r("tr"),Ml=r("td"),Uq=r("strong"),US=u("no parameter"),JS=u(" (required)"),WS=f(),$h=r("td"),YS=u("a binary representation of the audio file. No other parameters are currently allowed."),r2=f(),qh=r("p"),VS=u("Return value is either a dict or a list of dicts if you sent a list of inputs"),o2=f(),_h=r("p"),XS=u("Response:"),l2=f(),_(ma.$$.fragment),i2=f(),$a=r("table"),Jq=r("thead"),zl=r("tr"),vh=r("th"),QS=u("Returned values"),ZS=f(),Wq=r("th"),eN=f(),Yq=r("tbody"),Kl=r("tr"),yh=r("td"),Vq=r("strong"),tN=u("text"),sN=f(),Eh=r("td"),aN=u("The string that was recognized within the audio file."),u2=f(),et=r("h3"),qa=r("a"),Xq=r("span"),_(Fl.$$.fragment),nN=f(),Qq=r("span"),rN=u("Audio Classification task"),c2=f(),wh=r("p"),oN=u("This task reads some audio input and outputs the likelihood of classes."),p2=f(),_(_a.$$.fragment),f2=f(),tt=r("p"),lN=u("Available with: "),Ul=r("a"),iN=u("\u{1F917} Transformers"),uN=f(),Jl=r("a"),cN=u("SpeechBrain"),h2=f(),bh=r("p"),pN=u("Request:"),d2=f(),_(va.$$.fragment),g2=f(),Th=r("p"),fN=u(`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),m2=f(),ya=r("table"),Zq=r("thead"),Wl=r("tr"),jh=r("th"),hN=u("All parameters"),dN=f(),e_=r("th"),gN=f(),t_=r("tbody"),Yl=r("tr"),Vl=r("td"),s_=r("strong"),mN=u("no parameter"),$N=u(" (required)"),qN=f(),kh=r("td"),_N=u("a binary representation of the audio file. No other parameters are currently allowed."),$2=f(),Ah=r("p"),vN=u("Return value is a dict"),q2=f(),_(Ea.$$.fragment),_2=f(),wa=r("table"),a_=r("thead"),Xl=r("tr"),Dh=r("th"),yN=u("Returned values"),EN=f(),n_=r("th"),wN=f(),Ql=r("tbody"),Zl=r("tr"),Oh=r("td"),r_=r("strong"),bN=u("label"),TN=f(),Rh=r("td"),jN=u("The label for the class (model specific)"),kN=f(),ei=r("tr"),Ph=r("td"),o_=r("strong"),AN=u("score"),DN=f(),Sh=r("td"),ON=u("A float that represents how likely it is that the audio file belongs to this class."),v2=f(),st=r("h2"),ba=r("a"),l_=r("span"),_(ti.$$.fragment),RN=f(),i_=r("span"),PN=u("Computer Vision"),y2=f(),at=r("h3"),Ta=r("a"),u_=r("span"),_(si.$$.fragment),SN=f(),c_=r("span"),NN=u("Image Classification task"),E2=f(),Nh=r("p"),xN=u("This task reads some image input and outputs the likelihood of classes."),w2=f(),_(ja.$$.fragment),b2=f(),ai=r("p"),IN=u("Available with: "),ni=r("a"),HN=u("\u{1F917} Transformers"),T2=f(),xh=r("p"),BN=u("Request:"),j2=f(),_(ka.$$.fragment),k2=f(),Aa=r("p"),CN=u(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ri=r("a"),LN=u(`Pillow
supports`),GN=u("."),A2=f(),Da=r("table"),p_=r("thead"),oi=r("tr"),Ih=r("th"),MN=u("All parameters"),zN=f(),f_=r("th"),KN=f(),h_=r("tbody"),li=r("tr"),ii=r("td"),d_=r("strong"),FN=u("no parameter"),UN=u(" (required)"),JN=f(),Hh=r("td"),WN=u("a binary representation of the image file. No other parameters are currently allowed."),D2=f(),Bh=r("p"),YN=u("Return value is a dict"),O2=f(),_(Oa.$$.fragment),R2=f(),Ra=r("table"),g_=r("thead"),ui=r("tr"),Ch=r("th"),VN=u("Returned values"),XN=f(),m_=r("th"),QN=f(),ci=r("tbody"),pi=r("tr"),Lh=r("td"),$_=r("strong"),ZN=u("label"),ex=f(),Gh=r("td"),tx=u("The label for the class (model specific)"),sx=f(),fi=r("tr"),Mh=r("td"),q_=r("strong"),ax=u("score"),nx=f(),zh=r("td"),rx=u("A float that represents how likely it is that the image file belongs to this class."),P2=f(),nt=r("h3"),Pa=r("a"),__=r("span"),_(hi.$$.fragment),ox=f(),v_=r("span"),lx=u("Object Detection task"),S2=f(),Kh=r("p"),ix=u(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),N2=f(),_(Sa.$$.fragment),x2=f(),di=r("p"),ux=u("Available with: "),gi=r("a"),cx=u("\u{1F917} Transformers"),I2=f(),Fh=r("p"),px=u("Request:"),H2=f(),_(Na.$$.fragment),B2=f(),xa=r("p"),fx=u(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),mi=r("a"),hx=u(`Pillow
supports`),dx=u("."),C2=f(),Ia=r("table"),y_=r("thead"),$i=r("tr"),Uh=r("th"),gx=u("All parameters"),mx=f(),E_=r("th"),$x=f(),w_=r("tbody"),qi=r("tr"),_i=r("td"),b_=r("strong"),qx=u("no parameter"),_x=u(" (required)"),vx=f(),Jh=r("td"),yx=u("a binary representation of the image file. No other parameters are currently allowed."),L2=f(),Wh=r("p"),Ex=u("Return value is a dict"),G2=f(),_(Ha.$$.fragment),M2=f(),Ba=r("table"),T_=r("thead"),vi=r("tr"),Yh=r("th"),wx=u("Returned values"),bx=f(),j_=r("th"),Tx=f(),rt=r("tbody"),yi=r("tr"),Vh=r("td"),k_=r("strong"),jx=u("label"),kx=f(),Xh=r("td"),Ax=u("The label for the class (model specific) of a detected object."),Dx=f(),Ei=r("tr"),Qh=r("td"),A_=r("strong"),Ox=u("score"),Rx=f(),Zh=r("td"),Px=u("A float that represents how likely it is that the detected object belongs to the given class."),Sx=f(),wi=r("tr"),ed=r("td"),D_=r("strong"),Nx=u("box"),xx=f(),td=r("td"),Ix=u("A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),z2=f(),ot=r("h3"),Ca=r("a"),O_=r("span"),_(bi.$$.fragment),Hx=f(),R_=r("span"),Bx=u("Image Segmentation task"),K2=f(),sd=r("p"),Cx=u(`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),F2=f(),_(La.$$.fragment),U2=f(),Ti=r("p"),Lx=u("Available with: "),ji=r("a"),Gx=u("\u{1F917} Transformers"),J2=f(),ad=r("p"),Mx=u("Request:"),W2=f(),_(Ga.$$.fragment),Y2=f(),Ma=r("p"),zx=u(`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ki=r("a"),Kx=u(`Pillow
supports`),Fx=u("."),V2=f(),za=r("table"),P_=r("thead"),Ai=r("tr"),nd=r("th"),Ux=u("All parameters"),Jx=f(),S_=r("th"),Wx=f(),N_=r("tbody"),Di=r("tr"),Oi=r("td"),x_=r("strong"),Yx=u("no parameter"),Vx=u(" (required)"),Xx=f(),rd=r("td"),Qx=u("a binary representation of the image file. No other parameters are currently allowed."),X2=f(),od=r("p"),Zx=u("Return value is a dict"),Q2=f(),_(Ka.$$.fragment),Z2=f(),Fa=r("table"),I_=r("thead"),Ri=r("tr"),ld=r("th"),eI=u("Returned values"),tI=f(),H_=r("th"),sI=f(),lt=r("tbody"),Pi=r("tr"),id=r("td"),B_=r("strong"),aI=u("label"),nI=f(),ud=r("td"),rI=u("The label for the class (model specific) of a segment."),oI=f(),Si=r("tr"),cd=r("td"),C_=r("strong"),lI=u("score"),iI=f(),pd=r("td"),uI=u("A float that represents how likely it is that the segment belongs to the given class."),cI=f(),Ni=r("tr"),fd=r("td"),L_=r("strong"),pI=u("mask"),fI=f(),hd=r("td"),hI=u("A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),this.h()},l(a){const g=cJ('[data-svelte="svelte-1phssyn"]',document.head);n=o(g,"META",{name:!0,content:!0}),g.forEach(s),i=h(a),t=o(a,"H1",{class:!0});var xi=l(t);d=o(xi,"A",{id:!0,class:!0,href:!0});var G_=l(d);$=o(G_,"SPAN",{});var M_=l($);v(k.$$.fragment,M_),M_.forEach(s),G_.forEach(s),A=h(xi),j=o(xi,"SPAN",{});var z_=l(j);T=c(z_,"Detailed parameters"),z_.forEach(s),xi.forEach(s),S=h(a),D=o(a,"H2",{class:!0});var Ii=l(D);Z=o(Ii,"A",{id:!0,class:!0,href:!0});var K_=l(Z);Re=o(K_,"SPAN",{});var F_=l(Re);v(X.$$.fragment,F_),F_.forEach(s),K_.forEach(s),U=h(Ii),it=o(Ii,"SPAN",{});var U_=l(it);Yi=c(U_,"Which task is used by this model ?"),U_.forEach(s),Ii.forEach(s),rn=h(a),Pe=o(a,"P",{});var J_=l(Pe);x3=c(J_,`In general the \u{1F917} Hosted API Inference accepts a simple string as an
input. However, more advanced usage depends on the \u201Ctask\u201D that the
model solves.`),J_.forEach(s),E1=h(a),Vi=o(a,"P",{});var W_=l(Vi);I3=c(W_,"The \u201Ctask\u201D of a model is defined here on it\u2019s model page:"),W_.forEach(s),w1=h(a),ut=o(a,"IMG",{class:!0,src:!0,width:!0}),b1=h(a),ct=o(a,"IMG",{class:!0,src:!0,width:!0}),T1=h(a),Se=o(a,"H2",{class:!0});var Hi=l(Se);pt=o(Hi,"A",{id:!0,class:!0,href:!0});var Y_=l(pt);Od=o(Y_,"SPAN",{});var V_=l(Od);v(on.$$.fragment,V_),V_.forEach(s),Y_.forEach(s),H3=h(Hi),Rd=o(Hi,"SPAN",{});var X_=l(Rd);B3=c(X_,"Natural Language Processing"),X_.forEach(s),Hi.forEach(s),j1=h(a),Ne=o(a,"H3",{class:!0});var Bi=l(Ne);ft=o(Bi,"A",{id:!0,class:!0,href:!0});var Q_=l(ft);Pd=o(Q_,"SPAN",{});var Z_=l(Pd);v(ln.$$.fragment,Z_),Z_.forEach(s),Q_.forEach(s),C3=h(Bi),Sd=o(Bi,"SPAN",{});var e1=l(Sd);L3=c(e1,"Fill Mask task"),e1.forEach(s),Bi.forEach(s),k1=h(a),Xi=o(a,"P",{});var t1=l(Xi);G3=c(t1,`Tries to fill in a hole with a missing word (token to be precise).
That\u2019s the base task for BERT models.`),t1.forEach(s),A1=h(a),v(ht.$$.fragment,a),D1=h(a),un=o(a,"P",{});var dd=l(un);M3=c(dd,"Available with: "),cn=o(dd,"A",{href:!0,rel:!0});var s1=l(cn);z3=c(s1,"\u{1F917} Transformers"),s1.forEach(s),dd.forEach(s),O1=h(a),Qi=o(a,"P",{});var a1=l(Qi);K3=c(a1,"Example:"),a1.forEach(s),R1=h(a),v(dt.$$.fragment,a),P1=h(a),Zi=o(a,"P",{});var n1=l(Zi);F3=c(n1,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),n1.forEach(s),S1=h(a),gt=o(a,"TABLE",{});var Ci=l(gt);Nd=o(Ci,"THEAD",{});var r1=l(Nd);pn=o(r1,"TR",{});var Li=l(pn);eu=o(Li,"TH",{align:!0});var o1=l(eu);U3=c(o1,"All parameters"),o1.forEach(s),J3=h(Li),xd=o(Li,"TH",{align:!0}),l(xd).forEach(s),Li.forEach(s),r1.forEach(s),W3=h(Ci),ue=o(Ci,"TBODY",{});var qe=l(ue);fn=o(qe,"TR",{});var Gi=l(fn);hn=o(Gi,"TD",{align:!0});var gd=l(hn);Id=o(gd,"STRONG",{});var l1=l(Id);Y3=c(l1,"inputs"),l1.forEach(s),V3=c(gd," (required):"),gd.forEach(s),X3=h(Gi),tu=o(Gi,"TD",{align:!0});var i1=l(tu);Q3=c(i1,"a string to be filled from, must contain the [MASK] token (check model card for exact name of the mask)"),i1.forEach(s),Gi.forEach(s),Z3=h(qe),dn=o(qe,"TR",{});var Mi=l(dn);su=o(Mi,"TD",{align:!0});var u1=l(su);Hd=o(u1,"STRONG",{});var c1=l(Hd);eT=c(c1,"options"),c1.forEach(s),u1.forEach(s),tT=h(Mi),au=o(Mi,"TD",{align:!0});var p1=l(au);sT=c(p1,"a dict containing the following keys:"),p1.forEach(s),Mi.forEach(s),aT=h(qe),gn=o(qe,"TR",{});var zi=l(gn);nu=o(zi,"TD",{align:!0});var f1=l(nu);nT=c(f1,"use_cache"),f1.forEach(s),rT=h(zi),mt=o(zi,"TD",{align:!0});var Ki=l(mt);oT=c(Ki,"(Default: "),Bd=o(Ki,"CODE",{});var h1=l(Bd);lT=c(h1,"true"),h1.forEach(s),iT=c(Ki,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ki.forEach(s),zi.forEach(s),uT=h(qe),mn=o(qe,"TR",{});var Fi=l(mn);ru=o(Fi,"TD",{align:!0});var d1=l(ru);cT=c(d1,"wait_for_model"),d1.forEach(s),pT=h(Fi),$t=o(Fi,"TD",{align:!0});var Ui=l($t);fT=c(Ui,"(Default: "),Cd=o(Ui,"CODE",{});var g1=l(Cd);hT=c(g1,"false"),g1.forEach(s),dT=c(Ui,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Ui.forEach(s),Fi.forEach(s),qe.forEach(s),Ci.forEach(s),N1=h(a),ou=o(a,"P",{});var m1=l(ou);gT=c(m1,"Return value is either a dict or a list of dicts if you sent a list of inputs"),m1.forEach(s),x1=h(a),v(qt.$$.fragment,a),I1=h(a),_t=o(a,"TABLE",{});var Ji=l(_t);Ld=o(Ji,"THEAD",{});var $1=l(Ld);$n=o($1,"TR",{});var tE=l($n);lu=o(tE,"TH",{align:!0});var KI=l(lu);mT=c(KI,"Returned values"),KI.forEach(s),$T=h(tE),Gd=o(tE,"TH",{align:!0}),l(Gd).forEach(s),tE.forEach(s),$1.forEach(s),qT=h(Ji),ce=o(Ji,"TBODY",{});var Ua=l(ce);qn=o(Ua,"TR",{});var sE=l(qn);iu=o(sE,"TD",{align:!0});var FI=l(iu);Md=o(FI,"STRONG",{});var UI=l(Md);_T=c(UI,"sequence"),UI.forEach(s),FI.forEach(s),vT=h(sE),uu=o(sE,"TD",{align:!0});var JI=l(uu);yT=c(JI,"The actual sequence of tokens that ran against the model (may contain special tokens)"),JI.forEach(s),sE.forEach(s),ET=h(Ua),_n=o(Ua,"TR",{});var aE=l(_n);cu=o(aE,"TD",{align:!0});var WI=l(cu);zd=o(WI,"STRONG",{});var YI=l(zd);wT=c(YI,"score"),YI.forEach(s),WI.forEach(s),bT=h(aE),pu=o(aE,"TD",{align:!0});var VI=l(pu);TT=c(VI,"The probability for this token."),VI.forEach(s),aE.forEach(s),jT=h(Ua),vn=o(Ua,"TR",{});var nE=l(vn);fu=o(nE,"TD",{align:!0});var XI=l(fu);Kd=o(XI,"STRONG",{});var QI=l(Kd);kT=c(QI,"token"),QI.forEach(s),XI.forEach(s),AT=h(nE),hu=o(nE,"TD",{align:!0});var ZI=l(hu);DT=c(ZI,"The id of the token"),ZI.forEach(s),nE.forEach(s),OT=h(Ua),yn=o(Ua,"TR",{});var rE=l(yn);du=o(rE,"TD",{align:!0});var eH=l(du);Fd=o(eH,"STRONG",{});var tH=l(Fd);RT=c(tH,"token_str"),tH.forEach(s),eH.forEach(s),PT=h(rE),gu=o(rE,"TD",{align:!0});var sH=l(gu);ST=c(sH,"The string representation of the token"),sH.forEach(s),rE.forEach(s),Ua.forEach(s),Ji.forEach(s),H1=h(a),xe=o(a,"H3",{class:!0});var oE=l(xe);vt=o(oE,"A",{id:!0,class:!0,href:!0});var aH=l(vt);Ud=o(aH,"SPAN",{});var nH=l(Ud);v(En.$$.fragment,nH),nH.forEach(s),aH.forEach(s),NT=h(oE),Jd=o(oE,"SPAN",{});var rH=l(Jd);xT=c(rH,"Summarization task"),rH.forEach(s),oE.forEach(s),B1=h(a),yt=o(a,"P",{});var lE=l(yt);IT=c(lE,`This task is well known to summarize longer text into shorter text.
Be careful, some models have a maximum length of input. That means that
the summary cannot handle full books for instance. Be careful when
choosing your model. If you want to discuss your summarization needs,
please get in touch with us: <`),mu=o(lE,"A",{href:!0});var oH=l(mu);HT=c(oH,"api-enterprise@huggingface.co"),oH.forEach(s),BT=c(lE,">"),lE.forEach(s),C1=h(a),v(Et.$$.fragment,a),L1=h(a),wn=o(a,"P",{});var dI=l(wn);CT=c(dI,"Available with: "),bn=o(dI,"A",{href:!0,rel:!0});var lH=l(bn);LT=c(lH,"\u{1F917} Transformers"),lH.forEach(s),dI.forEach(s),G1=h(a),$u=o(a,"P",{});var iH=l($u);GT=c(iH,"Example:"),iH.forEach(s),M1=h(a),v(wt.$$.fragment,a),z1=h(a),qu=o(a,"P",{});var uH=l(qu);MT=c(uH,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),uH.forEach(s),K1=h(a),bt=o(a,"TABLE",{});var iE=l(bt);Wd=o(iE,"THEAD",{});var cH=l(Wd);Tn=o(cH,"TR",{});var uE=l(Tn);_u=o(uE,"TH",{align:!0});var pH=l(_u);zT=c(pH,"All parameters"),pH.forEach(s),KT=h(uE),Yd=o(uE,"TH",{align:!0}),l(Yd).forEach(s),uE.forEach(s),cH.forEach(s),FT=h(iE),G=o(iE,"TBODY",{});var M=l(G);jn=o(M,"TR",{});var cE=l(jn);kn=o(cE,"TD",{align:!0});var gI=l(kn);Vd=o(gI,"STRONG",{});var fH=l(Vd);UT=c(fH,"inputs"),fH.forEach(s),JT=c(gI," (required)"),gI.forEach(s),WT=h(cE),vu=o(cE,"TD",{align:!0});var hH=l(vu);YT=c(hH,"a string to be summarized"),hH.forEach(s),cE.forEach(s),VT=h(M),An=o(M,"TR",{});var pE=l(An);yu=o(pE,"TD",{align:!0});var dH=l(yu);Xd=o(dH,"STRONG",{});var gH=l(Xd);XT=c(gH,"parameters"),gH.forEach(s),dH.forEach(s),QT=h(pE),Eu=o(pE,"TD",{align:!0});var mH=l(Eu);ZT=c(mH,"a dict containing the following keys:"),mH.forEach(s),pE.forEach(s),ej=h(M),Dn=o(M,"TR",{});var fE=l(Dn);wu=o(fE,"TD",{align:!0});var $H=l(wu);tj=c($H,"min_length"),$H.forEach(s),sj=h(fE),_e=o(fE,"TD",{align:!0});var md=l(_e);aj=c(md,"(Default: "),Qd=o(md,"CODE",{});var qH=l(Qd);nj=c(qH,"None"),qH.forEach(s),rj=c(md,"). Integer to define the minimum length "),Zd=o(md,"STRONG",{});var _H=l(Zd);oj=c(_H,"in tokens"),_H.forEach(s),lj=c(md," of the output summary."),md.forEach(s),fE.forEach(s),ij=h(M),On=o(M,"TR",{});var hE=l(On);bu=o(hE,"TD",{align:!0});var vH=l(bu);uj=c(vH,"max_length"),vH.forEach(s),cj=h(hE),ve=o(hE,"TD",{align:!0});var $d=l(ve);pj=c($d,"(Default: "),eg=o($d,"CODE",{});var yH=l(eg);fj=c(yH,"None"),yH.forEach(s),hj=c($d,"). Integer to define the maximum length "),tg=o($d,"STRONG",{});var EH=l(tg);dj=c(EH,"in tokens"),EH.forEach(s),gj=c($d," of the output summary."),$d.forEach(s),hE.forEach(s),mj=h(M),Rn=o(M,"TR",{});var dE=l(Rn);Tu=o(dE,"TD",{align:!0});var wH=l(Tu);$j=c(wH,"top_k"),wH.forEach(s),qj=h(dE),ye=o(dE,"TD",{align:!0});var qd=l(ye);_j=c(qd,"(Default: "),sg=o(qd,"CODE",{});var bH=l(sg);vj=c(bH,"None"),bH.forEach(s),yj=c(qd,"). Integer to define the top tokens considered within the "),ag=o(qd,"CODE",{});var TH=l(ag);Ej=c(TH,"sample"),TH.forEach(s),wj=c(qd," operation to create new text."),qd.forEach(s),dE.forEach(s),bj=h(M),Pn=o(M,"TR",{});var gE=l(Pn);ju=o(gE,"TD",{align:!0});var jH=l(ju);Tj=c(jH,"top_p"),jH.forEach(s),jj=h(gE),ee=o(gE,"TD",{align:!0});var Ja=l(ee);kj=c(Ja,"(Default: "),ng=o(Ja,"CODE",{});var kH=l(ng);Aj=c(kH,"None"),kH.forEach(s),Dj=c(Ja,"). Float to define the tokens that are within the "),rg=o(Ja,"CODE",{});var AH=l(rg);Oj=c(AH,"sample"),AH.forEach(s),Rj=c(Ja," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),og=o(Ja,"CODE",{});var DH=l(og);Pj=c(DH,"top_p"),DH.forEach(s),Sj=c(Ja,"."),Ja.forEach(s),gE.forEach(s),Nj=h(M),Sn=o(M,"TR",{});var mE=l(Sn);ku=o(mE,"TD",{align:!0});var OH=l(ku);xj=c(OH,"temperature"),OH.forEach(s),Ij=h(mE),te=o(mE,"TD",{align:!0});var Wa=l(te);Hj=c(Wa,"(Default: "),lg=o(Wa,"CODE",{});var RH=l(lg);Bj=c(RH,"1.0"),RH.forEach(s),Cj=c(Wa,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),ig=o(Wa,"CODE",{});var PH=l(ig);Lj=c(PH,"0"),PH.forEach(s),Gj=c(Wa," means always take the highest score, "),ug=o(Wa,"CODE",{});var SH=l(ug);Mj=c(SH,"100.0"),SH.forEach(s),zj=c(Wa," is getting closer to uniform probability."),Wa.forEach(s),mE.forEach(s),Kj=h(M),Nn=o(M,"TR",{});var $E=l(Nn);Au=o($E,"TD",{align:!0});var NH=l(Au);Fj=c(NH,"repetition_penalty"),NH.forEach(s),Uj=h($E),Tt=o($E,"TD",{align:!0});var qE=l(Tt);Jj=c(qE,"(Default: "),cg=o(qE,"CODE",{});var xH=l(cg);Wj=c(xH,"None"),xH.forEach(s),Yj=c(qE,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),qE.forEach(s),$E.forEach(s),Vj=h(M),xn=o(M,"TR",{});var _E=l(xn);Du=o(_E,"TD",{align:!0});var IH=l(Du);Xj=c(IH,"max_time"),IH.forEach(s),Qj=h(_E),jt=o(_E,"TD",{align:!0});var vE=l(jt);Zj=c(vE,"(Default: "),pg=o(vE,"CODE",{});var HH=l(pg);e4=c(HH,"None"),HH.forEach(s),t4=c(vE,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),vE.forEach(s),_E.forEach(s),s4=h(M),In=o(M,"TR",{});var yE=l(In);Ou=o(yE,"TD",{align:!0});var BH=l(Ou);fg=o(BH,"STRONG",{});var CH=l(fg);a4=c(CH,"options"),CH.forEach(s),BH.forEach(s),n4=h(yE),Ru=o(yE,"TD",{align:!0});var LH=l(Ru);r4=c(LH,"a dict containing the following keys:"),LH.forEach(s),yE.forEach(s),o4=h(M),Hn=o(M,"TR",{});var EE=l(Hn);Pu=o(EE,"TD",{align:!0});var GH=l(Pu);l4=c(GH,"use_cache"),GH.forEach(s),i4=h(EE),kt=o(EE,"TD",{align:!0});var wE=l(kt);u4=c(wE,"(Default: "),hg=o(wE,"CODE",{});var MH=l(hg);c4=c(MH,"true"),MH.forEach(s),p4=c(wE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),wE.forEach(s),EE.forEach(s),f4=h(M),Bn=o(M,"TR",{});var bE=l(Bn);Su=o(bE,"TD",{align:!0});var zH=l(Su);h4=c(zH,"wait_for_model"),zH.forEach(s),d4=h(bE),At=o(bE,"TD",{align:!0});var TE=l(At);g4=c(TE,"(Default: "),dg=o(TE,"CODE",{});var KH=l(dg);m4=c(KH,"false"),KH.forEach(s),$4=c(TE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),TE.forEach(s),bE.forEach(s),M.forEach(s),iE.forEach(s),F1=h(a),Nu=o(a,"P",{});var FH=l(Nu);q4=c(FH,"Return value is either a dict or a list of dicts if you sent a list of inputs"),FH.forEach(s),U1=h(a),Dt=o(a,"TABLE",{});var jE=l(Dt);gg=o(jE,"THEAD",{});var UH=l(gg);Cn=o(UH,"TR",{});var kE=l(Cn);xu=o(kE,"TH",{align:!0});var JH=l(xu);_4=c(JH,"Returned values"),JH.forEach(s),v4=h(kE),mg=o(kE,"TH",{align:!0}),l(mg).forEach(s),kE.forEach(s),UH.forEach(s),y4=h(jE),$g=o(jE,"TBODY",{});var WH=l($g);Ln=o(WH,"TR",{});var AE=l(Ln);Iu=o(AE,"TD",{align:!0});var YH=l(Iu);qg=o(YH,"STRONG",{});var VH=l(qg);E4=c(VH,"summarization_text"),VH.forEach(s),YH.forEach(s),w4=h(AE),Hu=o(AE,"TD",{align:!0});var XH=l(Hu);b4=c(XH,"The string after translation"),XH.forEach(s),AE.forEach(s),WH.forEach(s),jE.forEach(s),J1=h(a),Ie=o(a,"H3",{class:!0});var DE=l(Ie);Ot=o(DE,"A",{id:!0,class:!0,href:!0});var QH=l(Ot);_g=o(QH,"SPAN",{});var ZH=l(_g);v(Gn.$$.fragment,ZH),ZH.forEach(s),QH.forEach(s),T4=h(DE),vg=o(DE,"SPAN",{});var eB=l(vg);j4=c(eB,"Question Answering task"),eB.forEach(s),DE.forEach(s),W1=h(a),Bu=o(a,"P",{});var tB=l(Bu);k4=c(tB,"Want to have a nice know-it-all bot that can answer any question?"),tB.forEach(s),Y1=h(a),v(Rt.$$.fragment,a),V1=h(a),He=o(a,"P",{});var q1=l(He);A4=c(q1,"Available with: "),Mn=o(q1,"A",{href:!0,rel:!0});var sB=l(Mn);D4=c(sB,"\u{1F917}Transformers"),sB.forEach(s),O4=c(q1,` and
`),zn=o(q1,"A",{href:!0,rel:!0});var aB=l(zn);R4=c(aB,"AllenNLP"),aB.forEach(s),q1.forEach(s),X1=h(a),Cu=o(a,"P",{});var nB=l(Cu);P4=c(nB,"Example:"),nB.forEach(s),Q1=h(a),v(Pt.$$.fragment,a),Z1=h(a),Lu=o(a,"P",{});var rB=l(Lu);S4=c(rB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),rB.forEach(s),ev=h(a),Gu=o(a,"P",{});var oB=l(Gu);N4=c(oB,"Return value is a dict."),oB.forEach(s),tv=h(a),v(St.$$.fragment,a),sv=h(a),Nt=o(a,"TABLE",{});var OE=l(Nt);yg=o(OE,"THEAD",{});var lB=l(yg);Kn=o(lB,"TR",{});var RE=l(Kn);Mu=o(RE,"TH",{align:!0});var iB=l(Mu);x4=c(iB,"Returned values"),iB.forEach(s),I4=h(RE),Eg=o(RE,"TH",{align:!0}),l(Eg).forEach(s),RE.forEach(s),lB.forEach(s),H4=h(OE),pe=o(OE,"TBODY",{});var Ya=l(pe);Fn=o(Ya,"TR",{});var PE=l(Fn);zu=o(PE,"TD",{align:!0});var uB=l(zu);wg=o(uB,"STRONG",{});var cB=l(wg);B4=c(cB,"answer"),cB.forEach(s),uB.forEach(s),C4=h(PE),Ku=o(PE,"TD",{align:!0});var pB=l(Ku);L4=c(pB,"A string that\u2019s the answer within the text."),pB.forEach(s),PE.forEach(s),G4=h(Ya),Un=o(Ya,"TR",{});var SE=l(Un);Fu=o(SE,"TD",{align:!0});var fB=l(Fu);bg=o(fB,"STRONG",{});var hB=l(bg);M4=c(hB,"score"),hB.forEach(s),fB.forEach(s),z4=h(SE),Uu=o(SE,"TD",{align:!0});var dB=l(Uu);K4=c(dB,"A float that represents how likely that the answer is correct"),dB.forEach(s),SE.forEach(s),F4=h(Ya),Jn=o(Ya,"TR",{});var NE=l(Jn);Ju=o(NE,"TD",{align:!0});var gB=l(Ju);Tg=o(gB,"STRONG",{});var mB=l(Tg);U4=c(mB,"start"),mB.forEach(s),gB.forEach(s),J4=h(NE),xt=o(NE,"TD",{align:!0});var xE=l(xt);W4=c(xE,"The index (string wise) of the start of the answer within "),jg=o(xE,"CODE",{});var $B=l(jg);Y4=c($B,"context"),$B.forEach(s),V4=c(xE,"."),xE.forEach(s),NE.forEach(s),X4=h(Ya),Wn=o(Ya,"TR",{});var IE=l(Wn);Wu=o(IE,"TD",{align:!0});var qB=l(Wu);kg=o(qB,"STRONG",{});var _B=l(kg);Q4=c(_B,"stop"),_B.forEach(s),qB.forEach(s),Z4=h(IE),It=o(IE,"TD",{align:!0});var HE=l(It);e5=c(HE,"The index (string wise) of the stop of the answer within "),Ag=o(HE,"CODE",{});var vB=l(Ag);t5=c(vB,"context"),vB.forEach(s),s5=c(HE,"."),HE.forEach(s),IE.forEach(s),Ya.forEach(s),OE.forEach(s),av=h(a),Be=o(a,"H3",{class:!0});var BE=l(Be);Ht=o(BE,"A",{id:!0,class:!0,href:!0});var yB=l(Ht);Dg=o(yB,"SPAN",{});var EB=l(Dg);v(Yn.$$.fragment,EB),EB.forEach(s),yB.forEach(s),a5=h(BE),Og=o(BE,"SPAN",{});var wB=l(Og);n5=c(wB,"Table Question Answering task"),wB.forEach(s),BE.forEach(s),nv=h(a),Yu=o(a,"P",{});var bB=l(Yu);r5=c(bB,`Don\u2019t know SQL? Don\u2019t want to dive into a large spreadsheet? Ask
questions in plain english!`),bB.forEach(s),rv=h(a),v(Bt.$$.fragment,a),ov=h(a),Vn=o(a,"P",{});var mI=l(Vn);o5=c(mI,"Available with: "),Xn=o(mI,"A",{href:!0,rel:!0});var TB=l(Xn);l5=c(TB,"\u{1F917} Transformers"),TB.forEach(s),mI.forEach(s),lv=h(a),Vu=o(a,"P",{});var jB=l(Vu);i5=c(jB,"Example:"),jB.forEach(s),iv=h(a),v(Ct.$$.fragment,a),uv=h(a),Xu=o(a,"P",{});var kB=l(Xu);u5=c(kB,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),kB.forEach(s),cv=h(a),Lt=o(a,"TABLE",{});var CE=l(Lt);Rg=o(CE,"THEAD",{});var AB=l(Rg);Qn=o(AB,"TR",{});var LE=l(Qn);Qu=o(LE,"TH",{align:!0});var DB=l(Qu);c5=c(DB,"All parameters"),DB.forEach(s),p5=h(LE),Pg=o(LE,"TH",{align:!0}),l(Pg).forEach(s),LE.forEach(s),AB.forEach(s),f5=h(CE),J=o(CE,"TBODY",{});var oe=l(J);Zn=o(oe,"TR",{});var GE=l(Zn);er=o(GE,"TD",{align:!0});var $I=l(er);Sg=o($I,"STRONG",{});var OB=l(Sg);h5=c(OB,"inputs"),OB.forEach(s),d5=c($I," (required)"),$I.forEach(s),g5=h(GE),Ng=o(GE,"TD",{align:!0}),l(Ng).forEach(s),GE.forEach(s),m5=h(oe),tr=o(oe,"TR",{});var ME=l(tr);Zu=o(ME,"TD",{align:!0});var RB=l(Zu);$5=c(RB,"query (required)"),RB.forEach(s),q5=h(ME),ec=o(ME,"TD",{align:!0});var PB=l(ec);_5=c(PB,"The query in plain text that you want to ask the table"),PB.forEach(s),ME.forEach(s),v5=h(oe),sr=o(oe,"TR",{});var zE=l(sr);tc=o(zE,"TD",{align:!0});var SB=l(tc);y5=c(SB,"table (required)"),SB.forEach(s),E5=h(zE),sc=o(zE,"TD",{align:!0});var NB=l(sc);w5=c(NB,"A table of data represented as a dict of list where entries are headers and the lists are all the values, all lists must have the same size."),NB.forEach(s),zE.forEach(s),b5=h(oe),ar=o(oe,"TR",{});var KE=l(ar);ac=o(KE,"TD",{align:!0});var xB=l(ac);xg=o(xB,"STRONG",{});var IB=l(xg);T5=c(IB,"options"),IB.forEach(s),xB.forEach(s),j5=h(KE),nc=o(KE,"TD",{align:!0});var HB=l(nc);k5=c(HB,"a dict containing the following keys:"),HB.forEach(s),KE.forEach(s),A5=h(oe),nr=o(oe,"TR",{});var FE=l(nr);rc=o(FE,"TD",{align:!0});var BB=l(rc);D5=c(BB,"use_cache"),BB.forEach(s),O5=h(FE),Gt=o(FE,"TD",{align:!0});var UE=l(Gt);R5=c(UE,"(Default: "),Ig=o(UE,"CODE",{});var CB=l(Ig);P5=c(CB,"true"),CB.forEach(s),S5=c(UE,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),UE.forEach(s),FE.forEach(s),N5=h(oe),rr=o(oe,"TR",{});var JE=l(rr);oc=o(JE,"TD",{align:!0});var LB=l(oc);x5=c(LB,"wait_for_model"),LB.forEach(s),I5=h(JE),Mt=o(JE,"TD",{align:!0});var WE=l(Mt);H5=c(WE,"(Default: "),Hg=o(WE,"CODE",{});var GB=l(Hg);B5=c(GB,"false"),GB.forEach(s),C5=c(WE,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),WE.forEach(s),JE.forEach(s),oe.forEach(s),CE.forEach(s),pv=h(a),lc=o(a,"P",{});var MB=l(lc);L5=c(MB,"Return value is either a dict or a list of dicts if you sent a list of inputs"),MB.forEach(s),fv=h(a),v(zt.$$.fragment,a),hv=h(a),Kt=o(a,"TABLE",{});var YE=l(Kt);Bg=o(YE,"THEAD",{});var zB=l(Bg);or=o(zB,"TR",{});var VE=l(or);ic=o(VE,"TH",{align:!0});var KB=l(ic);G5=c(KB,"Returned values"),KB.forEach(s),M5=h(VE),Cg=o(VE,"TH",{align:!0}),l(Cg).forEach(s),VE.forEach(s),zB.forEach(s),z5=h(YE),fe=o(YE,"TBODY",{});var Va=l(fe);lr=o(Va,"TR",{});var XE=l(lr);uc=o(XE,"TD",{align:!0});var FB=l(uc);Lg=o(FB,"STRONG",{});var UB=l(Lg);K5=c(UB,"answer"),UB.forEach(s),FB.forEach(s),F5=h(XE),cc=o(XE,"TD",{align:!0});var JB=l(cc);U5=c(JB,"The plaintext answer"),JB.forEach(s),XE.forEach(s),J5=h(Va),ir=o(Va,"TR",{});var QE=l(ir);pc=o(QE,"TD",{align:!0});var WB=l(pc);Gg=o(WB,"STRONG",{});var YB=l(Gg);W5=c(YB,"coordinates"),YB.forEach(s),WB.forEach(s),Y5=h(QE),fc=o(QE,"TD",{align:!0});var VB=l(fc);V5=c(VB,"a list of coordinates of the cells referenced in the answer"),VB.forEach(s),QE.forEach(s),X5=h(Va),ur=o(Va,"TR",{});var ZE=l(ur);hc=o(ZE,"TD",{align:!0});var XB=l(hc);Mg=o(XB,"STRONG",{});var QB=l(Mg);Q5=c(QB,"cells"),QB.forEach(s),XB.forEach(s),Z5=h(ZE),dc=o(ZE,"TD",{align:!0});var ZB=l(dc);ek=c(ZB,"a list of coordinates of the cells contents"),ZB.forEach(s),ZE.forEach(s),tk=h(Va),cr=o(Va,"TR",{});var ew=l(cr);gc=o(ew,"TD",{align:!0});var eC=l(gc);zg=o(eC,"STRONG",{});var tC=l(zg);sk=c(tC,"aggregator"),tC.forEach(s),eC.forEach(s),ak=h(ew),mc=o(ew,"TD",{align:!0});var sC=l(mc);nk=c(sC,"The aggregator used to get the answer"),sC.forEach(s),ew.forEach(s),Va.forEach(s),YE.forEach(s),dv=h(a),Ce=o(a,"H3",{class:!0});var tw=l(Ce);Ft=o(tw,"A",{id:!0,class:!0,href:!0});var aC=l(Ft);Kg=o(aC,"SPAN",{});var nC=l(Kg);v(pr.$$.fragment,nC),nC.forEach(s),aC.forEach(s),rk=h(tw),Fg=o(tw,"SPAN",{});var rC=l(Fg);ok=c(rC,"Sentence Similarity task"),rC.forEach(s),tw.forEach(s),gv=h(a),$c=o(a,"P",{});var oC=l($c);lk=c(oC,"Calculate the semantic similarity between one text and a list of other sentences by comparing their embeddings."),oC.forEach(s),mv=h(a),v(Ut.$$.fragment,a),$v=h(a),fr=o(a,"P",{});var qI=l(fr);ik=c(qI,"Available with: "),hr=o(qI,"A",{href:!0,rel:!0});var lC=l(hr);uk=c(lC,"Sentence Transformers"),lC.forEach(s),qI.forEach(s),qv=h(a),qc=o(a,"P",{});var iC=l(qc);ck=c(iC,"Example:"),iC.forEach(s),_v=h(a),v(Jt.$$.fragment,a),vv=h(a),_c=o(a,"P",{});var uC=l(_c);pk=c(uC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),uC.forEach(s),yv=h(a),Wt=o(a,"TABLE",{});var sw=l(Wt);Ug=o(sw,"THEAD",{});var cC=l(Ug);dr=o(cC,"TR",{});var aw=l(dr);vc=o(aw,"TH",{align:!0});var pC=l(vc);fk=c(pC,"All parameters"),pC.forEach(s),hk=h(aw),Jg=o(aw,"TH",{align:!0}),l(Jg).forEach(s),aw.forEach(s),cC.forEach(s),dk=h(sw),W=o(sw,"TBODY",{});var le=l(W);gr=o(le,"TR",{});var nw=l(gr);mr=o(nw,"TD",{align:!0});var _I=l(mr);Wg=o(_I,"STRONG",{});var fC=l(Wg);gk=c(fC,"inputs"),fC.forEach(s),mk=c(_I," (required)"),_I.forEach(s),$k=h(nw),Yg=o(nw,"TD",{align:!0}),l(Yg).forEach(s),nw.forEach(s),qk=h(le),$r=o(le,"TR",{});var rw=l($r);yc=o(rw,"TD",{align:!0});var hC=l(yc);_k=c(hC,"source_sentence (required)"),hC.forEach(s),vk=h(rw),Ec=o(rw,"TD",{align:!0});var dC=l(Ec);yk=c(dC,"The string that you wish to compare the other strings with. This can be a phrase, sentence, or longer passage, depending on the model being used."),dC.forEach(s),rw.forEach(s),Ek=h(le),qr=o(le,"TR",{});var ow=l(qr);wc=o(ow,"TD",{align:!0});var gC=l(wc);wk=c(gC,"sentences (required)"),gC.forEach(s),bk=h(ow),bc=o(ow,"TD",{align:!0});var mC=l(bc);Tk=c(mC,"A list of strings which will be compared against the source_sentence."),mC.forEach(s),ow.forEach(s),jk=h(le),_r=o(le,"TR",{});var lw=l(_r);Tc=o(lw,"TD",{align:!0});var $C=l(Tc);Vg=o($C,"STRONG",{});var qC=l(Vg);kk=c(qC,"options"),qC.forEach(s),$C.forEach(s),Ak=h(lw),jc=o(lw,"TD",{align:!0});var _C=l(jc);Dk=c(_C,"a dict containing the following keys:"),_C.forEach(s),lw.forEach(s),Ok=h(le),vr=o(le,"TR",{});var iw=l(vr);kc=o(iw,"TD",{align:!0});var vC=l(kc);Rk=c(vC,"use_cache"),vC.forEach(s),Pk=h(iw),Yt=o(iw,"TD",{align:!0});var uw=l(Yt);Sk=c(uw,"(Default: "),Xg=o(uw,"CODE",{});var yC=l(Xg);Nk=c(yC,"true"),yC.forEach(s),xk=c(uw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),uw.forEach(s),iw.forEach(s),Ik=h(le),yr=o(le,"TR",{});var cw=l(yr);Ac=o(cw,"TD",{align:!0});var EC=l(Ac);Hk=c(EC,"wait_for_model"),EC.forEach(s),Bk=h(cw),Vt=o(cw,"TD",{align:!0});var pw=l(Vt);Ck=c(pw,"(Default: "),Qg=o(pw,"CODE",{});var wC=l(Qg);Lk=c(wC,"false"),wC.forEach(s),Gk=c(pw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),pw.forEach(s),cw.forEach(s),le.forEach(s),sw.forEach(s),Ev=h(a),Dc=o(a,"P",{});var bC=l(Dc);Mk=c(bC,"The return value is a list of similarity scores, given as floats."),bC.forEach(s),wv=h(a),v(Xt.$$.fragment,a),bv=h(a),Qt=o(a,"TABLE",{});var fw=l(Qt);Zg=o(fw,"THEAD",{});var TC=l(Zg);Er=o(TC,"TR",{});var hw=l(Er);Oc=o(hw,"TH",{align:!0});var jC=l(Oc);zk=c(jC,"Returned values"),jC.forEach(s),Kk=h(hw),em=o(hw,"TH",{align:!0}),l(em).forEach(s),hw.forEach(s),TC.forEach(s),Fk=h(fw),tm=o(fw,"TBODY",{});var kC=l(tm);wr=o(kC,"TR",{});var dw=l(wr);Rc=o(dw,"TD",{align:!0});var AC=l(Rc);sm=o(AC,"STRONG",{});var DC=l(sm);Uk=c(DC,"Scores"),DC.forEach(s),AC.forEach(s),Jk=h(dw),Pc=o(dw,"TD",{align:!0});var OC=l(Pc);Wk=c(OC,"The associated similarity score for each of the given strings"),OC.forEach(s),dw.forEach(s),kC.forEach(s),fw.forEach(s),Tv=h(a),Le=o(a,"H3",{class:!0});var gw=l(Le);Zt=o(gw,"A",{id:!0,class:!0,href:!0});var RC=l(Zt);am=o(RC,"SPAN",{});var PC=l(am);v(br.$$.fragment,PC),PC.forEach(s),RC.forEach(s),Yk=h(gw),nm=o(gw,"SPAN",{});var SC=l(nm);Vk=c(SC,"Text Classification task"),SC.forEach(s),gw.forEach(s),jv=h(a),Sc=o(a,"P",{});var NC=l(Sc);Xk=c(NC,`Usually used for sentiment-analysis this will output the likelihood of
classes of an input.`),NC.forEach(s),kv=h(a),v(es.$$.fragment,a),Av=h(a),Tr=o(a,"P",{});var vI=l(Tr);Qk=c(vI,"Available with: "),jr=o(vI,"A",{href:!0,rel:!0});var xC=l(jr);Zk=c(xC,"\u{1F917} Transformers"),xC.forEach(s),vI.forEach(s),Dv=h(a),Nc=o(a,"P",{});var IC=l(Nc);e6=c(IC,"Example:"),IC.forEach(s),Ov=h(a),v(ts.$$.fragment,a),Rv=h(a),xc=o(a,"P",{});var HC=l(xc);t6=c(HC,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),HC.forEach(s),Pv=h(a),ss=o(a,"TABLE",{});var mw=l(ss);rm=o(mw,"THEAD",{});var BC=l(rm);kr=o(BC,"TR",{});var $w=l(kr);Ic=o($w,"TH",{align:!0});var CC=l(Ic);s6=c(CC,"All parameters"),CC.forEach(s),a6=h($w),om=o($w,"TH",{align:!0}),l(om).forEach(s),$w.forEach(s),BC.forEach(s),n6=h(mw),he=o(mw,"TBODY",{});var Xa=l(he);Ar=o(Xa,"TR",{});var qw=l(Ar);Dr=o(qw,"TD",{align:!0});var yI=l(Dr);lm=o(yI,"STRONG",{});var LC=l(lm);r6=c(LC,"inputs"),LC.forEach(s),o6=c(yI," (required)"),yI.forEach(s),l6=h(qw),Hc=o(qw,"TD",{align:!0});var GC=l(Hc);i6=c(GC,"a string to be classified"),GC.forEach(s),qw.forEach(s),u6=h(Xa),Or=o(Xa,"TR",{});var _w=l(Or);Bc=o(_w,"TD",{align:!0});var MC=l(Bc);im=o(MC,"STRONG",{});var zC=l(im);c6=c(zC,"options"),zC.forEach(s),MC.forEach(s),p6=h(_w),Cc=o(_w,"TD",{align:!0});var KC=l(Cc);f6=c(KC,"a dict containing the following keys:"),KC.forEach(s),_w.forEach(s),h6=h(Xa),Rr=o(Xa,"TR",{});var vw=l(Rr);Lc=o(vw,"TD",{align:!0});var FC=l(Lc);d6=c(FC,"use_cache"),FC.forEach(s),g6=h(vw),as=o(vw,"TD",{align:!0});var yw=l(as);m6=c(yw,"(Default: "),um=o(yw,"CODE",{});var UC=l(um);$6=c(UC,"true"),UC.forEach(s),q6=c(yw,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),yw.forEach(s),vw.forEach(s),_6=h(Xa),Pr=o(Xa,"TR",{});var Ew=l(Pr);Gc=o(Ew,"TD",{align:!0});var JC=l(Gc);v6=c(JC,"wait_for_model"),JC.forEach(s),y6=h(Ew),ns=o(Ew,"TD",{align:!0});var ww=l(ns);E6=c(ww,"(Default: "),cm=o(ww,"CODE",{});var WC=l(cm);w6=c(WC,"false"),WC.forEach(s),b6=c(ww,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),ww.forEach(s),Ew.forEach(s),Xa.forEach(s),mw.forEach(s),Sv=h(a),Mc=o(a,"P",{});var YC=l(Mc);T6=c(YC,"Return value is either a dict or a list of dicts if you sent a list of inputs"),YC.forEach(s),Nv=h(a),v(rs.$$.fragment,a),xv=h(a),os=o(a,"TABLE",{});var bw=l(os);pm=o(bw,"THEAD",{});var VC=l(pm);Sr=o(VC,"TR",{});var Tw=l(Sr);zc=o(Tw,"TH",{align:!0});var XC=l(zc);j6=c(XC,"Returned values"),XC.forEach(s),k6=h(Tw),fm=o(Tw,"TH",{align:!0}),l(fm).forEach(s),Tw.forEach(s),VC.forEach(s),A6=h(bw),Nr=o(bw,"TBODY",{});var jw=l(Nr);xr=o(jw,"TR",{});var kw=l(xr);Kc=o(kw,"TD",{align:!0});var QC=l(Kc);hm=o(QC,"STRONG",{});var ZC=l(hm);D6=c(ZC,"label"),ZC.forEach(s),QC.forEach(s),O6=h(kw),Fc=o(kw,"TD",{align:!0});var eL=l(Fc);R6=c(eL,"The label for the class (model specific)"),eL.forEach(s),kw.forEach(s),P6=h(jw),Ir=o(jw,"TR",{});var Aw=l(Ir);Uc=o(Aw,"TD",{align:!0});var tL=l(Uc);dm=o(tL,"STRONG",{});var sL=l(dm);S6=c(sL,"score"),sL.forEach(s),tL.forEach(s),N6=h(Aw),Jc=o(Aw,"TD",{align:!0});var aL=l(Jc);x6=c(aL,"A floats that represents how likely is that the text belongs the this class."),aL.forEach(s),Aw.forEach(s),jw.forEach(s),bw.forEach(s),Iv=h(a),Ge=o(a,"H3",{class:!0});var Dw=l(Ge);ls=o(Dw,"A",{id:!0,class:!0,href:!0});var nL=l(ls);gm=o(nL,"SPAN",{});var rL=l(gm);v(Hr.$$.fragment,rL),rL.forEach(s),nL.forEach(s),I6=h(Dw),mm=o(Dw,"SPAN",{});var oL=l(mm);H6=c(oL,"Text Generation task"),oL.forEach(s),Dw.forEach(s),Hv=h(a),Wc=o(a,"P",{});var lL=l(Wc);B6=c(lL,"Use to continue text from a prompt. This is a very generic task."),lL.forEach(s),Bv=h(a),v(is.$$.fragment,a),Cv=h(a),Br=o(a,"P",{});var EI=l(Br);C6=c(EI,"Available with: "),Cr=o(EI,"A",{href:!0,rel:!0});var iL=l(Cr);L6=c(iL,"\u{1F917} Transformers"),iL.forEach(s),EI.forEach(s),Lv=h(a),Yc=o(a,"P",{});var uL=l(Yc);G6=c(uL,"Example:"),uL.forEach(s),Gv=h(a),v(us.$$.fragment,a),Mv=h(a),Vc=o(a,"P",{});var cL=l(Vc);M6=c(cL,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),cL.forEach(s),zv=h(a),cs=o(a,"TABLE",{});var Ow=l(cs);$m=o(Ow,"THEAD",{});var pL=l($m);Lr=o(pL,"TR",{});var Rw=l(Lr);Xc=o(Rw,"TH",{align:!0});var fL=l(Xc);z6=c(fL,"All parameters"),fL.forEach(s),K6=h(Rw),qm=o(Rw,"TH",{align:!0}),l(qm).forEach(s),Rw.forEach(s),pL.forEach(s),F6=h(Ow),I=o(Ow,"TBODY",{});var L=l(I);Gr=o(L,"TR",{});var Pw=l(Gr);Mr=o(Pw,"TD",{align:!0});var wI=l(Mr);_m=o(wI,"STRONG",{});var hL=l(_m);U6=c(hL,"inputs"),hL.forEach(s),J6=c(wI," (required):"),wI.forEach(s),W6=h(Pw),Qc=o(Pw,"TD",{align:!0});var dL=l(Qc);Y6=c(dL,"a string to be generated from"),dL.forEach(s),Pw.forEach(s),V6=h(L),zr=o(L,"TR",{});var Sw=l(zr);Zc=o(Sw,"TD",{align:!0});var gL=l(Zc);vm=o(gL,"STRONG",{});var mL=l(vm);X6=c(mL,"parameters"),mL.forEach(s),gL.forEach(s),Q6=h(Sw),ep=o(Sw,"TD",{align:!0});var $L=l(ep);Z6=c($L,"dict containing the following keys:"),$L.forEach(s),Sw.forEach(s),e7=h(L),Kr=o(L,"TR",{});var Nw=l(Kr);tp=o(Nw,"TD",{align:!0});var qL=l(tp);t7=c(qL,"top_k"),qL.forEach(s),s7=h(Nw),Ee=o(Nw,"TD",{align:!0});var _d=l(Ee);a7=c(_d,"(Default: "),ym=o(_d,"CODE",{});var _L=l(ym);n7=c(_L,"None"),_L.forEach(s),r7=c(_d,"). Integer to define the top tokens considered within the "),Em=o(_d,"CODE",{});var vL=l(Em);o7=c(vL,"sample"),vL.forEach(s),l7=c(_d," operation to create new text."),_d.forEach(s),Nw.forEach(s),i7=h(L),Fr=o(L,"TR",{});var xw=l(Fr);sp=o(xw,"TD",{align:!0});var yL=l(sp);u7=c(yL,"top_p"),yL.forEach(s),c7=h(xw),se=o(xw,"TD",{align:!0});var Qa=l(se);p7=c(Qa,"(Default: "),wm=o(Qa,"CODE",{});var EL=l(wm);f7=c(EL,"None"),EL.forEach(s),h7=c(Qa,"). Float to define the tokens that are within the "),bm=o(Qa,"CODE",{});var wL=l(bm);d7=c(wL,"sample"),wL.forEach(s),g7=c(Qa," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),Tm=o(Qa,"CODE",{});var bL=l(Tm);m7=c(bL,"top_p"),bL.forEach(s),$7=c(Qa,"."),Qa.forEach(s),xw.forEach(s),q7=h(L),Ur=o(L,"TR",{});var Iw=l(Ur);ap=o(Iw,"TD",{align:!0});var TL=l(ap);_7=c(TL,"temperature"),TL.forEach(s),v7=h(Iw),ae=o(Iw,"TD",{align:!0});var Za=l(ae);y7=c(Za,"(Default: "),jm=o(Za,"CODE",{});var jL=l(jm);E7=c(jL,"1.0"),jL.forEach(s),w7=c(Za,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),km=o(Za,"CODE",{});var kL=l(km);b7=c(kL,"0"),kL.forEach(s),T7=c(Za," means always take the highest score, "),Am=o(Za,"CODE",{});var AL=l(Am);j7=c(AL,"100.0"),AL.forEach(s),k7=c(Za," is getting closer to uniform probability."),Za.forEach(s),Iw.forEach(s),A7=h(L),Jr=o(L,"TR",{});var Hw=l(Jr);np=o(Hw,"TD",{align:!0});var DL=l(np);D7=c(DL,"repetition_penalty"),DL.forEach(s),O7=h(Hw),ps=o(Hw,"TD",{align:!0});var Bw=l(ps);R7=c(Bw,"(Default: "),Dm=o(Bw,"CODE",{});var OL=l(Dm);P7=c(OL,"None"),OL.forEach(s),S7=c(Bw,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),Bw.forEach(s),Hw.forEach(s),N7=h(L),Wr=o(L,"TR",{});var Cw=l(Wr);rp=o(Cw,"TD",{align:!0});var RL=l(rp);x7=c(RL,"max_new_tokens"),RL.forEach(s),I7=h(Cw),we=o(Cw,"TD",{align:!0});var vd=l(we);H7=c(vd,"(Default: "),Om=o(vd,"CODE",{});var PL=l(Om);B7=c(PL,"None"),PL.forEach(s),C7=c(vd,"). Int (0-250). The amount of new tokens to be generated, this does "),Rm=o(vd,"STRONG",{});var SL=l(Rm);L7=c(SL,"not"),SL.forEach(s),G7=c(vd," include the input length it is a estimate of the size of generated text you want. Each new tokens slows down the request, so look for balance between response times and length of text generated."),vd.forEach(s),Cw.forEach(s),M7=h(L),Yr=o(L,"TR",{});var Lw=l(Yr);op=o(Lw,"TD",{align:!0});var NL=l(op);z7=c(NL,"max_time"),NL.forEach(s),K7=h(Lw),be=o(Lw,"TD",{align:!0});var yd=l(be);F7=c(yd,"(Default: "),Pm=o(yd,"CODE",{});var xL=l(Pm);U7=c(xL,"None"),xL.forEach(s),J7=c(yd,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit. Use that in combination with "),Sm=o(yd,"CODE",{});var IL=l(Sm);W7=c(IL,"max_new_tokens"),IL.forEach(s),Y7=c(yd," for best results."),yd.forEach(s),Lw.forEach(s),V7=h(L),Vr=o(L,"TR",{});var Gw=l(Vr);lp=o(Gw,"TD",{align:!0});var HL=l(lp);X7=c(HL,"return_full_text"),HL.forEach(s),Q7=h(Gw),Te=o(Gw,"TD",{align:!0});var Ed=l(Te);Z7=c(Ed,"(Default: "),Nm=o(Ed,"CODE",{});var BL=l(Nm);e9=c(BL,"True"),BL.forEach(s),t9=c(Ed,"). Bool. If set to False, the return results will "),xm=o(Ed,"STRONG",{});var CL=l(xm);s9=c(CL,"not"),CL.forEach(s),a9=c(Ed," contain the original query making it easier for prompting."),Ed.forEach(s),Gw.forEach(s),n9=h(L),Xr=o(L,"TR",{});var Mw=l(Xr);ip=o(Mw,"TD",{align:!0});var LL=l(ip);r9=c(LL,"num_return_sequences"),LL.forEach(s),o9=h(Mw),fs=o(Mw,"TD",{align:!0});var zw=l(fs);l9=c(zw,"(Default: "),Im=o(zw,"CODE",{});var GL=l(Im);i9=c(GL,"1"),GL.forEach(s),u9=c(zw,"). Integer. The number of proposition you want to be returned."),zw.forEach(s),Mw.forEach(s),c9=h(L),Qr=o(L,"TR",{});var Kw=l(Qr);up=o(Kw,"TD",{align:!0});var ML=l(up);p9=c(ML,"do_sample"),ML.forEach(s),f9=h(Kw),hs=o(Kw,"TD",{align:!0});var Fw=l(hs);h9=c(Fw,"(Optional: "),Hm=o(Fw,"CODE",{});var zL=l(Hm);d9=c(zL,"True"),zL.forEach(s),g9=c(Fw,"). Bool. Whether or not to use sampling, use greedy decoding otherwise."),Fw.forEach(s),Kw.forEach(s),m9=h(L),Zr=o(L,"TR",{});var Uw=l(Zr);cp=o(Uw,"TD",{align:!0});var KL=l(cp);Bm=o(KL,"STRONG",{});var FL=l(Bm);$9=c(FL,"options"),FL.forEach(s),KL.forEach(s),q9=h(Uw),pp=o(Uw,"TD",{align:!0});var UL=l(pp);_9=c(UL,"a dict containing the following keys:"),UL.forEach(s),Uw.forEach(s),v9=h(L),eo=o(L,"TR",{});var Jw=l(eo);fp=o(Jw,"TD",{align:!0});var JL=l(fp);y9=c(JL,"use_cache"),JL.forEach(s),E9=h(Jw),ds=o(Jw,"TD",{align:!0});var Ww=l(ds);w9=c(Ww,"(Default: "),Cm=o(Ww,"CODE",{});var WL=l(Cm);b9=c(WL,"true"),WL.forEach(s),T9=c(Ww,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),Ww.forEach(s),Jw.forEach(s),j9=h(L),to=o(L,"TR",{});var Yw=l(to);hp=o(Yw,"TD",{align:!0});var YL=l(hp);k9=c(YL,"wait_for_model"),YL.forEach(s),A9=h(Yw),gs=o(Yw,"TD",{align:!0});var Vw=l(gs);D9=c(Vw,"(Default: "),Lm=o(Vw,"CODE",{});var VL=l(Lm);O9=c(VL,"false"),VL.forEach(s),R9=c(Vw,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Vw.forEach(s),Yw.forEach(s),L.forEach(s),Ow.forEach(s),Kv=h(a),dp=o(a,"P",{});var XL=l(dp);P9=c(XL,"Return value is either a dict or a list of dicts if you sent a list of inputs"),XL.forEach(s),Fv=h(a),v(ms.$$.fragment,a),Uv=h(a),$s=o(a,"TABLE",{});var Xw=l($s);Gm=o(Xw,"THEAD",{});var QL=l(Gm);so=o(QL,"TR",{});var Qw=l(so);gp=o(Qw,"TH",{align:!0});var ZL=l(gp);S9=c(ZL,"Returned values"),ZL.forEach(s),N9=h(Qw),Mm=o(Qw,"TH",{align:!0}),l(Mm).forEach(s),Qw.forEach(s),QL.forEach(s),x9=h(Xw),zm=o(Xw,"TBODY",{});var eG=l(zm);ao=o(eG,"TR",{});var Zw=l(ao);mp=o(Zw,"TD",{align:!0});var tG=l(mp);Km=o(tG,"STRONG",{});var sG=l(Km);I9=c(sG,"generated_text"),sG.forEach(s),tG.forEach(s),H9=h(Zw),$p=o(Zw,"TD",{align:!0});var aG=l($p);B9=c(aG,"The continuated string"),aG.forEach(s),Zw.forEach(s),eG.forEach(s),Xw.forEach(s),Jv=h(a),Me=o(a,"H3",{class:!0});var e0=l(Me);qs=o(e0,"A",{id:!0,class:!0,href:!0});var nG=l(qs);Fm=o(nG,"SPAN",{});var rG=l(Fm);v(no.$$.fragment,rG),rG.forEach(s),nG.forEach(s),C9=h(e0),Um=o(e0,"SPAN",{});var oG=l(Um);L9=c(oG,"Text2Text Generation task"),oG.forEach(s),e0.forEach(s),Wv=h(a),_s=o(a,"P",{});var t0=l(_s);G9=c(t0,"Essentially "),qp=o(t0,"A",{href:!0});var lG=l(qp);M9=c(lG,"Text-generation task"),lG.forEach(s),z9=c(t0,`. But uses
Encoder-Decoder architecture, so might change in the future for more
options.`),t0.forEach(s),Yv=h(a),ze=o(a,"H3",{class:!0});var s0=l(ze);vs=o(s0,"A",{id:!0,class:!0,href:!0});var iG=l(vs);Jm=o(iG,"SPAN",{});var uG=l(Jm);v(ro.$$.fragment,uG),uG.forEach(s),iG.forEach(s),K9=h(s0),Wm=o(s0,"SPAN",{});var cG=l(Wm);F9=c(cG,"Token Classification task"),cG.forEach(s),s0.forEach(s),Vv=h(a),_p=o(a,"P",{});var pG=l(_p);U9=c(pG,`Usually used for sentence parsing, either grammatical, or Named Entity
Recognition (NER) to understand keywords contained within text.`),pG.forEach(s),Xv=h(a),v(ys.$$.fragment,a),Qv=h(a),Ke=o(a,"P",{});var _1=l(Ke);J9=c(_1,"Available with: "),oo=o(_1,"A",{href:!0,rel:!0});var fG=l(oo);W9=c(fG,"\u{1F917} Transformers"),fG.forEach(s),Y9=c(_1,`,
`),lo=o(_1,"A",{href:!0,rel:!0});var hG=l(lo);V9=c(hG,"Flair"),hG.forEach(s),_1.forEach(s),Zv=h(a),vp=o(a,"P",{});var dG=l(vp);X9=c(dG,"Example:"),dG.forEach(s),ey=h(a),v(Es.$$.fragment,a),ty=h(a),yp=o(a,"P",{});var gG=l(yp);Q9=c(gG,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),gG.forEach(s),sy=h(a),ws=o(a,"TABLE",{});var a0=l(ws);Ym=o(a0,"THEAD",{});var mG=l(Ym);io=o(mG,"TR",{});var n0=l(io);Ep=o(n0,"TH",{align:!0});var $G=l(Ep);Z9=c($G,"All parameters"),$G.forEach(s),e8=h(n0),Vm=o(n0,"TH",{align:!0}),l(Vm).forEach(s),n0.forEach(s),mG.forEach(s),t8=h(a0),Y=o(a0,"TBODY",{});var ie=l(Y);uo=o(ie,"TR",{});var r0=l(uo);co=o(r0,"TD",{align:!0});var bI=l(co);Xm=o(bI,"STRONG",{});var qG=l(Xm);s8=c(qG,"inputs"),qG.forEach(s),a8=c(bI," (required)"),bI.forEach(s),n8=h(r0),wp=o(r0,"TD",{align:!0});var _G=l(wp);r8=c(_G,"a string to be classified"),_G.forEach(s),r0.forEach(s),o8=h(ie),po=o(ie,"TR",{});var o0=l(po);bp=o(o0,"TD",{align:!0});var vG=l(bp);Qm=o(vG,"STRONG",{});var yG=l(Qm);l8=c(yG,"parameters"),yG.forEach(s),vG.forEach(s),i8=h(o0),Tp=o(o0,"TD",{align:!0});var EG=l(Tp);u8=c(EG,"a dict containing the following key:"),EG.forEach(s),o0.forEach(s),c8=h(ie),fo=o(ie,"TR",{});var l0=l(fo);jp=o(l0,"TD",{align:!0});var wG=l(jp);p8=c(wG,"aggregation_strategy"),wG.forEach(s),f8=h(l0),N=o(l0,"TD",{align:!0});var H=l(N);h8=c(H,"(Default: "),Zm=o(H,"CODE",{});var bG=l(Zm);d8=c(bG,"simple"),bG.forEach(s),g8=c(H,"). There are several aggregation strategies: "),m8=o(H,"BR",{}),$8=h(H),e$=o(H,"CODE",{});var TG=l(e$);q8=c(TG,"none"),TG.forEach(s),_8=c(H,": Every token gets classified without further aggregation. "),v8=o(H,"BR",{}),y8=h(H),t$=o(H,"CODE",{});var jG=l(t$);E8=c(jG,"simple"),jG.forEach(s),w8=c(H,": Entities are grouped according to the default schema (B-, I- tags get merged when the tag is similar). "),b8=o(H,"BR",{}),T8=h(H),s$=o(H,"CODE",{});var kG=l(s$);j8=c(kG,"first"),kG.forEach(s),k8=c(H,": Same as the "),a$=o(H,"CODE",{});var AG=l(a$);A8=c(AG,"simple"),AG.forEach(s),D8=c(H," strategy except words cannot end up with different tags. Words will use the tag of the first token when there is ambiguity. "),O8=o(H,"BR",{}),R8=h(H),n$=o(H,"CODE",{});var DG=l(n$);P8=c(DG,"average"),DG.forEach(s),S8=c(H,": Same as the "),r$=o(H,"CODE",{});var OG=l(r$);N8=c(OG,"simple"),OG.forEach(s),x8=c(H," strategy except words cannot end up with different tags. Scores are averaged across tokens and then the maximum label is applied. "),I8=o(H,"BR",{}),H8=h(H),o$=o(H,"CODE",{});var RG=l(o$);B8=c(RG,"max"),RG.forEach(s),C8=c(H,": Same as the "),l$=o(H,"CODE",{});var PG=l(l$);L8=c(PG,"simple"),PG.forEach(s),G8=c(H," strategy except words cannot end up with different tags. Word entity will be the token with the maximum score."),H.forEach(s),l0.forEach(s),M8=h(ie),ho=o(ie,"TR",{});var i0=l(ho);kp=o(i0,"TD",{align:!0});var SG=l(kp);i$=o(SG,"STRONG",{});var NG=l(i$);z8=c(NG,"options"),NG.forEach(s),SG.forEach(s),K8=h(i0),Ap=o(i0,"TD",{align:!0});var xG=l(Ap);F8=c(xG,"a dict containing the following keys:"),xG.forEach(s),i0.forEach(s),U8=h(ie),go=o(ie,"TR",{});var u0=l(go);Dp=o(u0,"TD",{align:!0});var IG=l(Dp);J8=c(IG,"use_cache"),IG.forEach(s),W8=h(u0),bs=o(u0,"TD",{align:!0});var c0=l(bs);Y8=c(c0,"(Default: "),u$=o(c0,"CODE",{});var HG=l(u$);V8=c(HG,"true"),HG.forEach(s),X8=c(c0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),c0.forEach(s),u0.forEach(s),Q8=h(ie),mo=o(ie,"TR",{});var p0=l(mo);Op=o(p0,"TD",{align:!0});var BG=l(Op);Z8=c(BG,"wait_for_model"),BG.forEach(s),eA=h(p0),Ts=o(p0,"TD",{align:!0});var f0=l(Ts);tA=c(f0,"(Default: "),c$=o(f0,"CODE",{});var CG=l(c$);sA=c(CG,"false"),CG.forEach(s),aA=c(f0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),f0.forEach(s),p0.forEach(s),ie.forEach(s),a0.forEach(s),ay=h(a),Rp=o(a,"P",{});var LG=l(Rp);nA=c(LG,"Return value is either a dict or a list of dicts if you sent a list of inputs"),LG.forEach(s),ny=h(a),v(js.$$.fragment,a),ry=h(a),ks=o(a,"TABLE",{});var h0=l(ks);p$=o(h0,"THEAD",{});var GG=l(p$);$o=o(GG,"TR",{});var d0=l($o);Pp=o(d0,"TH",{align:!0});var MG=l(Pp);rA=c(MG,"Returned values"),MG.forEach(s),oA=h(d0),f$=o(d0,"TH",{align:!0}),l(f$).forEach(s),d0.forEach(s),GG.forEach(s),lA=h(h0),Q=o(h0,"TBODY",{});var Oe=l(Q);qo=o(Oe,"TR",{});var g0=l(qo);Sp=o(g0,"TD",{align:!0});var zG=l(Sp);h$=o(zG,"STRONG",{});var KG=l(h$);iA=c(KG,"entity_group"),KG.forEach(s),zG.forEach(s),uA=h(g0),Np=o(g0,"TD",{align:!0});var FG=l(Np);cA=c(FG,"The type for the entity being recognized (model specific)."),FG.forEach(s),g0.forEach(s),pA=h(Oe),_o=o(Oe,"TR",{});var m0=l(_o);xp=o(m0,"TD",{align:!0});var UG=l(xp);d$=o(UG,"STRONG",{});var JG=l(d$);fA=c(JG,"score"),JG.forEach(s),UG.forEach(s),hA=h(m0),Ip=o(m0,"TD",{align:!0});var WG=l(Ip);dA=c(WG,"How likely the entity was recognized."),WG.forEach(s),m0.forEach(s),gA=h(Oe),vo=o(Oe,"TR",{});var $0=l(vo);Hp=o($0,"TD",{align:!0});var YG=l(Hp);g$=o(YG,"STRONG",{});var VG=l(g$);mA=c(VG,"word"),VG.forEach(s),YG.forEach(s),$A=h($0),Bp=o($0,"TD",{align:!0});var XG=l(Bp);qA=c(XG,"The string that was captured"),XG.forEach(s),$0.forEach(s),_A=h(Oe),yo=o(Oe,"TR",{});var q0=l(yo);Cp=o(q0,"TD",{align:!0});var QG=l(Cp);m$=o(QG,"STRONG",{});var ZG=l(m$);vA=c(ZG,"start"),ZG.forEach(s),QG.forEach(s),yA=h(q0),As=o(q0,"TD",{align:!0});var _0=l(As);EA=c(_0,"The offset stringwise where the answer is located. Useful to disambiguate if "),$$=o(_0,"CODE",{});var eM=l($$);wA=c(eM,"word"),eM.forEach(s),bA=c(_0," occurs multiple times."),_0.forEach(s),q0.forEach(s),TA=h(Oe),Eo=o(Oe,"TR",{});var v0=l(Eo);Lp=o(v0,"TD",{align:!0});var tM=l(Lp);q$=o(tM,"STRONG",{});var sM=l(q$);jA=c(sM,"end"),sM.forEach(s),tM.forEach(s),kA=h(v0),Ds=o(v0,"TD",{align:!0});var y0=l(Ds);AA=c(y0,"The offset stringwise where the answer is located. Useful to disambiguate if "),_$=o(y0,"CODE",{});var aM=l(_$);DA=c(aM,"word"),aM.forEach(s),OA=c(y0," occurs multiple times."),y0.forEach(s),v0.forEach(s),Oe.forEach(s),h0.forEach(s),oy=h(a),Fe=o(a,"H3",{class:!0});var E0=l(Fe);Os=o(E0,"A",{id:!0,class:!0,href:!0});var nM=l(Os);v$=o(nM,"SPAN",{});var rM=l(v$);v(wo.$$.fragment,rM),rM.forEach(s),nM.forEach(s),RA=h(E0),y$=o(E0,"SPAN",{});var oM=l(y$);PA=c(oM,"Named Entity Recognition (NER) task"),oM.forEach(s),E0.forEach(s),ly=h(a),bo=o(a,"P",{});var TI=l(bo);SA=c(TI,"See "),Gp=o(TI,"A",{href:!0});var lM=l(Gp);NA=c(lM,"Token-classification task"),lM.forEach(s),TI.forEach(s),iy=h(a),Ue=o(a,"H3",{class:!0});var w0=l(Ue);Rs=o(w0,"A",{id:!0,class:!0,href:!0});var iM=l(Rs);E$=o(iM,"SPAN",{});var uM=l(E$);v(To.$$.fragment,uM),uM.forEach(s),iM.forEach(s),xA=h(w0),w$=o(w0,"SPAN",{});var cM=l(w$);IA=c(cM,"Translation task"),cM.forEach(s),w0.forEach(s),uy=h(a),Mp=o(a,"P",{});var pM=l(Mp);HA=c(pM,"This task is well known to translate text from one language to another"),pM.forEach(s),cy=h(a),v(Ps.$$.fragment,a),py=h(a),jo=o(a,"P",{});var jI=l(jo);BA=c(jI,"Available with: "),ko=o(jI,"A",{href:!0,rel:!0});var fM=l(ko);CA=c(fM,"\u{1F917} Transformers"),fM.forEach(s),jI.forEach(s),fy=h(a),zp=o(a,"P",{});var hM=l(zp);LA=c(hM,"Example:"),hM.forEach(s),hy=h(a),v(Ss.$$.fragment,a),dy=h(a),Kp=o(a,"P",{});var dM=l(Kp);GA=c(dM,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),dM.forEach(s),gy=h(a),Ns=o(a,"TABLE",{});var b0=l(Ns);b$=o(b0,"THEAD",{});var gM=l(b$);Ao=o(gM,"TR",{});var T0=l(Ao);Fp=o(T0,"TH",{align:!0});var mM=l(Fp);MA=c(mM,"All parameters"),mM.forEach(s),zA=h(T0),T$=o(T0,"TH",{align:!0}),l(T$).forEach(s),T0.forEach(s),gM.forEach(s),KA=h(b0),de=o(b0,"TBODY",{});var en=l(de);Do=o(en,"TR",{});var j0=l(Do);Oo=o(j0,"TD",{align:!0});var kI=l(Oo);j$=o(kI,"STRONG",{});var $M=l(j$);FA=c($M,"inputs"),$M.forEach(s),UA=c(kI," (required)"),kI.forEach(s),JA=h(j0),Up=o(j0,"TD",{align:!0});var qM=l(Up);WA=c(qM,"a string to be translated in the original languages"),qM.forEach(s),j0.forEach(s),YA=h(en),Ro=o(en,"TR",{});var k0=l(Ro);Jp=o(k0,"TD",{align:!0});var _M=l(Jp);k$=o(_M,"STRONG",{});var vM=l(k$);VA=c(vM,"options"),vM.forEach(s),_M.forEach(s),XA=h(k0),Wp=o(k0,"TD",{align:!0});var yM=l(Wp);QA=c(yM,"a dict containing the following keys:"),yM.forEach(s),k0.forEach(s),ZA=h(en),Po=o(en,"TR",{});var A0=l(Po);Yp=o(A0,"TD",{align:!0});var EM=l(Yp);eD=c(EM,"use_cache"),EM.forEach(s),tD=h(A0),xs=o(A0,"TD",{align:!0});var D0=l(xs);sD=c(D0,"(Default: "),A$=o(D0,"CODE",{});var wM=l(A$);aD=c(wM,"true"),wM.forEach(s),nD=c(D0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),D0.forEach(s),A0.forEach(s),rD=h(en),So=o(en,"TR",{});var O0=l(So);Vp=o(O0,"TD",{align:!0});var bM=l(Vp);oD=c(bM,"wait_for_model"),bM.forEach(s),lD=h(O0),Is=o(O0,"TD",{align:!0});var R0=l(Is);iD=c(R0,"(Default: "),D$=o(R0,"CODE",{});var TM=l(D$);uD=c(TM,"false"),TM.forEach(s),cD=c(R0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),R0.forEach(s),O0.forEach(s),en.forEach(s),b0.forEach(s),my=h(a),Xp=o(a,"P",{});var jM=l(Xp);pD=c(jM,"Return value is either a dict or a list of dicts if you sent a list of inputs"),jM.forEach(s),$y=h(a),Hs=o(a,"TABLE",{});var P0=l(Hs);O$=o(P0,"THEAD",{});var kM=l(O$);No=o(kM,"TR",{});var S0=l(No);Qp=o(S0,"TH",{align:!0});var AM=l(Qp);fD=c(AM,"Returned values"),AM.forEach(s),hD=h(S0),R$=o(S0,"TH",{align:!0}),l(R$).forEach(s),S0.forEach(s),kM.forEach(s),dD=h(P0),P$=o(P0,"TBODY",{});var DM=l(P$);xo=o(DM,"TR",{});var N0=l(xo);Zp=o(N0,"TD",{align:!0});var OM=l(Zp);S$=o(OM,"STRONG",{});var RM=l(S$);gD=c(RM,"translation_text"),RM.forEach(s),OM.forEach(s),mD=h(N0),ef=o(N0,"TD",{align:!0});var PM=l(ef);$D=c(PM,"The string after translation"),PM.forEach(s),N0.forEach(s),DM.forEach(s),P0.forEach(s),qy=h(a),Je=o(a,"H3",{class:!0});var x0=l(Je);Bs=o(x0,"A",{id:!0,class:!0,href:!0});var SM=l(Bs);N$=o(SM,"SPAN",{});var NM=l(N$);v(Io.$$.fragment,NM),NM.forEach(s),SM.forEach(s),qD=h(x0),x$=o(x0,"SPAN",{});var xM=l(x$);_D=c(xM,"Zero-Shot Classification task"),xM.forEach(s),x0.forEach(s),_y=h(a),tf=o(a,"P",{});var IM=l(tf);vD=c(IM,`This task is super useful to try out classification with zero code,
you simply pass a sentence/paragraph and the possible labels for that
sentence, and you get a result.`),IM.forEach(s),vy=h(a),v(Cs.$$.fragment,a),yy=h(a),Ho=o(a,"P",{});var AI=l(Ho);yD=c(AI,"Available with: "),Bo=o(AI,"A",{href:!0,rel:!0});var HM=l(Bo);ED=c(HM,"\u{1F917} Transformers"),HM.forEach(s),AI.forEach(s),Ey=h(a),sf=o(a,"P",{});var BM=l(sf);wD=c(BM,"Request:"),BM.forEach(s),wy=h(a),v(Ls.$$.fragment,a),by=h(a),af=o(a,"P",{});var CM=l(af);bD=c(CM,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),CM.forEach(s),Ty=h(a),Gs=o(a,"TABLE",{});var I0=l(Gs);I$=o(I0,"THEAD",{});var LM=l(I$);Co=o(LM,"TR",{});var H0=l(Co);nf=o(H0,"TH",{align:!0});var GM=l(nf);TD=c(GM,"All parameters"),GM.forEach(s),jD=h(H0),H$=o(H0,"TH",{align:!0}),l(H$).forEach(s),H0.forEach(s),LM.forEach(s),kD=h(I0),F=o(I0,"TBODY",{});var V=l(F);Lo=o(V,"TR",{});var B0=l(Lo);Go=o(B0,"TD",{align:!0});var DI=l(Go);B$=o(DI,"STRONG",{});var MM=l(B$);AD=c(MM,"inputs"),MM.forEach(s),DD=c(DI," (required)"),DI.forEach(s),OD=h(B0),rf=o(B0,"TD",{align:!0});var zM=l(rf);RD=c(zM,"a string or list of strings"),zM.forEach(s),B0.forEach(s),PD=h(V),Mo=o(V,"TR",{});var C0=l(Mo);zo=o(C0,"TD",{align:!0});var OI=l(zo);C$=o(OI,"STRONG",{});var KM=l(C$);SD=c(KM,"parameters"),KM.forEach(s),ND=c(OI," (required)"),OI.forEach(s),xD=h(C0),of=o(C0,"TD",{align:!0});var FM=l(of);ID=c(FM,"a dict containing the following keys:"),FM.forEach(s),C0.forEach(s),HD=h(V),Ko=o(V,"TR",{});var L0=l(Ko);lf=o(L0,"TD",{align:!0});var UM=l(lf);BD=c(UM,"candidate_labels (required)"),UM.forEach(s),CD=h(L0),je=o(L0,"TD",{align:!0});var wd=l(je);LD=c(wd,"a list of strings that are potential classes for "),L$=o(wd,"CODE",{});var JM=l(L$);GD=c(JM,"inputs"),JM.forEach(s),MD=c(wd,". (max 10 candidate_labels, for more, simply run multiple requests, results are going to be misleading if using too many candidate_labels anyway. If you want to keep the exact same, you can simply run "),G$=o(wd,"CODE",{});var WM=l(G$);zD=c(WM,"multi_label=True"),WM.forEach(s),KD=c(wd," and do the scaling on your end. )"),wd.forEach(s),L0.forEach(s),FD=h(V),Fo=o(V,"TR",{});var G0=l(Fo);uf=o(G0,"TD",{align:!0});var YM=l(uf);UD=c(YM,"multi_label"),YM.forEach(s),JD=h(G0),Ms=o(G0,"TD",{align:!0});var M0=l(Ms);WD=c(M0,"(Default: "),M$=o(M0,"CODE",{});var VM=l(M$);YD=c(VM,"false"),VM.forEach(s),VD=c(M0,") Boolean that is set to True if classes can overlap"),M0.forEach(s),G0.forEach(s),XD=h(V),Uo=o(V,"TR",{});var z0=l(Uo);cf=o(z0,"TD",{align:!0});var XM=l(cf);z$=o(XM,"STRONG",{});var QM=l(z$);QD=c(QM,"options"),QM.forEach(s),XM.forEach(s),ZD=h(z0),pf=o(z0,"TD",{align:!0});var ZM=l(pf);eO=c(ZM,"a dict containing the following keys:"),ZM.forEach(s),z0.forEach(s),tO=h(V),Jo=o(V,"TR",{});var K0=l(Jo);ff=o(K0,"TD",{align:!0});var ez=l(ff);sO=c(ez,"use_cache"),ez.forEach(s),aO=h(K0),zs=o(K0,"TD",{align:!0});var F0=l(zs);nO=c(F0,"(Default: "),K$=o(F0,"CODE",{});var tz=l(K$);rO=c(tz,"true"),tz.forEach(s),oO=c(F0,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),F0.forEach(s),K0.forEach(s),lO=h(V),Wo=o(V,"TR",{});var U0=l(Wo);hf=o(U0,"TD",{align:!0});var sz=l(hf);iO=c(sz,"wait_for_model"),sz.forEach(s),uO=h(U0),Ks=o(U0,"TD",{align:!0});var J0=l(Ks);cO=c(J0,"(Default: "),F$=o(J0,"CODE",{});var az=l(F$);pO=c(az,"false"),az.forEach(s),fO=c(J0,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),J0.forEach(s),U0.forEach(s),V.forEach(s),I0.forEach(s),jy=h(a),df=o(a,"P",{});var nz=l(df);hO=c(nz,"Return value is either a dict or a list of dicts if you sent a list of inputs"),nz.forEach(s),ky=h(a),gf=o(a,"P",{});var rz=l(gf);dO=c(rz,"Response:"),rz.forEach(s),Ay=h(a),v(Fs.$$.fragment,a),Dy=h(a),Us=o(a,"TABLE",{});var W0=l(Us);U$=o(W0,"THEAD",{});var oz=l(U$);Yo=o(oz,"TR",{});var Y0=l(Yo);mf=o(Y0,"TH",{align:!0});var lz=l(mf);gO=c(lz,"Returned values"),lz.forEach(s),mO=h(Y0),J$=o(Y0,"TH",{align:!0}),l(J$).forEach(s),Y0.forEach(s),oz.forEach(s),$O=h(W0),We=o(W0,"TBODY",{});var bd=l(We);Vo=o(bd,"TR",{});var V0=l(Vo);$f=o(V0,"TD",{align:!0});var iz=l($f);W$=o(iz,"STRONG",{});var uz=l(W$);qO=c(uz,"sequence"),uz.forEach(s),iz.forEach(s),_O=h(V0),qf=o(V0,"TD",{align:!0});var cz=l(qf);vO=c(cz,"The string sent as an input"),cz.forEach(s),V0.forEach(s),yO=h(bd),Xo=o(bd,"TR",{});var X0=l(Xo);_f=o(X0,"TD",{align:!0});var pz=l(_f);Y$=o(pz,"STRONG",{});var fz=l(Y$);EO=c(fz,"labels"),fz.forEach(s),pz.forEach(s),wO=h(X0),vf=o(X0,"TD",{align:!0});var hz=l(vf);bO=c(hz,"The list of strings for labels that you sent (in order)"),hz.forEach(s),X0.forEach(s),TO=h(bd),Qo=o(bd,"TR",{});var Q0=l(Qo);yf=o(Q0,"TD",{align:!0});var dz=l(yf);V$=o(dz,"STRONG",{});var gz=l(V$);jO=c(gz,"scores"),gz.forEach(s),dz.forEach(s),kO=h(Q0),Js=o(Q0,"TD",{align:!0});var Z0=l(Js);AO=c(Z0,"a list of floats that correspond the the probability of label, in the same order as "),X$=o(Z0,"CODE",{});var mz=l(X$);DO=c(mz,"labels"),mz.forEach(s),OO=c(Z0,"."),Z0.forEach(s),Q0.forEach(s),bd.forEach(s),W0.forEach(s),Oy=h(a),Ye=o(a,"H3",{class:!0});var eb=l(Ye);Ws=o(eb,"A",{id:!0,class:!0,href:!0});var $z=l(Ws);Q$=o($z,"SPAN",{});var qz=l(Q$);v(Zo.$$.fragment,qz),qz.forEach(s),$z.forEach(s),RO=h(eb),Z$=o(eb,"SPAN",{});var _z=l(Z$);PO=c(_z,"Conversational task"),_z.forEach(s),eb.forEach(s),Ry=h(a),Ef=o(a,"P",{});var vz=l(Ef);SO=c(vz,`This task corresponds to any chatbot like structure. Models tend to have
shorter max_length, so please check with caution when using a given
model if you need long range dependency or not.`),vz.forEach(s),Py=h(a),v(Ys.$$.fragment,a),Sy=h(a),el=o(a,"P",{});var RI=l(el);NO=c(RI,"Available with: "),tl=o(RI,"A",{href:!0,rel:!0});var yz=l(tl);xO=c(yz,"\u{1F917} Transformers"),yz.forEach(s),RI.forEach(s),Ny=h(a),wf=o(a,"P",{});var Ez=l(wf);IO=c(Ez,"Example:"),Ez.forEach(s),xy=h(a),v(Vs.$$.fragment,a),Iy=h(a),bf=o(a,"P",{});var wz=l(bf);HO=c(wz,`When sending your request, you should send a JSON encoded payload. Here
are all the options`),wz.forEach(s),Hy=h(a),Xs=o(a,"TABLE",{});var tb=l(Xs);eq=o(tb,"THEAD",{});var bz=l(eq);sl=o(bz,"TR",{});var sb=l(sl);Tf=o(sb,"TH",{align:!0});var Tz=l(Tf);BO=c(Tz,"All parameters"),Tz.forEach(s),CO=h(sb),tq=o(sb,"TH",{align:!0}),l(tq).forEach(s),sb.forEach(s),bz.forEach(s),LO=h(tb),x=o(tb,"TBODY",{});var B=l(x);al=o(B,"TR",{});var ab=l(al);nl=o(ab,"TD",{align:!0});var PI=l(nl);sq=o(PI,"STRONG",{});var jz=l(sq);GO=c(jz,"inputs"),jz.forEach(s),MO=c(PI," (required)"),PI.forEach(s),zO=h(ab),aq=o(ab,"TD",{align:!0}),l(aq).forEach(s),ab.forEach(s),KO=h(B),rl=o(B,"TR",{});var nb=l(rl);jf=o(nb,"TD",{align:!0});var kz=l(jf);FO=c(kz,"text (required)"),kz.forEach(s),UO=h(nb),kf=o(nb,"TD",{align:!0});var Az=l(kf);JO=c(Az,"The last input from the user in the conversation."),Az.forEach(s),nb.forEach(s),WO=h(B),ol=o(B,"TR",{});var rb=l(ol);Af=o(rb,"TD",{align:!0});var Dz=l(Af);YO=c(Dz,"generated_responses"),Dz.forEach(s),VO=h(rb),Df=o(rb,"TD",{align:!0});var Oz=l(Df);XO=c(Oz,"A list of strings corresponding to the earlier replies from the model."),Oz.forEach(s),rb.forEach(s),QO=h(B),ll=o(B,"TR",{});var ob=l(ll);Of=o(ob,"TD",{align:!0});var Rz=l(Of);ZO=c(Rz,"past_user_inputs"),Rz.forEach(s),eR=h(ob),Qs=o(ob,"TD",{align:!0});var lb=l(Qs);tR=c(lb,"A list of strings corresponding to the earlier replies from the user. Should be of the same length of "),nq=o(lb,"CODE",{});var Pz=l(nq);sR=c(Pz,"generated_responses"),Pz.forEach(s),aR=c(lb,"."),lb.forEach(s),ob.forEach(s),nR=h(B),il=o(B,"TR",{});var ib=l(il);Rf=o(ib,"TD",{align:!0});var Sz=l(Rf);rq=o(Sz,"STRONG",{});var Nz=l(rq);rR=c(Nz,"parameters"),Nz.forEach(s),Sz.forEach(s),oR=h(ib),Pf=o(ib,"TD",{align:!0});var xz=l(Pf);lR=c(xz,"a dict containing the following keys:"),xz.forEach(s),ib.forEach(s),iR=h(B),ul=o(B,"TR",{});var ub=l(ul);Sf=o(ub,"TD",{align:!0});var Iz=l(Sf);uR=c(Iz,"min_length"),Iz.forEach(s),cR=h(ub),ke=o(ub,"TD",{align:!0});var Td=l(ke);pR=c(Td,"(Default: "),oq=o(Td,"CODE",{});var Hz=l(oq);fR=c(Hz,"None"),Hz.forEach(s),hR=c(Td,"). Integer to define the minimum length "),lq=o(Td,"STRONG",{});var Bz=l(lq);dR=c(Bz,"in tokens"),Bz.forEach(s),gR=c(Td," of the output summary."),Td.forEach(s),ub.forEach(s),mR=h(B),cl=o(B,"TR",{});var cb=l(cl);Nf=o(cb,"TD",{align:!0});var Cz=l(Nf);$R=c(Cz,"max_length"),Cz.forEach(s),qR=h(cb),Ae=o(cb,"TD",{align:!0});var jd=l(Ae);_R=c(jd,"(Default: "),iq=o(jd,"CODE",{});var Lz=l(iq);vR=c(Lz,"None"),Lz.forEach(s),yR=c(jd,"). Integer to define the maximum length "),uq=o(jd,"STRONG",{});var Gz=l(uq);ER=c(Gz,"in tokens"),Gz.forEach(s),wR=c(jd," of the output summary."),jd.forEach(s),cb.forEach(s),bR=h(B),pl=o(B,"TR",{});var pb=l(pl);xf=o(pb,"TD",{align:!0});var Mz=l(xf);TR=c(Mz,"top_k"),Mz.forEach(s),jR=h(pb),De=o(pb,"TD",{align:!0});var kd=l(De);kR=c(kd,"(Default: "),cq=o(kd,"CODE",{});var zz=l(cq);AR=c(zz,"None"),zz.forEach(s),DR=c(kd,"). Integer to define the top tokens considered within the "),pq=o(kd,"CODE",{});var Kz=l(pq);OR=c(Kz,"sample"),Kz.forEach(s),RR=c(kd," operation to create new text."),kd.forEach(s),pb.forEach(s),PR=h(B),fl=o(B,"TR",{});var fb=l(fl);If=o(fb,"TD",{align:!0});var Fz=l(If);SR=c(Fz,"top_p"),Fz.forEach(s),NR=h(fb),ne=o(fb,"TD",{align:!0});var tn=l(ne);xR=c(tn,"(Default: "),fq=o(tn,"CODE",{});var Uz=l(fq);IR=c(Uz,"None"),Uz.forEach(s),HR=c(tn,"). Float to define the tokens that are within the "),hq=o(tn,"CODE",{});var Jz=l(hq);BR=c(Jz,"sample"),Jz.forEach(s),CR=c(tn," operation of text generation. Add tokens in the sample for more probable to least probable until the sum of the probabilities is greater than "),dq=o(tn,"CODE",{});var Wz=l(dq);LR=c(Wz,"top_p"),Wz.forEach(s),GR=c(tn,"."),tn.forEach(s),fb.forEach(s),MR=h(B),hl=o(B,"TR",{});var hb=l(hl);Hf=o(hb,"TD",{align:!0});var Yz=l(Hf);zR=c(Yz,"temperature"),Yz.forEach(s),KR=h(hb),re=o(hb,"TD",{align:!0});var sn=l(re);FR=c(sn,"(Default: "),gq=o(sn,"CODE",{});var Vz=l(gq);UR=c(Vz,"1.0"),Vz.forEach(s),JR=c(sn,"). Float (0.0-100.0). The temperature of the sampling operation. 1 means regular sampling, "),mq=o(sn,"CODE",{});var Xz=l(mq);WR=c(Xz,"0"),Xz.forEach(s),YR=c(sn," means always take the highest score, "),$q=o(sn,"CODE",{});var Qz=l($q);VR=c(Qz,"100.0"),Qz.forEach(s),XR=c(sn," is getting closer to uniform probability."),sn.forEach(s),hb.forEach(s),QR=h(B),dl=o(B,"TR",{});var db=l(dl);Bf=o(db,"TD",{align:!0});var Zz=l(Bf);ZR=c(Zz,"repetition_penalty"),Zz.forEach(s),eP=h(db),Zs=o(db,"TD",{align:!0});var gb=l(Zs);tP=c(gb,"(Default: "),qq=o(gb,"CODE",{});var eK=l(qq);sP=c(eK,"None"),eK.forEach(s),aP=c(gb,"). Float (0.0-100.0). The more a token is used within generation the more it is penalized to not be picked in successive generation passes."),gb.forEach(s),db.forEach(s),nP=h(B),gl=o(B,"TR",{});var mb=l(gl);Cf=o(mb,"TD",{align:!0});var tK=l(Cf);rP=c(tK,"max_time"),tK.forEach(s),oP=h(mb),ea=o(mb,"TD",{align:!0});var $b=l(ea);lP=c($b,"(Default: "),_q=o($b,"CODE",{});var sK=l(_q);iP=c(sK,"None"),sK.forEach(s),uP=c($b,"). Float (0-120.0). The amount of time in seconds that the query should take maximum. Network can cause some overhead so it will be a soft limit."),$b.forEach(s),mb.forEach(s),cP=h(B),ml=o(B,"TR",{});var qb=l(ml);Lf=o(qb,"TD",{align:!0});var aK=l(Lf);vq=o(aK,"STRONG",{});var nK=l(vq);pP=c(nK,"options"),nK.forEach(s),aK.forEach(s),fP=h(qb),Gf=o(qb,"TD",{align:!0});var rK=l(Gf);hP=c(rK,"a dict containing the following keys:"),rK.forEach(s),qb.forEach(s),dP=h(B),$l=o(B,"TR",{});var _b=l($l);Mf=o(_b,"TD",{align:!0});var oK=l(Mf);gP=c(oK,"use_cache"),oK.forEach(s),mP=h(_b),ta=o(_b,"TD",{align:!0});var vb=l(ta);$P=c(vb,"(Default: "),yq=o(vb,"CODE",{});var lK=l(yq);qP=c(lK,"true"),lK.forEach(s),_P=c(vb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),vb.forEach(s),_b.forEach(s),vP=h(B),ql=o(B,"TR",{});var yb=l(ql);zf=o(yb,"TD",{align:!0});var iK=l(zf);yP=c(iK,"wait_for_model"),iK.forEach(s),EP=h(yb),sa=o(yb,"TD",{align:!0});var Eb=l(sa);wP=c(Eb,"(Default: "),Eq=o(Eb,"CODE",{});var uK=l(Eq);bP=c(uK,"false"),uK.forEach(s),TP=c(Eb,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Eb.forEach(s),yb.forEach(s),B.forEach(s),tb.forEach(s),By=h(a),Kf=o(a,"P",{});var cK=l(Kf);jP=c(cK,"Return value is either a dict or a list of dicts if you sent a list of inputs"),cK.forEach(s),Cy=h(a),aa=o(a,"TABLE",{});var wb=l(aa);wq=o(wb,"THEAD",{});var pK=l(wq);_l=o(pK,"TR",{});var bb=l(_l);Ff=o(bb,"TH",{align:!0});var fK=l(Ff);kP=c(fK,"Returned values"),fK.forEach(s),AP=h(bb),bq=o(bb,"TH",{align:!0}),l(bq).forEach(s),bb.forEach(s),pK.forEach(s),DP=h(wb),ge=o(wb,"TBODY",{});var an=l(ge);vl=o(an,"TR",{});var Tb=l(vl);Uf=o(Tb,"TD",{align:!0});var hK=l(Uf);Tq=o(hK,"STRONG",{});var dK=l(Tq);OP=c(dK,"generated_text"),dK.forEach(s),hK.forEach(s),RP=h(Tb),Jf=o(Tb,"TD",{align:!0});var gK=l(Jf);PP=c(gK,"The answer of the bot"),gK.forEach(s),Tb.forEach(s),SP=h(an),yl=o(an,"TR",{});var jb=l(yl);Wf=o(jb,"TD",{align:!0});var mK=l(Wf);jq=o(mK,"STRONG",{});var $K=l(jq);NP=c($K,"conversation"),$K.forEach(s),mK.forEach(s),xP=h(jb),Yf=o(jb,"TD",{align:!0});var qK=l(Yf);IP=c(qK,"A facility dictionnary to send back for the next input (with the new user input addition)."),qK.forEach(s),jb.forEach(s),HP=h(an),El=o(an,"TR",{});var kb=l(El);Vf=o(kb,"TD",{align:!0});var _K=l(Vf);BP=c(_K,"past_user_inputs"),_K.forEach(s),CP=h(kb),Xf=o(kb,"TD",{align:!0});var vK=l(Xf);LP=c(vK,"List of strings. The last inputs from the user in the conversation, <em>after the model has run."),vK.forEach(s),kb.forEach(s),GP=h(an),wl=o(an,"TR",{});var Ab=l(wl);Qf=o(Ab,"TD",{align:!0});var yK=l(Qf);MP=c(yK,"generated_responses"),yK.forEach(s),zP=h(Ab),Zf=o(Ab,"TD",{align:!0});var EK=l(Zf);KP=c(EK,"List of strings. The last outputs from the model in the conversation, <em>after the model has run."),EK.forEach(s),Ab.forEach(s),an.forEach(s),wb.forEach(s),Ly=h(a),Ve=o(a,"H3",{class:!0});var Db=l(Ve);na=o(Db,"A",{id:!0,class:!0,href:!0});var wK=l(na);kq=o(wK,"SPAN",{});var bK=l(kq);v(bl.$$.fragment,bK),bK.forEach(s),wK.forEach(s),FP=h(Db),Aq=o(Db,"SPAN",{});var TK=l(Aq);UP=c(TK,"Feature Extraction task"),TK.forEach(s),Db.forEach(s),Gy=h(a),eh=o(a,"P",{});var jK=l(eh);JP=c(jK,`This task reads some text and outputs raw float values, that are usually
consumed as part of a semantic database/semantic search.`),jK.forEach(s),My=h(a),v(ra.$$.fragment,a),zy=h(a),Xe=o(a,"P",{});var v1=l(Xe);WP=c(v1,"Available with: "),Tl=o(v1,"A",{href:!0,rel:!0});var kK=l(Tl);YP=c(kK,"\u{1F917} Transformers"),kK.forEach(s),VP=h(v1),jl=o(v1,"A",{href:!0,rel:!0});var AK=l(jl);XP=c(AK,"Sentence-transformers"),AK.forEach(s),v1.forEach(s),Ky=h(a),th=o(a,"P",{});var DK=l(th);QP=c(DK,"Request:"),DK.forEach(s),Fy=h(a),oa=o(a,"TABLE",{});var Ob=l(oa);Dq=o(Ob,"THEAD",{});var OK=l(Dq);kl=o(OK,"TR",{});var Rb=l(kl);sh=o(Rb,"TH",{align:!0});var RK=l(sh);ZP=c(RK,"All parameters"),RK.forEach(s),eS=h(Rb),Oq=o(Rb,"TH",{align:!0}),l(Oq).forEach(s),Rb.forEach(s),OK.forEach(s),tS=h(Ob),me=o(Ob,"TBODY",{});var nn=l(me);Al=o(nn,"TR",{});var Pb=l(Al);Dl=o(Pb,"TD",{align:!0});var SI=l(Dl);Rq=o(SI,"STRONG",{});var PK=l(Rq);sS=c(PK,"inputs"),PK.forEach(s),aS=c(SI," (required):"),SI.forEach(s),nS=h(Pb),ah=o(Pb,"TD",{align:!0});var SK=l(ah);rS=c(SK,"a string or a list of strings to get the features from."),SK.forEach(s),Pb.forEach(s),oS=h(nn),Ol=o(nn,"TR",{});var Sb=l(Ol);nh=o(Sb,"TD",{align:!0});var NK=l(nh);Pq=o(NK,"STRONG",{});var xK=l(Pq);lS=c(xK,"options"),xK.forEach(s),NK.forEach(s),iS=h(Sb),rh=o(Sb,"TD",{align:!0});var IK=l(rh);uS=c(IK,"a dict containing the following keys:"),IK.forEach(s),Sb.forEach(s),cS=h(nn),Rl=o(nn,"TR",{});var Nb=l(Rl);oh=o(Nb,"TD",{align:!0});var HK=l(oh);pS=c(HK,"use_cache"),HK.forEach(s),fS=h(Nb),la=o(Nb,"TD",{align:!0});var xb=l(la);hS=c(xb,"(Default: "),Sq=o(xb,"CODE",{});var BK=l(Sq);dS=c(BK,"true"),BK.forEach(s),gS=c(xb,"). Boolean. There is a cache layer on the inference API to speedup requests we have already seen. Most models can use those results as is as models are deterministic (meaning the results will be the same anyway). However if you use a non deterministic model, you can set this parameter to prevent the caching mechanism from being used resulting in a real new query."),xb.forEach(s),Nb.forEach(s),mS=h(nn),Pl=o(nn,"TR",{});var Ib=l(Pl);lh=o(Ib,"TD",{align:!0});var CK=l(lh);$S=c(CK,"wait_for_model"),CK.forEach(s),qS=h(Ib),ia=o(Ib,"TD",{align:!0});var Hb=l(ia);_S=c(Hb,"(Default: "),Nq=o(Hb,"CODE",{});var LK=l(Nq);vS=c(LK,"false"),LK.forEach(s),yS=c(Hb,") Boolean. If the model is not ready, wait for it instead of receiving 503. It limits the number of requests required to get your inference done. It is advised to only set this flag to true after receiving a 503 error as it will limit hanging in your application to known places."),Hb.forEach(s),Ib.forEach(s),nn.forEach(s),Ob.forEach(s),Uy=h(a),ih=o(a,"P",{});var GK=l(ih);ES=c(GK,"Return value is either a dict or a list of dicts if you sent a list of inputs"),GK.forEach(s),Jy=h(a),ua=o(a,"TABLE",{});var Bb=l(ua);xq=o(Bb,"THEAD",{});var MK=l(xq);Sl=o(MK,"TR",{});var Cb=l(Sl);uh=o(Cb,"TH",{align:!0});var zK=l(uh);wS=c(zK,"Returned values"),zK.forEach(s),bS=h(Cb),Iq=o(Cb,"TH",{align:!0}),l(Iq).forEach(s),Cb.forEach(s),MK.forEach(s),TS=h(Bb),Hq=o(Bb,"TBODY",{});var KK=l(Hq);Nl=o(KK,"TR",{});var Lb=l(Nl);ch=o(Lb,"TD",{align:!0});var FK=l(ch);Bq=o(FK,"STRONG",{});var UK=l(Bq);jS=c(UK,"A list of float (or list of list of floats)"),UK.forEach(s),FK.forEach(s),kS=h(Lb),ph=o(Lb,"TD",{align:!0});var JK=l(ph);AS=c(JK,"The numbers that are the representation features of the input."),JK.forEach(s),Lb.forEach(s),KK.forEach(s),Bb.forEach(s),Wy=h(a),fh=o(a,"SMALL",{});var WK=l(fh);DS=c(WK,`Returned values are a list of floats, or a list of list of floats (depending
  on if you sent a string or a list of string, and if the automatic reduction,
  usually mean_pooling for instance was applied for you or not. This should be
  explained on the model's README.`),WK.forEach(s),Yy=h(a),Qe=o(a,"H2",{class:!0});var Gb=l(Qe);ca=o(Gb,"A",{id:!0,class:!0,href:!0});var YK=l(ca);Cq=o(YK,"SPAN",{});var VK=l(Cq);v(xl.$$.fragment,VK),VK.forEach(s),YK.forEach(s),OS=h(Gb),Lq=o(Gb,"SPAN",{});var XK=l(Lq);RS=c(XK,"Audio"),XK.forEach(s),Gb.forEach(s),Vy=h(a),Ze=o(a,"H3",{class:!0});var Mb=l(Ze);pa=o(Mb,"A",{id:!0,class:!0,href:!0});var QK=l(pa);Gq=o(QK,"SPAN",{});var ZK=l(Gq);v(Il.$$.fragment,ZK),ZK.forEach(s),QK.forEach(s),PS=h(Mb),Mq=o(Mb,"SPAN",{});var eF=l(Mq);SS=c(eF,"Automatic Speech Recognition task"),eF.forEach(s),Mb.forEach(s),Xy=h(a),hh=o(a,"P",{});var tF=l(hh);NS=c(tF,`This task reads some audio input and outputs the said words within the
audio files.`),tF.forEach(s),Qy=h(a),v(fa.$$.fragment,a),Zy=h(a),v(ha.$$.fragment,a),e2=h(a),$e=o(a,"P",{});var Wi=l($e);xS=c(Wi,"Available with: "),Hl=o(Wi,"A",{href:!0,rel:!0});var sF=l(Hl);IS=c(sF,"\u{1F917} Transformers"),sF.forEach(s),HS=h(Wi),Bl=o(Wi,"A",{href:!0,rel:!0});var aF=l(Bl);BS=c(aF,"ESPnet"),aF.forEach(s),CS=c(Wi,` and
`),Cl=o(Wi,"A",{href:!0,rel:!0});var nF=l(Cl);LS=c(nF,"SpeechBrain"),nF.forEach(s),Wi.forEach(s),t2=h(a),dh=o(a,"P",{});var rF=l(dh);GS=c(rF,"Request:"),rF.forEach(s),s2=h(a),v(da.$$.fragment,a),a2=h(a),gh=o(a,"P",{});var oF=l(gh);MS=c(oF,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),oF.forEach(s),n2=h(a),ga=o(a,"TABLE",{});var zb=l(ga);zq=o(zb,"THEAD",{});var lF=l(zq);Ll=o(lF,"TR",{});var Kb=l(Ll);mh=o(Kb,"TH",{align:!0});var iF=l(mh);zS=c(iF,"All parameters"),iF.forEach(s),KS=h(Kb),Kq=o(Kb,"TH",{align:!0}),l(Kq).forEach(s),Kb.forEach(s),lF.forEach(s),FS=h(zb),Fq=o(zb,"TBODY",{});var uF=l(Fq);Gl=o(uF,"TR",{});var Fb=l(Gl);Ml=o(Fb,"TD",{align:!0});var NI=l(Ml);Uq=o(NI,"STRONG",{});var cF=l(Uq);US=c(cF,"no parameter"),cF.forEach(s),JS=c(NI," (required)"),NI.forEach(s),WS=h(Fb),$h=o(Fb,"TD",{align:!0});var pF=l($h);YS=c(pF,"a binary representation of the audio file. No other parameters are currently allowed."),pF.forEach(s),Fb.forEach(s),uF.forEach(s),zb.forEach(s),r2=h(a),qh=o(a,"P",{});var fF=l(qh);VS=c(fF,"Return value is either a dict or a list of dicts if you sent a list of inputs"),fF.forEach(s),o2=h(a),_h=o(a,"P",{});var hF=l(_h);XS=c(hF,"Response:"),hF.forEach(s),l2=h(a),v(ma.$$.fragment,a),i2=h(a),$a=o(a,"TABLE",{});var Ub=l($a);Jq=o(Ub,"THEAD",{});var dF=l(Jq);zl=o(dF,"TR",{});var Jb=l(zl);vh=o(Jb,"TH",{align:!0});var gF=l(vh);QS=c(gF,"Returned values"),gF.forEach(s),ZS=h(Jb),Wq=o(Jb,"TH",{align:!0}),l(Wq).forEach(s),Jb.forEach(s),dF.forEach(s),eN=h(Ub),Yq=o(Ub,"TBODY",{});var mF=l(Yq);Kl=o(mF,"TR",{});var Wb=l(Kl);yh=o(Wb,"TD",{align:!0});var $F=l(yh);Vq=o($F,"STRONG",{});var qF=l(Vq);tN=c(qF,"text"),qF.forEach(s),$F.forEach(s),sN=h(Wb),Eh=o(Wb,"TD",{align:!0});var _F=l(Eh);aN=c(_F,"The string that was recognized within the audio file."),_F.forEach(s),Wb.forEach(s),mF.forEach(s),Ub.forEach(s),u2=h(a),et=o(a,"H3",{class:!0});var Yb=l(et);qa=o(Yb,"A",{id:!0,class:!0,href:!0});var vF=l(qa);Xq=o(vF,"SPAN",{});var yF=l(Xq);v(Fl.$$.fragment,yF),yF.forEach(s),vF.forEach(s),nN=h(Yb),Qq=o(Yb,"SPAN",{});var EF=l(Qq);rN=c(EF,"Audio Classification task"),EF.forEach(s),Yb.forEach(s),c2=h(a),wh=o(a,"P",{});var wF=l(wh);oN=c(wF,"This task reads some audio input and outputs the likelihood of classes."),wF.forEach(s),p2=h(a),v(_a.$$.fragment,a),f2=h(a),tt=o(a,"P",{});var y1=l(tt);lN=c(y1,"Available with: "),Ul=o(y1,"A",{href:!0,rel:!0});var bF=l(Ul);iN=c(bF,"\u{1F917} Transformers"),bF.forEach(s),uN=h(y1),Jl=o(y1,"A",{href:!0,rel:!0});var TF=l(Jl);cN=c(TF,"SpeechBrain"),TF.forEach(s),y1.forEach(s),h2=h(a),bh=o(a,"P",{});var jF=l(bh);pN=c(jF,"Request:"),jF.forEach(s),d2=h(a),v(va.$$.fragment,a),g2=h(a),Th=o(a,"P",{});var kF=l(Th);fN=c(kF,`When sending your request, you should send a binary payload that simply
contains your audio file. We try to support most formats (Flac, Wav,
Mp3, Ogg etc...). And we automatically rescale the sampling rate to the
appropriate rate for the given model (usually 16KHz).`),kF.forEach(s),m2=h(a),ya=o(a,"TABLE",{});var Vb=l(ya);Zq=o(Vb,"THEAD",{});var AF=l(Zq);Wl=o(AF,"TR",{});var Xb=l(Wl);jh=o(Xb,"TH",{align:!0});var DF=l(jh);hN=c(DF,"All parameters"),DF.forEach(s),dN=h(Xb),e_=o(Xb,"TH",{align:!0}),l(e_).forEach(s),Xb.forEach(s),AF.forEach(s),gN=h(Vb),t_=o(Vb,"TBODY",{});var OF=l(t_);Yl=o(OF,"TR",{});var Qb=l(Yl);Vl=o(Qb,"TD",{align:!0});var xI=l(Vl);s_=o(xI,"STRONG",{});var RF=l(s_);mN=c(RF,"no parameter"),RF.forEach(s),$N=c(xI," (required)"),xI.forEach(s),qN=h(Qb),kh=o(Qb,"TD",{align:!0});var PF=l(kh);_N=c(PF,"a binary representation of the audio file. No other parameters are currently allowed."),PF.forEach(s),Qb.forEach(s),OF.forEach(s),Vb.forEach(s),$2=h(a),Ah=o(a,"P",{});var SF=l(Ah);vN=c(SF,"Return value is a dict"),SF.forEach(s),q2=h(a),v(Ea.$$.fragment,a),_2=h(a),wa=o(a,"TABLE",{});var Zb=l(wa);a_=o(Zb,"THEAD",{});var NF=l(a_);Xl=o(NF,"TR",{});var e3=l(Xl);Dh=o(e3,"TH",{align:!0});var xF=l(Dh);yN=c(xF,"Returned values"),xF.forEach(s),EN=h(e3),n_=o(e3,"TH",{align:!0}),l(n_).forEach(s),e3.forEach(s),NF.forEach(s),wN=h(Zb),Ql=o(Zb,"TBODY",{});var t3=l(Ql);Zl=o(t3,"TR",{});var s3=l(Zl);Oh=o(s3,"TD",{align:!0});var IF=l(Oh);r_=o(IF,"STRONG",{});var HF=l(r_);bN=c(HF,"label"),HF.forEach(s),IF.forEach(s),TN=h(s3),Rh=o(s3,"TD",{align:!0});var BF=l(Rh);jN=c(BF,"The label for the class (model specific)"),BF.forEach(s),s3.forEach(s),kN=h(t3),ei=o(t3,"TR",{});var a3=l(ei);Ph=o(a3,"TD",{align:!0});var CF=l(Ph);o_=o(CF,"STRONG",{});var LF=l(o_);AN=c(LF,"score"),LF.forEach(s),CF.forEach(s),DN=h(a3),Sh=o(a3,"TD",{align:!0});var GF=l(Sh);ON=c(GF,"A float that represents how likely it is that the audio file belongs to this class."),GF.forEach(s),a3.forEach(s),t3.forEach(s),Zb.forEach(s),v2=h(a),st=o(a,"H2",{class:!0});var n3=l(st);ba=o(n3,"A",{id:!0,class:!0,href:!0});var MF=l(ba);l_=o(MF,"SPAN",{});var zF=l(l_);v(ti.$$.fragment,zF),zF.forEach(s),MF.forEach(s),RN=h(n3),i_=o(n3,"SPAN",{});var KF=l(i_);PN=c(KF,"Computer Vision"),KF.forEach(s),n3.forEach(s),y2=h(a),at=o(a,"H3",{class:!0});var r3=l(at);Ta=o(r3,"A",{id:!0,class:!0,href:!0});var FF=l(Ta);u_=o(FF,"SPAN",{});var UF=l(u_);v(si.$$.fragment,UF),UF.forEach(s),FF.forEach(s),SN=h(r3),c_=o(r3,"SPAN",{});var JF=l(c_);NN=c(JF,"Image Classification task"),JF.forEach(s),r3.forEach(s),E2=h(a),Nh=o(a,"P",{});var WF=l(Nh);xN=c(WF,"This task reads some image input and outputs the likelihood of classes."),WF.forEach(s),w2=h(a),v(ja.$$.fragment,a),b2=h(a),ai=o(a,"P",{});var II=l(ai);IN=c(II,"Available with: "),ni=o(II,"A",{href:!0,rel:!0});var YF=l(ni);HN=c(YF,"\u{1F917} Transformers"),YF.forEach(s),II.forEach(s),T2=h(a),xh=o(a,"P",{});var VF=l(xh);BN=c(VF,"Request:"),VF.forEach(s),j2=h(a),v(ka.$$.fragment,a),k2=h(a),Aa=o(a,"P",{});var o3=l(Aa);CN=c(o3,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ri=o(o3,"A",{href:!0,rel:!0});var XF=l(ri);LN=c(XF,`Pillow
supports`),XF.forEach(s),GN=c(o3,"."),o3.forEach(s),A2=h(a),Da=o(a,"TABLE",{});var l3=l(Da);p_=o(l3,"THEAD",{});var QF=l(p_);oi=o(QF,"TR",{});var i3=l(oi);Ih=o(i3,"TH",{align:!0});var ZF=l(Ih);MN=c(ZF,"All parameters"),ZF.forEach(s),zN=h(i3),f_=o(i3,"TH",{align:!0}),l(f_).forEach(s),i3.forEach(s),QF.forEach(s),KN=h(l3),h_=o(l3,"TBODY",{});var eU=l(h_);li=o(eU,"TR",{});var u3=l(li);ii=o(u3,"TD",{align:!0});var HI=l(ii);d_=o(HI,"STRONG",{});var tU=l(d_);FN=c(tU,"no parameter"),tU.forEach(s),UN=c(HI," (required)"),HI.forEach(s),JN=h(u3),Hh=o(u3,"TD",{align:!0});var sU=l(Hh);WN=c(sU,"a binary representation of the image file. No other parameters are currently allowed."),sU.forEach(s),u3.forEach(s),eU.forEach(s),l3.forEach(s),D2=h(a),Bh=o(a,"P",{});var aU=l(Bh);YN=c(aU,"Return value is a dict"),aU.forEach(s),O2=h(a),v(Oa.$$.fragment,a),R2=h(a),Ra=o(a,"TABLE",{});var c3=l(Ra);g_=o(c3,"THEAD",{});var nU=l(g_);ui=o(nU,"TR",{});var p3=l(ui);Ch=o(p3,"TH",{align:!0});var rU=l(Ch);VN=c(rU,"Returned values"),rU.forEach(s),XN=h(p3),m_=o(p3,"TH",{align:!0}),l(m_).forEach(s),p3.forEach(s),nU.forEach(s),QN=h(c3),ci=o(c3,"TBODY",{});var f3=l(ci);pi=o(f3,"TR",{});var h3=l(pi);Lh=o(h3,"TD",{align:!0});var oU=l(Lh);$_=o(oU,"STRONG",{});var lU=l($_);ZN=c(lU,"label"),lU.forEach(s),oU.forEach(s),ex=h(h3),Gh=o(h3,"TD",{align:!0});var iU=l(Gh);tx=c(iU,"The label for the class (model specific)"),iU.forEach(s),h3.forEach(s),sx=h(f3),fi=o(f3,"TR",{});var d3=l(fi);Mh=o(d3,"TD",{align:!0});var uU=l(Mh);q_=o(uU,"STRONG",{});var cU=l(q_);ax=c(cU,"score"),cU.forEach(s),uU.forEach(s),nx=h(d3),zh=o(d3,"TD",{align:!0});var pU=l(zh);rx=c(pU,"A float that represents how likely it is that the image file belongs to this class."),pU.forEach(s),d3.forEach(s),f3.forEach(s),c3.forEach(s),P2=h(a),nt=o(a,"H3",{class:!0});var g3=l(nt);Pa=o(g3,"A",{id:!0,class:!0,href:!0});var fU=l(Pa);__=o(fU,"SPAN",{});var hU=l(__);v(hi.$$.fragment,hU),hU.forEach(s),fU.forEach(s),ox=h(g3),v_=o(g3,"SPAN",{});var dU=l(v_);lx=c(dU,"Object Detection task"),dU.forEach(s),g3.forEach(s),S2=h(a),Kh=o(a,"P",{});var gU=l(Kh);ix=c(gU,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),gU.forEach(s),N2=h(a),v(Sa.$$.fragment,a),x2=h(a),di=o(a,"P",{});var BI=l(di);ux=c(BI,"Available with: "),gi=o(BI,"A",{href:!0,rel:!0});var mU=l(gi);cx=c(mU,"\u{1F917} Transformers"),mU.forEach(s),BI.forEach(s),I2=h(a),Fh=o(a,"P",{});var $U=l(Fh);px=c($U,"Request:"),$U.forEach(s),H2=h(a),v(Na.$$.fragment,a),B2=h(a),xa=o(a,"P",{});var m3=l(xa);fx=c(m3,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),mi=o(m3,"A",{href:!0,rel:!0});var qU=l(mi);hx=c(qU,`Pillow
supports`),qU.forEach(s),dx=c(m3,"."),m3.forEach(s),C2=h(a),Ia=o(a,"TABLE",{});var $3=l(Ia);y_=o($3,"THEAD",{});var _U=l(y_);$i=o(_U,"TR",{});var q3=l($i);Uh=o(q3,"TH",{align:!0});var vU=l(Uh);gx=c(vU,"All parameters"),vU.forEach(s),mx=h(q3),E_=o(q3,"TH",{align:!0}),l(E_).forEach(s),q3.forEach(s),_U.forEach(s),$x=h($3),w_=o($3,"TBODY",{});var yU=l(w_);qi=o(yU,"TR",{});var _3=l(qi);_i=o(_3,"TD",{align:!0});var CI=l(_i);b_=o(CI,"STRONG",{});var EU=l(b_);qx=c(EU,"no parameter"),EU.forEach(s),_x=c(CI," (required)"),CI.forEach(s),vx=h(_3),Jh=o(_3,"TD",{align:!0});var wU=l(Jh);yx=c(wU,"a binary representation of the image file. No other parameters are currently allowed."),wU.forEach(s),_3.forEach(s),yU.forEach(s),$3.forEach(s),L2=h(a),Wh=o(a,"P",{});var bU=l(Wh);Ex=c(bU,"Return value is a dict"),bU.forEach(s),G2=h(a),v(Ha.$$.fragment,a),M2=h(a),Ba=o(a,"TABLE",{});var v3=l(Ba);T_=o(v3,"THEAD",{});var TU=l(T_);vi=o(TU,"TR",{});var y3=l(vi);Yh=o(y3,"TH",{align:!0});var jU=l(Yh);wx=c(jU,"Returned values"),jU.forEach(s),bx=h(y3),j_=o(y3,"TH",{align:!0}),l(j_).forEach(s),y3.forEach(s),TU.forEach(s),Tx=h(v3),rt=o(v3,"TBODY",{});var Ad=l(rt);yi=o(Ad,"TR",{});var E3=l(yi);Vh=o(E3,"TD",{align:!0});var kU=l(Vh);k_=o(kU,"STRONG",{});var AU=l(k_);jx=c(AU,"label"),AU.forEach(s),kU.forEach(s),kx=h(E3),Xh=o(E3,"TD",{align:!0});var DU=l(Xh);Ax=c(DU,"The label for the class (model specific) of a detected object."),DU.forEach(s),E3.forEach(s),Dx=h(Ad),Ei=o(Ad,"TR",{});var w3=l(Ei);Qh=o(w3,"TD",{align:!0});var OU=l(Qh);A_=o(OU,"STRONG",{});var RU=l(A_);Ox=c(RU,"score"),RU.forEach(s),OU.forEach(s),Rx=h(w3),Zh=o(w3,"TD",{align:!0});var PU=l(Zh);Px=c(PU,"A float that represents how likely it is that the detected object belongs to the given class."),PU.forEach(s),w3.forEach(s),Sx=h(Ad),wi=o(Ad,"TR",{});var b3=l(wi);ed=o(b3,"TD",{align:!0});var SU=l(ed);D_=o(SU,"STRONG",{});var NU=l(D_);Nx=c(NU,"box"),NU.forEach(s),SU.forEach(s),xx=h(b3),td=o(b3,"TD",{align:!0});var xU=l(td);Ix=c(xU,"A dict (with keys [xmin,ymin,xmax,ymax]) representing the bounding box of a detected object."),xU.forEach(s),b3.forEach(s),Ad.forEach(s),v3.forEach(s),z2=h(a),ot=o(a,"H3",{class:!0});var T3=l(ot);Ca=o(T3,"A",{id:!0,class:!0,href:!0});var IU=l(Ca);O_=o(IU,"SPAN",{});var HU=l(O_);v(bi.$$.fragment,HU),HU.forEach(s),IU.forEach(s),Hx=h(T3),R_=o(T3,"SPAN",{});var BU=l(R_);Bx=c(BU,"Image Segmentation task"),BU.forEach(s),T3.forEach(s),K2=h(a),sd=o(a,"P",{});var CU=l(sd);Cx=c(CU,`This task reads some image input and outputs the likelihood of classes &
bounding boxes of detected objects.`),CU.forEach(s),F2=h(a),v(La.$$.fragment,a),U2=h(a),Ti=o(a,"P",{});var LI=l(Ti);Lx=c(LI,"Available with: "),ji=o(LI,"A",{href:!0,rel:!0});var LU=l(ji);Gx=c(LU,"\u{1F917} Transformers"),LU.forEach(s),LI.forEach(s),J2=h(a),ad=o(a,"P",{});var GU=l(ad);Mx=c(GU,"Request:"),GU.forEach(s),W2=h(a),v(Ga.$$.fragment,a),Y2=h(a),Ma=o(a,"P",{});var j3=l(Ma);zx=c(j3,`When sending your request, you should send a binary payload that simply
contains your image file. We support all image formats `),ki=o(j3,"A",{href:!0,rel:!0});var MU=l(ki);Kx=c(MU,`Pillow
supports`),MU.forEach(s),Fx=c(j3,"."),j3.forEach(s),V2=h(a),za=o(a,"TABLE",{});var k3=l(za);P_=o(k3,"THEAD",{});var zU=l(P_);Ai=o(zU,"TR",{});var A3=l(Ai);nd=o(A3,"TH",{align:!0});var KU=l(nd);Ux=c(KU,"All parameters"),KU.forEach(s),Jx=h(A3),S_=o(A3,"TH",{align:!0}),l(S_).forEach(s),A3.forEach(s),zU.forEach(s),Wx=h(k3),N_=o(k3,"TBODY",{});var FU=l(N_);Di=o(FU,"TR",{});var D3=l(Di);Oi=o(D3,"TD",{align:!0});var GI=l(Oi);x_=o(GI,"STRONG",{});var UU=l(x_);Yx=c(UU,"no parameter"),UU.forEach(s),Vx=c(GI," (required)"),GI.forEach(s),Xx=h(D3),rd=o(D3,"TD",{align:!0});var JU=l(rd);Qx=c(JU,"a binary representation of the image file. No other parameters are currently allowed."),JU.forEach(s),D3.forEach(s),FU.forEach(s),k3.forEach(s),X2=h(a),od=o(a,"P",{});var WU=l(od);Zx=c(WU,"Return value is a dict"),WU.forEach(s),Q2=h(a),v(Ka.$$.fragment,a),Z2=h(a),Fa=o(a,"TABLE",{});var O3=l(Fa);I_=o(O3,"THEAD",{});var YU=l(I_);Ri=o(YU,"TR",{});var R3=l(Ri);ld=o(R3,"TH",{align:!0});var VU=l(ld);eI=c(VU,"Returned values"),VU.forEach(s),tI=h(R3),H_=o(R3,"TH",{align:!0}),l(H_).forEach(s),R3.forEach(s),YU.forEach(s),sI=h(O3),lt=o(O3,"TBODY",{});var Dd=l(lt);Pi=o(Dd,"TR",{});var P3=l(Pi);id=o(P3,"TD",{align:!0});var XU=l(id);B_=o(XU,"STRONG",{});var QU=l(B_);aI=c(QU,"label"),QU.forEach(s),XU.forEach(s),nI=h(P3),ud=o(P3,"TD",{align:!0});var ZU=l(ud);rI=c(ZU,"The label for the class (model specific) of a segment."),ZU.forEach(s),P3.forEach(s),oI=h(Dd),Si=o(Dd,"TR",{});var S3=l(Si);cd=o(S3,"TD",{align:!0});var eJ=l(cd);C_=o(eJ,"STRONG",{});var tJ=l(C_);lI=c(tJ,"score"),tJ.forEach(s),eJ.forEach(s),iI=h(S3),pd=o(S3,"TD",{align:!0});var sJ=l(pd);uI=c(sJ,"A float that represents how likely it is that the segment belongs to the given class."),sJ.forEach(s),S3.forEach(s),cI=h(Dd),Ni=o(Dd,"TR",{});var N3=l(Ni);fd=o(N3,"TD",{align:!0});var aJ=l(fd);L_=o(aJ,"STRONG",{});var nJ=l(L_);pI=c(nJ,"mask"),nJ.forEach(s),aJ.forEach(s),fI=h(N3),hd=o(N3,"TD",{align:!0});var rJ=l(hd);hI=c(rJ,"A str (base64 str of a single channel black-and-white img) representing the mask of a segment."),rJ.forEach(s),N3.forEach(s),Dd.forEach(s),O3.forEach(s),this.h()},h(){p(n,"name","hf:doc:metadata"),p(n,"content",JSON.stringify(FY)),p(d,"id","detailed-parameters"),p(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(d,"href","#detailed-parameters"),p(t,"class","relative group"),p(Z,"id","which-task-is-used-by-this-model"),p(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Z,"href","#which-task-is-used-by-this-model"),p(D,"class","relative group"),p(ut,"class","block dark:hidden"),oJ(ut.src,MI="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task.png")||p(ut,"src",MI),p(ut,"width","300"),p(ct,"class","hidden dark:block invert"),oJ(ct.src,zI="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/inference-api/task-dark.png")||p(ct,"src",zI),p(ct,"width","300"),p(pt,"id","natural-language-processing"),p(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(pt,"href","#natural-language-processing"),p(Se,"class","relative group"),p(ft,"id","fill-mask-task"),p(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ft,"href","#fill-mask-task"),p(Ne,"class","relative group"),p(cn,"href","https://github.com/huggingface/transformers"),p(cn,"rel","nofollow"),p(eu,"align","left"),p(xd,"align","left"),p(hn,"align","left"),p(tu,"align","left"),p(su,"align","left"),p(au,"align","left"),p(nu,"align","left"),p(mt,"align","left"),p(ru,"align","left"),p($t,"align","left"),p(lu,"align","left"),p(Gd,"align","left"),p(iu,"align","left"),p(uu,"align","left"),p(cu,"align","left"),p(pu,"align","left"),p(fu,"align","left"),p(hu,"align","left"),p(du,"align","left"),p(gu,"align","left"),p(vt,"id","summarization-task"),p(vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(vt,"href","#summarization-task"),p(xe,"class","relative group"),p(mu,"href","mailto:api-enterprise@huggingface.co"),p(bn,"href","https://github.com/huggingface/transformers"),p(bn,"rel","nofollow"),p(_u,"align","left"),p(Yd,"align","left"),p(kn,"align","left"),p(vu,"align","left"),p(yu,"align","left"),p(Eu,"align","left"),p(wu,"align","left"),p(_e,"align","left"),p(bu,"align","left"),p(ve,"align","left"),p(Tu,"align","left"),p(ye,"align","left"),p(ju,"align","left"),p(ee,"align","left"),p(ku,"align","left"),p(te,"align","left"),p(Au,"align","left"),p(Tt,"align","left"),p(Du,"align","left"),p(jt,"align","left"),p(Ou,"align","left"),p(Ru,"align","left"),p(Pu,"align","left"),p(kt,"align","left"),p(Su,"align","left"),p(At,"align","left"),p(xu,"align","left"),p(mg,"align","left"),p(Iu,"align","left"),p(Hu,"align","left"),p(Ot,"id","question-answering-task"),p(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ot,"href","#question-answering-task"),p(Ie,"class","relative group"),p(Mn,"href","https://github.com/huggingface/transformers"),p(Mn,"rel","nofollow"),p(zn,"href","https://github.com/allenai/allennlp"),p(zn,"rel","nofollow"),p(Mu,"align","left"),p(Eg,"align","left"),p(zu,"align","left"),p(Ku,"align","left"),p(Fu,"align","left"),p(Uu,"align","left"),p(Ju,"align","left"),p(xt,"align","left"),p(Wu,"align","left"),p(It,"align","left"),p(Ht,"id","table-question-answering-task"),p(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ht,"href","#table-question-answering-task"),p(Be,"class","relative group"),p(Xn,"href","https://github.com/huggingface/transformers"),p(Xn,"rel","nofollow"),p(Qu,"align","left"),p(Pg,"align","left"),p(er,"align","left"),p(Ng,"align","left"),p(Zu,"align","left"),p(ec,"align","left"),p(tc,"align","left"),p(sc,"align","left"),p(ac,"align","left"),p(nc,"align","left"),p(rc,"align","left"),p(Gt,"align","left"),p(oc,"align","left"),p(Mt,"align","left"),p(ic,"align","left"),p(Cg,"align","left"),p(uc,"align","left"),p(cc,"align","left"),p(pc,"align","left"),p(fc,"align","left"),p(hc,"align","left"),p(dc,"align","left"),p(gc,"align","left"),p(mc,"align","left"),p(Ft,"id","sentence-similarity-task"),p(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ft,"href","#sentence-similarity-task"),p(Ce,"class","relative group"),p(hr,"href","https://www.sbert.net/index.html"),p(hr,"rel","nofollow"),p(vc,"align","left"),p(Jg,"align","left"),p(mr,"align","left"),p(Yg,"align","left"),p(yc,"align","left"),p(Ec,"align","left"),p(wc,"align","left"),p(bc,"align","left"),p(Tc,"align","left"),p(jc,"align","left"),p(kc,"align","left"),p(Yt,"align","left"),p(Ac,"align","left"),p(Vt,"align","left"),p(Oc,"align","left"),p(em,"align","left"),p(Rc,"align","left"),p(Pc,"align","left"),p(Zt,"id","text-classification-task"),p(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Zt,"href","#text-classification-task"),p(Le,"class","relative group"),p(jr,"href","https://github.com/huggingface/transformers"),p(jr,"rel","nofollow"),p(Ic,"align","left"),p(om,"align","left"),p(Dr,"align","left"),p(Hc,"align","left"),p(Bc,"align","left"),p(Cc,"align","left"),p(Lc,"align","left"),p(as,"align","left"),p(Gc,"align","left"),p(ns,"align","left"),p(zc,"align","left"),p(fm,"align","left"),p(Kc,"align","left"),p(Fc,"align","left"),p(Uc,"align","left"),p(Jc,"align","left"),p(ls,"id","text-generation-task"),p(ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ls,"href","#text-generation-task"),p(Ge,"class","relative group"),p(Cr,"href","https://github.com/huggingface/transformers"),p(Cr,"rel","nofollow"),p(Xc,"align","left"),p(qm,"align","left"),p(Mr,"align","left"),p(Qc,"align","left"),p(Zc,"align","left"),p(ep,"align","left"),p(tp,"align","left"),p(Ee,"align","left"),p(sp,"align","left"),p(se,"align","left"),p(ap,"align","left"),p(ae,"align","left"),p(np,"align","left"),p(ps,"align","left"),p(rp,"align","left"),p(we,"align","left"),p(op,"align","left"),p(be,"align","left"),p(lp,"align","left"),p(Te,"align","left"),p(ip,"align","left"),p(fs,"align","left"),p(up,"align","left"),p(hs,"align","left"),p(cp,"align","left"),p(pp,"align","left"),p(fp,"align","left"),p(ds,"align","left"),p(hp,"align","left"),p(gs,"align","left"),p(gp,"align","left"),p(Mm,"align","left"),p(mp,"align","left"),p($p,"align","left"),p(qs,"id","text2text-generation-task"),p(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(qs,"href","#text2text-generation-task"),p(Me,"class","relative group"),p(qp,"href","#text-generation-task"),p(vs,"id","token-classification-task"),p(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(vs,"href","#token-classification-task"),p(ze,"class","relative group"),p(oo,"href","https://github.com/huggingface/transformers"),p(oo,"rel","nofollow"),p(lo,"href","https://github.com/flairNLP/flair"),p(lo,"rel","nofollow"),p(Ep,"align","left"),p(Vm,"align","left"),p(co,"align","left"),p(wp,"align","left"),p(bp,"align","left"),p(Tp,"align","left"),p(jp,"align","left"),p(N,"align","left"),p(kp,"align","left"),p(Ap,"align","left"),p(Dp,"align","left"),p(bs,"align","left"),p(Op,"align","left"),p(Ts,"align","left"),p(Pp,"align","left"),p(f$,"align","left"),p(Sp,"align","left"),p(Np,"align","left"),p(xp,"align","left"),p(Ip,"align","left"),p(Hp,"align","left"),p(Bp,"align","left"),p(Cp,"align","left"),p(As,"align","left"),p(Lp,"align","left"),p(Ds,"align","left"),p(Os,"id","named-entity-recognition-ner-task"),p(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Os,"href","#named-entity-recognition-ner-task"),p(Fe,"class","relative group"),p(Gp,"href","#token-classification-task"),p(Rs,"id","translation-task"),p(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Rs,"href","#translation-task"),p(Ue,"class","relative group"),p(ko,"href","https://github.com/huggingface/transformers"),p(ko,"rel","nofollow"),p(Fp,"align","left"),p(T$,"align","left"),p(Oo,"align","left"),p(Up,"align","left"),p(Jp,"align","left"),p(Wp,"align","left"),p(Yp,"align","left"),p(xs,"align","left"),p(Vp,"align","left"),p(Is,"align","left"),p(Qp,"align","left"),p(R$,"align","left"),p(Zp,"align","left"),p(ef,"align","left"),p(Bs,"id","zeroshot-classification-task"),p(Bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Bs,"href","#zeroshot-classification-task"),p(Je,"class","relative group"),p(Bo,"href","https://github.com/huggingface/transformers"),p(Bo,"rel","nofollow"),p(nf,"align","left"),p(H$,"align","left"),p(Go,"align","left"),p(rf,"align","left"),p(zo,"align","left"),p(of,"align","left"),p(lf,"align","left"),p(je,"align","left"),p(uf,"align","left"),p(Ms,"align","left"),p(cf,"align","left"),p(pf,"align","left"),p(ff,"align","left"),p(zs,"align","left"),p(hf,"align","left"),p(Ks,"align","left"),p(mf,"align","left"),p(J$,"align","left"),p($f,"align","left"),p(qf,"align","left"),p(_f,"align","left"),p(vf,"align","left"),p(yf,"align","left"),p(Js,"align","left"),p(Ws,"id","conversational-task"),p(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ws,"href","#conversational-task"),p(Ye,"class","relative group"),p(tl,"href","https://github.com/huggingface/transformers"),p(tl,"rel","nofollow"),p(Tf,"align","left"),p(tq,"align","left"),p(nl,"align","left"),p(aq,"align","left"),p(jf,"align","left"),p(kf,"align","left"),p(Af,"align","left"),p(Df,"align","left"),p(Of,"align","left"),p(Qs,"align","left"),p(Rf,"align","left"),p(Pf,"align","left"),p(Sf,"align","left"),p(ke,"align","left"),p(Nf,"align","left"),p(Ae,"align","left"),p(xf,"align","left"),p(De,"align","left"),p(If,"align","left"),p(ne,"align","left"),p(Hf,"align","left"),p(re,"align","left"),p(Bf,"align","left"),p(Zs,"align","left"),p(Cf,"align","left"),p(ea,"align","left"),p(Lf,"align","left"),p(Gf,"align","left"),p(Mf,"align","left"),p(ta,"align","left"),p(zf,"align","left"),p(sa,"align","left"),p(Ff,"align","left"),p(bq,"align","left"),p(Uf,"align","left"),p(Jf,"align","left"),p(Wf,"align","left"),p(Yf,"align","left"),p(Vf,"align","left"),p(Xf,"align","left"),p(Qf,"align","left"),p(Zf,"align","left"),p(na,"id","feature-extraction-task"),p(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(na,"href","#feature-extraction-task"),p(Ve,"class","relative group"),p(Tl,"href","https://github.com/huggingface/transformers"),p(Tl,"rel","nofollow"),p(jl,"href","https://github.com/UKPLab/sentence-transformers"),p(jl,"rel","nofollow"),p(sh,"align","left"),p(Oq,"align","left"),p(Dl,"align","left"),p(ah,"align","left"),p(nh,"align","left"),p(rh,"align","left"),p(oh,"align","left"),p(la,"align","left"),p(lh,"align","left"),p(ia,"align","left"),p(uh,"align","left"),p(Iq,"align","left"),p(ch,"align","left"),p(ph,"align","left"),p(ca,"id","audio"),p(ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ca,"href","#audio"),p(Qe,"class","relative group"),p(pa,"id","automatic-speech-recognition-task"),p(pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(pa,"href","#automatic-speech-recognition-task"),p(Ze,"class","relative group"),p(Hl,"href","https://github.com/huggingface/transformers"),p(Hl,"rel","nofollow"),p(Bl,"href","https://github.com/espnet/espnet"),p(Bl,"rel","nofollow"),p(Cl,"href","https://github.com/speechbrain/speechbrain"),p(Cl,"rel","nofollow"),p(mh,"align","left"),p(Kq,"align","left"),p(Ml,"align","left"),p($h,"align","left"),p(vh,"align","left"),p(Wq,"align","left"),p(yh,"align","left"),p(Eh,"align","left"),p(qa,"id","audio-classification-task"),p(qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(qa,"href","#audio-classification-task"),p(et,"class","relative group"),p(Ul,"href","https://github.com/huggingface/transformers"),p(Ul,"rel","nofollow"),p(Jl,"href","https://github.com/speechbrain/speechbrain"),p(Jl,"rel","nofollow"),p(jh,"align","left"),p(e_,"align","left"),p(Vl,"align","left"),p(kh,"align","left"),p(Dh,"align","left"),p(n_,"align","left"),p(Oh,"align","left"),p(Rh,"align","left"),p(Ph,"align","left"),p(Sh,"align","left"),p(ba,"id","computer-vision"),p(ba,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ba,"href","#computer-vision"),p(st,"class","relative group"),p(Ta,"id","image-classification-task"),p(Ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ta,"href","#image-classification-task"),p(at,"class","relative group"),p(ni,"href","https://github.com/huggingface/transformers"),p(ni,"rel","nofollow"),p(ri,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),p(ri,"rel","nofollow"),p(Ih,"align","left"),p(f_,"align","left"),p(ii,"align","left"),p(Hh,"align","left"),p(Ch,"align","left"),p(m_,"align","left"),p(Lh,"align","left"),p(Gh,"align","left"),p(Mh,"align","left"),p(zh,"align","left"),p(Pa,"id","object-detection-task"),p(Pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Pa,"href","#object-detection-task"),p(nt,"class","relative group"),p(gi,"href","https://github.com/huggingface/transformers"),p(gi,"rel","nofollow"),p(mi,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),p(mi,"rel","nofollow"),p(Uh,"align","left"),p(E_,"align","left"),p(_i,"align","left"),p(Jh,"align","left"),p(Yh,"align","left"),p(j_,"align","left"),p(Vh,"align","left"),p(Xh,"align","left"),p(Qh,"align","left"),p(Zh,"align","left"),p(ed,"align","left"),p(td,"align","left"),p(Ca,"id","image-segmentation-task"),p(Ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ca,"href","#image-segmentation-task"),p(ot,"class","relative group"),p(ji,"href","https://github.com/huggingface/transformers"),p(ji,"rel","nofollow"),p(ki,"href","https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html"),p(ki,"rel","nofollow"),p(nd,"align","left"),p(S_,"align","left"),p(Oi,"align","left"),p(rd,"align","left"),p(ld,"align","left"),p(H_,"align","left"),p(id,"align","left"),p(ud,"align","left"),p(cd,"align","left"),p(pd,"align","left"),p(fd,"align","left"),p(hd,"align","left")},m(a,g){e(document.head,n),m(a,i,g),m(a,t,g),e(t,d),e(d,$),y(k,$,null),e(t,A),e(t,j),e(j,T),m(a,S,g),m(a,D,g),e(D,Z),e(Z,Re),y(X,Re,null),e(D,U),e(D,it),e(it,Yi),m(a,rn,g),m(a,Pe,g),e(Pe,x3),m(a,E1,g),m(a,Vi,g),e(Vi,I3),m(a,w1,g),m(a,ut,g),m(a,b1,g),m(a,ct,g),m(a,T1,g),m(a,Se,g),e(Se,pt),e(pt,Od),y(on,Od,null),e(Se,H3),e(Se,Rd),e(Rd,B3),m(a,j1,g),m(a,Ne,g),e(Ne,ft),e(ft,Pd),y(ln,Pd,null),e(Ne,C3),e(Ne,Sd),e(Sd,L3),m(a,k1,g),m(a,Xi,g),e(Xi,G3),m(a,A1,g),y(ht,a,g),m(a,D1,g),m(a,un,g),e(un,M3),e(un,cn),e(cn,z3),m(a,O1,g),m(a,Qi,g),e(Qi,K3),m(a,R1,g),y(dt,a,g),m(a,P1,g),m(a,Zi,g),e(Zi,F3),m(a,S1,g),m(a,gt,g),e(gt,Nd),e(Nd,pn),e(pn,eu),e(eu,U3),e(pn,J3),e(pn,xd),e(gt,W3),e(gt,ue),e(ue,fn),e(fn,hn),e(hn,Id),e(Id,Y3),e(hn,V3),e(fn,X3),e(fn,tu),e(tu,Q3),e(ue,Z3),e(ue,dn),e(dn,su),e(su,Hd),e(Hd,eT),e(dn,tT),e(dn,au),e(au,sT),e(ue,aT),e(ue,gn),e(gn,nu),e(nu,nT),e(gn,rT),e(gn,mt),e(mt,oT),e(mt,Bd),e(Bd,lT),e(mt,iT),e(ue,uT),e(ue,mn),e(mn,ru),e(ru,cT),e(mn,pT),e(mn,$t),e($t,fT),e($t,Cd),e(Cd,hT),e($t,dT),m(a,N1,g),m(a,ou,g),e(ou,gT),m(a,x1,g),y(qt,a,g),m(a,I1,g),m(a,_t,g),e(_t,Ld),e(Ld,$n),e($n,lu),e(lu,mT),e($n,$T),e($n,Gd),e(_t,qT),e(_t,ce),e(ce,qn),e(qn,iu),e(iu,Md),e(Md,_T),e(qn,vT),e(qn,uu),e(uu,yT),e(ce,ET),e(ce,_n),e(_n,cu),e(cu,zd),e(zd,wT),e(_n,bT),e(_n,pu),e(pu,TT),e(ce,jT),e(ce,vn),e(vn,fu),e(fu,Kd),e(Kd,kT),e(vn,AT),e(vn,hu),e(hu,DT),e(ce,OT),e(ce,yn),e(yn,du),e(du,Fd),e(Fd,RT),e(yn,PT),e(yn,gu),e(gu,ST),m(a,H1,g),m(a,xe,g),e(xe,vt),e(vt,Ud),y(En,Ud,null),e(xe,NT),e(xe,Jd),e(Jd,xT),m(a,B1,g),m(a,yt,g),e(yt,IT),e(yt,mu),e(mu,HT),e(yt,BT),m(a,C1,g),y(Et,a,g),m(a,L1,g),m(a,wn,g),e(wn,CT),e(wn,bn),e(bn,LT),m(a,G1,g),m(a,$u,g),e($u,GT),m(a,M1,g),y(wt,a,g),m(a,z1,g),m(a,qu,g),e(qu,MT),m(a,K1,g),m(a,bt,g),e(bt,Wd),e(Wd,Tn),e(Tn,_u),e(_u,zT),e(Tn,KT),e(Tn,Yd),e(bt,FT),e(bt,G),e(G,jn),e(jn,kn),e(kn,Vd),e(Vd,UT),e(kn,JT),e(jn,WT),e(jn,vu),e(vu,YT),e(G,VT),e(G,An),e(An,yu),e(yu,Xd),e(Xd,XT),e(An,QT),e(An,Eu),e(Eu,ZT),e(G,ej),e(G,Dn),e(Dn,wu),e(wu,tj),e(Dn,sj),e(Dn,_e),e(_e,aj),e(_e,Qd),e(Qd,nj),e(_e,rj),e(_e,Zd),e(Zd,oj),e(_e,lj),e(G,ij),e(G,On),e(On,bu),e(bu,uj),e(On,cj),e(On,ve),e(ve,pj),e(ve,eg),e(eg,fj),e(ve,hj),e(ve,tg),e(tg,dj),e(ve,gj),e(G,mj),e(G,Rn),e(Rn,Tu),e(Tu,$j),e(Rn,qj),e(Rn,ye),e(ye,_j),e(ye,sg),e(sg,vj),e(ye,yj),e(ye,ag),e(ag,Ej),e(ye,wj),e(G,bj),e(G,Pn),e(Pn,ju),e(ju,Tj),e(Pn,jj),e(Pn,ee),e(ee,kj),e(ee,ng),e(ng,Aj),e(ee,Dj),e(ee,rg),e(rg,Oj),e(ee,Rj),e(ee,og),e(og,Pj),e(ee,Sj),e(G,Nj),e(G,Sn),e(Sn,ku),e(ku,xj),e(Sn,Ij),e(Sn,te),e(te,Hj),e(te,lg),e(lg,Bj),e(te,Cj),e(te,ig),e(ig,Lj),e(te,Gj),e(te,ug),e(ug,Mj),e(te,zj),e(G,Kj),e(G,Nn),e(Nn,Au),e(Au,Fj),e(Nn,Uj),e(Nn,Tt),e(Tt,Jj),e(Tt,cg),e(cg,Wj),e(Tt,Yj),e(G,Vj),e(G,xn),e(xn,Du),e(Du,Xj),e(xn,Qj),e(xn,jt),e(jt,Zj),e(jt,pg),e(pg,e4),e(jt,t4),e(G,s4),e(G,In),e(In,Ou),e(Ou,fg),e(fg,a4),e(In,n4),e(In,Ru),e(Ru,r4),e(G,o4),e(G,Hn),e(Hn,Pu),e(Pu,l4),e(Hn,i4),e(Hn,kt),e(kt,u4),e(kt,hg),e(hg,c4),e(kt,p4),e(G,f4),e(G,Bn),e(Bn,Su),e(Su,h4),e(Bn,d4),e(Bn,At),e(At,g4),e(At,dg),e(dg,m4),e(At,$4),m(a,F1,g),m(a,Nu,g),e(Nu,q4),m(a,U1,g),m(a,Dt,g),e(Dt,gg),e(gg,Cn),e(Cn,xu),e(xu,_4),e(Cn,v4),e(Cn,mg),e(Dt,y4),e(Dt,$g),e($g,Ln),e(Ln,Iu),e(Iu,qg),e(qg,E4),e(Ln,w4),e(Ln,Hu),e(Hu,b4),m(a,J1,g),m(a,Ie,g),e(Ie,Ot),e(Ot,_g),y(Gn,_g,null),e(Ie,T4),e(Ie,vg),e(vg,j4),m(a,W1,g),m(a,Bu,g),e(Bu,k4),m(a,Y1,g),y(Rt,a,g),m(a,V1,g),m(a,He,g),e(He,A4),e(He,Mn),e(Mn,D4),e(He,O4),e(He,zn),e(zn,R4),m(a,X1,g),m(a,Cu,g),e(Cu,P4),m(a,Q1,g),y(Pt,a,g),m(a,Z1,g),m(a,Lu,g),e(Lu,S4),m(a,ev,g),m(a,Gu,g),e(Gu,N4),m(a,tv,g),y(St,a,g),m(a,sv,g),m(a,Nt,g),e(Nt,yg),e(yg,Kn),e(Kn,Mu),e(Mu,x4),e(Kn,I4),e(Kn,Eg),e(Nt,H4),e(Nt,pe),e(pe,Fn),e(Fn,zu),e(zu,wg),e(wg,B4),e(Fn,C4),e(Fn,Ku),e(Ku,L4),e(pe,G4),e(pe,Un),e(Un,Fu),e(Fu,bg),e(bg,M4),e(Un,z4),e(Un,Uu),e(Uu,K4),e(pe,F4),e(pe,Jn),e(Jn,Ju),e(Ju,Tg),e(Tg,U4),e(Jn,J4),e(Jn,xt),e(xt,W4),e(xt,jg),e(jg,Y4),e(xt,V4),e(pe,X4),e(pe,Wn),e(Wn,Wu),e(Wu,kg),e(kg,Q4),e(Wn,Z4),e(Wn,It),e(It,e5),e(It,Ag),e(Ag,t5),e(It,s5),m(a,av,g),m(a,Be,g),e(Be,Ht),e(Ht,Dg),y(Yn,Dg,null),e(Be,a5),e(Be,Og),e(Og,n5),m(a,nv,g),m(a,Yu,g),e(Yu,r5),m(a,rv,g),y(Bt,a,g),m(a,ov,g),m(a,Vn,g),e(Vn,o5),e(Vn,Xn),e(Xn,l5),m(a,lv,g),m(a,Vu,g),e(Vu,i5),m(a,iv,g),y(Ct,a,g),m(a,uv,g),m(a,Xu,g),e(Xu,u5),m(a,cv,g),m(a,Lt,g),e(Lt,Rg),e(Rg,Qn),e(Qn,Qu),e(Qu,c5),e(Qn,p5),e(Qn,Pg),e(Lt,f5),e(Lt,J),e(J,Zn),e(Zn,er),e(er,Sg),e(Sg,h5),e(er,d5),e(Zn,g5),e(Zn,Ng),e(J,m5),e(J,tr),e(tr,Zu),e(Zu,$5),e(tr,q5),e(tr,ec),e(ec,_5),e(J,v5),e(J,sr),e(sr,tc),e(tc,y5),e(sr,E5),e(sr,sc),e(sc,w5),e(J,b5),e(J,ar),e(ar,ac),e(ac,xg),e(xg,T5),e(ar,j5),e(ar,nc),e(nc,k5),e(J,A5),e(J,nr),e(nr,rc),e(rc,D5),e(nr,O5),e(nr,Gt),e(Gt,R5),e(Gt,Ig),e(Ig,P5),e(Gt,S5),e(J,N5),e(J,rr),e(rr,oc),e(oc,x5),e(rr,I5),e(rr,Mt),e(Mt,H5),e(Mt,Hg),e(Hg,B5),e(Mt,C5),m(a,pv,g),m(a,lc,g),e(lc,L5),m(a,fv,g),y(zt,a,g),m(a,hv,g),m(a,Kt,g),e(Kt,Bg),e(Bg,or),e(or,ic),e(ic,G5),e(or,M5),e(or,Cg),e(Kt,z5),e(Kt,fe),e(fe,lr),e(lr,uc),e(uc,Lg),e(Lg,K5),e(lr,F5),e(lr,cc),e(cc,U5),e(fe,J5),e(fe,ir),e(ir,pc),e(pc,Gg),e(Gg,W5),e(ir,Y5),e(ir,fc),e(fc,V5),e(fe,X5),e(fe,ur),e(ur,hc),e(hc,Mg),e(Mg,Q5),e(ur,Z5),e(ur,dc),e(dc,ek),e(fe,tk),e(fe,cr),e(cr,gc),e(gc,zg),e(zg,sk),e(cr,ak),e(cr,mc),e(mc,nk),m(a,dv,g),m(a,Ce,g),e(Ce,Ft),e(Ft,Kg),y(pr,Kg,null),e(Ce,rk),e(Ce,Fg),e(Fg,ok),m(a,gv,g),m(a,$c,g),e($c,lk),m(a,mv,g),y(Ut,a,g),m(a,$v,g),m(a,fr,g),e(fr,ik),e(fr,hr),e(hr,uk),m(a,qv,g),m(a,qc,g),e(qc,ck),m(a,_v,g),y(Jt,a,g),m(a,vv,g),m(a,_c,g),e(_c,pk),m(a,yv,g),m(a,Wt,g),e(Wt,Ug),e(Ug,dr),e(dr,vc),e(vc,fk),e(dr,hk),e(dr,Jg),e(Wt,dk),e(Wt,W),e(W,gr),e(gr,mr),e(mr,Wg),e(Wg,gk),e(mr,mk),e(gr,$k),e(gr,Yg),e(W,qk),e(W,$r),e($r,yc),e(yc,_k),e($r,vk),e($r,Ec),e(Ec,yk),e(W,Ek),e(W,qr),e(qr,wc),e(wc,wk),e(qr,bk),e(qr,bc),e(bc,Tk),e(W,jk),e(W,_r),e(_r,Tc),e(Tc,Vg),e(Vg,kk),e(_r,Ak),e(_r,jc),e(jc,Dk),e(W,Ok),e(W,vr),e(vr,kc),e(kc,Rk),e(vr,Pk),e(vr,Yt),e(Yt,Sk),e(Yt,Xg),e(Xg,Nk),e(Yt,xk),e(W,Ik),e(W,yr),e(yr,Ac),e(Ac,Hk),e(yr,Bk),e(yr,Vt),e(Vt,Ck),e(Vt,Qg),e(Qg,Lk),e(Vt,Gk),m(a,Ev,g),m(a,Dc,g),e(Dc,Mk),m(a,wv,g),y(Xt,a,g),m(a,bv,g),m(a,Qt,g),e(Qt,Zg),e(Zg,Er),e(Er,Oc),e(Oc,zk),e(Er,Kk),e(Er,em),e(Qt,Fk),e(Qt,tm),e(tm,wr),e(wr,Rc),e(Rc,sm),e(sm,Uk),e(wr,Jk),e(wr,Pc),e(Pc,Wk),m(a,Tv,g),m(a,Le,g),e(Le,Zt),e(Zt,am),y(br,am,null),e(Le,Yk),e(Le,nm),e(nm,Vk),m(a,jv,g),m(a,Sc,g),e(Sc,Xk),m(a,kv,g),y(es,a,g),m(a,Av,g),m(a,Tr,g),e(Tr,Qk),e(Tr,jr),e(jr,Zk),m(a,Dv,g),m(a,Nc,g),e(Nc,e6),m(a,Ov,g),y(ts,a,g),m(a,Rv,g),m(a,xc,g),e(xc,t6),m(a,Pv,g),m(a,ss,g),e(ss,rm),e(rm,kr),e(kr,Ic),e(Ic,s6),e(kr,a6),e(kr,om),e(ss,n6),e(ss,he),e(he,Ar),e(Ar,Dr),e(Dr,lm),e(lm,r6),e(Dr,o6),e(Ar,l6),e(Ar,Hc),e(Hc,i6),e(he,u6),e(he,Or),e(Or,Bc),e(Bc,im),e(im,c6),e(Or,p6),e(Or,Cc),e(Cc,f6),e(he,h6),e(he,Rr),e(Rr,Lc),e(Lc,d6),e(Rr,g6),e(Rr,as),e(as,m6),e(as,um),e(um,$6),e(as,q6),e(he,_6),e(he,Pr),e(Pr,Gc),e(Gc,v6),e(Pr,y6),e(Pr,ns),e(ns,E6),e(ns,cm),e(cm,w6),e(ns,b6),m(a,Sv,g),m(a,Mc,g),e(Mc,T6),m(a,Nv,g),y(rs,a,g),m(a,xv,g),m(a,os,g),e(os,pm),e(pm,Sr),e(Sr,zc),e(zc,j6),e(Sr,k6),e(Sr,fm),e(os,A6),e(os,Nr),e(Nr,xr),e(xr,Kc),e(Kc,hm),e(hm,D6),e(xr,O6),e(xr,Fc),e(Fc,R6),e(Nr,P6),e(Nr,Ir),e(Ir,Uc),e(Uc,dm),e(dm,S6),e(Ir,N6),e(Ir,Jc),e(Jc,x6),m(a,Iv,g),m(a,Ge,g),e(Ge,ls),e(ls,gm),y(Hr,gm,null),e(Ge,I6),e(Ge,mm),e(mm,H6),m(a,Hv,g),m(a,Wc,g),e(Wc,B6),m(a,Bv,g),y(is,a,g),m(a,Cv,g),m(a,Br,g),e(Br,C6),e(Br,Cr),e(Cr,L6),m(a,Lv,g),m(a,Yc,g),e(Yc,G6),m(a,Gv,g),y(us,a,g),m(a,Mv,g),m(a,Vc,g),e(Vc,M6),m(a,zv,g),m(a,cs,g),e(cs,$m),e($m,Lr),e(Lr,Xc),e(Xc,z6),e(Lr,K6),e(Lr,qm),e(cs,F6),e(cs,I),e(I,Gr),e(Gr,Mr),e(Mr,_m),e(_m,U6),e(Mr,J6),e(Gr,W6),e(Gr,Qc),e(Qc,Y6),e(I,V6),e(I,zr),e(zr,Zc),e(Zc,vm),e(vm,X6),e(zr,Q6),e(zr,ep),e(ep,Z6),e(I,e7),e(I,Kr),e(Kr,tp),e(tp,t7),e(Kr,s7),e(Kr,Ee),e(Ee,a7),e(Ee,ym),e(ym,n7),e(Ee,r7),e(Ee,Em),e(Em,o7),e(Ee,l7),e(I,i7),e(I,Fr),e(Fr,sp),e(sp,u7),e(Fr,c7),e(Fr,se),e(se,p7),e(se,wm),e(wm,f7),e(se,h7),e(se,bm),e(bm,d7),e(se,g7),e(se,Tm),e(Tm,m7),e(se,$7),e(I,q7),e(I,Ur),e(Ur,ap),e(ap,_7),e(Ur,v7),e(Ur,ae),e(ae,y7),e(ae,jm),e(jm,E7),e(ae,w7),e(ae,km),e(km,b7),e(ae,T7),e(ae,Am),e(Am,j7),e(ae,k7),e(I,A7),e(I,Jr),e(Jr,np),e(np,D7),e(Jr,O7),e(Jr,ps),e(ps,R7),e(ps,Dm),e(Dm,P7),e(ps,S7),e(I,N7),e(I,Wr),e(Wr,rp),e(rp,x7),e(Wr,I7),e(Wr,we),e(we,H7),e(we,Om),e(Om,B7),e(we,C7),e(we,Rm),e(Rm,L7),e(we,G7),e(I,M7),e(I,Yr),e(Yr,op),e(op,z7),e(Yr,K7),e(Yr,be),e(be,F7),e(be,Pm),e(Pm,U7),e(be,J7),e(be,Sm),e(Sm,W7),e(be,Y7),e(I,V7),e(I,Vr),e(Vr,lp),e(lp,X7),e(Vr,Q7),e(Vr,Te),e(Te,Z7),e(Te,Nm),e(Nm,e9),e(Te,t9),e(Te,xm),e(xm,s9),e(Te,a9),e(I,n9),e(I,Xr),e(Xr,ip),e(ip,r9),e(Xr,o9),e(Xr,fs),e(fs,l9),e(fs,Im),e(Im,i9),e(fs,u9),e(I,c9),e(I,Qr),e(Qr,up),e(up,p9),e(Qr,f9),e(Qr,hs),e(hs,h9),e(hs,Hm),e(Hm,d9),e(hs,g9),e(I,m9),e(I,Zr),e(Zr,cp),e(cp,Bm),e(Bm,$9),e(Zr,q9),e(Zr,pp),e(pp,_9),e(I,v9),e(I,eo),e(eo,fp),e(fp,y9),e(eo,E9),e(eo,ds),e(ds,w9),e(ds,Cm),e(Cm,b9),e(ds,T9),e(I,j9),e(I,to),e(to,hp),e(hp,k9),e(to,A9),e(to,gs),e(gs,D9),e(gs,Lm),e(Lm,O9),e(gs,R9),m(a,Kv,g),m(a,dp,g),e(dp,P9),m(a,Fv,g),y(ms,a,g),m(a,Uv,g),m(a,$s,g),e($s,Gm),e(Gm,so),e(so,gp),e(gp,S9),e(so,N9),e(so,Mm),e($s,x9),e($s,zm),e(zm,ao),e(ao,mp),e(mp,Km),e(Km,I9),e(ao,H9),e(ao,$p),e($p,B9),m(a,Jv,g),m(a,Me,g),e(Me,qs),e(qs,Fm),y(no,Fm,null),e(Me,C9),e(Me,Um),e(Um,L9),m(a,Wv,g),m(a,_s,g),e(_s,G9),e(_s,qp),e(qp,M9),e(_s,z9),m(a,Yv,g),m(a,ze,g),e(ze,vs),e(vs,Jm),y(ro,Jm,null),e(ze,K9),e(ze,Wm),e(Wm,F9),m(a,Vv,g),m(a,_p,g),e(_p,U9),m(a,Xv,g),y(ys,a,g),m(a,Qv,g),m(a,Ke,g),e(Ke,J9),e(Ke,oo),e(oo,W9),e(Ke,Y9),e(Ke,lo),e(lo,V9),m(a,Zv,g),m(a,vp,g),e(vp,X9),m(a,ey,g),y(Es,a,g),m(a,ty,g),m(a,yp,g),e(yp,Q9),m(a,sy,g),m(a,ws,g),e(ws,Ym),e(Ym,io),e(io,Ep),e(Ep,Z9),e(io,e8),e(io,Vm),e(ws,t8),e(ws,Y),e(Y,uo),e(uo,co),e(co,Xm),e(Xm,s8),e(co,a8),e(uo,n8),e(uo,wp),e(wp,r8),e(Y,o8),e(Y,po),e(po,bp),e(bp,Qm),e(Qm,l8),e(po,i8),e(po,Tp),e(Tp,u8),e(Y,c8),e(Y,fo),e(fo,jp),e(jp,p8),e(fo,f8),e(fo,N),e(N,h8),e(N,Zm),e(Zm,d8),e(N,g8),e(N,m8),e(N,$8),e(N,e$),e(e$,q8),e(N,_8),e(N,v8),e(N,y8),e(N,t$),e(t$,E8),e(N,w8),e(N,b8),e(N,T8),e(N,s$),e(s$,j8),e(N,k8),e(N,a$),e(a$,A8),e(N,D8),e(N,O8),e(N,R8),e(N,n$),e(n$,P8),e(N,S8),e(N,r$),e(r$,N8),e(N,x8),e(N,I8),e(N,H8),e(N,o$),e(o$,B8),e(N,C8),e(N,l$),e(l$,L8),e(N,G8),e(Y,M8),e(Y,ho),e(ho,kp),e(kp,i$),e(i$,z8),e(ho,K8),e(ho,Ap),e(Ap,F8),e(Y,U8),e(Y,go),e(go,Dp),e(Dp,J8),e(go,W8),e(go,bs),e(bs,Y8),e(bs,u$),e(u$,V8),e(bs,X8),e(Y,Q8),e(Y,mo),e(mo,Op),e(Op,Z8),e(mo,eA),e(mo,Ts),e(Ts,tA),e(Ts,c$),e(c$,sA),e(Ts,aA),m(a,ay,g),m(a,Rp,g),e(Rp,nA),m(a,ny,g),y(js,a,g),m(a,ry,g),m(a,ks,g),e(ks,p$),e(p$,$o),e($o,Pp),e(Pp,rA),e($o,oA),e($o,f$),e(ks,lA),e(ks,Q),e(Q,qo),e(qo,Sp),e(Sp,h$),e(h$,iA),e(qo,uA),e(qo,Np),e(Np,cA),e(Q,pA),e(Q,_o),e(_o,xp),e(xp,d$),e(d$,fA),e(_o,hA),e(_o,Ip),e(Ip,dA),e(Q,gA),e(Q,vo),e(vo,Hp),e(Hp,g$),e(g$,mA),e(vo,$A),e(vo,Bp),e(Bp,qA),e(Q,_A),e(Q,yo),e(yo,Cp),e(Cp,m$),e(m$,vA),e(yo,yA),e(yo,As),e(As,EA),e(As,$$),e($$,wA),e(As,bA),e(Q,TA),e(Q,Eo),e(Eo,Lp),e(Lp,q$),e(q$,jA),e(Eo,kA),e(Eo,Ds),e(Ds,AA),e(Ds,_$),e(_$,DA),e(Ds,OA),m(a,oy,g),m(a,Fe,g),e(Fe,Os),e(Os,v$),y(wo,v$,null),e(Fe,RA),e(Fe,y$),e(y$,PA),m(a,ly,g),m(a,bo,g),e(bo,SA),e(bo,Gp),e(Gp,NA),m(a,iy,g),m(a,Ue,g),e(Ue,Rs),e(Rs,E$),y(To,E$,null),e(Ue,xA),e(Ue,w$),e(w$,IA),m(a,uy,g),m(a,Mp,g),e(Mp,HA),m(a,cy,g),y(Ps,a,g),m(a,py,g),m(a,jo,g),e(jo,BA),e(jo,ko),e(ko,CA),m(a,fy,g),m(a,zp,g),e(zp,LA),m(a,hy,g),y(Ss,a,g),m(a,dy,g),m(a,Kp,g),e(Kp,GA),m(a,gy,g),m(a,Ns,g),e(Ns,b$),e(b$,Ao),e(Ao,Fp),e(Fp,MA),e(Ao,zA),e(Ao,T$),e(Ns,KA),e(Ns,de),e(de,Do),e(Do,Oo),e(Oo,j$),e(j$,FA),e(Oo,UA),e(Do,JA),e(Do,Up),e(Up,WA),e(de,YA),e(de,Ro),e(Ro,Jp),e(Jp,k$),e(k$,VA),e(Ro,XA),e(Ro,Wp),e(Wp,QA),e(de,ZA),e(de,Po),e(Po,Yp),e(Yp,eD),e(Po,tD),e(Po,xs),e(xs,sD),e(xs,A$),e(A$,aD),e(xs,nD),e(de,rD),e(de,So),e(So,Vp),e(Vp,oD),e(So,lD),e(So,Is),e(Is,iD),e(Is,D$),e(D$,uD),e(Is,cD),m(a,my,g),m(a,Xp,g),e(Xp,pD),m(a,$y,g),m(a,Hs,g),e(Hs,O$),e(O$,No),e(No,Qp),e(Qp,fD),e(No,hD),e(No,R$),e(Hs,dD),e(Hs,P$),e(P$,xo),e(xo,Zp),e(Zp,S$),e(S$,gD),e(xo,mD),e(xo,ef),e(ef,$D),m(a,qy,g),m(a,Je,g),e(Je,Bs),e(Bs,N$),y(Io,N$,null),e(Je,qD),e(Je,x$),e(x$,_D),m(a,_y,g),m(a,tf,g),e(tf,vD),m(a,vy,g),y(Cs,a,g),m(a,yy,g),m(a,Ho,g),e(Ho,yD),e(Ho,Bo),e(Bo,ED),m(a,Ey,g),m(a,sf,g),e(sf,wD),m(a,wy,g),y(Ls,a,g),m(a,by,g),m(a,af,g),e(af,bD),m(a,Ty,g),m(a,Gs,g),e(Gs,I$),e(I$,Co),e(Co,nf),e(nf,TD),e(Co,jD),e(Co,H$),e(Gs,kD),e(Gs,F),e(F,Lo),e(Lo,Go),e(Go,B$),e(B$,AD),e(Go,DD),e(Lo,OD),e(Lo,rf),e(rf,RD),e(F,PD),e(F,Mo),e(Mo,zo),e(zo,C$),e(C$,SD),e(zo,ND),e(Mo,xD),e(Mo,of),e(of,ID),e(F,HD),e(F,Ko),e(Ko,lf),e(lf,BD),e(Ko,CD),e(Ko,je),e(je,LD),e(je,L$),e(L$,GD),e(je,MD),e(je,G$),e(G$,zD),e(je,KD),e(F,FD),e(F,Fo),e(Fo,uf),e(uf,UD),e(Fo,JD),e(Fo,Ms),e(Ms,WD),e(Ms,M$),e(M$,YD),e(Ms,VD),e(F,XD),e(F,Uo),e(Uo,cf),e(cf,z$),e(z$,QD),e(Uo,ZD),e(Uo,pf),e(pf,eO),e(F,tO),e(F,Jo),e(Jo,ff),e(ff,sO),e(Jo,aO),e(Jo,zs),e(zs,nO),e(zs,K$),e(K$,rO),e(zs,oO),e(F,lO),e(F,Wo),e(Wo,hf),e(hf,iO),e(Wo,uO),e(Wo,Ks),e(Ks,cO),e(Ks,F$),e(F$,pO),e(Ks,fO),m(a,jy,g),m(a,df,g),e(df,hO),m(a,ky,g),m(a,gf,g),e(gf,dO),m(a,Ay,g),y(Fs,a,g),m(a,Dy,g),m(a,Us,g),e(Us,U$),e(U$,Yo),e(Yo,mf),e(mf,gO),e(Yo,mO),e(Yo,J$),e(Us,$O),e(Us,We),e(We,Vo),e(Vo,$f),e($f,W$),e(W$,qO),e(Vo,_O),e(Vo,qf),e(qf,vO),e(We,yO),e(We,Xo),e(Xo,_f),e(_f,Y$),e(Y$,EO),e(Xo,wO),e(Xo,vf),e(vf,bO),e(We,TO),e(We,Qo),e(Qo,yf),e(yf,V$),e(V$,jO),e(Qo,kO),e(Qo,Js),e(Js,AO),e(Js,X$),e(X$,DO),e(Js,OO),m(a,Oy,g),m(a,Ye,g),e(Ye,Ws),e(Ws,Q$),y(Zo,Q$,null),e(Ye,RO),e(Ye,Z$),e(Z$,PO),m(a,Ry,g),m(a,Ef,g),e(Ef,SO),m(a,Py,g),y(Ys,a,g),m(a,Sy,g),m(a,el,g),e(el,NO),e(el,tl),e(tl,xO),m(a,Ny,g),m(a,wf,g),e(wf,IO),m(a,xy,g),y(Vs,a,g),m(a,Iy,g),m(a,bf,g),e(bf,HO),m(a,Hy,g),m(a,Xs,g),e(Xs,eq),e(eq,sl),e(sl,Tf),e(Tf,BO),e(sl,CO),e(sl,tq),e(Xs,LO),e(Xs,x),e(x,al),e(al,nl),e(nl,sq),e(sq,GO),e(nl,MO),e(al,zO),e(al,aq),e(x,KO),e(x,rl),e(rl,jf),e(jf,FO),e(rl,UO),e(rl,kf),e(kf,JO),e(x,WO),e(x,ol),e(ol,Af),e(Af,YO),e(ol,VO),e(ol,Df),e(Df,XO),e(x,QO),e(x,ll),e(ll,Of),e(Of,ZO),e(ll,eR),e(ll,Qs),e(Qs,tR),e(Qs,nq),e(nq,sR),e(Qs,aR),e(x,nR),e(x,il),e(il,Rf),e(Rf,rq),e(rq,rR),e(il,oR),e(il,Pf),e(Pf,lR),e(x,iR),e(x,ul),e(ul,Sf),e(Sf,uR),e(ul,cR),e(ul,ke),e(ke,pR),e(ke,oq),e(oq,fR),e(ke,hR),e(ke,lq),e(lq,dR),e(ke,gR),e(x,mR),e(x,cl),e(cl,Nf),e(Nf,$R),e(cl,qR),e(cl,Ae),e(Ae,_R),e(Ae,iq),e(iq,vR),e(Ae,yR),e(Ae,uq),e(uq,ER),e(Ae,wR),e(x,bR),e(x,pl),e(pl,xf),e(xf,TR),e(pl,jR),e(pl,De),e(De,kR),e(De,cq),e(cq,AR),e(De,DR),e(De,pq),e(pq,OR),e(De,RR),e(x,PR),e(x,fl),e(fl,If),e(If,SR),e(fl,NR),e(fl,ne),e(ne,xR),e(ne,fq),e(fq,IR),e(ne,HR),e(ne,hq),e(hq,BR),e(ne,CR),e(ne,dq),e(dq,LR),e(ne,GR),e(x,MR),e(x,hl),e(hl,Hf),e(Hf,zR),e(hl,KR),e(hl,re),e(re,FR),e(re,gq),e(gq,UR),e(re,JR),e(re,mq),e(mq,WR),e(re,YR),e(re,$q),e($q,VR),e(re,XR),e(x,QR),e(x,dl),e(dl,Bf),e(Bf,ZR),e(dl,eP),e(dl,Zs),e(Zs,tP),e(Zs,qq),e(qq,sP),e(Zs,aP),e(x,nP),e(x,gl),e(gl,Cf),e(Cf,rP),e(gl,oP),e(gl,ea),e(ea,lP),e(ea,_q),e(_q,iP),e(ea,uP),e(x,cP),e(x,ml),e(ml,Lf),e(Lf,vq),e(vq,pP),e(ml,fP),e(ml,Gf),e(Gf,hP),e(x,dP),e(x,$l),e($l,Mf),e(Mf,gP),e($l,mP),e($l,ta),e(ta,$P),e(ta,yq),e(yq,qP),e(ta,_P),e(x,vP),e(x,ql),e(ql,zf),e(zf,yP),e(ql,EP),e(ql,sa),e(sa,wP),e(sa,Eq),e(Eq,bP),e(sa,TP),m(a,By,g),m(a,Kf,g),e(Kf,jP),m(a,Cy,g),m(a,aa,g),e(aa,wq),e(wq,_l),e(_l,Ff),e(Ff,kP),e(_l,AP),e(_l,bq),e(aa,DP),e(aa,ge),e(ge,vl),e(vl,Uf),e(Uf,Tq),e(Tq,OP),e(vl,RP),e(vl,Jf),e(Jf,PP),e(ge,SP),e(ge,yl),e(yl,Wf),e(Wf,jq),e(jq,NP),e(yl,xP),e(yl,Yf),e(Yf,IP),e(ge,HP),e(ge,El),e(El,Vf),e(Vf,BP),e(El,CP),e(El,Xf),e(Xf,LP),e(ge,GP),e(ge,wl),e(wl,Qf),e(Qf,MP),e(wl,zP),e(wl,Zf),e(Zf,KP),m(a,Ly,g),m(a,Ve,g),e(Ve,na),e(na,kq),y(bl,kq,null),e(Ve,FP),e(Ve,Aq),e(Aq,UP),m(a,Gy,g),m(a,eh,g),e(eh,JP),m(a,My,g),y(ra,a,g),m(a,zy,g),m(a,Xe,g),e(Xe,WP),e(Xe,Tl),e(Tl,YP),e(Xe,VP),e(Xe,jl),e(jl,XP),m(a,Ky,g),m(a,th,g),e(th,QP),m(a,Fy,g),m(a,oa,g),e(oa,Dq),e(Dq,kl),e(kl,sh),e(sh,ZP),e(kl,eS),e(kl,Oq),e(oa,tS),e(oa,me),e(me,Al),e(Al,Dl),e(Dl,Rq),e(Rq,sS),e(Dl,aS),e(Al,nS),e(Al,ah),e(ah,rS),e(me,oS),e(me,Ol),e(Ol,nh),e(nh,Pq),e(Pq,lS),e(Ol,iS),e(Ol,rh),e(rh,uS),e(me,cS),e(me,Rl),e(Rl,oh),e(oh,pS),e(Rl,fS),e(Rl,la),e(la,hS),e(la,Sq),e(Sq,dS),e(la,gS),e(me,mS),e(me,Pl),e(Pl,lh),e(lh,$S),e(Pl,qS),e(Pl,ia),e(ia,_S),e(ia,Nq),e(Nq,vS),e(ia,yS),m(a,Uy,g),m(a,ih,g),e(ih,ES),m(a,Jy,g),m(a,ua,g),e(ua,xq),e(xq,Sl),e(Sl,uh),e(uh,wS),e(Sl,bS),e(Sl,Iq),e(ua,TS),e(ua,Hq),e(Hq,Nl),e(Nl,ch),e(ch,Bq),e(Bq,jS),e(Nl,kS),e(Nl,ph),e(ph,AS),m(a,Wy,g),m(a,fh,g),e(fh,DS),m(a,Yy,g),m(a,Qe,g),e(Qe,ca),e(ca,Cq),y(xl,Cq,null),e(Qe,OS),e(Qe,Lq),e(Lq,RS),m(a,Vy,g),m(a,Ze,g),e(Ze,pa),e(pa,Gq),y(Il,Gq,null),e(Ze,PS),e(Ze,Mq),e(Mq,SS),m(a,Xy,g),m(a,hh,g),e(hh,NS),m(a,Qy,g),y(fa,a,g),m(a,Zy,g),y(ha,a,g),m(a,e2,g),m(a,$e,g),e($e,xS),e($e,Hl),e(Hl,IS),e($e,HS),e($e,Bl),e(Bl,BS),e($e,CS),e($e,Cl),e(Cl,LS),m(a,t2,g),m(a,dh,g),e(dh,GS),m(a,s2,g),y(da,a,g),m(a,a2,g),m(a,gh,g),e(gh,MS),m(a,n2,g),m(a,ga,g),e(ga,zq),e(zq,Ll),e(Ll,mh),e(mh,zS),e(Ll,KS),e(Ll,Kq),e(ga,FS),e(ga,Fq),e(Fq,Gl),e(Gl,Ml),e(Ml,Uq),e(Uq,US),e(Ml,JS),e(Gl,WS),e(Gl,$h),e($h,YS),m(a,r2,g),m(a,qh,g),e(qh,VS),m(a,o2,g),m(a,_h,g),e(_h,XS),m(a,l2,g),y(ma,a,g),m(a,i2,g),m(a,$a,g),e($a,Jq),e(Jq,zl),e(zl,vh),e(vh,QS),e(zl,ZS),e(zl,Wq),e($a,eN),e($a,Yq),e(Yq,Kl),e(Kl,yh),e(yh,Vq),e(Vq,tN),e(Kl,sN),e(Kl,Eh),e(Eh,aN),m(a,u2,g),m(a,et,g),e(et,qa),e(qa,Xq),y(Fl,Xq,null),e(et,nN),e(et,Qq),e(Qq,rN),m(a,c2,g),m(a,wh,g),e(wh,oN),m(a,p2,g),y(_a,a,g),m(a,f2,g),m(a,tt,g),e(tt,lN),e(tt,Ul),e(Ul,iN),e(tt,uN),e(tt,Jl),e(Jl,cN),m(a,h2,g),m(a,bh,g),e(bh,pN),m(a,d2,g),y(va,a,g),m(a,g2,g),m(a,Th,g),e(Th,fN),m(a,m2,g),m(a,ya,g),e(ya,Zq),e(Zq,Wl),e(Wl,jh),e(jh,hN),e(Wl,dN),e(Wl,e_),e(ya,gN),e(ya,t_),e(t_,Yl),e(Yl,Vl),e(Vl,s_),e(s_,mN),e(Vl,$N),e(Yl,qN),e(Yl,kh),e(kh,_N),m(a,$2,g),m(a,Ah,g),e(Ah,vN),m(a,q2,g),y(Ea,a,g),m(a,_2,g),m(a,wa,g),e(wa,a_),e(a_,Xl),e(Xl,Dh),e(Dh,yN),e(Xl,EN),e(Xl,n_),e(wa,wN),e(wa,Ql),e(Ql,Zl),e(Zl,Oh),e(Oh,r_),e(r_,bN),e(Zl,TN),e(Zl,Rh),e(Rh,jN),e(Ql,kN),e(Ql,ei),e(ei,Ph),e(Ph,o_),e(o_,AN),e(ei,DN),e(ei,Sh),e(Sh,ON),m(a,v2,g),m(a,st,g),e(st,ba),e(ba,l_),y(ti,l_,null),e(st,RN),e(st,i_),e(i_,PN),m(a,y2,g),m(a,at,g),e(at,Ta),e(Ta,u_),y(si,u_,null),e(at,SN),e(at,c_),e(c_,NN),m(a,E2,g),m(a,Nh,g),e(Nh,xN),m(a,w2,g),y(ja,a,g),m(a,b2,g),m(a,ai,g),e(ai,IN),e(ai,ni),e(ni,HN),m(a,T2,g),m(a,xh,g),e(xh,BN),m(a,j2,g),y(ka,a,g),m(a,k2,g),m(a,Aa,g),e(Aa,CN),e(Aa,ri),e(ri,LN),e(Aa,GN),m(a,A2,g),m(a,Da,g),e(Da,p_),e(p_,oi),e(oi,Ih),e(Ih,MN),e(oi,zN),e(oi,f_),e(Da,KN),e(Da,h_),e(h_,li),e(li,ii),e(ii,d_),e(d_,FN),e(ii,UN),e(li,JN),e(li,Hh),e(Hh,WN),m(a,D2,g),m(a,Bh,g),e(Bh,YN),m(a,O2,g),y(Oa,a,g),m(a,R2,g),m(a,Ra,g),e(Ra,g_),e(g_,ui),e(ui,Ch),e(Ch,VN),e(ui,XN),e(ui,m_),e(Ra,QN),e(Ra,ci),e(ci,pi),e(pi,Lh),e(Lh,$_),e($_,ZN),e(pi,ex),e(pi,Gh),e(Gh,tx),e(ci,sx),e(ci,fi),e(fi,Mh),e(Mh,q_),e(q_,ax),e(fi,nx),e(fi,zh),e(zh,rx),m(a,P2,g),m(a,nt,g),e(nt,Pa),e(Pa,__),y(hi,__,null),e(nt,ox),e(nt,v_),e(v_,lx),m(a,S2,g),m(a,Kh,g),e(Kh,ix),m(a,N2,g),y(Sa,a,g),m(a,x2,g),m(a,di,g),e(di,ux),e(di,gi),e(gi,cx),m(a,I2,g),m(a,Fh,g),e(Fh,px),m(a,H2,g),y(Na,a,g),m(a,B2,g),m(a,xa,g),e(xa,fx),e(xa,mi),e(mi,hx),e(xa,dx),m(a,C2,g),m(a,Ia,g),e(Ia,y_),e(y_,$i),e($i,Uh),e(Uh,gx),e($i,mx),e($i,E_),e(Ia,$x),e(Ia,w_),e(w_,qi),e(qi,_i),e(_i,b_),e(b_,qx),e(_i,_x),e(qi,vx),e(qi,Jh),e(Jh,yx),m(a,L2,g),m(a,Wh,g),e(Wh,Ex),m(a,G2,g),y(Ha,a,g),m(a,M2,g),m(a,Ba,g),e(Ba,T_),e(T_,vi),e(vi,Yh),e(Yh,wx),e(vi,bx),e(vi,j_),e(Ba,Tx),e(Ba,rt),e(rt,yi),e(yi,Vh),e(Vh,k_),e(k_,jx),e(yi,kx),e(yi,Xh),e(Xh,Ax),e(rt,Dx),e(rt,Ei),e(Ei,Qh),e(Qh,A_),e(A_,Ox),e(Ei,Rx),e(Ei,Zh),e(Zh,Px),e(rt,Sx),e(rt,wi),e(wi,ed),e(ed,D_),e(D_,Nx),e(wi,xx),e(wi,td),e(td,Ix),m(a,z2,g),m(a,ot,g),e(ot,Ca),e(Ca,O_),y(bi,O_,null),e(ot,Hx),e(ot,R_),e(R_,Bx),m(a,K2,g),m(a,sd,g),e(sd,Cx),m(a,F2,g),y(La,a,g),m(a,U2,g),m(a,Ti,g),e(Ti,Lx),e(Ti,ji),e(ji,Gx),m(a,J2,g),m(a,ad,g),e(ad,Mx),m(a,W2,g),y(Ga,a,g),m(a,Y2,g),m(a,Ma,g),e(Ma,zx),e(Ma,ki),e(ki,Kx),e(Ma,Fx),m(a,V2,g),m(a,za,g),e(za,P_),e(P_,Ai),e(Ai,nd),e(nd,Ux),e(Ai,Jx),e(Ai,S_),e(za,Wx),e(za,N_),e(N_,Di),e(Di,Oi),e(Oi,x_),e(x_,Yx),e(Oi,Vx),e(Di,Xx),e(Di,rd),e(rd,Qx),m(a,X2,g),m(a,od,g),e(od,Zx),m(a,Q2,g),y(Ka,a,g),m(a,Z2,g),m(a,Fa,g),e(Fa,I_),e(I_,Ri),e(Ri,ld),e(ld,eI),e(Ri,tI),e(Ri,H_),e(Fa,sI),e(Fa,lt),e(lt,Pi),e(Pi,id),e(id,B_),e(B_,aI),e(Pi,nI),e(Pi,ud),e(ud,rI),e(lt,oI),e(lt,Si),e(Si,cd),e(cd,C_),e(C_,lI),e(Si,iI),e(Si,pd),e(pd,uI),e(lt,cI),e(lt,Ni),e(Ni,fd),e(fd,L_),e(L_,pI),e(Ni,fI),e(Ni,hd),e(hd,hI),eE=!0},p(a,[g]){const xi={};g&2&&(xi.$$scope={dirty:g,ctx:a}),ht.$set(xi);const G_={};g&2&&(G_.$$scope={dirty:g,ctx:a}),dt.$set(G_);const M_={};g&2&&(M_.$$scope={dirty:g,ctx:a}),qt.$set(M_);const z_={};g&2&&(z_.$$scope={dirty:g,ctx:a}),Et.$set(z_);const Ii={};g&2&&(Ii.$$scope={dirty:g,ctx:a}),wt.$set(Ii);const K_={};g&2&&(K_.$$scope={dirty:g,ctx:a}),Rt.$set(K_);const F_={};g&2&&(F_.$$scope={dirty:g,ctx:a}),Pt.$set(F_);const U_={};g&2&&(U_.$$scope={dirty:g,ctx:a}),St.$set(U_);const J_={};g&2&&(J_.$$scope={dirty:g,ctx:a}),Bt.$set(J_);const W_={};g&2&&(W_.$$scope={dirty:g,ctx:a}),Ct.$set(W_);const Hi={};g&2&&(Hi.$$scope={dirty:g,ctx:a}),zt.$set(Hi);const Y_={};g&2&&(Y_.$$scope={dirty:g,ctx:a}),Ut.$set(Y_);const V_={};g&2&&(V_.$$scope={dirty:g,ctx:a}),Jt.$set(V_);const X_={};g&2&&(X_.$$scope={dirty:g,ctx:a}),Xt.$set(X_);const Bi={};g&2&&(Bi.$$scope={dirty:g,ctx:a}),es.$set(Bi);const Q_={};g&2&&(Q_.$$scope={dirty:g,ctx:a}),ts.$set(Q_);const Z_={};g&2&&(Z_.$$scope={dirty:g,ctx:a}),rs.$set(Z_);const e1={};g&2&&(e1.$$scope={dirty:g,ctx:a}),is.$set(e1);const t1={};g&2&&(t1.$$scope={dirty:g,ctx:a}),us.$set(t1);const dd={};g&2&&(dd.$$scope={dirty:g,ctx:a}),ms.$set(dd);const s1={};g&2&&(s1.$$scope={dirty:g,ctx:a}),ys.$set(s1);const a1={};g&2&&(a1.$$scope={dirty:g,ctx:a}),Es.$set(a1);const n1={};g&2&&(n1.$$scope={dirty:g,ctx:a}),js.$set(n1);const Ci={};g&2&&(Ci.$$scope={dirty:g,ctx:a}),Ps.$set(Ci);const r1={};g&2&&(r1.$$scope={dirty:g,ctx:a}),Ss.$set(r1);const Li={};g&2&&(Li.$$scope={dirty:g,ctx:a}),Cs.$set(Li);const o1={};g&2&&(o1.$$scope={dirty:g,ctx:a}),Ls.$set(o1);const qe={};g&2&&(qe.$$scope={dirty:g,ctx:a}),Fs.$set(qe);const Gi={};g&2&&(Gi.$$scope={dirty:g,ctx:a}),Ys.$set(Gi);const gd={};g&2&&(gd.$$scope={dirty:g,ctx:a}),Vs.$set(gd);const l1={};g&2&&(l1.$$scope={dirty:g,ctx:a}),ra.$set(l1);const i1={};g&2&&(i1.$$scope={dirty:g,ctx:a}),fa.$set(i1);const Mi={};g&2&&(Mi.$$scope={dirty:g,ctx:a}),ha.$set(Mi);const u1={};g&2&&(u1.$$scope={dirty:g,ctx:a}),da.$set(u1);const c1={};g&2&&(c1.$$scope={dirty:g,ctx:a}),ma.$set(c1);const p1={};g&2&&(p1.$$scope={dirty:g,ctx:a}),_a.$set(p1);const zi={};g&2&&(zi.$$scope={dirty:g,ctx:a}),va.$set(zi);const f1={};g&2&&(f1.$$scope={dirty:g,ctx:a}),Ea.$set(f1);const Ki={};g&2&&(Ki.$$scope={dirty:g,ctx:a}),ja.$set(Ki);const h1={};g&2&&(h1.$$scope={dirty:g,ctx:a}),ka.$set(h1);const Fi={};g&2&&(Fi.$$scope={dirty:g,ctx:a}),Oa.$set(Fi);const d1={};g&2&&(d1.$$scope={dirty:g,ctx:a}),Sa.$set(d1);const Ui={};g&2&&(Ui.$$scope={dirty:g,ctx:a}),Na.$set(Ui);const g1={};g&2&&(g1.$$scope={dirty:g,ctx:a}),Ha.$set(g1);const m1={};g&2&&(m1.$$scope={dirty:g,ctx:a}),La.$set(m1);const Ji={};g&2&&(Ji.$$scope={dirty:g,ctx:a}),Ga.$set(Ji);const $1={};g&2&&($1.$$scope={dirty:g,ctx:a}),Ka.$set($1)},i(a){eE||(E(k.$$.fragment,a),E(X.$$.fragment,a),E(on.$$.fragment,a),E(ln.$$.fragment,a),E(ht.$$.fragment,a),E(dt.$$.fragment,a),E(qt.$$.fragment,a),E(En.$$.fragment,a),E(Et.$$.fragment,a),E(wt.$$.fragment,a),E(Gn.$$.fragment,a),E(Rt.$$.fragment,a),E(Pt.$$.fragment,a),E(St.$$.fragment,a),E(Yn.$$.fragment,a),E(Bt.$$.fragment,a),E(Ct.$$.fragment,a),E(zt.$$.fragment,a),E(pr.$$.fragment,a),E(Ut.$$.fragment,a),E(Jt.$$.fragment,a),E(Xt.$$.fragment,a),E(br.$$.fragment,a),E(es.$$.fragment,a),E(ts.$$.fragment,a),E(rs.$$.fragment,a),E(Hr.$$.fragment,a),E(is.$$.fragment,a),E(us.$$.fragment,a),E(ms.$$.fragment,a),E(no.$$.fragment,a),E(ro.$$.fragment,a),E(ys.$$.fragment,a),E(Es.$$.fragment,a),E(js.$$.fragment,a),E(wo.$$.fragment,a),E(To.$$.fragment,a),E(Ps.$$.fragment,a),E(Ss.$$.fragment,a),E(Io.$$.fragment,a),E(Cs.$$.fragment,a),E(Ls.$$.fragment,a),E(Fs.$$.fragment,a),E(Zo.$$.fragment,a),E(Ys.$$.fragment,a),E(Vs.$$.fragment,a),E(bl.$$.fragment,a),E(ra.$$.fragment,a),E(xl.$$.fragment,a),E(Il.$$.fragment,a),E(fa.$$.fragment,a),E(ha.$$.fragment,a),E(da.$$.fragment,a),E(ma.$$.fragment,a),E(Fl.$$.fragment,a),E(_a.$$.fragment,a),E(va.$$.fragment,a),E(Ea.$$.fragment,a),E(ti.$$.fragment,a),E(si.$$.fragment,a),E(ja.$$.fragment,a),E(ka.$$.fragment,a),E(Oa.$$.fragment,a),E(hi.$$.fragment,a),E(Sa.$$.fragment,a),E(Na.$$.fragment,a),E(Ha.$$.fragment,a),E(bi.$$.fragment,a),E(La.$$.fragment,a),E(Ga.$$.fragment,a),E(Ka.$$.fragment,a),eE=!0)},o(a){w(k.$$.fragment,a),w(X.$$.fragment,a),w(on.$$.fragment,a),w(ln.$$.fragment,a),w(ht.$$.fragment,a),w(dt.$$.fragment,a),w(qt.$$.fragment,a),w(En.$$.fragment,a),w(Et.$$.fragment,a),w(wt.$$.fragment,a),w(Gn.$$.fragment,a),w(Rt.$$.fragment,a),w(Pt.$$.fragment,a),w(St.$$.fragment,a),w(Yn.$$.fragment,a),w(Bt.$$.fragment,a),w(Ct.$$.fragment,a),w(zt.$$.fragment,a),w(pr.$$.fragment,a),w(Ut.$$.fragment,a),w(Jt.$$.fragment,a),w(Xt.$$.fragment,a),w(br.$$.fragment,a),w(es.$$.fragment,a),w(ts.$$.fragment,a),w(rs.$$.fragment,a),w(Hr.$$.fragment,a),w(is.$$.fragment,a),w(us.$$.fragment,a),w(ms.$$.fragment,a),w(no.$$.fragment,a),w(ro.$$.fragment,a),w(ys.$$.fragment,a),w(Es.$$.fragment,a),w(js.$$.fragment,a),w(wo.$$.fragment,a),w(To.$$.fragment,a),w(Ps.$$.fragment,a),w(Ss.$$.fragment,a),w(Io.$$.fragment,a),w(Cs.$$.fragment,a),w(Ls.$$.fragment,a),w(Fs.$$.fragment,a),w(Zo.$$.fragment,a),w(Ys.$$.fragment,a),w(Vs.$$.fragment,a),w(bl.$$.fragment,a),w(ra.$$.fragment,a),w(xl.$$.fragment,a),w(Il.$$.fragment,a),w(fa.$$.fragment,a),w(ha.$$.fragment,a),w(da.$$.fragment,a),w(ma.$$.fragment,a),w(Fl.$$.fragment,a),w(_a.$$.fragment,a),w(va.$$.fragment,a),w(Ea.$$.fragment,a),w(ti.$$.fragment,a),w(si.$$.fragment,a),w(ja.$$.fragment,a),w(ka.$$.fragment,a),w(Oa.$$.fragment,a),w(hi.$$.fragment,a),w(Sa.$$.fragment,a),w(Na.$$.fragment,a),w(Ha.$$.fragment,a),w(bi.$$.fragment,a),w(La.$$.fragment,a),w(Ga.$$.fragment,a),w(Ka.$$.fragment,a),eE=!1},d(a){s(n),a&&s(i),a&&s(t),b(k),a&&s(S),a&&s(D),b(X),a&&s(rn),a&&s(Pe),a&&s(E1),a&&s(Vi),a&&s(w1),a&&s(ut),a&&s(b1),a&&s(ct),a&&s(T1),a&&s(Se),b(on),a&&s(j1),a&&s(Ne),b(ln),a&&s(k1),a&&s(Xi),a&&s(A1),b(ht,a),a&&s(D1),a&&s(un),a&&s(O1),a&&s(Qi),a&&s(R1),b(dt,a),a&&s(P1),a&&s(Zi),a&&s(S1),a&&s(gt),a&&s(N1),a&&s(ou),a&&s(x1),b(qt,a),a&&s(I1),a&&s(_t),a&&s(H1),a&&s(xe),b(En),a&&s(B1),a&&s(yt),a&&s(C1),b(Et,a),a&&s(L1),a&&s(wn),a&&s(G1),a&&s($u),a&&s(M1),b(wt,a),a&&s(z1),a&&s(qu),a&&s(K1),a&&s(bt),a&&s(F1),a&&s(Nu),a&&s(U1),a&&s(Dt),a&&s(J1),a&&s(Ie),b(Gn),a&&s(W1),a&&s(Bu),a&&s(Y1),b(Rt,a),a&&s(V1),a&&s(He),a&&s(X1),a&&s(Cu),a&&s(Q1),b(Pt,a),a&&s(Z1),a&&s(Lu),a&&s(ev),a&&s(Gu),a&&s(tv),b(St,a),a&&s(sv),a&&s(Nt),a&&s(av),a&&s(Be),b(Yn),a&&s(nv),a&&s(Yu),a&&s(rv),b(Bt,a),a&&s(ov),a&&s(Vn),a&&s(lv),a&&s(Vu),a&&s(iv),b(Ct,a),a&&s(uv),a&&s(Xu),a&&s(cv),a&&s(Lt),a&&s(pv),a&&s(lc),a&&s(fv),b(zt,a),a&&s(hv),a&&s(Kt),a&&s(dv),a&&s(Ce),b(pr),a&&s(gv),a&&s($c),a&&s(mv),b(Ut,a),a&&s($v),a&&s(fr),a&&s(qv),a&&s(qc),a&&s(_v),b(Jt,a),a&&s(vv),a&&s(_c),a&&s(yv),a&&s(Wt),a&&s(Ev),a&&s(Dc),a&&s(wv),b(Xt,a),a&&s(bv),a&&s(Qt),a&&s(Tv),a&&s(Le),b(br),a&&s(jv),a&&s(Sc),a&&s(kv),b(es,a),a&&s(Av),a&&s(Tr),a&&s(Dv),a&&s(Nc),a&&s(Ov),b(ts,a),a&&s(Rv),a&&s(xc),a&&s(Pv),a&&s(ss),a&&s(Sv),a&&s(Mc),a&&s(Nv),b(rs,a),a&&s(xv),a&&s(os),a&&s(Iv),a&&s(Ge),b(Hr),a&&s(Hv),a&&s(Wc),a&&s(Bv),b(is,a),a&&s(Cv),a&&s(Br),a&&s(Lv),a&&s(Yc),a&&s(Gv),b(us,a),a&&s(Mv),a&&s(Vc),a&&s(zv),a&&s(cs),a&&s(Kv),a&&s(dp),a&&s(Fv),b(ms,a),a&&s(Uv),a&&s($s),a&&s(Jv),a&&s(Me),b(no),a&&s(Wv),a&&s(_s),a&&s(Yv),a&&s(ze),b(ro),a&&s(Vv),a&&s(_p),a&&s(Xv),b(ys,a),a&&s(Qv),a&&s(Ke),a&&s(Zv),a&&s(vp),a&&s(ey),b(Es,a),a&&s(ty),a&&s(yp),a&&s(sy),a&&s(ws),a&&s(ay),a&&s(Rp),a&&s(ny),b(js,a),a&&s(ry),a&&s(ks),a&&s(oy),a&&s(Fe),b(wo),a&&s(ly),a&&s(bo),a&&s(iy),a&&s(Ue),b(To),a&&s(uy),a&&s(Mp),a&&s(cy),b(Ps,a),a&&s(py),a&&s(jo),a&&s(fy),a&&s(zp),a&&s(hy),b(Ss,a),a&&s(dy),a&&s(Kp),a&&s(gy),a&&s(Ns),a&&s(my),a&&s(Xp),a&&s($y),a&&s(Hs),a&&s(qy),a&&s(Je),b(Io),a&&s(_y),a&&s(tf),a&&s(vy),b(Cs,a),a&&s(yy),a&&s(Ho),a&&s(Ey),a&&s(sf),a&&s(wy),b(Ls,a),a&&s(by),a&&s(af),a&&s(Ty),a&&s(Gs),a&&s(jy),a&&s(df),a&&s(ky),a&&s(gf),a&&s(Ay),b(Fs,a),a&&s(Dy),a&&s(Us),a&&s(Oy),a&&s(Ye),b(Zo),a&&s(Ry),a&&s(Ef),a&&s(Py),b(Ys,a),a&&s(Sy),a&&s(el),a&&s(Ny),a&&s(wf),a&&s(xy),b(Vs,a),a&&s(Iy),a&&s(bf),a&&s(Hy),a&&s(Xs),a&&s(By),a&&s(Kf),a&&s(Cy),a&&s(aa),a&&s(Ly),a&&s(Ve),b(bl),a&&s(Gy),a&&s(eh),a&&s(My),b(ra,a),a&&s(zy),a&&s(Xe),a&&s(Ky),a&&s(th),a&&s(Fy),a&&s(oa),a&&s(Uy),a&&s(ih),a&&s(Jy),a&&s(ua),a&&s(Wy),a&&s(fh),a&&s(Yy),a&&s(Qe),b(xl),a&&s(Vy),a&&s(Ze),b(Il),a&&s(Xy),a&&s(hh),a&&s(Qy),b(fa,a),a&&s(Zy),b(ha,a),a&&s(e2),a&&s($e),a&&s(t2),a&&s(dh),a&&s(s2),b(da,a),a&&s(a2),a&&s(gh),a&&s(n2),a&&s(ga),a&&s(r2),a&&s(qh),a&&s(o2),a&&s(_h),a&&s(l2),b(ma,a),a&&s(i2),a&&s($a),a&&s(u2),a&&s(et),b(Fl),a&&s(c2),a&&s(wh),a&&s(p2),b(_a,a),a&&s(f2),a&&s(tt),a&&s(h2),a&&s(bh),a&&s(d2),b(va,a),a&&s(g2),a&&s(Th),a&&s(m2),a&&s(ya),a&&s($2),a&&s(Ah),a&&s(q2),b(Ea,a),a&&s(_2),a&&s(wa),a&&s(v2),a&&s(st),b(ti),a&&s(y2),a&&s(at),b(si),a&&s(E2),a&&s(Nh),a&&s(w2),b(ja,a),a&&s(b2),a&&s(ai),a&&s(T2),a&&s(xh),a&&s(j2),b(ka,a),a&&s(k2),a&&s(Aa),a&&s(A2),a&&s(Da),a&&s(D2),a&&s(Bh),a&&s(O2),b(Oa,a),a&&s(R2),a&&s(Ra),a&&s(P2),a&&s(nt),b(hi),a&&s(S2),a&&s(Kh),a&&s(N2),b(Sa,a),a&&s(x2),a&&s(di),a&&s(I2),a&&s(Fh),a&&s(H2),b(Na,a),a&&s(B2),a&&s(xa),a&&s(C2),a&&s(Ia),a&&s(L2),a&&s(Wh),a&&s(G2),b(Ha,a),a&&s(M2),a&&s(Ba),a&&s(z2),a&&s(ot),b(bi),a&&s(K2),a&&s(sd),a&&s(F2),b(La,a),a&&s(U2),a&&s(Ti),a&&s(J2),a&&s(ad),a&&s(W2),b(Ga,a),a&&s(Y2),a&&s(Ma),a&&s(V2),a&&s(za),a&&s(X2),a&&s(od),a&&s(Q2),b(Ka,a),a&&s(Z2),a&&s(Fa)}}}const FY={local:"detailed-parameters",sections:[{local:"which-task-is-used-by-this-model",title:"Which task is used by this model ?"},{local:"natural-language-processing",sections:[{local:"fill-mask-task",title:"Fill Mask task"},{local:"summarization-task",title:"Summarization task"},{local:"question-answering-task",title:"Question Answering task"},{local:"table-question-answering-task",title:"Table Question Answering task"},{local:"sentence-similarity-task",title:"Sentence Similarity task"},{local:"text-classification-task",title:"Text Classification task"},{local:"text-generation-task",title:"Text Generation task"},{local:"text2text-generation-task",title:"Text2Text Generation task"},{local:"token-classification-task",title:"Token Classification task"},{local:"named-entity-recognition-ner-task",title:"Named Entity Recognition (NER) task"},{local:"translation-task",title:"Translation task"},{local:"zeroshot-classification-task",title:"Zero-Shot Classification task"},{local:"conversational-task",title:"Conversational task"},{local:"feature-extraction-task",title:"Feature Extraction task"}],title:"Natural Language Processing"},{local:"audio",sections:[{local:"automatic-speech-recognition-task",title:"Automatic Speech Recognition task"},{local:"audio-classification-task",title:"Audio Classification task"}],title:"Audio"},{local:"computer-vision",sections:[{local:"image-classification-task",title:"Image Classification task"},{local:"object-detection-task",title:"Object Detection task"},{local:"image-segmentation-task",title:"Image Segmentation task"}],title:"Computer Vision"}],title:"Detailed parameters"};function UY(q){return pJ(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class XY extends lJ{constructor(n){super();iJ(this,n,UY,KY,uJ,{})}}export{XY as default,FY as metadata};
