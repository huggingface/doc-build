import{S as Ta,i as za,s as La,e as o,k as c,w as u,t as h,M as Va,c as a,d as t,m as d,a as i,x as f,h as p,b as r,G as n,g as l,y,L as Fa,q as v,o as m,B as w,v as Wa}from"../chunks/vendor-hf-doc-builder.js";import{I as g}from"../chunks/IconCopyLink-hf-doc-builder.js";function Ya(_o){let _,vt,b,F,We,oe,dn,Ye,un,mt,A,W,Ue,ae,fn,Oe,yn,wt,ke,vn,gt,P,Y,Ge,ie,mn,Me,wn,Et,_e,gn,$t,I,U,Be,re,En,De,$n,kt,be,kn,_t,q,O,Re,se,_n,Ze,bn,bt,$,An,Ae,Pn,In,Pe,qn,Sn,At,S,G,Je,le,Qn,je,Nn,Pt,Ie,Hn,It,Q,M,Ke,he,xn,Xe,Cn,qt,qe,Tn,St,N,B,et,pe,zn,tt,Ln,Qt,k,Vn,ce,Fn,Wn,Se,Yn,Un,Nt,H,D,nt,de,On,ot,Gn,Ht,Qe,Mn,xt,x,R,at,ue,Bn,it,Dn,Ct,Ne,Rn,Tt,C,Z,rt,fe,Zn,st,Jn,zt,J,jn,ye,Kn,Xn,Lt,T,j,lt,ve,eo,ht,to,Vt,E,no,me,oo,ao,He,io,ro,xe,so,lo,Ft,z,K,pt,we,ho,ct,po,Wt,Ce,co,Yt,L,X,dt,ge,uo,ut,fo,Ut,ee,yo,Ee,vo,mo,Ot,V,te,ft,$e,wo,yt,go,Gt,Te,Eo,Mt,ze,$o,Bt,Le,ko,Dt;return oe=new g({}),ae=new g({}),ie=new g({}),re=new g({}),se=new g({}),le=new g({}),he=new g({}),pe=new g({}),de=new g({}),ue=new g({}),fe=new g({}),ve=new g({}),we=new g({}),ge=new g({}),$e=new g({}),{c(){_=o("meta"),vt=c(),b=o("h1"),F=o("a"),We=o("span"),u(oe.$$.fragment),dn=c(),Ye=o("span"),un=h("FAQs"),mt=c(),A=o("h3"),W=o("a"),Ue=o("span"),u(ae.$$.fragment),fn=c(),Oe=o("span"),yn=h("Q: In which regions are Inference Endpoints available?"),wt=c(),ke=o("p"),vn=h("A: Inference Endpoints are currently available on AWS in us-east-1 (N. Virginia) & eu-west-1 (Ireland) and on Azure in eastus (Virginia). If you need to deploy in a different region, please let us know."),gt=c(),P=o("h3"),Y=o("a"),Ge=o("span"),u(ie.$$.fragment),mn=c(),Me=o("span"),wn=h("Q: Can I access the instance my Endpoint is running on?"),Et=c(),_e=o("p"),gn=h("A: No, you cannot access the instance hosting your Endpoint. But if you are missing information or need more insights on the machine where the Endpoint is running, please contact us."),$t=c(),I=o("h3"),U=o("a"),Be=o("span"),u(re.$$.fragment),En=c(),De=o("span"),$n=h("Q: Can I see my Private Endpoint running on my VPC account?"),kt=c(),be=o("p"),kn=h("A: No, when creating a Private Endpoint (a Hugging Face Inference Endpoint linked to your VPC via AWS/Azure PrivateLink), you can only see the ENI in your VPC where the Endpoint is available."),_t=c(),q=o("h3"),O=o("a"),Re=o("span"),u(se.$$.fragment),_n=c(),Ze=o("span"),bn=h("Q: Can I run inference in batches?"),bt=c(),$=o("p"),An=h("A: It depends on the Task. The "),Ae=o("a"),Pn=h("supported Tasks"),In=h(" are using the transformers or sentence-transformers pipelines under the hood. If your Task pipeline supports batching, e.g. Zero-Shot Classification then batch inference is supported. In any case, you can always create your own "),Pe=o("a"),qn=h("inference handler"),Sn=h(" and implement batching."),At=c(),S=o("h3"),G=o("a"),Je=o("span"),u(le.$$.fragment),Qn=c(),je=o("span"),Nn=h("Q: How can I scale my deployment?"),Pt=c(),Ie=o("p"),Hn=h("A: The Endpoints are scaled automatically for you, the only information you need to provide is a min replica target and a max replica target. Then the system will scale your Endpoint based on the load. Scaling to zero is currently not supported."),It=c(),Q=o("h3"),M=o("a"),Ke=o("span"),u(he.$$.fragment),xn=c(),Xe=o("span"),Cn=h("Q: Will my endpoint still be running if no more requests are processed?"),qt=c(),qe=o("p"),Tn=h("A: Yes, your Endpoint will always stay available/up with the number of min replicas defined in the Advanced configuration."),St=c(),N=o("h3"),B=o("a"),et=o("span"),u(pe.$$.fragment),zn=c(),tt=o("span"),Ln=h("Q: I would like to deploy a model which is not in the supported tasks, is this possible?"),Qt=c(),k=o("p"),Vn=h("A:  Yes, you can deploy any repository from the "),ce=o("a"),Fn=h("Hugging Face Hub"),Wn=h(" and if your task/model/framework is not supported out of the box, you can "),Se=o("a"),Yn=h("create your own inference handler"),Un=h(" and then deploy your model to an Endpoint."),Nt=c(),H=o("h3"),D=o("a"),nt=o("span"),u(de.$$.fragment),On=c(),ot=o("span"),Gn=h("Q: How much does it cost to run my Endpoint?"),Ht=c(),Qe=o("p"),Mn=h("A: The Endpoints are billed based on the compute hours of your Running Endpoints, and the associated instance types. We may add usage costs for load balancers and Private Links in the future."),xt=c(),x=o("h3"),R=o("a"),at=o("span"),u(ue.$$.fragment),Bn=c(),it=o("span"),Dn=h("Q: Is the data transiting to the Endpoint encrypted?"),Ct=c(),Ne=o("p"),Rn=h("A: Yes, data is encrypted during transit with TLS/SSL."),Tt=c(),C=o("h3"),Z=o("a"),rt=o("span"),u(fe.$$.fragment),Zn=c(),st=o("span"),Jn=h("Q: How can I reduce the latency of my Endpoint?"),zt=c(),J=o("p"),jn=h("A: There are several ways to reduce the latency of your Endpoint. One is to deploy your Endpoint in a region close to your application to reduce the network overhead. Another is to optimize your model using "),ye=o("a"),Kn=h("Hugging Face Optimum"),Xn=h(" before creating your Endpoint. If you need help or have more questions about reducing latency, please contact us."),Lt=c(),T=o("h3"),j=o("a"),lt=o("span"),u(ve.$$.fragment),eo=c(),ht=o("span"),to=h("Q: How do I monitor my deployed Endpoint?"),Vt=c(),E=o("p"),no=h("A: You can currently monitor your Endpoint through the "),me=o("a"),oo=h("\u{1F917} Inference Endpoints web application"),ao=h(", where you have access to the "),He=o("a"),io=h("Logs of your Endpoints"),ro=h(" as well as a "),xe=o("a"),so=h("metrics dashboard"),lo=h(". If you need programmatic access or more information, please contact us."),Ft=c(),z=o("h3"),K=o("a"),pt=o("span"),u(we.$$.fragment),ho=c(),ct=o("span"),po=h("Q: What if I would like to deploy to a different instance type that is not listed?"),Wt=c(),Ce=o("p"),co=h("A: Please contact us if you feel your model would do better on a different instance type than what is listed."),Yt=c(),L=o("h3"),X=o("a"),dt=o("span"),u(ge.$$.fragment),uo=c(),ut=o("span"),fo=h("Q: I accidentally leaked my token. Do I need to delete my endpoint?"),Ut=c(),ee=o("p"),yo=h("A: You can invalidate existing personal tokens and create new ones in your settings here: "),Ee=o("a"),vo=h("https://huggingface.co/settings/tokens"),mo=h(". For organization tokens, go to the organization settings."),Ot=c(),V=o("h3"),te=o("a"),ft=o("span"),u($e.$$.fragment),wo=c(),yt=o("span"),go=h("Q: How does autoscaling work?"),Gt=c(),Te=o("p"),Eo=h("A: If your Endpoint is using a CPU accelerator, once the average CPU utilization of all your endpoints hits 80%, a new endpoint will be added."),Mt=c(),ze=o("p"),$o=h("As for the GPU accelerator, once the average GPU utilization of all your endpoints averaged over a 2 minute window reaches 80%, a new endpoint will be added. One Endpoint can be scaled up every 3 minutes."),Bt=c(),Le=o("p"),ko=h("Note that if we do not have enough resources available for your new endpoint, a new VM needs to be created which generally takes 1 to 5 minutes."),this.h()},l(e){const s=Va('[data-svelte="svelte-1phssyn"]',document.head);_=a(s,"META",{name:!0,content:!0}),s.forEach(t),vt=d(e),b=a(e,"H1",{class:!0});var Rt=i(b);F=a(Rt,"A",{id:!0,class:!0,href:!0});var bo=i(F);We=a(bo,"SPAN",{});var Ao=i(We);f(oe.$$.fragment,Ao),Ao.forEach(t),bo.forEach(t),dn=d(Rt),Ye=a(Rt,"SPAN",{});var Po=i(Ye);un=p(Po,"FAQs"),Po.forEach(t),Rt.forEach(t),mt=d(e),A=a(e,"H3",{class:!0});var Zt=i(A);W=a(Zt,"A",{id:!0,class:!0,href:!0});var Io=i(W);Ue=a(Io,"SPAN",{});var qo=i(Ue);f(ae.$$.fragment,qo),qo.forEach(t),Io.forEach(t),fn=d(Zt),Oe=a(Zt,"SPAN",{});var So=i(Oe);yn=p(So,"Q: In which regions are Inference Endpoints available?"),So.forEach(t),Zt.forEach(t),wt=d(e),ke=a(e,"P",{});var Qo=i(ke);vn=p(Qo,"A: Inference Endpoints are currently available on AWS in us-east-1 (N. Virginia) & eu-west-1 (Ireland) and on Azure in eastus (Virginia). If you need to deploy in a different region, please let us know."),Qo.forEach(t),gt=d(e),P=a(e,"H3",{class:!0});var Jt=i(P);Y=a(Jt,"A",{id:!0,class:!0,href:!0});var No=i(Y);Ge=a(No,"SPAN",{});var Ho=i(Ge);f(ie.$$.fragment,Ho),Ho.forEach(t),No.forEach(t),mn=d(Jt),Me=a(Jt,"SPAN",{});var xo=i(Me);wn=p(xo,"Q: Can I access the instance my Endpoint is running on?"),xo.forEach(t),Jt.forEach(t),Et=d(e),_e=a(e,"P",{});var Co=i(_e);gn=p(Co,"A: No, you cannot access the instance hosting your Endpoint. But if you are missing information or need more insights on the machine where the Endpoint is running, please contact us."),Co.forEach(t),$t=d(e),I=a(e,"H3",{class:!0});var jt=i(I);U=a(jt,"A",{id:!0,class:!0,href:!0});var To=i(U);Be=a(To,"SPAN",{});var zo=i(Be);f(re.$$.fragment,zo),zo.forEach(t),To.forEach(t),En=d(jt),De=a(jt,"SPAN",{});var Lo=i(De);$n=p(Lo,"Q: Can I see my Private Endpoint running on my VPC account?"),Lo.forEach(t),jt.forEach(t),kt=d(e),be=a(e,"P",{});var Vo=i(be);kn=p(Vo,"A: No, when creating a Private Endpoint (a Hugging Face Inference Endpoint linked to your VPC via AWS/Azure PrivateLink), you can only see the ENI in your VPC where the Endpoint is available."),Vo.forEach(t),_t=d(e),q=a(e,"H3",{class:!0});var Kt=i(q);O=a(Kt,"A",{id:!0,class:!0,href:!0});var Fo=i(O);Re=a(Fo,"SPAN",{});var Wo=i(Re);f(se.$$.fragment,Wo),Wo.forEach(t),Fo.forEach(t),_n=d(Kt),Ze=a(Kt,"SPAN",{});var Yo=i(Ze);bn=p(Yo,"Q: Can I run inference in batches?"),Yo.forEach(t),Kt.forEach(t),bt=d(e),$=a(e,"P",{});var Ve=i($);An=p(Ve,"A: It depends on the Task. The "),Ae=a(Ve,"A",{href:!0});var Uo=i(Ae);Pn=p(Uo,"supported Tasks"),Uo.forEach(t),In=p(Ve," are using the transformers or sentence-transformers pipelines under the hood. If your Task pipeline supports batching, e.g. Zero-Shot Classification then batch inference is supported. In any case, you can always create your own "),Pe=a(Ve,"A",{href:!0});var Oo=i(Pe);qn=p(Oo,"inference handler"),Oo.forEach(t),Sn=p(Ve," and implement batching."),Ve.forEach(t),At=d(e),S=a(e,"H3",{class:!0});var Xt=i(S);G=a(Xt,"A",{id:!0,class:!0,href:!0});var Go=i(G);Je=a(Go,"SPAN",{});var Mo=i(Je);f(le.$$.fragment,Mo),Mo.forEach(t),Go.forEach(t),Qn=d(Xt),je=a(Xt,"SPAN",{});var Bo=i(je);Nn=p(Bo,"Q: How can I scale my deployment?"),Bo.forEach(t),Xt.forEach(t),Pt=d(e),Ie=a(e,"P",{});var Do=i(Ie);Hn=p(Do,"A: The Endpoints are scaled automatically for you, the only information you need to provide is a min replica target and a max replica target. Then the system will scale your Endpoint based on the load. Scaling to zero is currently not supported."),Do.forEach(t),It=d(e),Q=a(e,"H3",{class:!0});var en=i(Q);M=a(en,"A",{id:!0,class:!0,href:!0});var Ro=i(M);Ke=a(Ro,"SPAN",{});var Zo=i(Ke);f(he.$$.fragment,Zo),Zo.forEach(t),Ro.forEach(t),xn=d(en),Xe=a(en,"SPAN",{});var Jo=i(Xe);Cn=p(Jo,"Q: Will my endpoint still be running if no more requests are processed?"),Jo.forEach(t),en.forEach(t),qt=d(e),qe=a(e,"P",{});var jo=i(qe);Tn=p(jo,"A: Yes, your Endpoint will always stay available/up with the number of min replicas defined in the Advanced configuration."),jo.forEach(t),St=d(e),N=a(e,"H3",{class:!0});var tn=i(N);B=a(tn,"A",{id:!0,class:!0,href:!0});var Ko=i(B);et=a(Ko,"SPAN",{});var Xo=i(et);f(pe.$$.fragment,Xo),Xo.forEach(t),Ko.forEach(t),zn=d(tn),tt=a(tn,"SPAN",{});var ea=i(tt);Ln=p(ea,"Q: I would like to deploy a model which is not in the supported tasks, is this possible?"),ea.forEach(t),tn.forEach(t),Qt=d(e),k=a(e,"P",{});var Fe=i(k);Vn=p(Fe,"A:  Yes, you can deploy any repository from the "),ce=a(Fe,"A",{href:!0,rel:!0});var ta=i(ce);Fn=p(ta,"Hugging Face Hub"),ta.forEach(t),Wn=p(Fe," and if your task/model/framework is not supported out of the box, you can "),Se=a(Fe,"A",{href:!0});var na=i(Se);Yn=p(na,"create your own inference handler"),na.forEach(t),Un=p(Fe," and then deploy your model to an Endpoint."),Fe.forEach(t),Nt=d(e),H=a(e,"H3",{class:!0});var nn=i(H);D=a(nn,"A",{id:!0,class:!0,href:!0});var oa=i(D);nt=a(oa,"SPAN",{});var aa=i(nt);f(de.$$.fragment,aa),aa.forEach(t),oa.forEach(t),On=d(nn),ot=a(nn,"SPAN",{});var ia=i(ot);Gn=p(ia,"Q: How much does it cost to run my Endpoint?"),ia.forEach(t),nn.forEach(t),Ht=d(e),Qe=a(e,"P",{});var ra=i(Qe);Mn=p(ra,"A: The Endpoints are billed based on the compute hours of your Running Endpoints, and the associated instance types. We may add usage costs for load balancers and Private Links in the future."),ra.forEach(t),xt=d(e),x=a(e,"H3",{class:!0});var on=i(x);R=a(on,"A",{id:!0,class:!0,href:!0});var sa=i(R);at=a(sa,"SPAN",{});var la=i(at);f(ue.$$.fragment,la),la.forEach(t),sa.forEach(t),Bn=d(on),it=a(on,"SPAN",{});var ha=i(it);Dn=p(ha,"Q: Is the data transiting to the Endpoint encrypted?"),ha.forEach(t),on.forEach(t),Ct=d(e),Ne=a(e,"P",{});var pa=i(Ne);Rn=p(pa,"A: Yes, data is encrypted during transit with TLS/SSL."),pa.forEach(t),Tt=d(e),C=a(e,"H3",{class:!0});var an=i(C);Z=a(an,"A",{id:!0,class:!0,href:!0});var ca=i(Z);rt=a(ca,"SPAN",{});var da=i(rt);f(fe.$$.fragment,da),da.forEach(t),ca.forEach(t),Zn=d(an),st=a(an,"SPAN",{});var ua=i(st);Jn=p(ua,"Q: How can I reduce the latency of my Endpoint?"),ua.forEach(t),an.forEach(t),zt=d(e),J=a(e,"P",{});var rn=i(J);jn=p(rn,"A: There are several ways to reduce the latency of your Endpoint. One is to deploy your Endpoint in a region close to your application to reduce the network overhead. Another is to optimize your model using "),ye=a(rn,"A",{href:!0,rel:!0});var fa=i(ye);Kn=p(fa,"Hugging Face Optimum"),fa.forEach(t),Xn=p(rn," before creating your Endpoint. If you need help or have more questions about reducing latency, please contact us."),rn.forEach(t),Lt=d(e),T=a(e,"H3",{class:!0});var sn=i(T);j=a(sn,"A",{id:!0,class:!0,href:!0});var ya=i(j);lt=a(ya,"SPAN",{});var va=i(lt);f(ve.$$.fragment,va),va.forEach(t),ya.forEach(t),eo=d(sn),ht=a(sn,"SPAN",{});var ma=i(ht);to=p(ma,"Q: How do I monitor my deployed Endpoint?"),ma.forEach(t),sn.forEach(t),Vt=d(e),E=a(e,"P",{});var ne=i(E);no=p(ne,"A: You can currently monitor your Endpoint through the "),me=a(ne,"A",{href:!0,rel:!0});var wa=i(me);oo=p(wa,"\u{1F917} Inference Endpoints web application"),wa.forEach(t),ao=p(ne,", where you have access to the "),He=a(ne,"A",{href:!0});var ga=i(He);io=p(ga,"Logs of your Endpoints"),ga.forEach(t),ro=p(ne," as well as a "),xe=a(ne,"A",{href:!0});var Ea=i(xe);so=p(Ea,"metrics dashboard"),Ea.forEach(t),lo=p(ne,". If you need programmatic access or more information, please contact us."),ne.forEach(t),Ft=d(e),z=a(e,"H3",{class:!0});var ln=i(z);K=a(ln,"A",{id:!0,class:!0,href:!0});var $a=i(K);pt=a($a,"SPAN",{});var ka=i(pt);f(we.$$.fragment,ka),ka.forEach(t),$a.forEach(t),ho=d(ln),ct=a(ln,"SPAN",{});var _a=i(ct);po=p(_a,"Q: What if I would like to deploy to a different instance type that is not listed?"),_a.forEach(t),ln.forEach(t),Wt=d(e),Ce=a(e,"P",{});var ba=i(Ce);co=p(ba,"A: Please contact us if you feel your model would do better on a different instance type than what is listed."),ba.forEach(t),Yt=d(e),L=a(e,"H3",{class:!0});var hn=i(L);X=a(hn,"A",{id:!0,class:!0,href:!0});var Aa=i(X);dt=a(Aa,"SPAN",{});var Pa=i(dt);f(ge.$$.fragment,Pa),Pa.forEach(t),Aa.forEach(t),uo=d(hn),ut=a(hn,"SPAN",{});var Ia=i(ut);fo=p(Ia,"Q: I accidentally leaked my token. Do I need to delete my endpoint?"),Ia.forEach(t),hn.forEach(t),Ut=d(e),ee=a(e,"P",{});var pn=i(ee);yo=p(pn,"A: You can invalidate existing personal tokens and create new ones in your settings here: "),Ee=a(pn,"A",{href:!0,rel:!0});var qa=i(Ee);vo=p(qa,"https://huggingface.co/settings/tokens"),qa.forEach(t),mo=p(pn,". For organization tokens, go to the organization settings."),pn.forEach(t),Ot=d(e),V=a(e,"H3",{class:!0});var cn=i(V);te=a(cn,"A",{id:!0,class:!0,href:!0});var Sa=i(te);ft=a(Sa,"SPAN",{});var Qa=i(ft);f($e.$$.fragment,Qa),Qa.forEach(t),Sa.forEach(t),wo=d(cn),yt=a(cn,"SPAN",{});var Na=i(yt);go=p(Na,"Q: How does autoscaling work?"),Na.forEach(t),cn.forEach(t),Gt=d(e),Te=a(e,"P",{});var Ha=i(Te);Eo=p(Ha,"A: If your Endpoint is using a CPU accelerator, once the average CPU utilization of all your endpoints hits 80%, a new endpoint will be added."),Ha.forEach(t),Mt=d(e),ze=a(e,"P",{});var xa=i(ze);$o=p(xa,"As for the GPU accelerator, once the average GPU utilization of all your endpoints averaged over a 2 minute window reaches 80%, a new endpoint will be added. One Endpoint can be scaled up every 3 minutes."),xa.forEach(t),Bt=d(e),Le=a(e,"P",{});var Ca=i(Le);ko=p(Ca,"Note that if we do not have enough resources available for your new endpoint, a new VM needs to be created which generally takes 1 to 5 minutes."),Ca.forEach(t),this.h()},h(){r(_,"name","hf:doc:metadata"),r(_,"content",JSON.stringify(Ua)),r(F,"id","faqs"),r(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(F,"href","#faqs"),r(b,"class","relative group"),r(W,"id","q-in-which-regions-are-inference-endpoints-available"),r(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(W,"href","#q-in-which-regions-are-inference-endpoints-available"),r(A,"class","relative group"),r(Y,"id","q-can-i-access-the-instance-my-endpoint-is-running-on"),r(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Y,"href","#q-can-i-access-the-instance-my-endpoint-is-running-on"),r(P,"class","relative group"),r(U,"id","q-can-i-see-my-private-endpoint-running-on-my-vpc-account"),r(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(U,"href","#q-can-i-see-my-private-endpoint-running-on-my-vpc-account"),r(I,"class","relative group"),r(O,"id","q-can-i-run-inference-in-batches"),r(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(O,"href","#q-can-i-run-inference-in-batches"),r(q,"class","relative group"),r(Ae,"href","/docs/inference-endpoints/supported_tasks"),r(Pe,"href","/docs/inference-endpoints/guides/custom_handler"),r(G,"id","q-how-can-i-scale-my-deployment"),r(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(G,"href","#q-how-can-i-scale-my-deployment"),r(S,"class","relative group"),r(M,"id","q-will-my-endpoint-still-be-running-if-no-more-requests-are-processed"),r(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(M,"href","#q-will-my-endpoint-still-be-running-if-no-more-requests-are-processed"),r(Q,"class","relative group"),r(B,"id","q-i-would-like-to-deploy-a-model-which-is-not-in-the-supported-tasks-is-this-possible"),r(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(B,"href","#q-i-would-like-to-deploy-a-model-which-is-not-in-the-supported-tasks-is-this-possible"),r(N,"class","relative group"),r(ce,"href","https://huggingface.co/models"),r(ce,"rel","nofollow"),r(Se,"href","/docs/inference-endpoints/guides/custom_handler"),r(D,"id","q-how-much-does-it-cost-to-run-my-endpoint"),r(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(D,"href","#q-how-much-does-it-cost-to-run-my-endpoint"),r(H,"class","relative group"),r(R,"id","q-is-the-data-transiting-to-the-endpoint-encrypted"),r(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(R,"href","#q-is-the-data-transiting-to-the-endpoint-encrypted"),r(x,"class","relative group"),r(Z,"id","q-how-can-i-reduce-the-latency-of-my-endpoint"),r(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Z,"href","#q-how-can-i-reduce-the-latency-of-my-endpoint"),r(C,"class","relative group"),r(ye,"href","https://huggingface.co/docs/optimum/index"),r(ye,"rel","nofollow"),r(j,"id","q-how-do-i-monitor-my-deployed-endpoint"),r(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(j,"href","#q-how-do-i-monitor-my-deployed-endpoint"),r(T,"class","relative group"),r(me,"href","https://ui.endpoints.huggingface.co/endpoints"),r(me,"rel","nofollow"),r(He,"href","/docs/inference-endpoints/guides/logs"),r(xe,"href","/docs/inference-endpoints/guides/metrics"),r(K,"id","q-what-if-i-would-like-to-deploy-to-a-different-instance-type-that-is-not-listed"),r(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(K,"href","#q-what-if-i-would-like-to-deploy-to-a-different-instance-type-that-is-not-listed"),r(z,"class","relative group"),r(X,"id","q-i-accidentally-leaked-my-token-do-i-need-to-delete-my-endpoint"),r(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(X,"href","#q-i-accidentally-leaked-my-token-do-i-need-to-delete-my-endpoint"),r(L,"class","relative group"),r(Ee,"href","https://huggingface.co/settings/tokens"),r(Ee,"rel","nofollow"),r(te,"id","q-how-does-autoscaling-work"),r(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(te,"href","#q-how-does-autoscaling-work"),r(V,"class","relative group")},m(e,s){n(document.head,_),l(e,vt,s),l(e,b,s),n(b,F),n(F,We),y(oe,We,null),n(b,dn),n(b,Ye),n(Ye,un),l(e,mt,s),l(e,A,s),n(A,W),n(W,Ue),y(ae,Ue,null),n(A,fn),n(A,Oe),n(Oe,yn),l(e,wt,s),l(e,ke,s),n(ke,vn),l(e,gt,s),l(e,P,s),n(P,Y),n(Y,Ge),y(ie,Ge,null),n(P,mn),n(P,Me),n(Me,wn),l(e,Et,s),l(e,_e,s),n(_e,gn),l(e,$t,s),l(e,I,s),n(I,U),n(U,Be),y(re,Be,null),n(I,En),n(I,De),n(De,$n),l(e,kt,s),l(e,be,s),n(be,kn),l(e,_t,s),l(e,q,s),n(q,O),n(O,Re),y(se,Re,null),n(q,_n),n(q,Ze),n(Ze,bn),l(e,bt,s),l(e,$,s),n($,An),n($,Ae),n(Ae,Pn),n($,In),n($,Pe),n(Pe,qn),n($,Sn),l(e,At,s),l(e,S,s),n(S,G),n(G,Je),y(le,Je,null),n(S,Qn),n(S,je),n(je,Nn),l(e,Pt,s),l(e,Ie,s),n(Ie,Hn),l(e,It,s),l(e,Q,s),n(Q,M),n(M,Ke),y(he,Ke,null),n(Q,xn),n(Q,Xe),n(Xe,Cn),l(e,qt,s),l(e,qe,s),n(qe,Tn),l(e,St,s),l(e,N,s),n(N,B),n(B,et),y(pe,et,null),n(N,zn),n(N,tt),n(tt,Ln),l(e,Qt,s),l(e,k,s),n(k,Vn),n(k,ce),n(ce,Fn),n(k,Wn),n(k,Se),n(Se,Yn),n(k,Un),l(e,Nt,s),l(e,H,s),n(H,D),n(D,nt),y(de,nt,null),n(H,On),n(H,ot),n(ot,Gn),l(e,Ht,s),l(e,Qe,s),n(Qe,Mn),l(e,xt,s),l(e,x,s),n(x,R),n(R,at),y(ue,at,null),n(x,Bn),n(x,it),n(it,Dn),l(e,Ct,s),l(e,Ne,s),n(Ne,Rn),l(e,Tt,s),l(e,C,s),n(C,Z),n(Z,rt),y(fe,rt,null),n(C,Zn),n(C,st),n(st,Jn),l(e,zt,s),l(e,J,s),n(J,jn),n(J,ye),n(ye,Kn),n(J,Xn),l(e,Lt,s),l(e,T,s),n(T,j),n(j,lt),y(ve,lt,null),n(T,eo),n(T,ht),n(ht,to),l(e,Vt,s),l(e,E,s),n(E,no),n(E,me),n(me,oo),n(E,ao),n(E,He),n(He,io),n(E,ro),n(E,xe),n(xe,so),n(E,lo),l(e,Ft,s),l(e,z,s),n(z,K),n(K,pt),y(we,pt,null),n(z,ho),n(z,ct),n(ct,po),l(e,Wt,s),l(e,Ce,s),n(Ce,co),l(e,Yt,s),l(e,L,s),n(L,X),n(X,dt),y(ge,dt,null),n(L,uo),n(L,ut),n(ut,fo),l(e,Ut,s),l(e,ee,s),n(ee,yo),n(ee,Ee),n(Ee,vo),n(ee,mo),l(e,Ot,s),l(e,V,s),n(V,te),n(te,ft),y($e,ft,null),n(V,wo),n(V,yt),n(yt,go),l(e,Gt,s),l(e,Te,s),n(Te,Eo),l(e,Mt,s),l(e,ze,s),n(ze,$o),l(e,Bt,s),l(e,Le,s),n(Le,ko),Dt=!0},p:Fa,i(e){Dt||(v(oe.$$.fragment,e),v(ae.$$.fragment,e),v(ie.$$.fragment,e),v(re.$$.fragment,e),v(se.$$.fragment,e),v(le.$$.fragment,e),v(he.$$.fragment,e),v(pe.$$.fragment,e),v(de.$$.fragment,e),v(ue.$$.fragment,e),v(fe.$$.fragment,e),v(ve.$$.fragment,e),v(we.$$.fragment,e),v(ge.$$.fragment,e),v($e.$$.fragment,e),Dt=!0)},o(e){m(oe.$$.fragment,e),m(ae.$$.fragment,e),m(ie.$$.fragment,e),m(re.$$.fragment,e),m(se.$$.fragment,e),m(le.$$.fragment,e),m(he.$$.fragment,e),m(pe.$$.fragment,e),m(de.$$.fragment,e),m(ue.$$.fragment,e),m(fe.$$.fragment,e),m(ve.$$.fragment,e),m(we.$$.fragment,e),m(ge.$$.fragment,e),m($e.$$.fragment,e),Dt=!1},d(e){t(_),e&&t(vt),e&&t(b),w(oe),e&&t(mt),e&&t(A),w(ae),e&&t(wt),e&&t(ke),e&&t(gt),e&&t(P),w(ie),e&&t(Et),e&&t(_e),e&&t($t),e&&t(I),w(re),e&&t(kt),e&&t(be),e&&t(_t),e&&t(q),w(se),e&&t(bt),e&&t($),e&&t(At),e&&t(S),w(le),e&&t(Pt),e&&t(Ie),e&&t(It),e&&t(Q),w(he),e&&t(qt),e&&t(qe),e&&t(St),e&&t(N),w(pe),e&&t(Qt),e&&t(k),e&&t(Nt),e&&t(H),w(de),e&&t(Ht),e&&t(Qe),e&&t(xt),e&&t(x),w(ue),e&&t(Ct),e&&t(Ne),e&&t(Tt),e&&t(C),w(fe),e&&t(zt),e&&t(J),e&&t(Lt),e&&t(T),w(ve),e&&t(Vt),e&&t(E),e&&t(Ft),e&&t(z),w(we),e&&t(Wt),e&&t(Ce),e&&t(Yt),e&&t(L),w(ge),e&&t(Ut),e&&t(ee),e&&t(Ot),e&&t(V),w($e),e&&t(Gt),e&&t(Te),e&&t(Mt),e&&t(ze),e&&t(Bt),e&&t(Le)}}}const Ua={local:"faqs",sections:[{local:"q-in-which-regions-are-inference-endpoints-available",title:"Q: In which regions are Inference Endpoints available?"},{local:"q-can-i-access-the-instance-my-endpoint-is-running-on",title:"Q: Can I access the instance my Endpoint is running on?"},{local:"q-can-i-see-my-private-endpoint-running-on-my-vpc-account",title:"Q: Can I see my Private Endpoint running on my VPC account?"},{local:"q-can-i-run-inference-in-batches",title:"Q: Can I run inference in batches?"},{local:"q-how-can-i-scale-my-deployment",title:"Q: How can I scale my deployment?"},{local:"q-will-my-endpoint-still-be-running-if-no-more-requests-are-processed",title:"Q: Will my endpoint still be running if no more requests are processed?"},{local:"q-i-would-like-to-deploy-a-model-which-is-not-in-the-supported-tasks-is-this-possible",title:"Q: I would like to deploy a model which is not in the supported tasks, is this possible?"},{local:"q-how-much-does-it-cost-to-run-my-endpoint",title:"Q: How much does it cost to run my Endpoint?"},{local:"q-is-the-data-transiting-to-the-endpoint-encrypted",title:"Q: Is the data transiting to the Endpoint encrypted?"},{local:"q-how-can-i-reduce-the-latency-of-my-endpoint",title:"Q: How can I reduce the latency of my Endpoint?"},{local:"q-how-do-i-monitor-my-deployed-endpoint",title:"Q: How do I monitor my deployed Endpoint?"},{local:"q-what-if-i-would-like-to-deploy-to-a-different-instance-type-that-is-not-listed",title:"Q: What if I would like to deploy to a different instance type that is not listed?"},{local:"q-i-accidentally-leaked-my-token-do-i-need-to-delete-my-endpoint",title:"Q: I accidentally leaked my token. Do I need to delete my endpoint?"},{local:"q-how-does-autoscaling-work",title:"Q: How does autoscaling work?"}],title:"FAQs "};function Oa(_o){return Wa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ba extends Ta{constructor(_){super();za(this,_,Oa,Ya,La,{})}}export{Ba as default,Ua as metadata};
